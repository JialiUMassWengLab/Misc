{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bafa4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/efs/home/iet5740/Projects/UKBPPP/torch_venv/lib64/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "260533a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 601 ms, sys: 89.7 ms, total: 691 ms\n",
      "Wall time: 768 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_embedding = torch.load('embedding_tensors.training.pt')\n",
    "testing_embedding = torch.load('embedding_tensors.testing.pt')\n",
    "wt_embedding = torch.load('embedding_tensors.wildtype.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eafadd50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38559, 6)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('Orphanet_LoF_GoF_clinvarVariants.csv',index_col=0)\n",
    "df2 = pd.read_csv('Benign_clinvarVariants.csv',index_col=0)\n",
    "df2['direction'] = ['Benign'] * df2.shape[0]\n",
    "training = pd.concat([df1,df2],join='inner',axis=0)\n",
    "training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3e56e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24176, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('OMIM_LoF_GoF_clinvarVariants.csv',index_col=0)\n",
    "df2 = pd.read_csv('Benign_clinvarVariants_OMIM.csv',index_col=0)\n",
    "df2['direction'] = ['Benign'] * df2.shape[0]\n",
    "testing = pd.concat([df1,df2],join='inner',axis=0)\n",
    "testing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53ad999",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.index.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "810efd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {'Benign': 0, 'LoF':1, 'GoF':2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28a4cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class variantDataset(Dataset):\n",
    "    def __init__(self,tensor_dict,wt_tensor_dict,label_df):\n",
    "        tensor_array = []\n",
    "        labels = []\n",
    "        for key,embedding in tensor_dict.items():\n",
    "            vid,txID,sub = key.split(':')\n",
    "            direction = label_df.loc[int(vid),'direction']\n",
    "            if direction != 'conflicted':\n",
    "                wt_embedding = wt_tensor_dict[txID+'_WT']\n",
    "                tensor_array.append(torch.cat((embedding,wt_embedding),dim=-1))\n",
    "                labels.append(label_mapping[direction])\n",
    "            \n",
    "        self.embeddings = tensor_array\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        embedding = self.embeddings[idx]\n",
    "        label = self.labels[idx]\n",
    "        return embedding,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "334fc88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10990\n"
     ]
    }
   ],
   "source": [
    "training_data = variantDataset(training_embedding,wt_embedding,training)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "825e9ca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.1094,  0.0968,  0.1457,  ...,  0.0722, -0.0161,  0.0375]), 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d03b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7362\n"
     ]
    }
   ],
   "source": [
    "testing_data = variantDataset(testing_embedding,wt_embedding,testing)\n",
    "print(len(testing_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddd3fce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "INPUT_DIM = 2560\n",
    "Z_DIM = 3\n",
    "H1_DIM = 128\n",
    "H2_DIM = 16\n",
    "NUM_EPOCHS = 300\n",
    "BATCH_SIZE = 64\n",
    "LR_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d729b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1863193f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, z_dim, h1_dim, h2_dim):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_dim, h1_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h1_dim, h2_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(h2_dim, z_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "#model = NeuralNetwork(INPUT_DIM, Z_DIM, H1_DIM, H2_DIM).to(device)\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ff46b",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d0252705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train function\n",
    "def train(num_epochs, dataloader, model, optimizer, loss_fn):\n",
    "    trainingEpoch_loss = []\n",
    "    n_total_steps = len(dataloader)\n",
    "    # Start training\n",
    "    for epoch in range(num_epochs):\n",
    "        step_loss = []\n",
    "        for i, (X,y) in enumerate(dataloader):\n",
    "            # Forward pass\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            loss = loss_fn(pred,y)\n",
    "\n",
    "            # Backprop and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "            #record loss\n",
    "            step_loss.append(loss.item())\n",
    "            if (i+1) % 1 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "                \n",
    "        trainingEpoch_loss.append(np.array(step_loss).mean())\n",
    "        \n",
    "    return trainingEpoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b7c7f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, loss\n",
    "model = NeuralNetwork(INPUT_DIM, Z_DIM, H1_DIM, H2_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b693514",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Step [1/172], Loss: 68.9848\n",
      "Epoch [1/300], Step [2/172], Loss: 67.8992\n",
      "Epoch [1/300], Step [3/172], Loss: 68.4125\n",
      "Epoch [1/300], Step [4/172], Loss: 66.7385\n",
      "Epoch [1/300], Step [5/172], Loss: 67.0300\n",
      "Epoch [1/300], Step [6/172], Loss: 64.7885\n",
      "Epoch [1/300], Step [7/172], Loss: 62.4279\n",
      "Epoch [1/300], Step [8/172], Loss: 62.6803\n",
      "Epoch [1/300], Step [9/172], Loss: 61.5183\n",
      "Epoch [1/300], Step [10/172], Loss: 61.9408\n",
      "Epoch [1/300], Step [11/172], Loss: 61.5468\n",
      "Epoch [1/300], Step [12/172], Loss: 59.2755\n",
      "Epoch [1/300], Step [13/172], Loss: 58.4219\n",
      "Epoch [1/300], Step [14/172], Loss: 57.8058\n",
      "Epoch [1/300], Step [15/172], Loss: 56.2284\n",
      "Epoch [1/300], Step [16/172], Loss: 54.2144\n",
      "Epoch [1/300], Step [17/172], Loss: 52.6863\n",
      "Epoch [1/300], Step [18/172], Loss: 53.5813\n",
      "Epoch [1/300], Step [19/172], Loss: 51.0874\n",
      "Epoch [1/300], Step [20/172], Loss: 72.0540\n",
      "Epoch [1/300], Step [21/172], Loss: 49.2530\n",
      "Epoch [1/300], Step [22/172], Loss: 51.6451\n",
      "Epoch [1/300], Step [23/172], Loss: 44.7008\n",
      "Epoch [1/300], Step [24/172], Loss: 47.9785\n",
      "Epoch [1/300], Step [25/172], Loss: 44.0526\n",
      "Epoch [1/300], Step [26/172], Loss: 46.0856\n",
      "Epoch [1/300], Step [27/172], Loss: 47.7316\n",
      "Epoch [1/300], Step [28/172], Loss: 42.0492\n",
      "Epoch [1/300], Step [29/172], Loss: 46.5035\n",
      "Epoch [1/300], Step [30/172], Loss: 42.5741\n",
      "Epoch [1/300], Step [31/172], Loss: 37.1601\n",
      "Epoch [1/300], Step [32/172], Loss: 37.1837\n",
      "Epoch [1/300], Step [33/172], Loss: 41.2699\n",
      "Epoch [1/300], Step [34/172], Loss: 33.0913\n",
      "Epoch [1/300], Step [35/172], Loss: 54.5786\n",
      "Epoch [1/300], Step [36/172], Loss: 41.0326\n",
      "Epoch [1/300], Step [37/172], Loss: 36.4455\n",
      "Epoch [1/300], Step [38/172], Loss: 34.3862\n",
      "Epoch [1/300], Step [39/172], Loss: 37.0056\n",
      "Epoch [1/300], Step [40/172], Loss: 30.0153\n",
      "Epoch [1/300], Step [41/172], Loss: 31.2569\n",
      "Epoch [1/300], Step [42/172], Loss: 26.7908\n",
      "Epoch [1/300], Step [43/172], Loss: 27.7607\n",
      "Epoch [1/300], Step [44/172], Loss: 27.0624\n",
      "Epoch [1/300], Step [45/172], Loss: 25.0972\n",
      "Epoch [1/300], Step [46/172], Loss: 47.1179\n",
      "Epoch [1/300], Step [47/172], Loss: 44.2182\n",
      "Epoch [1/300], Step [48/172], Loss: 69.9678\n",
      "Epoch [1/300], Step [49/172], Loss: 25.2707\n",
      "Epoch [1/300], Step [50/172], Loss: 36.4734\n",
      "Epoch [1/300], Step [51/172], Loss: 21.6541\n",
      "Epoch [1/300], Step [52/172], Loss: 20.2272\n",
      "Epoch [1/300], Step [53/172], Loss: 24.6987\n",
      "Epoch [1/300], Step [54/172], Loss: 17.5154\n",
      "Epoch [1/300], Step [55/172], Loss: 17.8680\n",
      "Epoch [1/300], Step [56/172], Loss: 18.4036\n",
      "Epoch [1/300], Step [57/172], Loss: 43.5166\n",
      "Epoch [1/300], Step [58/172], Loss: 25.4327\n",
      "Epoch [1/300], Step [59/172], Loss: 31.1607\n",
      "Epoch [1/300], Step [60/172], Loss: 62.9963\n",
      "Epoch [1/300], Step [61/172], Loss: 18.6044\n",
      "Epoch [1/300], Step [62/172], Loss: 18.5517\n",
      "Epoch [1/300], Step [63/172], Loss: 14.2324\n",
      "Epoch [1/300], Step [64/172], Loss: 13.7345\n",
      "Epoch [1/300], Step [65/172], Loss: 26.9306\n",
      "Epoch [1/300], Step [66/172], Loss: 16.1706\n",
      "Epoch [1/300], Step [67/172], Loss: 24.3139\n",
      "Epoch [1/300], Step [68/172], Loss: 9.4735\n",
      "Epoch [1/300], Step [69/172], Loss: 88.1102\n",
      "Epoch [1/300], Step [70/172], Loss: 180.1652\n",
      "Epoch [1/300], Step [71/172], Loss: 171.0421\n",
      "Epoch [1/300], Step [72/172], Loss: 181.4302\n",
      "Epoch [1/300], Step [73/172], Loss: 174.1557\n",
      "Epoch [1/300], Step [74/172], Loss: 177.4428\n",
      "Epoch [1/300], Step [75/172], Loss: 161.3927\n",
      "Epoch [1/300], Step [76/172], Loss: 160.3272\n",
      "Epoch [1/300], Step [77/172], Loss: 158.2691\n",
      "Epoch [1/300], Step [78/172], Loss: 158.7818\n",
      "Epoch [1/300], Step [79/172], Loss: 147.5403\n",
      "Epoch [1/300], Step [80/172], Loss: 147.4944\n",
      "Epoch [1/300], Step [81/172], Loss: 142.1339\n",
      "Epoch [1/300], Step [82/172], Loss: 142.9567\n",
      "Epoch [1/300], Step [83/172], Loss: 132.8854\n",
      "Epoch [1/300], Step [84/172], Loss: 129.5396\n",
      "Epoch [1/300], Step [85/172], Loss: 127.0888\n",
      "Epoch [1/300], Step [86/172], Loss: 123.7394\n",
      "Epoch [1/300], Step [87/172], Loss: 117.7459\n",
      "Epoch [1/300], Step [88/172], Loss: 116.1337\n",
      "Epoch [1/300], Step [89/172], Loss: 113.1349\n",
      "Epoch [1/300], Step [90/172], Loss: 109.9534\n",
      "Epoch [1/300], Step [91/172], Loss: 105.0943\n",
      "Epoch [1/300], Step [92/172], Loss: 103.3173\n",
      "Epoch [1/300], Step [93/172], Loss: 100.9247\n",
      "Epoch [1/300], Step [94/172], Loss: 98.0139\n",
      "Epoch [1/300], Step [95/172], Loss: 94.5514\n",
      "Epoch [1/300], Step [96/172], Loss: 92.0047\n",
      "Epoch [1/300], Step [97/172], Loss: 89.8366\n",
      "Epoch [1/300], Step [98/172], Loss: 87.6079\n",
      "Epoch [1/300], Step [99/172], Loss: 85.5169\n",
      "Epoch [1/300], Step [100/172], Loss: 83.1712\n",
      "Epoch [1/300], Step [101/172], Loss: 81.5038\n",
      "Epoch [1/300], Step [102/172], Loss: 79.8860\n",
      "Epoch [1/300], Step [103/172], Loss: 78.3436\n",
      "Epoch [1/300], Step [104/172], Loss: 77.1819\n",
      "Epoch [1/300], Step [105/172], Loss: 76.2446\n",
      "Epoch [1/300], Step [106/172], Loss: 75.2147\n",
      "Epoch [1/300], Step [107/172], Loss: 74.2583\n",
      "Epoch [1/300], Step [108/172], Loss: 73.5280\n",
      "Epoch [1/300], Step [109/172], Loss: 72.2069\n",
      "Epoch [1/300], Step [110/172], Loss: 71.0410\n",
      "Epoch [1/300], Step [111/172], Loss: 69.5800\n",
      "Epoch [1/300], Step [112/172], Loss: 68.0746\n",
      "Epoch [1/300], Step [113/172], Loss: 66.6545\n",
      "Epoch [1/300], Step [114/172], Loss: 65.1579\n",
      "Epoch [1/300], Step [115/172], Loss: 63.7862\n",
      "Epoch [1/300], Step [116/172], Loss: 62.4956\n",
      "Epoch [1/300], Step [117/172], Loss: 61.3134\n",
      "Epoch [1/300], Step [118/172], Loss: 60.1553\n",
      "Epoch [1/300], Step [119/172], Loss: 58.6474\n",
      "Epoch [1/300], Step [120/172], Loss: 57.8020\n",
      "Epoch [1/300], Step [121/172], Loss: 56.2185\n",
      "Epoch [1/300], Step [122/172], Loss: 54.7914\n",
      "Epoch [1/300], Step [123/172], Loss: 53.8840\n",
      "Epoch [1/300], Step [124/172], Loss: 52.8450\n",
      "Epoch [1/300], Step [125/172], Loss: 51.6875\n",
      "Epoch [1/300], Step [126/172], Loss: 50.2018\n",
      "Epoch [1/300], Step [127/172], Loss: 48.7994\n",
      "Epoch [1/300], Step [128/172], Loss: 47.2572\n",
      "Epoch [1/300], Step [129/172], Loss: 46.7488\n",
      "Epoch [1/300], Step [130/172], Loss: 45.8762\n",
      "Epoch [1/300], Step [131/172], Loss: 44.7012\n",
      "Epoch [1/300], Step [132/172], Loss: 43.3168\n",
      "Epoch [1/300], Step [133/172], Loss: 42.3368\n",
      "Epoch [1/300], Step [134/172], Loss: 41.1895\n",
      "Epoch [1/300], Step [135/172], Loss: 39.9071\n",
      "Epoch [1/300], Step [136/172], Loss: 37.0147\n",
      "Epoch [1/300], Step [137/172], Loss: 37.2171\n",
      "Epoch [1/300], Step [138/172], Loss: 36.4102\n",
      "Epoch [1/300], Step [139/172], Loss: 36.0334\n",
      "Epoch [1/300], Step [140/172], Loss: 34.3350\n",
      "Epoch [1/300], Step [141/172], Loss: 33.7819\n",
      "Epoch [1/300], Step [142/172], Loss: 32.7455\n",
      "Epoch [1/300], Step [143/172], Loss: 31.6598\n",
      "Epoch [1/300], Step [144/172], Loss: 31.0846\n",
      "Epoch [1/300], Step [145/172], Loss: 29.9886\n",
      "Epoch [1/300], Step [146/172], Loss: 28.6446\n",
      "Epoch [1/300], Step [147/172], Loss: 29.7419\n",
      "Epoch [1/300], Step [148/172], Loss: 28.1928\n",
      "Epoch [1/300], Step [149/172], Loss: 26.8243\n",
      "Epoch [1/300], Step [150/172], Loss: 26.1224\n",
      "Epoch [1/300], Step [151/172], Loss: 25.4162\n",
      "Epoch [1/300], Step [152/172], Loss: 24.1441\n",
      "Epoch [1/300], Step [153/172], Loss: 24.0499\n",
      "Epoch [1/300], Step [154/172], Loss: 22.2224\n",
      "Epoch [1/300], Step [155/172], Loss: 22.5226\n",
      "Epoch [1/300], Step [156/172], Loss: 21.9500\n",
      "Epoch [1/300], Step [157/172], Loss: 20.3144\n",
      "Epoch [1/300], Step [158/172], Loss: 20.1848\n",
      "Epoch [1/300], Step [159/172], Loss: 19.6380\n",
      "Epoch [1/300], Step [160/172], Loss: 19.4673\n",
      "Epoch [1/300], Step [161/172], Loss: 18.4306\n",
      "Epoch [1/300], Step [162/172], Loss: 18.2836\n",
      "Epoch [1/300], Step [163/172], Loss: 17.6153\n",
      "Epoch [1/300], Step [164/172], Loss: 17.1892\n",
      "Epoch [1/300], Step [165/172], Loss: 15.9989\n",
      "Epoch [1/300], Step [166/172], Loss: 16.4254\n",
      "Epoch [1/300], Step [167/172], Loss: 14.6068\n",
      "Epoch [1/300], Step [168/172], Loss: 15.6234\n",
      "Epoch [1/300], Step [169/172], Loss: 15.0851\n",
      "Epoch [1/300], Step [170/172], Loss: 14.4166\n",
      "Epoch [1/300], Step [171/172], Loss: 13.1444\n",
      "Epoch [1/300], Step [172/172], Loss: 9.2592\n",
      "Epoch [2/300], Step [1/172], Loss: 138.9502\n",
      "Epoch [2/300], Step [2/172], Loss: 137.3331\n",
      "Epoch [2/300], Step [3/172], Loss: 162.1691\n",
      "Epoch [2/300], Step [4/172], Loss: 140.0534\n",
      "Epoch [2/300], Step [5/172], Loss: 163.5450\n",
      "Epoch [2/300], Step [6/172], Loss: 122.6852\n",
      "Epoch [2/300], Step [7/172], Loss: 134.4203\n",
      "Epoch [2/300], Step [8/172], Loss: 119.4288\n",
      "Epoch [2/300], Step [9/172], Loss: 120.4697\n",
      "Epoch [2/300], Step [10/172], Loss: 130.7193\n",
      "Epoch [2/300], Step [11/172], Loss: 118.1182\n",
      "Epoch [2/300], Step [12/172], Loss: 110.5479\n",
      "Epoch [2/300], Step [13/172], Loss: 105.2714\n",
      "Epoch [2/300], Step [14/172], Loss: 117.8445\n",
      "Epoch [2/300], Step [15/172], Loss: 113.1887\n",
      "Epoch [2/300], Step [16/172], Loss: 101.9087\n",
      "Epoch [2/300], Step [17/172], Loss: 101.3506\n",
      "Epoch [2/300], Step [18/172], Loss: 100.1946\n",
      "Epoch [2/300], Step [19/172], Loss: 95.9854\n",
      "Epoch [2/300], Step [20/172], Loss: 151.9359\n",
      "Epoch [2/300], Step [21/172], Loss: 100.1660\n",
      "Epoch [2/300], Step [22/172], Loss: 100.8509\n",
      "Epoch [2/300], Step [23/172], Loss: 94.7936\n",
      "Epoch [2/300], Step [24/172], Loss: 93.2275\n",
      "Epoch [2/300], Step [25/172], Loss: 84.4424\n",
      "Epoch [2/300], Step [26/172], Loss: 85.3779\n",
      "Epoch [2/300], Step [27/172], Loss: 91.5033\n",
      "Epoch [2/300], Step [28/172], Loss: 78.8111\n",
      "Epoch [2/300], Step [29/172], Loss: 92.1632\n",
      "Epoch [2/300], Step [30/172], Loss: 81.2155\n",
      "Epoch [2/300], Step [31/172], Loss: 74.8442\n",
      "Epoch [2/300], Step [32/172], Loss: 71.8835\n",
      "Epoch [2/300], Step [33/172], Loss: 78.2876\n",
      "Epoch [2/300], Step [34/172], Loss: 69.9171\n",
      "Epoch [2/300], Step [35/172], Loss: 87.4245\n",
      "Epoch [2/300], Step [36/172], Loss: 70.2852\n",
      "Epoch [2/300], Step [37/172], Loss: 61.3371\n",
      "Epoch [2/300], Step [38/172], Loss: 62.2690\n",
      "Epoch [2/300], Step [39/172], Loss: 68.2338\n",
      "Epoch [2/300], Step [40/172], Loss: 59.7642\n",
      "Epoch [2/300], Step [41/172], Loss: 61.2267\n",
      "Epoch [2/300], Step [42/172], Loss: 58.1279\n",
      "Epoch [2/300], Step [43/172], Loss: 56.2334\n",
      "Epoch [2/300], Step [44/172], Loss: 59.0855\n",
      "Epoch [2/300], Step [45/172], Loss: 54.3447\n",
      "Epoch [2/300], Step [46/172], Loss: 72.7985\n",
      "Epoch [2/300], Step [47/172], Loss: 71.0721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/300], Step [48/172], Loss: 93.8703\n",
      "Epoch [2/300], Step [49/172], Loss: 51.9793\n",
      "Epoch [2/300], Step [50/172], Loss: 61.7483\n",
      "Epoch [2/300], Step [51/172], Loss: 45.8520\n",
      "Epoch [2/300], Step [52/172], Loss: 45.8161\n",
      "Epoch [2/300], Step [53/172], Loss: 49.3775\n",
      "Epoch [2/300], Step [54/172], Loss: 44.7260\n",
      "Epoch [2/300], Step [55/172], Loss: 43.5120\n",
      "Epoch [2/300], Step [56/172], Loss: 41.8833\n",
      "Epoch [2/300], Step [57/172], Loss: 63.9382\n",
      "Epoch [2/300], Step [58/172], Loss: 46.2879\n",
      "Epoch [2/300], Step [59/172], Loss: 52.8371\n",
      "Epoch [2/300], Step [60/172], Loss: 78.6928\n",
      "Epoch [2/300], Step [61/172], Loss: 40.1889\n",
      "Epoch [2/300], Step [62/172], Loss: 41.2628\n",
      "Epoch [2/300], Step [63/172], Loss: 34.1597\n",
      "Epoch [2/300], Step [64/172], Loss: 32.8355\n",
      "Epoch [2/300], Step [65/172], Loss: 43.5268\n",
      "Epoch [2/300], Step [66/172], Loss: 33.0164\n",
      "Epoch [2/300], Step [67/172], Loss: 40.8700\n",
      "Epoch [2/300], Step [68/172], Loss: 30.5829\n",
      "Epoch [2/300], Step [69/172], Loss: 67.7271\n",
      "Epoch [2/300], Step [70/172], Loss: 89.0772\n",
      "Epoch [2/300], Step [71/172], Loss: 88.2025\n",
      "Epoch [2/300], Step [72/172], Loss: 90.7204\n",
      "Epoch [2/300], Step [73/172], Loss: 89.6659\n",
      "Epoch [2/300], Step [74/172], Loss: 89.5647\n",
      "Epoch [2/300], Step [75/172], Loss: 83.2281\n",
      "Epoch [2/300], Step [76/172], Loss: 83.4644\n",
      "Epoch [2/300], Step [77/172], Loss: 82.3604\n",
      "Epoch [2/300], Step [78/172], Loss: 81.6402\n",
      "Epoch [2/300], Step [79/172], Loss: 78.1864\n",
      "Epoch [2/300], Step [80/172], Loss: 77.4544\n",
      "Epoch [2/300], Step [81/172], Loss: 75.1442\n",
      "Epoch [2/300], Step [82/172], Loss: 74.5886\n",
      "Epoch [2/300], Step [83/172], Loss: 72.1526\n",
      "Epoch [2/300], Step [84/172], Loss: 71.1728\n",
      "Epoch [2/300], Step [85/172], Loss: 70.1643\n",
      "Epoch [2/300], Step [86/172], Loss: 66.8795\n",
      "Epoch [2/300], Step [87/172], Loss: 64.8725\n",
      "Epoch [2/300], Step [88/172], Loss: 64.3896\n",
      "Epoch [2/300], Step [89/172], Loss: 63.1509\n",
      "Epoch [2/300], Step [90/172], Loss: 60.8050\n",
      "Epoch [2/300], Step [91/172], Loss: 59.5940\n",
      "Epoch [2/300], Step [92/172], Loss: 57.7872\n",
      "Epoch [2/300], Step [93/172], Loss: 56.4006\n",
      "Epoch [2/300], Step [94/172], Loss: 55.4585\n",
      "Epoch [2/300], Step [95/172], Loss: 54.0525\n",
      "Epoch [2/300], Step [96/172], Loss: 53.1783\n",
      "Epoch [2/300], Step [97/172], Loss: 52.6979\n",
      "Epoch [2/300], Step [98/172], Loss: 51.2726\n",
      "Epoch [2/300], Step [99/172], Loss: 50.0507\n",
      "Epoch [2/300], Step [100/172], Loss: 49.4556\n",
      "Epoch [2/300], Step [101/172], Loss: 47.6179\n",
      "Epoch [2/300], Step [102/172], Loss: 47.4999\n",
      "Epoch [2/300], Step [103/172], Loss: 46.3448\n",
      "Epoch [2/300], Step [104/172], Loss: 44.7708\n",
      "Epoch [2/300], Step [105/172], Loss: 44.4562\n",
      "Epoch [2/300], Step [106/172], Loss: 42.7565\n",
      "Epoch [2/300], Step [107/172], Loss: 41.8474\n",
      "Epoch [2/300], Step [108/172], Loss: 41.9628\n",
      "Epoch [2/300], Step [109/172], Loss: 40.5562\n",
      "Epoch [2/300], Step [110/172], Loss: 40.3974\n",
      "Epoch [2/300], Step [111/172], Loss: 39.2499\n",
      "Epoch [2/300], Step [112/172], Loss: 38.2953\n",
      "Epoch [2/300], Step [113/172], Loss: 37.7981\n",
      "Epoch [2/300], Step [114/172], Loss: 36.4425\n",
      "Epoch [2/300], Step [115/172], Loss: 35.6213\n",
      "Epoch [2/300], Step [116/172], Loss: 35.0558\n",
      "Epoch [2/300], Step [117/172], Loss: 34.7790\n",
      "Epoch [2/300], Step [118/172], Loss: 34.4379\n",
      "Epoch [2/300], Step [119/172], Loss: 33.1877\n",
      "Epoch [2/300], Step [120/172], Loss: 33.3949\n",
      "Epoch [2/300], Step [121/172], Loss: 32.3328\n",
      "Epoch [2/300], Step [122/172], Loss: 30.9107\n",
      "Epoch [2/300], Step [123/172], Loss: 30.4414\n",
      "Epoch [2/300], Step [124/172], Loss: 30.4816\n",
      "Epoch [2/300], Step [125/172], Loss: 30.1231\n",
      "Epoch [2/300], Step [126/172], Loss: 29.2174\n",
      "Epoch [2/300], Step [127/172], Loss: 28.3621\n",
      "Epoch [2/300], Step [128/172], Loss: 27.5256\n",
      "Epoch [2/300], Step [129/172], Loss: 27.5615\n",
      "Epoch [2/300], Step [130/172], Loss: 27.6435\n",
      "Epoch [2/300], Step [131/172], Loss: 26.7927\n",
      "Epoch [2/300], Step [132/172], Loss: 26.0282\n",
      "Epoch [2/300], Step [133/172], Loss: 25.6896\n",
      "Epoch [2/300], Step [134/172], Loss: 25.2098\n",
      "Epoch [2/300], Step [135/172], Loss: 24.2510\n",
      "Epoch [2/300], Step [136/172], Loss: 21.9779\n",
      "Epoch [2/300], Step [137/172], Loss: 23.0190\n",
      "Epoch [2/300], Step [138/172], Loss: 22.5843\n",
      "Epoch [2/300], Step [139/172], Loss: 23.0454\n",
      "Epoch [2/300], Step [140/172], Loss: 21.9260\n",
      "Epoch [2/300], Step [141/172], Loss: 21.8951\n",
      "Epoch [2/300], Step [142/172], Loss: 21.5027\n",
      "Epoch [2/300], Step [143/172], Loss: 20.7293\n",
      "Epoch [2/300], Step [144/172], Loss: 20.5172\n",
      "Epoch [2/300], Step [145/172], Loss: 20.0200\n",
      "Epoch [2/300], Step [146/172], Loss: 19.2056\n",
      "Epoch [2/300], Step [147/172], Loss: 20.6630\n",
      "Epoch [2/300], Step [148/172], Loss: 19.6429\n",
      "Epoch [2/300], Step [149/172], Loss: 18.8886\n",
      "Epoch [2/300], Step [150/172], Loss: 18.7515\n",
      "Epoch [2/300], Step [151/172], Loss: 18.1873\n",
      "Epoch [2/300], Step [152/172], Loss: 17.4907\n",
      "Epoch [2/300], Step [153/172], Loss: 17.7676\n",
      "Epoch [2/300], Step [154/172], Loss: 16.6067\n",
      "Epoch [2/300], Step [155/172], Loss: 17.0452\n",
      "Epoch [2/300], Step [156/172], Loss: 16.8992\n",
      "Epoch [2/300], Step [157/172], Loss: 15.9232\n",
      "Epoch [2/300], Step [158/172], Loss: 16.0121\n",
      "Epoch [2/300], Step [159/172], Loss: 15.7484\n",
      "Epoch [2/300], Step [160/172], Loss: 15.8708\n",
      "Epoch [2/300], Step [161/172], Loss: 15.1312\n",
      "Epoch [2/300], Step [162/172], Loss: 15.2124\n",
      "Epoch [2/300], Step [163/172], Loss: 14.8829\n",
      "Epoch [2/300], Step [164/172], Loss: 14.8139\n",
      "Epoch [2/300], Step [165/172], Loss: 13.8757\n",
      "Epoch [2/300], Step [166/172], Loss: 14.6262\n",
      "Epoch [2/300], Step [167/172], Loss: 13.1784\n",
      "Epoch [2/300], Step [168/172], Loss: 14.3086\n",
      "Epoch [2/300], Step [169/172], Loss: 13.7895\n",
      "Epoch [2/300], Step [170/172], Loss: 13.4138\n",
      "Epoch [2/300], Step [171/172], Loss: 12.3251\n",
      "Epoch [2/300], Step [172/172], Loss: 8.9101\n",
      "Epoch [3/300], Step [1/172], Loss: 127.7263\n",
      "Epoch [3/300], Step [2/172], Loss: 125.7699\n",
      "Epoch [3/300], Step [3/172], Loss: 173.4332\n",
      "Epoch [3/300], Step [4/172], Loss: 135.2129\n",
      "Epoch [3/300], Step [5/172], Loss: 166.4461\n",
      "Epoch [3/300], Step [6/172], Loss: 109.6232\n",
      "Epoch [3/300], Step [7/172], Loss: 120.2195\n",
      "Epoch [3/300], Step [8/172], Loss: 105.8499\n",
      "Epoch [3/300], Step [9/172], Loss: 110.0187\n",
      "Epoch [3/300], Step [10/172], Loss: 126.3012\n",
      "Epoch [3/300], Step [11/172], Loss: 113.8552\n",
      "Epoch [3/300], Step [12/172], Loss: 104.5237\n",
      "Epoch [3/300], Step [13/172], Loss: 97.1009\n",
      "Epoch [3/300], Step [14/172], Loss: 116.3586\n",
      "Epoch [3/300], Step [15/172], Loss: 111.2671\n",
      "Epoch [3/300], Step [16/172], Loss: 96.1529\n",
      "Epoch [3/300], Step [17/172], Loss: 96.1602\n",
      "Epoch [3/300], Step [18/172], Loss: 98.7126\n",
      "Epoch [3/300], Step [19/172], Loss: 92.9891\n",
      "Epoch [3/300], Step [20/172], Loss: 174.0028\n",
      "Epoch [3/300], Step [21/172], Loss: 101.5220\n",
      "Epoch [3/300], Step [22/172], Loss: 104.3668\n",
      "Epoch [3/300], Step [23/172], Loss: 93.4083\n",
      "Epoch [3/300], Step [24/172], Loss: 95.7978\n",
      "Epoch [3/300], Step [25/172], Loss: 83.3432\n",
      "Epoch [3/300], Step [26/172], Loss: 85.8496\n",
      "Epoch [3/300], Step [27/172], Loss: 96.0865\n",
      "Epoch [3/300], Step [28/172], Loss: 80.1103\n",
      "Epoch [3/300], Step [29/172], Loss: 99.1262\n",
      "Epoch [3/300], Step [30/172], Loss: 85.1328\n",
      "Epoch [3/300], Step [31/172], Loss: 76.0791\n",
      "Epoch [3/300], Step [32/172], Loss: 72.6021\n",
      "Epoch [3/300], Step [33/172], Loss: 83.5373\n",
      "Epoch [3/300], Step [34/172], Loss: 71.2734\n",
      "Epoch [3/300], Step [35/172], Loss: 93.9656\n",
      "Epoch [3/300], Step [36/172], Loss: 70.3101\n",
      "Epoch [3/300], Step [37/172], Loss: 58.4448\n",
      "Epoch [3/300], Step [38/172], Loss: 62.4894\n",
      "Epoch [3/300], Step [39/172], Loss: 71.4720\n",
      "Epoch [3/300], Step [40/172], Loss: 60.7683\n",
      "Epoch [3/300], Step [41/172], Loss: 63.0658\n",
      "Epoch [3/300], Step [42/172], Loss: 60.3682\n",
      "Epoch [3/300], Step [43/172], Loss: 57.0869\n",
      "Epoch [3/300], Step [44/172], Loss: 62.4616\n",
      "Epoch [3/300], Step [45/172], Loss: 55.5832\n",
      "Epoch [3/300], Step [46/172], Loss: 78.9011\n",
      "Epoch [3/300], Step [47/172], Loss: 78.3846\n",
      "Epoch [3/300], Step [48/172], Loss: 105.8624\n",
      "Epoch [3/300], Step [49/172], Loss: 53.7864\n",
      "Epoch [3/300], Step [50/172], Loss: 66.7635\n",
      "Epoch [3/300], Step [51/172], Loss: 44.5912\n",
      "Epoch [3/300], Step [52/172], Loss: 46.7476\n",
      "Epoch [3/300], Step [53/172], Loss: 51.7368\n",
      "Epoch [3/300], Step [54/172], Loss: 47.4015\n",
      "Epoch [3/300], Step [55/172], Loss: 45.4213\n",
      "Epoch [3/300], Step [56/172], Loss: 42.1701\n",
      "Epoch [3/300], Step [57/172], Loss: 70.4123\n",
      "Epoch [3/300], Step [58/172], Loss: 47.5480\n",
      "Epoch [3/300], Step [59/172], Loss: 57.3564\n",
      "Epoch [3/300], Step [60/172], Loss: 88.7233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/300], Step [61/172], Loss: 41.2769\n",
      "Epoch [3/300], Step [62/172], Loss: 44.7489\n",
      "Epoch [3/300], Step [63/172], Loss: 34.5520\n",
      "Epoch [3/300], Step [64/172], Loss: 32.7220\n",
      "Epoch [3/300], Step [65/172], Loss: 46.3984\n",
      "Epoch [3/300], Step [66/172], Loss: 33.0370\n",
      "Epoch [3/300], Step [67/172], Loss: 45.8024\n",
      "Epoch [3/300], Step [68/172], Loss: 37.0944\n",
      "Epoch [3/300], Step [69/172], Loss: 68.6731\n",
      "Epoch [3/300], Step [70/172], Loss: 73.3040\n",
      "Epoch [3/300], Step [71/172], Loss: 73.1814\n",
      "Epoch [3/300], Step [72/172], Loss: 73.4603\n",
      "Epoch [3/300], Step [73/172], Loss: 74.6144\n",
      "Epoch [3/300], Step [74/172], Loss: 73.0796\n",
      "Epoch [3/300], Step [75/172], Loss: 68.0342\n",
      "Epoch [3/300], Step [76/172], Loss: 70.9018\n",
      "Epoch [3/300], Step [77/172], Loss: 71.4391\n",
      "Epoch [3/300], Step [78/172], Loss: 71.5484\n",
      "Epoch [3/300], Step [79/172], Loss: 69.8667\n",
      "Epoch [3/300], Step [80/172], Loss: 69.2100\n",
      "Epoch [3/300], Step [81/172], Loss: 66.9071\n",
      "Epoch [3/300], Step [82/172], Loss: 66.2441\n",
      "Epoch [3/300], Step [83/172], Loss: 64.8814\n",
      "Epoch [3/300], Step [84/172], Loss: 64.4955\n",
      "Epoch [3/300], Step [85/172], Loss: 63.9492\n",
      "Epoch [3/300], Step [86/172], Loss: 59.2397\n",
      "Epoch [3/300], Step [87/172], Loss: 57.4367\n",
      "Epoch [3/300], Step [88/172], Loss: 57.6336\n",
      "Epoch [3/300], Step [89/172], Loss: 56.6206\n",
      "Epoch [3/300], Step [90/172], Loss: 53.7420\n",
      "Epoch [3/300], Step [91/172], Loss: 52.9388\n",
      "Epoch [3/300], Step [92/172], Loss: 50.8639\n",
      "Epoch [3/300], Step [93/172], Loss: 49.7421\n",
      "Epoch [3/300], Step [94/172], Loss: 49.3689\n",
      "Epoch [3/300], Step [95/172], Loss: 47.9239\n",
      "Epoch [3/300], Step [96/172], Loss: 47.3574\n",
      "Epoch [3/300], Step [97/172], Loss: 47.4767\n",
      "Epoch [3/300], Step [98/172], Loss: 46.0727\n",
      "Epoch [3/300], Step [99/172], Loss: 44.9268\n",
      "Epoch [3/300], Step [100/172], Loss: 44.5402\n",
      "Epoch [3/300], Step [101/172], Loss: 42.7498\n",
      "Epoch [3/300], Step [102/172], Loss: 43.0083\n",
      "Epoch [3/300], Step [103/172], Loss: 42.0269\n",
      "Epoch [3/300], Step [104/172], Loss: 40.4618\n",
      "Epoch [3/300], Step [105/172], Loss: 40.4260\n",
      "Epoch [3/300], Step [106/172], Loss: 38.8421\n",
      "Epoch [3/300], Step [107/172], Loss: 37.7588\n",
      "Epoch [3/300], Step [108/172], Loss: 38.5844\n",
      "Epoch [3/300], Step [109/172], Loss: 37.0119\n",
      "Epoch [3/300], Step [110/172], Loss: 37.1402\n",
      "Epoch [3/300], Step [111/172], Loss: 36.0482\n",
      "Epoch [3/300], Step [112/172], Loss: 35.8660\n",
      "Epoch [3/300], Step [113/172], Loss: 34.7977\n",
      "Epoch [3/300], Step [114/172], Loss: 33.8633\n",
      "Epoch [3/300], Step [115/172], Loss: 33.3131\n",
      "Epoch [3/300], Step [116/172], Loss: 32.7746\n",
      "Epoch [3/300], Step [117/172], Loss: 32.3135\n",
      "Epoch [3/300], Step [118/172], Loss: 32.2899\n",
      "Epoch [3/300], Step [119/172], Loss: 31.0244\n",
      "Epoch [3/300], Step [120/172], Loss: 31.2769\n",
      "Epoch [3/300], Step [121/172], Loss: 30.4416\n",
      "Epoch [3/300], Step [122/172], Loss: 28.8910\n",
      "Epoch [3/300], Step [123/172], Loss: 28.6105\n",
      "Epoch [3/300], Step [124/172], Loss: 28.7869\n",
      "Epoch [3/300], Step [125/172], Loss: 28.6046\n",
      "Epoch [3/300], Step [126/172], Loss: 27.9616\n",
      "Epoch [3/300], Step [127/172], Loss: 27.4248\n",
      "Epoch [3/300], Step [128/172], Loss: 26.7995\n",
      "Epoch [3/300], Step [129/172], Loss: 26.3691\n",
      "Epoch [3/300], Step [130/172], Loss: 26.6959\n",
      "Epoch [3/300], Step [131/172], Loss: 25.8020\n",
      "Epoch [3/300], Step [132/172], Loss: 25.1750\n",
      "Epoch [3/300], Step [133/172], Loss: 24.8745\n",
      "Epoch [3/300], Step [134/172], Loss: 24.5239\n",
      "Epoch [3/300], Step [135/172], Loss: 23.4060\n",
      "Epoch [3/300], Step [136/172], Loss: 21.6841\n",
      "Epoch [3/300], Step [137/172], Loss: 22.6525\n",
      "Epoch [3/300], Step [138/172], Loss: 22.1014\n",
      "Epoch [3/300], Step [139/172], Loss: 22.7428\n",
      "Epoch [3/300], Step [140/172], Loss: 21.7557\n",
      "Epoch [3/300], Step [141/172], Loss: 21.7593\n",
      "Epoch [3/300], Step [142/172], Loss: 21.5136\n",
      "Epoch [3/300], Step [143/172], Loss: 20.6291\n",
      "Epoch [3/300], Step [144/172], Loss: 20.3202\n",
      "Epoch [3/300], Step [145/172], Loss: 19.9591\n",
      "Epoch [3/300], Step [146/172], Loss: 19.2428\n",
      "Epoch [3/300], Step [147/172], Loss: 20.6621\n",
      "Epoch [3/300], Step [148/172], Loss: 19.7227\n",
      "Epoch [3/300], Step [149/172], Loss: 19.1217\n",
      "Epoch [3/300], Step [150/172], Loss: 19.1276\n",
      "Epoch [3/300], Step [151/172], Loss: 18.3588\n",
      "Epoch [3/300], Step [152/172], Loss: 17.8401\n",
      "Epoch [3/300], Step [153/172], Loss: 18.1600\n",
      "Epoch [3/300], Step [154/172], Loss: 17.2196\n",
      "Epoch [3/300], Step [155/172], Loss: 17.5045\n",
      "Epoch [3/300], Step [156/172], Loss: 17.4101\n",
      "Epoch [3/300], Step [157/172], Loss: 16.7464\n",
      "Epoch [3/300], Step [158/172], Loss: 16.7550\n",
      "Epoch [3/300], Step [159/172], Loss: 16.4871\n",
      "Epoch [3/300], Step [160/172], Loss: 16.6331\n",
      "Epoch [3/300], Step [161/172], Loss: 15.8876\n",
      "Epoch [3/300], Step [162/172], Loss: 15.9426\n",
      "Epoch [3/300], Step [163/172], Loss: 15.6616\n",
      "Epoch [3/300], Step [164/172], Loss: 15.6477\n",
      "Epoch [3/300], Step [165/172], Loss: 14.7027\n",
      "Epoch [3/300], Step [166/172], Loss: 15.5398\n",
      "Epoch [3/300], Step [167/172], Loss: 14.1792\n",
      "Epoch [3/300], Step [168/172], Loss: 15.2551\n",
      "Epoch [3/300], Step [169/172], Loss: 14.6185\n",
      "Epoch [3/300], Step [170/172], Loss: 14.2807\n",
      "Epoch [3/300], Step [171/172], Loss: 13.1576\n",
      "Epoch [3/300], Step [172/172], Loss: 9.5886\n",
      "Epoch [4/300], Step [1/172], Loss: 118.3826\n",
      "Epoch [4/300], Step [2/172], Loss: 116.8138\n",
      "Epoch [4/300], Step [3/172], Loss: 170.3515\n",
      "Epoch [4/300], Step [4/172], Loss: 127.9224\n",
      "Epoch [4/300], Step [5/172], Loss: 158.6416\n",
      "Epoch [4/300], Step [6/172], Loss: 99.3428\n",
      "Epoch [4/300], Step [7/172], Loss: 109.2858\n",
      "Epoch [4/300], Step [8/172], Loss: 95.5612\n",
      "Epoch [4/300], Step [9/172], Loss: 101.8141\n",
      "Epoch [4/300], Step [10/172], Loss: 118.1006\n",
      "Epoch [4/300], Step [11/172], Loss: 107.7572\n",
      "Epoch [4/300], Step [12/172], Loss: 98.8244\n",
      "Epoch [4/300], Step [13/172], Loss: 89.9694\n",
      "Epoch [4/300], Step [14/172], Loss: 112.1832\n",
      "Epoch [4/300], Step [15/172], Loss: 106.9945\n",
      "Epoch [4/300], Step [16/172], Loss: 90.1964\n",
      "Epoch [4/300], Step [17/172], Loss: 89.7256\n",
      "Epoch [4/300], Step [18/172], Loss: 95.2959\n",
      "Epoch [4/300], Step [19/172], Loss: 88.6762\n",
      "Epoch [4/300], Step [20/172], Loss: 174.0023\n",
      "Epoch [4/300], Step [21/172], Loss: 99.1440\n",
      "Epoch [4/300], Step [22/172], Loss: 102.7871\n",
      "Epoch [4/300], Step [23/172], Loss: 88.7545\n",
      "Epoch [4/300], Step [24/172], Loss: 94.1193\n",
      "Epoch [4/300], Step [25/172], Loss: 79.9402\n",
      "Epoch [4/300], Step [26/172], Loss: 83.1755\n",
      "Epoch [4/300], Step [27/172], Loss: 95.2952\n",
      "Epoch [4/300], Step [28/172], Loss: 78.6661\n",
      "Epoch [4/300], Step [29/172], Loss: 97.8536\n",
      "Epoch [4/300], Step [30/172], Loss: 84.9300\n",
      "Epoch [4/300], Step [31/172], Loss: 74.2006\n",
      "Epoch [4/300], Step [32/172], Loss: 70.4753\n",
      "Epoch [4/300], Step [33/172], Loss: 83.7451\n",
      "Epoch [4/300], Step [34/172], Loss: 69.2568\n",
      "Epoch [4/300], Step [35/172], Loss: 93.0437\n",
      "Epoch [4/300], Step [36/172], Loss: 66.6202\n",
      "Epoch [4/300], Step [37/172], Loss: 54.4594\n",
      "Epoch [4/300], Step [38/172], Loss: 60.5190\n",
      "Epoch [4/300], Step [39/172], Loss: 70.4817\n",
      "Epoch [4/300], Step [40/172], Loss: 59.2074\n",
      "Epoch [4/300], Step [41/172], Loss: 61.8002\n",
      "Epoch [4/300], Step [42/172], Loss: 59.5697\n",
      "Epoch [4/300], Step [43/172], Loss: 55.6379\n",
      "Epoch [4/300], Step [44/172], Loss: 62.5178\n",
      "Epoch [4/300], Step [45/172], Loss: 54.4587\n",
      "Epoch [4/300], Step [46/172], Loss: 79.1776\n",
      "Epoch [4/300], Step [47/172], Loss: 79.9864\n",
      "Epoch [4/300], Step [48/172], Loss: 106.6588\n",
      "Epoch [4/300], Step [49/172], Loss: 52.9562\n",
      "Epoch [4/300], Step [50/172], Loss: 67.6892\n",
      "Epoch [4/300], Step [51/172], Loss: 41.8180\n",
      "Epoch [4/300], Step [52/172], Loss: 45.7294\n",
      "Epoch [4/300], Step [53/172], Loss: 51.5188\n",
      "Epoch [4/300], Step [54/172], Loss: 47.3947\n",
      "Epoch [4/300], Step [55/172], Loss: 45.4929\n",
      "Epoch [4/300], Step [56/172], Loss: 41.0125\n",
      "Epoch [4/300], Step [57/172], Loss: 71.7741\n",
      "Epoch [4/300], Step [58/172], Loss: 46.7812\n",
      "Epoch [4/300], Step [59/172], Loss: 58.7965\n",
      "Epoch [4/300], Step [60/172], Loss: 91.7617\n",
      "Epoch [4/300], Step [61/172], Loss: 41.0553\n",
      "Epoch [4/300], Step [62/172], Loss: 46.8806\n",
      "Epoch [4/300], Step [63/172], Loss: 34.1777\n",
      "Epoch [4/300], Step [64/172], Loss: 31.8127\n",
      "Epoch [4/300], Step [65/172], Loss: 47.0124\n",
      "Epoch [4/300], Step [66/172], Loss: 32.1742\n",
      "Epoch [4/300], Step [67/172], Loss: 47.8805\n",
      "Epoch [4/300], Step [68/172], Loss: 42.2303\n",
      "Epoch [4/300], Step [69/172], Loss: 69.2646\n",
      "Epoch [4/300], Step [70/172], Loss: 67.7066\n",
      "Epoch [4/300], Step [71/172], Loss: 67.8710\n",
      "Epoch [4/300], Step [72/172], Loss: 66.8763\n",
      "Epoch [4/300], Step [73/172], Loss: 68.8740\n",
      "Epoch [4/300], Step [74/172], Loss: 66.1284\n",
      "Epoch [4/300], Step [75/172], Loss: 60.8618\n",
      "Epoch [4/300], Step [76/172], Loss: 64.9863\n",
      "Epoch [4/300], Step [77/172], Loss: 66.2789\n",
      "Epoch [4/300], Step [78/172], Loss: 66.3845\n",
      "Epoch [4/300], Step [79/172], Loss: 65.3163\n",
      "Epoch [4/300], Step [80/172], Loss: 64.6459\n",
      "Epoch [4/300], Step [81/172], Loss: 62.4290\n",
      "Epoch [4/300], Step [82/172], Loss: 61.8764\n",
      "Epoch [4/300], Step [83/172], Loss: 61.1407\n",
      "Epoch [4/300], Step [84/172], Loss: 61.2986\n",
      "Epoch [4/300], Step [85/172], Loss: 61.1709\n",
      "Epoch [4/300], Step [86/172], Loss: 55.5913\n",
      "Epoch [4/300], Step [87/172], Loss: 54.0111\n",
      "Epoch [4/300], Step [88/172], Loss: 54.7799\n",
      "Epoch [4/300], Step [89/172], Loss: 54.0844\n",
      "Epoch [4/300], Step [90/172], Loss: 51.0739\n",
      "Epoch [4/300], Step [91/172], Loss: 50.5634\n",
      "Epoch [4/300], Step [92/172], Loss: 48.5289\n",
      "Epoch [4/300], Step [93/172], Loss: 47.7659\n",
      "Epoch [4/300], Step [94/172], Loss: 47.8515\n",
      "Epoch [4/300], Step [95/172], Loss: 46.3987\n",
      "Epoch [4/300], Step [96/172], Loss: 46.0809\n",
      "Epoch [4/300], Step [97/172], Loss: 46.5519\n",
      "Epoch [4/300], Step [98/172], Loss: 45.1198\n",
      "Epoch [4/300], Step [99/172], Loss: 44.0760\n",
      "Epoch [4/300], Step [100/172], Loss: 43.6786\n",
      "Epoch [4/300], Step [101/172], Loss: 41.9950\n",
      "Epoch [4/300], Step [102/172], Loss: 42.4392\n",
      "Epoch [4/300], Step [103/172], Loss: 41.5698\n",
      "Epoch [4/300], Step [104/172], Loss: 40.0055\n",
      "Epoch [4/300], Step [105/172], Loss: 40.1028\n",
      "Epoch [4/300], Step [106/172], Loss: 38.5862\n",
      "Epoch [4/300], Step [107/172], Loss: 37.3120\n",
      "Epoch [4/300], Step [108/172], Loss: 38.5694\n",
      "Epoch [4/300], Step [109/172], Loss: 36.8322\n",
      "Epoch [4/300], Step [110/172], Loss: 37.0735\n",
      "Epoch [4/300], Step [111/172], Loss: 35.9649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/300], Step [112/172], Loss: 36.3763\n",
      "Epoch [4/300], Step [113/172], Loss: 34.7224\n",
      "Epoch [4/300], Step [114/172], Loss: 34.1091\n",
      "Epoch [4/300], Step [115/172], Loss: 33.7394\n",
      "Epoch [4/300], Step [116/172], Loss: 33.1036\n",
      "Epoch [4/300], Step [117/172], Loss: 32.3923\n",
      "Epoch [4/300], Step [118/172], Loss: 32.5810\n",
      "Epoch [4/300], Step [119/172], Loss: 31.2197\n",
      "Epoch [4/300], Step [120/172], Loss: 31.4347\n",
      "Epoch [4/300], Step [121/172], Loss: 30.7411\n",
      "Epoch [4/300], Step [122/172], Loss: 29.0147\n",
      "Epoch [4/300], Step [123/172], Loss: 28.8497\n",
      "Epoch [4/300], Step [124/172], Loss: 29.0378\n",
      "Epoch [4/300], Step [125/172], Loss: 28.9674\n",
      "Epoch [4/300], Step [126/172], Loss: 28.4758\n",
      "Epoch [4/300], Step [127/172], Loss: 28.1748\n",
      "Epoch [4/300], Step [128/172], Loss: 27.6776\n",
      "Epoch [4/300], Step [129/172], Loss: 26.7875\n",
      "Epoch [4/300], Step [130/172], Loss: 27.2712\n",
      "Epoch [4/300], Step [131/172], Loss: 26.2971\n",
      "Epoch [4/300], Step [132/172], Loss: 25.7795\n",
      "Epoch [4/300], Step [133/172], Loss: 25.4893\n",
      "Epoch [4/300], Step [134/172], Loss: 25.2206\n",
      "Epoch [4/300], Step [135/172], Loss: 23.8999\n",
      "Epoch [4/300], Step [136/172], Loss: 22.7525\n",
      "Epoch [4/300], Step [137/172], Loss: 23.5493\n",
      "Epoch [4/300], Step [138/172], Loss: 22.8758\n",
      "Epoch [4/300], Step [139/172], Loss: 23.6126\n",
      "Epoch [4/300], Step [140/172], Loss: 22.6986\n",
      "Epoch [4/300], Step [141/172], Loss: 22.7349\n",
      "Epoch [4/300], Step [142/172], Loss: 22.5831\n",
      "Epoch [4/300], Step [143/172], Loss: 21.5696\n",
      "Epoch [4/300], Step [144/172], Loss: 21.1550\n",
      "Epoch [4/300], Step [145/172], Loss: 20.8680\n",
      "Epoch [4/300], Step [146/172], Loss: 20.2150\n",
      "Epoch [4/300], Step [147/172], Loss: 21.5245\n",
      "Epoch [4/300], Step [148/172], Loss: 20.5777\n",
      "Epoch [4/300], Step [149/172], Loss: 20.0909\n",
      "Epoch [4/300], Step [150/172], Loss: 20.1954\n",
      "Epoch [4/300], Step [151/172], Loss: 19.1956\n",
      "Epoch [4/300], Step [152/172], Loss: 18.7622\n",
      "Epoch [4/300], Step [153/172], Loss: 19.0706\n",
      "Epoch [4/300], Step [154/172], Loss: 18.2921\n",
      "Epoch [4/300], Step [155/172], Loss: 18.3692\n",
      "Epoch [4/300], Step [156/172], Loss: 18.2681\n",
      "Epoch [4/300], Step [157/172], Loss: 17.8249\n",
      "Epoch [4/300], Step [158/172], Loss: 17.6971\n",
      "Epoch [4/300], Step [159/172], Loss: 17.3998\n",
      "Epoch [4/300], Step [160/172], Loss: 17.5139\n",
      "Epoch [4/300], Step [161/172], Loss: 16.7140\n",
      "Epoch [4/300], Step [162/172], Loss: 16.6908\n",
      "Epoch [4/300], Step [163/172], Loss: 16.4203\n",
      "Epoch [4/300], Step [164/172], Loss: 16.4183\n",
      "Epoch [4/300], Step [165/172], Loss: 15.4190\n",
      "Epoch [4/300], Step [166/172], Loss: 16.3150\n",
      "Epoch [4/300], Step [167/172], Loss: 14.9936\n",
      "Epoch [4/300], Step [168/172], Loss: 15.9833\n",
      "Epoch [4/300], Step [169/172], Loss: 15.1996\n",
      "Epoch [4/300], Step [170/172], Loss: 14.8818\n",
      "Epoch [4/300], Step [171/172], Loss: 13.6565\n",
      "Epoch [4/300], Step [172/172], Loss: 10.0069\n",
      "Epoch [5/300], Step [1/172], Loss: 113.1211\n",
      "Epoch [5/300], Step [2/172], Loss: 112.0566\n",
      "Epoch [5/300], Step [3/172], Loss: 165.0707\n",
      "Epoch [5/300], Step [4/172], Loss: 122.5961\n",
      "Epoch [5/300], Step [5/172], Loss: 150.4393\n",
      "Epoch [5/300], Step [6/172], Loss: 92.7239\n",
      "Epoch [5/300], Step [7/172], Loss: 102.4207\n",
      "Epoch [5/300], Step [8/172], Loss: 89.3233\n",
      "Epoch [5/300], Step [9/172], Loss: 96.8799\n",
      "Epoch [5/300], Step [10/172], Loss: 111.3053\n",
      "Epoch [5/300], Step [11/172], Loss: 103.0396\n",
      "Epoch [5/300], Step [12/172], Loss: 95.1727\n",
      "Epoch [5/300], Step [13/172], Loss: 85.2027\n",
      "Epoch [5/300], Step [14/172], Loss: 109.2238\n",
      "Epoch [5/300], Step [15/172], Loss: 103.7599\n",
      "Epoch [5/300], Step [16/172], Loss: 86.0623\n",
      "Epoch [5/300], Step [17/172], Loss: 84.7589\n",
      "Epoch [5/300], Step [18/172], Loss: 92.5160\n",
      "Epoch [5/300], Step [19/172], Loss: 85.2883\n",
      "Epoch [5/300], Step [20/172], Loss: 168.9702\n",
      "Epoch [5/300], Step [21/172], Loss: 96.9803\n",
      "Epoch [5/300], Step [22/172], Loss: 100.7288\n",
      "Epoch [5/300], Step [23/172], Loss: 84.9272\n",
      "Epoch [5/300], Step [24/172], Loss: 92.0950\n",
      "Epoch [5/300], Step [25/172], Loss: 76.6896\n",
      "Epoch [5/300], Step [26/172], Loss: 80.3724\n",
      "Epoch [5/300], Step [27/172], Loss: 93.6548\n",
      "Epoch [5/300], Step [28/172], Loss: 77.1226\n",
      "Epoch [5/300], Step [29/172], Loss: 95.6594\n",
      "Epoch [5/300], Step [30/172], Loss: 84.1162\n",
      "Epoch [5/300], Step [31/172], Loss: 72.0764\n",
      "Epoch [5/300], Step [32/172], Loss: 68.1004\n",
      "Epoch [5/300], Step [33/172], Loss: 83.1721\n",
      "Epoch [5/300], Step [34/172], Loss: 66.8566\n",
      "Epoch [5/300], Step [35/172], Loss: 90.8091\n",
      "Epoch [5/300], Step [36/172], Loss: 62.5668\n",
      "Epoch [5/300], Step [37/172], Loss: 50.7067\n",
      "Epoch [5/300], Step [38/172], Loss: 58.5065\n",
      "Epoch [5/300], Step [39/172], Loss: 68.7783\n",
      "Epoch [5/300], Step [40/172], Loss: 57.4200\n",
      "Epoch [5/300], Step [41/172], Loss: 60.1126\n",
      "Epoch [5/300], Step [42/172], Loss: 58.4512\n",
      "Epoch [5/300], Step [43/172], Loss: 53.9358\n",
      "Epoch [5/300], Step [44/172], Loss: 61.9173\n",
      "Epoch [5/300], Step [45/172], Loss: 52.9286\n",
      "Epoch [5/300], Step [46/172], Loss: 78.5260\n",
      "Epoch [5/300], Step [47/172], Loss: 80.5885\n",
      "Epoch [5/300], Step [48/172], Loss: 105.6137\n",
      "Epoch [5/300], Step [49/172], Loss: 51.5816\n",
      "Epoch [5/300], Step [50/172], Loss: 67.9373\n",
      "Epoch [5/300], Step [51/172], Loss: 38.8447\n",
      "Epoch [5/300], Step [52/172], Loss: 44.3594\n",
      "Epoch [5/300], Step [53/172], Loss: 50.7423\n",
      "Epoch [5/300], Step [54/172], Loss: 46.7137\n",
      "Epoch [5/300], Step [55/172], Loss: 44.7316\n",
      "Epoch [5/300], Step [56/172], Loss: 39.3818\n",
      "Epoch [5/300], Step [57/172], Loss: 72.2364\n",
      "Epoch [5/300], Step [58/172], Loss: 45.2098\n",
      "Epoch [5/300], Step [59/172], Loss: 58.9888\n",
      "Epoch [5/300], Step [60/172], Loss: 93.2636\n",
      "Epoch [5/300], Step [61/172], Loss: 39.9839\n",
      "Epoch [5/300], Step [62/172], Loss: 47.1234\n",
      "Epoch [5/300], Step [63/172], Loss: 33.0677\n",
      "Epoch [5/300], Step [64/172], Loss: 30.1107\n",
      "Epoch [5/300], Step [65/172], Loss: 46.5056\n",
      "Epoch [5/300], Step [66/172], Loss: 30.9243\n",
      "Epoch [5/300], Step [67/172], Loss: 48.6565\n",
      "Epoch [5/300], Step [68/172], Loss: 45.0488\n",
      "Epoch [5/300], Step [69/172], Loss: 70.1731\n",
      "Epoch [5/300], Step [70/172], Loss: 65.6475\n",
      "Epoch [5/300], Step [71/172], Loss: 65.4826\n",
      "Epoch [5/300], Step [72/172], Loss: 63.7380\n",
      "Epoch [5/300], Step [73/172], Loss: 66.3457\n",
      "Epoch [5/300], Step [74/172], Loss: 62.6897\n",
      "Epoch [5/300], Step [75/172], Loss: 56.1640\n",
      "Epoch [5/300], Step [76/172], Loss: 61.8074\n",
      "Epoch [5/300], Step [77/172], Loss: 63.6837\n",
      "Epoch [5/300], Step [78/172], Loss: 63.9835\n",
      "Epoch [5/300], Step [79/172], Loss: 62.8812\n",
      "Epoch [5/300], Step [80/172], Loss: 62.4658\n",
      "Epoch [5/300], Step [81/172], Loss: 59.9657\n",
      "Epoch [5/300], Step [82/172], Loss: 59.6757\n",
      "Epoch [5/300], Step [83/172], Loss: 59.3036\n",
      "Epoch [5/300], Step [84/172], Loss: 59.8059\n",
      "Epoch [5/300], Step [85/172], Loss: 60.1765\n",
      "Epoch [5/300], Step [86/172], Loss: 53.5256\n",
      "Epoch [5/300], Step [87/172], Loss: 51.8081\n",
      "Epoch [5/300], Step [88/172], Loss: 53.1347\n",
      "Epoch [5/300], Step [89/172], Loss: 52.6182\n",
      "Epoch [5/300], Step [90/172], Loss: 49.3099\n",
      "Epoch [5/300], Step [91/172], Loss: 48.9635\n",
      "Epoch [5/300], Step [92/172], Loss: 46.7365\n",
      "Epoch [5/300], Step [93/172], Loss: 46.3325\n",
      "Epoch [5/300], Step [94/172], Loss: 46.7730\n",
      "Epoch [5/300], Step [95/172], Loss: 45.1425\n",
      "Epoch [5/300], Step [96/172], Loss: 44.8960\n",
      "Epoch [5/300], Step [97/172], Loss: 45.6801\n",
      "Epoch [5/300], Step [98/172], Loss: 44.1953\n",
      "Epoch [5/300], Step [99/172], Loss: 43.1120\n",
      "Epoch [5/300], Step [100/172], Loss: 42.6934\n",
      "Epoch [5/300], Step [101/172], Loss: 41.0351\n",
      "Epoch [5/300], Step [102/172], Loss: 41.6280\n",
      "Epoch [5/300], Step [103/172], Loss: 40.7976\n",
      "Epoch [5/300], Step [104/172], Loss: 39.1771\n",
      "Epoch [5/300], Step [105/172], Loss: 39.4263\n",
      "Epoch [5/300], Step [106/172], Loss: 37.8997\n",
      "Epoch [5/300], Step [107/172], Loss: 36.3602\n",
      "Epoch [5/300], Step [108/172], Loss: 38.1339\n",
      "Epoch [5/300], Step [109/172], Loss: 36.1721\n",
      "Epoch [5/300], Step [110/172], Loss: 36.5349\n",
      "Epoch [5/300], Step [111/172], Loss: 35.3917\n",
      "Epoch [5/300], Step [112/172], Loss: 36.4616\n",
      "Epoch [5/300], Step [113/172], Loss: 34.1549\n",
      "Epoch [5/300], Step [114/172], Loss: 33.8646\n",
      "Epoch [5/300], Step [115/172], Loss: 33.7106\n",
      "Epoch [5/300], Step [116/172], Loss: 32.9278\n",
      "Epoch [5/300], Step [117/172], Loss: 31.9446\n",
      "Epoch [5/300], Step [118/172], Loss: 32.3623\n",
      "Epoch [5/300], Step [119/172], Loss: 30.8767\n",
      "Epoch [5/300], Step [120/172], Loss: 31.0708\n",
      "Epoch [5/300], Step [121/172], Loss: 30.5354\n",
      "Epoch [5/300], Step [122/172], Loss: 28.5812\n",
      "Epoch [5/300], Step [123/172], Loss: 28.5353\n",
      "Epoch [5/300], Step [124/172], Loss: 28.7313\n",
      "Epoch [5/300], Step [125/172], Loss: 28.7944\n",
      "Epoch [5/300], Step [126/172], Loss: 28.4535\n",
      "Epoch [5/300], Step [127/172], Loss: 28.3813\n",
      "Epoch [5/300], Step [128/172], Loss: 28.0046\n",
      "Epoch [5/300], Step [129/172], Loss: 26.6536\n",
      "Epoch [5/300], Step [130/172], Loss: 27.3101\n",
      "Epoch [5/300], Step [131/172], Loss: 26.2239\n",
      "Epoch [5/300], Step [132/172], Loss: 25.8059\n",
      "Epoch [5/300], Step [133/172], Loss: 25.5166\n",
      "Epoch [5/300], Step [134/172], Loss: 25.3245\n",
      "Epoch [5/300], Step [135/172], Loss: 23.8165\n",
      "Epoch [5/300], Step [136/172], Loss: 23.0910\n",
      "Epoch [5/300], Step [137/172], Loss: 23.8008\n",
      "Epoch [5/300], Step [138/172], Loss: 23.0103\n",
      "Epoch [5/300], Step [139/172], Loss: 23.8389\n",
      "Epoch [5/300], Step [140/172], Loss: 22.9689\n",
      "Epoch [5/300], Step [141/172], Loss: 23.0370\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/300], Step [142/172], Loss: 22.9551\n",
      "Epoch [5/300], Step [143/172], Loss: 21.7900\n",
      "Epoch [5/300], Step [144/172], Loss: 21.2438\n",
      "Epoch [5/300], Step [145/172], Loss: 21.0516\n",
      "Epoch [5/300], Step [146/172], Loss: 20.4204\n",
      "Epoch [5/300], Step [147/172], Loss: 21.6252\n",
      "Epoch [5/300], Step [148/172], Loss: 20.6782\n",
      "Epoch [5/300], Step [149/172], Loss: 20.2783\n",
      "Epoch [5/300], Step [150/172], Loss: 20.4360\n",
      "Epoch [5/300], Step [151/172], Loss: 19.2289\n",
      "Epoch [5/300], Step [152/172], Loss: 18.8516\n",
      "Epoch [5/300], Step [153/172], Loss: 19.1268\n",
      "Epoch [5/300], Step [154/172], Loss: 18.5064\n",
      "Epoch [5/300], Step [155/172], Loss: 18.3599\n",
      "Epoch [5/300], Step [156/172], Loss: 18.2820\n",
      "Epoch [5/300], Step [157/172], Loss: 18.0004\n",
      "Epoch [5/300], Step [158/172], Loss: 17.7592\n",
      "Epoch [5/300], Step [159/172], Loss: 17.4194\n",
      "Epoch [5/300], Step [160/172], Loss: 17.5088\n",
      "Epoch [5/300], Step [161/172], Loss: 16.6422\n",
      "Epoch [5/300], Step [162/172], Loss: 16.5518\n",
      "Epoch [5/300], Step [163/172], Loss: 16.2950\n",
      "Epoch [5/300], Step [164/172], Loss: 16.3401\n",
      "Epoch [5/300], Step [165/172], Loss: 15.2350\n",
      "Epoch [5/300], Step [166/172], Loss: 16.1567\n",
      "Epoch [5/300], Step [167/172], Loss: 14.8996\n",
      "Epoch [5/300], Step [168/172], Loss: 15.8106\n",
      "Epoch [5/300], Step [169/172], Loss: 14.8342\n",
      "Epoch [5/300], Step [170/172], Loss: 14.5662\n",
      "Epoch [5/300], Step [171/172], Loss: 13.3078\n",
      "Epoch [5/300], Step [172/172], Loss: 9.7715\n",
      "Epoch [6/300], Step [1/172], Loss: 112.3745\n",
      "Epoch [6/300], Step [2/172], Loss: 111.6012\n",
      "Epoch [6/300], Step [3/172], Loss: 162.7844\n",
      "Epoch [6/300], Step [4/172], Loss: 120.3654\n",
      "Epoch [6/300], Step [5/172], Loss: 146.2223\n",
      "Epoch [6/300], Step [6/172], Loss: 89.0357\n",
      "Epoch [6/300], Step [7/172], Loss: 99.7101\n",
      "Epoch [6/300], Step [8/172], Loss: 85.3669\n",
      "Epoch [6/300], Step [9/172], Loss: 94.6094\n",
      "Epoch [6/300], Step [10/172], Loss: 107.1560\n",
      "Epoch [6/300], Step [11/172], Loss: 100.9320\n",
      "Epoch [6/300], Step [12/172], Loss: 94.1351\n",
      "Epoch [6/300], Step [13/172], Loss: 82.4426\n",
      "Epoch [6/300], Step [14/172], Loss: 108.6983\n",
      "Epoch [6/300], Step [15/172], Loss: 102.9931\n",
      "Epoch [6/300], Step [16/172], Loss: 83.9080\n",
      "Epoch [6/300], Step [17/172], Loss: 82.1026\n",
      "Epoch [6/300], Step [18/172], Loss: 91.7525\n",
      "Epoch [6/300], Step [19/172], Loss: 83.9731\n",
      "Epoch [6/300], Step [20/172], Loss: 164.2961\n",
      "Epoch [6/300], Step [21/172], Loss: 96.8257\n",
      "Epoch [6/300], Step [22/172], Loss: 100.0757\n",
      "Epoch [6/300], Step [23/172], Loss: 83.1225\n",
      "Epoch [6/300], Step [24/172], Loss: 91.5169\n",
      "Epoch [6/300], Step [25/172], Loss: 74.7098\n",
      "Epoch [6/300], Step [26/172], Loss: 78.5936\n",
      "Epoch [6/300], Step [27/172], Loss: 92.6345\n",
      "Epoch [6/300], Step [28/172], Loss: 76.4173\n",
      "Epoch [6/300], Step [29/172], Loss: 94.3088\n",
      "Epoch [6/300], Step [30/172], Loss: 83.7001\n",
      "Epoch [6/300], Step [31/172], Loss: 70.2807\n",
      "Epoch [6/300], Step [32/172], Loss: 65.9946\n",
      "Epoch [6/300], Step [33/172], Loss: 82.6581\n",
      "Epoch [6/300], Step [34/172], Loss: 64.5801\n",
      "Epoch [6/300], Step [35/172], Loss: 88.5162\n",
      "Epoch [6/300], Step [36/172], Loss: 58.7554\n",
      "Epoch [6/300], Step [37/172], Loss: 47.0744\n",
      "Epoch [6/300], Step [38/172], Loss: 56.5737\n",
      "Epoch [6/300], Step [39/172], Loss: 66.9871\n",
      "Epoch [6/300], Step [40/172], Loss: 55.4829\n",
      "Epoch [6/300], Step [41/172], Loss: 58.2627\n",
      "Epoch [6/300], Step [42/172], Loss: 56.9999\n",
      "Epoch [6/300], Step [43/172], Loss: 51.8567\n",
      "Epoch [6/300], Step [44/172], Loss: 60.9972\n",
      "Epoch [6/300], Step [45/172], Loss: 50.9656\n",
      "Epoch [6/300], Step [46/172], Loss: 76.8510\n",
      "Epoch [6/300], Step [47/172], Loss: 80.3182\n",
      "Epoch [6/300], Step [48/172], Loss: 103.7571\n",
      "Epoch [6/300], Step [49/172], Loss: 49.0194\n",
      "Epoch [6/300], Step [50/172], Loss: 67.1968\n",
      "Epoch [6/300], Step [51/172], Loss: 34.3031\n",
      "Epoch [6/300], Step [52/172], Loss: 41.3111\n",
      "Epoch [6/300], Step [53/172], Loss: 48.3605\n",
      "Epoch [6/300], Step [54/172], Loss: 44.0837\n",
      "Epoch [6/300], Step [55/172], Loss: 41.9132\n",
      "Epoch [6/300], Step [56/172], Loss: 35.2483\n",
      "Epoch [6/300], Step [57/172], Loss: 70.4155\n",
      "Epoch [6/300], Step [58/172], Loss: 41.3635\n",
      "Epoch [6/300], Step [59/172], Loss: 56.8732\n",
      "Epoch [6/300], Step [60/172], Loss: 93.7211\n",
      "Epoch [6/300], Step [61/172], Loss: 35.7628\n",
      "Epoch [6/300], Step [62/172], Loss: 43.9151\n",
      "Epoch [6/300], Step [63/172], Loss: 27.9940\n",
      "Epoch [6/300], Step [64/172], Loss: 24.5150\n",
      "Epoch [6/300], Step [65/172], Loss: 42.7417\n",
      "Epoch [6/300], Step [66/172], Loss: 25.1745\n",
      "Epoch [6/300], Step [67/172], Loss: 45.2589\n",
      "Epoch [6/300], Step [68/172], Loss: 41.8782\n",
      "Epoch [6/300], Step [69/172], Loss: 72.4538\n",
      "Epoch [6/300], Step [70/172], Loss: 72.1228\n",
      "Epoch [6/300], Step [71/172], Loss: 71.8316\n",
      "Epoch [6/300], Step [72/172], Loss: 70.1633\n",
      "Epoch [6/300], Step [73/172], Loss: 72.8457\n",
      "Epoch [6/300], Step [74/172], Loss: 67.9261\n",
      "Epoch [6/300], Step [75/172], Loss: 58.3179\n",
      "Epoch [6/300], Step [76/172], Loss: 65.6790\n",
      "Epoch [6/300], Step [77/172], Loss: 68.1419\n",
      "Epoch [6/300], Step [78/172], Loss: 68.4901\n",
      "Epoch [6/300], Step [79/172], Loss: 66.4059\n",
      "Epoch [6/300], Step [80/172], Loss: 65.6089\n",
      "Epoch [6/300], Step [81/172], Loss: 62.2649\n",
      "Epoch [6/300], Step [82/172], Loss: 61.5981\n",
      "Epoch [6/300], Step [83/172], Loss: 60.9437\n",
      "Epoch [6/300], Step [84/172], Loss: 61.3097\n",
      "Epoch [6/300], Step [85/172], Loss: 61.6953\n",
      "Epoch [6/300], Step [86/172], Loss: 53.7562\n",
      "Epoch [6/300], Step [87/172], Loss: 51.4031\n",
      "Epoch [6/300], Step [88/172], Loss: 52.8715\n",
      "Epoch [6/300], Step [89/172], Loss: 52.5199\n",
      "Epoch [6/300], Step [90/172], Loss: 48.4374\n",
      "Epoch [6/300], Step [91/172], Loss: 48.0312\n",
      "Epoch [6/300], Step [92/172], Loss: 45.3460\n",
      "Epoch [6/300], Step [93/172], Loss: 45.1149\n",
      "Epoch [6/300], Step [94/172], Loss: 45.6704\n",
      "Epoch [6/300], Step [95/172], Loss: 43.7973\n",
      "Epoch [6/300], Step [96/172], Loss: 43.4225\n",
      "Epoch [6/300], Step [97/172], Loss: 44.3286\n",
      "Epoch [6/300], Step [98/172], Loss: 42.6950\n",
      "Epoch [6/300], Step [99/172], Loss: 41.5510\n",
      "Epoch [6/300], Step [100/172], Loss: 41.2088\n",
      "Epoch [6/300], Step [101/172], Loss: 39.4837\n",
      "Epoch [6/300], Step [102/172], Loss: 40.1311\n",
      "Epoch [6/300], Step [103/172], Loss: 39.4316\n",
      "Epoch [6/300], Step [104/172], Loss: 37.8211\n",
      "Epoch [6/300], Step [105/172], Loss: 38.0877\n",
      "Epoch [6/300], Step [106/172], Loss: 36.6866\n",
      "Epoch [6/300], Step [107/172], Loss: 34.9064\n",
      "Epoch [6/300], Step [108/172], Loss: 37.0753\n",
      "Epoch [6/300], Step [109/172], Loss: 35.0598\n",
      "Epoch [6/300], Step [110/172], Loss: 35.4881\n",
      "Epoch [6/300], Step [111/172], Loss: 34.3561\n",
      "Epoch [6/300], Step [112/172], Loss: 36.0101\n",
      "Epoch [6/300], Step [113/172], Loss: 33.1541\n",
      "Epoch [6/300], Step [114/172], Loss: 33.1141\n",
      "Epoch [6/300], Step [115/172], Loss: 33.1886\n",
      "Epoch [6/300], Step [116/172], Loss: 32.2913\n",
      "Epoch [6/300], Step [117/172], Loss: 31.0938\n",
      "Epoch [6/300], Step [118/172], Loss: 31.6932\n",
      "Epoch [6/300], Step [119/172], Loss: 30.1362\n",
      "Epoch [6/300], Step [120/172], Loss: 30.2993\n",
      "Epoch [6/300], Step [121/172], Loss: 29.8683\n",
      "Epoch [6/300], Step [122/172], Loss: 27.7894\n",
      "Epoch [6/300], Step [123/172], Loss: 27.9385\n",
      "Epoch [6/300], Step [124/172], Loss: 28.0614\n",
      "Epoch [6/300], Step [125/172], Loss: 28.2686\n",
      "Epoch [6/300], Step [126/172], Loss: 28.0480\n",
      "Epoch [6/300], Step [127/172], Loss: 28.1953\n",
      "Epoch [6/300], Step [128/172], Loss: 27.9292\n",
      "Epoch [6/300], Step [129/172], Loss: 26.1593\n",
      "Epoch [6/300], Step [130/172], Loss: 26.9609\n",
      "Epoch [6/300], Step [131/172], Loss: 25.7992\n",
      "Epoch [6/300], Step [132/172], Loss: 25.4752\n",
      "Epoch [6/300], Step [133/172], Loss: 25.1750\n",
      "Epoch [6/300], Step [134/172], Loss: 25.0636\n",
      "Epoch [6/300], Step [135/172], Loss: 23.3974\n",
      "Epoch [6/300], Step [136/172], Loss: 23.0502\n",
      "Epoch [6/300], Step [137/172], Loss: 23.6658\n",
      "Epoch [6/300], Step [138/172], Loss: 22.7833\n",
      "Epoch [6/300], Step [139/172], Loss: 23.6851\n",
      "Epoch [6/300], Step [140/172], Loss: 22.8654\n",
      "Epoch [6/300], Step [141/172], Loss: 23.0100\n",
      "Epoch [6/300], Step [142/172], Loss: 22.9742\n",
      "Epoch [6/300], Step [143/172], Loss: 21.6992\n",
      "Epoch [6/300], Step [144/172], Loss: 21.1204\n",
      "Epoch [6/300], Step [145/172], Loss: 20.9761\n",
      "Epoch [6/300], Step [146/172], Loss: 20.4254\n",
      "Epoch [6/300], Step [147/172], Loss: 21.6166\n",
      "Epoch [6/300], Step [148/172], Loss: 20.7027\n",
      "Epoch [6/300], Step [149/172], Loss: 20.3810\n",
      "Epoch [6/300], Step [150/172], Loss: 20.6579\n",
      "Epoch [6/300], Step [151/172], Loss: 19.3153\n",
      "Epoch [6/300], Step [152/172], Loss: 19.0394\n",
      "Epoch [6/300], Step [153/172], Loss: 19.3723\n",
      "Epoch [6/300], Step [154/172], Loss: 18.9372\n",
      "Epoch [6/300], Step [155/172], Loss: 18.6655\n",
      "Epoch [6/300], Step [156/172], Loss: 18.6362\n",
      "Epoch [6/300], Step [157/172], Loss: 18.5876\n",
      "Epoch [6/300], Step [158/172], Loss: 18.2945\n",
      "Epoch [6/300], Step [159/172], Loss: 18.0124\n",
      "Epoch [6/300], Step [160/172], Loss: 18.1241\n",
      "Epoch [6/300], Step [161/172], Loss: 17.2633\n",
      "Epoch [6/300], Step [162/172], Loss: 17.1630\n",
      "Epoch [6/300], Step [163/172], Loss: 16.9786\n",
      "Epoch [6/300], Step [164/172], Loss: 17.0826\n",
      "Epoch [6/300], Step [165/172], Loss: 15.9615\n",
      "Epoch [6/300], Step [166/172], Loss: 16.9969\n",
      "Epoch [6/300], Step [167/172], Loss: 15.8522\n",
      "Epoch [6/300], Step [168/172], Loss: 16.7436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/300], Step [169/172], Loss: 15.6850\n",
      "Epoch [6/300], Step [170/172], Loss: 15.4968\n",
      "Epoch [6/300], Step [171/172], Loss: 14.1959\n",
      "Epoch [6/300], Step [172/172], Loss: 10.5035\n",
      "Epoch [7/300], Step [1/172], Loss: 106.2588\n",
      "Epoch [7/300], Step [2/172], Loss: 105.6626\n",
      "Epoch [7/300], Step [3/172], Loss: 156.3377\n",
      "Epoch [7/300], Step [4/172], Loss: 114.0030\n",
      "Epoch [7/300], Step [5/172], Loss: 137.6820\n",
      "Epoch [7/300], Step [6/172], Loss: 82.3841\n",
      "Epoch [7/300], Step [7/172], Loss: 93.2388\n",
      "Epoch [7/300], Step [8/172], Loss: 79.7362\n",
      "Epoch [7/300], Step [9/172], Loss: 89.9688\n",
      "Epoch [7/300], Step [10/172], Loss: 101.1221\n",
      "Epoch [7/300], Step [11/172], Loss: 96.7064\n",
      "Epoch [7/300], Step [12/172], Loss: 90.7963\n",
      "Epoch [7/300], Step [13/172], Loss: 78.7051\n",
      "Epoch [7/300], Step [14/172], Loss: 105.7000\n",
      "Epoch [7/300], Step [15/172], Loss: 100.5444\n",
      "Epoch [7/300], Step [16/172], Loss: 81.4530\n",
      "Epoch [7/300], Step [17/172], Loss: 79.1179\n",
      "Epoch [7/300], Step [18/172], Loss: 90.2931\n",
      "Epoch [7/300], Step [19/172], Loss: 82.4844\n",
      "Epoch [7/300], Step [20/172], Loss: 159.5550\n",
      "Epoch [7/300], Step [21/172], Loss: 96.5019\n",
      "Epoch [7/300], Step [22/172], Loss: 99.4910\n",
      "Epoch [7/300], Step [23/172], Loss: 82.1139\n",
      "Epoch [7/300], Step [24/172], Loss: 91.2769\n",
      "Epoch [7/300], Step [25/172], Loss: 74.1479\n",
      "Epoch [7/300], Step [26/172], Loss: 78.1386\n",
      "Epoch [7/300], Step [27/172], Loss: 92.9075\n",
      "Epoch [7/300], Step [28/172], Loss: 77.2928\n",
      "Epoch [7/300], Step [29/172], Loss: 94.9653\n",
      "Epoch [7/300], Step [30/172], Loss: 84.6997\n",
      "Epoch [7/300], Step [31/172], Loss: 70.5202\n",
      "Epoch [7/300], Step [32/172], Loss: 65.8338\n",
      "Epoch [7/300], Step [33/172], Loss: 83.7699\n",
      "Epoch [7/300], Step [34/172], Loss: 64.4786\n",
      "Epoch [7/300], Step [35/172], Loss: 87.0518\n",
      "Epoch [7/300], Step [36/172], Loss: 56.9919\n",
      "Epoch [7/300], Step [37/172], Loss: 45.5387\n",
      "Epoch [7/300], Step [38/172], Loss: 56.7227\n",
      "Epoch [7/300], Step [39/172], Loss: 66.8935\n",
      "Epoch [7/300], Step [40/172], Loss: 55.6277\n",
      "Epoch [7/300], Step [41/172], Loss: 58.3877\n",
      "Epoch [7/300], Step [42/172], Loss: 57.5930\n",
      "Epoch [7/300], Step [43/172], Loss: 52.1169\n",
      "Epoch [7/300], Step [44/172], Loss: 61.6282\n",
      "Epoch [7/300], Step [45/172], Loss: 51.2977\n",
      "Epoch [7/300], Step [46/172], Loss: 76.4997\n",
      "Epoch [7/300], Step [47/172], Loss: 81.2317\n",
      "Epoch [7/300], Step [48/172], Loss: 102.2630\n",
      "Epoch [7/300], Step [49/172], Loss: 49.5715\n",
      "Epoch [7/300], Step [50/172], Loss: 68.7882\n",
      "Epoch [7/300], Step [51/172], Loss: 34.0005\n",
      "Epoch [7/300], Step [52/172], Loss: 42.2155\n",
      "Epoch [7/300], Step [53/172], Loss: 49.7880\n",
      "Epoch [7/300], Step [54/172], Loss: 45.7768\n",
      "Epoch [7/300], Step [55/172], Loss: 43.6565\n",
      "Epoch [7/300], Step [56/172], Loss: 36.8155\n",
      "Epoch [7/300], Step [57/172], Loss: 71.8679\n",
      "Epoch [7/300], Step [58/172], Loss: 42.3036\n",
      "Epoch [7/300], Step [59/172], Loss: 58.8951\n",
      "Epoch [7/300], Step [60/172], Loss: 94.0249\n",
      "Epoch [7/300], Step [61/172], Loss: 37.6770\n",
      "Epoch [7/300], Step [62/172], Loss: 46.4599\n",
      "Epoch [7/300], Step [63/172], Loss: 30.6014\n",
      "Epoch [7/300], Step [64/172], Loss: 26.4021\n",
      "Epoch [7/300], Step [65/172], Loss: 44.6621\n",
      "Epoch [7/300], Step [66/172], Loss: 27.5547\n",
      "Epoch [7/300], Step [67/172], Loss: 48.8381\n",
      "Epoch [7/300], Step [68/172], Loss: 48.6636\n",
      "Epoch [7/300], Step [69/172], Loss: 71.9511\n",
      "Epoch [7/300], Step [70/172], Loss: 64.2022\n",
      "Epoch [7/300], Step [71/172], Loss: 63.5811\n",
      "Epoch [7/300], Step [72/172], Loss: 61.3897\n",
      "Epoch [7/300], Step [73/172], Loss: 64.5384\n",
      "Epoch [7/300], Step [74/172], Loss: 59.6905\n",
      "Epoch [7/300], Step [75/172], Loss: 49.7859\n",
      "Epoch [7/300], Step [76/172], Loss: 58.2619\n",
      "Epoch [7/300], Step [77/172], Loss: 61.0406\n",
      "Epoch [7/300], Step [78/172], Loss: 62.0743\n",
      "Epoch [7/300], Step [79/172], Loss: 60.1803\n",
      "Epoch [7/300], Step [80/172], Loss: 60.1196\n",
      "Epoch [7/300], Step [81/172], Loss: 57.0041\n",
      "Epoch [7/300], Step [82/172], Loss: 56.7406\n",
      "Epoch [7/300], Step [83/172], Loss: 57.0503\n",
      "Epoch [7/300], Step [84/172], Loss: 57.8961\n",
      "Epoch [7/300], Step [85/172], Loss: 59.0466\n",
      "Epoch [7/300], Step [86/172], Loss: 51.0386\n",
      "Epoch [7/300], Step [87/172], Loss: 48.7698\n",
      "Epoch [7/300], Step [88/172], Loss: 50.6983\n",
      "Epoch [7/300], Step [89/172], Loss: 50.9007\n",
      "Epoch [7/300], Step [90/172], Loss: 46.8458\n",
      "Epoch [7/300], Step [91/172], Loss: 46.7088\n",
      "Epoch [7/300], Step [92/172], Loss: 44.0120\n",
      "Epoch [7/300], Step [93/172], Loss: 44.2769\n",
      "Epoch [7/300], Step [94/172], Loss: 45.1852\n",
      "Epoch [7/300], Step [95/172], Loss: 43.2881\n",
      "Epoch [7/300], Step [96/172], Loss: 42.9698\n",
      "Epoch [7/300], Step [97/172], Loss: 44.2007\n",
      "Epoch [7/300], Step [98/172], Loss: 42.5388\n",
      "Epoch [7/300], Step [99/172], Loss: 41.4327\n",
      "Epoch [7/300], Step [100/172], Loss: 41.0328\n",
      "Epoch [7/300], Step [101/172], Loss: 39.3739\n",
      "Epoch [7/300], Step [102/172], Loss: 40.0924\n",
      "Epoch [7/300], Step [103/172], Loss: 39.3958\n",
      "Epoch [7/300], Step [104/172], Loss: 37.7264\n",
      "Epoch [7/300], Step [105/172], Loss: 38.0966\n",
      "Epoch [7/300], Step [106/172], Loss: 36.6073\n",
      "Epoch [7/300], Step [107/172], Loss: 34.6067\n",
      "Epoch [7/300], Step [108/172], Loss: 37.1950\n",
      "Epoch [7/300], Step [109/172], Loss: 35.0285\n",
      "Epoch [7/300], Step [110/172], Loss: 35.4557\n",
      "Epoch [7/300], Step [111/172], Loss: 34.2195\n",
      "Epoch [7/300], Step [112/172], Loss: 36.2867\n",
      "Epoch [7/300], Step [113/172], Loss: 32.9946\n",
      "Epoch [7/300], Step [114/172], Loss: 33.2464\n",
      "Epoch [7/300], Step [115/172], Loss: 33.5677\n",
      "Epoch [7/300], Step [116/172], Loss: 32.3682\n",
      "Epoch [7/300], Step [117/172], Loss: 30.9617\n",
      "Epoch [7/300], Step [118/172], Loss: 31.7457\n",
      "Epoch [7/300], Step [119/172], Loss: 30.1010\n",
      "Epoch [7/300], Step [120/172], Loss: 30.2022\n",
      "Epoch [7/300], Step [121/172], Loss: 29.9378\n",
      "Epoch [7/300], Step [122/172], Loss: 27.6327\n",
      "Epoch [7/300], Step [123/172], Loss: 27.9418\n",
      "Epoch [7/300], Step [124/172], Loss: 27.9655\n",
      "Epoch [7/300], Step [125/172], Loss: 28.3551\n",
      "Epoch [7/300], Step [126/172], Loss: 28.1591\n",
      "Epoch [7/300], Step [127/172], Loss: 28.4767\n",
      "Epoch [7/300], Step [128/172], Loss: 28.2658\n",
      "Epoch [7/300], Step [129/172], Loss: 26.1346\n",
      "Epoch [7/300], Step [130/172], Loss: 27.0680\n",
      "Epoch [7/300], Step [131/172], Loss: 25.7730\n",
      "Epoch [7/300], Step [132/172], Loss: 25.5446\n",
      "Epoch [7/300], Step [133/172], Loss: 25.2585\n",
      "Epoch [7/300], Step [134/172], Loss: 25.1659\n",
      "Epoch [7/300], Step [135/172], Loss: 23.3504\n",
      "Epoch [7/300], Step [136/172], Loss: 23.3417\n",
      "Epoch [7/300], Step [137/172], Loss: 23.8343\n",
      "Epoch [7/300], Step [138/172], Loss: 22.8210\n",
      "Epoch [7/300], Step [139/172], Loss: 23.8068\n",
      "Epoch [7/300], Step [140/172], Loss: 23.0369\n",
      "Epoch [7/300], Step [141/172], Loss: 23.2637\n",
      "Epoch [7/300], Step [142/172], Loss: 23.1934\n",
      "Epoch [7/300], Step [143/172], Loss: 21.8080\n",
      "Epoch [7/300], Step [144/172], Loss: 21.1426\n",
      "Epoch [7/300], Step [145/172], Loss: 21.0597\n",
      "Epoch [7/300], Step [146/172], Loss: 20.5344\n",
      "Epoch [7/300], Step [147/172], Loss: 21.6894\n",
      "Epoch [7/300], Step [148/172], Loss: 20.7821\n",
      "Epoch [7/300], Step [149/172], Loss: 20.4850\n",
      "Epoch [7/300], Step [150/172], Loss: 20.8159\n",
      "Epoch [7/300], Step [151/172], Loss: 19.3148\n",
      "Epoch [7/300], Step [152/172], Loss: 19.0981\n",
      "Epoch [7/300], Step [153/172], Loss: 19.4510\n",
      "Epoch [7/300], Step [154/172], Loss: 19.1550\n",
      "Epoch [7/300], Step [155/172], Loss: 18.6895\n",
      "Epoch [7/300], Step [156/172], Loss: 18.6983\n",
      "Epoch [7/300], Step [157/172], Loss: 18.8098\n",
      "Epoch [7/300], Step [158/172], Loss: 18.4463\n",
      "Epoch [7/300], Step [159/172], Loss: 18.1392\n",
      "Epoch [7/300], Step [160/172], Loss: 18.2462\n",
      "Epoch [7/300], Step [161/172], Loss: 17.3695\n",
      "Epoch [7/300], Step [162/172], Loss: 17.2184\n",
      "Epoch [7/300], Step [163/172], Loss: 17.0606\n",
      "Epoch [7/300], Step [164/172], Loss: 17.2000\n",
      "Epoch [7/300], Step [165/172], Loss: 15.9992\n",
      "Epoch [7/300], Step [166/172], Loss: 17.0980\n",
      "Epoch [7/300], Step [167/172], Loss: 16.0629\n",
      "Epoch [7/300], Step [168/172], Loss: 16.8822\n",
      "Epoch [7/300], Step [169/172], Loss: 15.7314\n",
      "Epoch [7/300], Step [170/172], Loss: 15.6230\n",
      "Epoch [7/300], Step [171/172], Loss: 14.3046\n",
      "Epoch [7/300], Step [172/172], Loss: 10.6481\n",
      "Epoch [8/300], Step [1/172], Loss: 103.4478\n",
      "Epoch [8/300], Step [2/172], Loss: 103.5005\n",
      "Epoch [8/300], Step [3/172], Loss: 152.0015\n",
      "Epoch [8/300], Step [4/172], Loss: 110.3796\n",
      "Epoch [8/300], Step [5/172], Loss: 132.3810\n",
      "Epoch [8/300], Step [6/172], Loss: 78.1754\n",
      "Epoch [8/300], Step [7/172], Loss: 90.1160\n",
      "Epoch [8/300], Step [8/172], Loss: 76.0731\n",
      "Epoch [8/300], Step [9/172], Loss: 87.5407\n",
      "Epoch [8/300], Step [10/172], Loss: 97.0553\n",
      "Epoch [8/300], Step [11/172], Loss: 94.2741\n",
      "Epoch [8/300], Step [12/172], Loss: 89.1053\n",
      "Epoch [8/300], Step [13/172], Loss: 75.8675\n",
      "Epoch [8/300], Step [14/172], Loss: 104.6428\n",
      "Epoch [8/300], Step [15/172], Loss: 99.1917\n",
      "Epoch [8/300], Step [16/172], Loss: 79.3631\n",
      "Epoch [8/300], Step [17/172], Loss: 76.1029\n",
      "Epoch [8/300], Step [18/172], Loss: 89.1473\n",
      "Epoch [8/300], Step [19/172], Loss: 80.8942\n",
      "Epoch [8/300], Step [20/172], Loss: 155.4735\n",
      "Epoch [8/300], Step [21/172], Loss: 96.1066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Step [22/172], Loss: 98.5178\n",
      "Epoch [8/300], Step [23/172], Loss: 80.7213\n",
      "Epoch [8/300], Step [24/172], Loss: 90.4241\n",
      "Epoch [8/300], Step [25/172], Loss: 72.2148\n",
      "Epoch [8/300], Step [26/172], Loss: 76.3773\n",
      "Epoch [8/300], Step [27/172], Loss: 92.1521\n",
      "Epoch [8/300], Step [28/172], Loss: 76.5386\n",
      "Epoch [8/300], Step [29/172], Loss: 94.4872\n",
      "Epoch [8/300], Step [30/172], Loss: 84.3459\n",
      "Epoch [8/300], Step [31/172], Loss: 68.8409\n",
      "Epoch [8/300], Step [32/172], Loss: 63.5222\n",
      "Epoch [8/300], Step [33/172], Loss: 83.4797\n",
      "Epoch [8/300], Step [34/172], Loss: 62.2371\n",
      "Epoch [8/300], Step [35/172], Loss: 84.3178\n",
      "Epoch [8/300], Step [36/172], Loss: 52.9105\n",
      "Epoch [8/300], Step [37/172], Loss: 41.2095\n",
      "Epoch [8/300], Step [38/172], Loss: 54.3285\n",
      "Epoch [8/300], Step [39/172], Loss: 64.8935\n",
      "Epoch [8/300], Step [40/172], Loss: 53.4213\n",
      "Epoch [8/300], Step [41/172], Loss: 56.3951\n",
      "Epoch [8/300], Step [42/172], Loss: 55.8711\n",
      "Epoch [8/300], Step [43/172], Loss: 49.5629\n",
      "Epoch [8/300], Step [44/172], Loss: 59.9198\n",
      "Epoch [8/300], Step [45/172], Loss: 48.4707\n",
      "Epoch [8/300], Step [46/172], Loss: 73.5657\n",
      "Epoch [8/300], Step [47/172], Loss: 80.0600\n",
      "Epoch [8/300], Step [48/172], Loss: 100.2513\n",
      "Epoch [8/300], Step [49/172], Loss: 45.6291\n",
      "Epoch [8/300], Step [50/172], Loss: 66.7179\n",
      "Epoch [8/300], Step [51/172], Loss: 27.8191\n",
      "Epoch [8/300], Step [52/172], Loss: 37.7614\n",
      "Epoch [8/300], Step [53/172], Loss: 45.5649\n",
      "Epoch [8/300], Step [54/172], Loss: 40.7375\n",
      "Epoch [8/300], Step [55/172], Loss: 38.2587\n",
      "Epoch [8/300], Step [56/172], Loss: 29.1450\n",
      "Epoch [8/300], Step [57/172], Loss: 66.6164\n",
      "Epoch [8/300], Step [58/172], Loss: 35.3536\n",
      "Epoch [8/300], Step [59/172], Loss: 52.9582\n",
      "Epoch [8/300], Step [60/172], Loss: 89.6672\n",
      "Epoch [8/300], Step [61/172], Loss: 28.6098\n",
      "Epoch [8/300], Step [62/172], Loss: 37.9524\n",
      "Epoch [8/300], Step [63/172], Loss: 20.6153\n",
      "Epoch [8/300], Step [64/172], Loss: 16.4279\n",
      "Epoch [8/300], Step [65/172], Loss: 36.0340\n",
      "Epoch [8/300], Step [66/172], Loss: 17.6598\n",
      "Epoch [8/300], Step [67/172], Loss: 38.5079\n",
      "Epoch [8/300], Step [68/172], Loss: 37.3160\n",
      "Epoch [8/300], Step [69/172], Loss: 75.8962\n",
      "Epoch [8/300], Step [70/172], Loss: 86.9921\n",
      "Epoch [8/300], Step [71/172], Loss: 86.5060\n",
      "Epoch [8/300], Step [72/172], Loss: 83.6518\n",
      "Epoch [8/300], Step [73/172], Loss: 87.2445\n",
      "Epoch [8/300], Step [74/172], Loss: 78.3214\n",
      "Epoch [8/300], Step [75/172], Loss: 61.6263\n",
      "Epoch [8/300], Step [76/172], Loss: 72.9763\n",
      "Epoch [8/300], Step [77/172], Loss: 76.7630\n",
      "Epoch [8/300], Step [78/172], Loss: 76.5113\n",
      "Epoch [8/300], Step [79/172], Loss: 71.8009\n",
      "Epoch [8/300], Step [80/172], Loss: 69.8936\n",
      "Epoch [8/300], Step [81/172], Loss: 64.3749\n",
      "Epoch [8/300], Step [82/172], Loss: 62.5283\n",
      "Epoch [8/300], Step [83/172], Loss: 60.8261\n",
      "Epoch [8/300], Step [84/172], Loss: 60.4665\n",
      "Epoch [8/300], Step [85/172], Loss: 60.5703\n",
      "Epoch [8/300], Step [86/172], Loss: 49.6452\n",
      "Epoch [8/300], Step [87/172], Loss: 45.9596\n",
      "Epoch [8/300], Step [88/172], Loss: 47.2814\n",
      "Epoch [8/300], Step [89/172], Loss: 47.7423\n",
      "Epoch [8/300], Step [90/172], Loss: 43.0442\n",
      "Epoch [8/300], Step [91/172], Loss: 43.0832\n",
      "Epoch [8/300], Step [92/172], Loss: 39.9390\n",
      "Epoch [8/300], Step [93/172], Loss: 40.7556\n",
      "Epoch [8/300], Step [94/172], Loss: 41.7597\n",
      "Epoch [8/300], Step [95/172], Loss: 40.0126\n",
      "Epoch [8/300], Step [96/172], Loss: 39.3257\n",
      "Epoch [8/300], Step [97/172], Loss: 40.4303\n",
      "Epoch [8/300], Step [98/172], Loss: 38.9179\n",
      "Epoch [8/300], Step [99/172], Loss: 37.7268\n",
      "Epoch [8/300], Step [100/172], Loss: 37.8176\n",
      "Epoch [8/300], Step [101/172], Loss: 36.0967\n",
      "Epoch [8/300], Step [102/172], Loss: 36.7725\n",
      "Epoch [8/300], Step [103/172], Loss: 36.4368\n",
      "Epoch [8/300], Step [104/172], Loss: 35.0035\n",
      "Epoch [8/300], Step [105/172], Loss: 35.2085\n",
      "Epoch [8/300], Step [106/172], Loss: 34.1286\n",
      "Epoch [8/300], Step [107/172], Loss: 31.9043\n",
      "Epoch [8/300], Step [108/172], Loss: 34.7462\n",
      "Epoch [8/300], Step [109/172], Loss: 32.7401\n",
      "Epoch [8/300], Step [110/172], Loss: 33.2216\n",
      "Epoch [8/300], Step [111/172], Loss: 32.0960\n",
      "Epoch [8/300], Step [112/172], Loss: 34.7246\n",
      "Epoch [8/300], Step [113/172], Loss: 30.9532\n",
      "Epoch [8/300], Step [114/172], Loss: 31.3959\n",
      "Epoch [8/300], Step [115/172], Loss: 31.9524\n",
      "Epoch [8/300], Step [116/172], Loss: 30.7352\n",
      "Epoch [8/300], Step [117/172], Loss: 29.1585\n",
      "Epoch [8/300], Step [118/172], Loss: 30.0266\n",
      "Epoch [8/300], Step [119/172], Loss: 28.4280\n",
      "Epoch [8/300], Step [120/172], Loss: 28.4564\n",
      "Epoch [8/300], Step [121/172], Loss: 28.2947\n",
      "Epoch [8/300], Step [122/172], Loss: 25.9768\n",
      "Epoch [8/300], Step [123/172], Loss: 26.5937\n",
      "Epoch [8/300], Step [124/172], Loss: 26.4317\n",
      "Epoch [8/300], Step [125/172], Loss: 26.9456\n",
      "Epoch [8/300], Step [126/172], Loss: 26.9172\n",
      "Epoch [8/300], Step [127/172], Loss: 27.4504\n",
      "Epoch [8/300], Step [128/172], Loss: 27.3783\n",
      "Epoch [8/300], Step [129/172], Loss: 24.8403\n",
      "Epoch [8/300], Step [130/172], Loss: 25.8858\n",
      "Epoch [8/300], Step [131/172], Loss: 24.5829\n",
      "Epoch [8/300], Step [132/172], Loss: 24.4442\n",
      "Epoch [8/300], Step [133/172], Loss: 24.1153\n",
      "Epoch [8/300], Step [134/172], Loss: 24.1233\n",
      "Epoch [8/300], Step [135/172], Loss: 22.2506\n",
      "Epoch [8/300], Step [136/172], Loss: 22.5697\n",
      "Epoch [8/300], Step [137/172], Loss: 23.0050\n",
      "Epoch [8/300], Step [138/172], Loss: 22.0189\n",
      "Epoch [8/300], Step [139/172], Loss: 22.9859\n",
      "Epoch [8/300], Step [140/172], Loss: 22.2404\n",
      "Epoch [8/300], Step [141/172], Loss: 22.5660\n",
      "Epoch [8/300], Step [142/172], Loss: 22.5356\n",
      "Epoch [8/300], Step [143/172], Loss: 21.0486\n",
      "Epoch [8/300], Step [144/172], Loss: 20.3518\n",
      "Epoch [8/300], Step [145/172], Loss: 20.3133\n",
      "Epoch [8/300], Step [146/172], Loss: 19.8799\n",
      "Epoch [8/300], Step [147/172], Loss: 20.8591\n",
      "Epoch [8/300], Step [148/172], Loss: 20.0109\n",
      "Epoch [8/300], Step [149/172], Loss: 19.8158\n",
      "Epoch [8/300], Step [150/172], Loss: 20.2422\n",
      "Epoch [8/300], Step [151/172], Loss: 18.6256\n",
      "Epoch [8/300], Step [152/172], Loss: 18.4976\n",
      "Epoch [8/300], Step [153/172], Loss: 18.8488\n",
      "Epoch [8/300], Step [154/172], Loss: 18.6849\n",
      "Epoch [8/300], Step [155/172], Loss: 18.1258\n",
      "Epoch [8/300], Step [156/172], Loss: 18.1206\n",
      "Epoch [8/300], Step [157/172], Loss: 18.4089\n",
      "Epoch [8/300], Step [158/172], Loss: 17.9754\n",
      "Epoch [8/300], Step [159/172], Loss: 17.6433\n",
      "Epoch [8/300], Step [160/172], Loss: 17.7575\n",
      "Epoch [8/300], Step [161/172], Loss: 16.8768\n",
      "Epoch [8/300], Step [162/172], Loss: 16.7058\n",
      "Epoch [8/300], Step [163/172], Loss: 16.5655\n",
      "Epoch [8/300], Step [164/172], Loss: 16.7103\n",
      "Epoch [8/300], Step [165/172], Loss: 15.5222\n",
      "Epoch [8/300], Step [166/172], Loss: 16.6411\n",
      "Epoch [8/300], Step [167/172], Loss: 15.6927\n",
      "Epoch [8/300], Step [168/172], Loss: 16.4362\n",
      "Epoch [8/300], Step [169/172], Loss: 15.2506\n",
      "Epoch [8/300], Step [170/172], Loss: 15.1567\n",
      "Epoch [8/300], Step [171/172], Loss: 13.8124\n",
      "Epoch [8/300], Step [172/172], Loss: 10.3606\n",
      "Epoch [9/300], Step [1/172], Loss: 103.6791\n",
      "Epoch [9/300], Step [2/172], Loss: 104.0426\n",
      "Epoch [9/300], Step [3/172], Loss: 149.8754\n",
      "Epoch [9/300], Step [4/172], Loss: 108.5741\n",
      "Epoch [9/300], Step [5/172], Loss: 129.3676\n",
      "Epoch [9/300], Step [6/172], Loss: 76.4991\n",
      "Epoch [9/300], Step [7/172], Loss: 88.8593\n",
      "Epoch [9/300], Step [8/172], Loss: 74.4096\n",
      "Epoch [9/300], Step [9/172], Loss: 87.1914\n",
      "Epoch [9/300], Step [10/172], Loss: 95.5896\n",
      "Epoch [9/300], Step [11/172], Loss: 94.0614\n",
      "Epoch [9/300], Step [12/172], Loss: 89.9011\n",
      "Epoch [9/300], Step [13/172], Loss: 75.1694\n",
      "Epoch [9/300], Step [14/172], Loss: 105.1193\n",
      "Epoch [9/300], Step [15/172], Loss: 99.5821\n",
      "Epoch [9/300], Step [16/172], Loss: 79.6573\n",
      "Epoch [9/300], Step [17/172], Loss: 75.8462\n",
      "Epoch [9/300], Step [18/172], Loss: 90.0059\n",
      "Epoch [9/300], Step [19/172], Loss: 81.6442\n",
      "Epoch [9/300], Step [20/172], Loss: 152.6393\n",
      "Epoch [9/300], Step [21/172], Loss: 97.4006\n",
      "Epoch [9/300], Step [22/172], Loss: 99.3077\n",
      "Epoch [9/300], Step [23/172], Loss: 80.7786\n",
      "Epoch [9/300], Step [24/172], Loss: 91.3355\n",
      "Epoch [9/300], Step [25/172], Loss: 72.7675\n",
      "Epoch [9/300], Step [26/172], Loss: 76.9522\n",
      "Epoch [9/300], Step [27/172], Loss: 93.0551\n",
      "Epoch [9/300], Step [28/172], Loss: 78.7213\n",
      "Epoch [9/300], Step [29/172], Loss: 96.6352\n",
      "Epoch [9/300], Step [30/172], Loss: 85.7779\n",
      "Epoch [9/300], Step [31/172], Loss: 69.8645\n",
      "Epoch [9/300], Step [32/172], Loss: 64.2691\n",
      "Epoch [9/300], Step [33/172], Loss: 84.9150\n",
      "Epoch [9/300], Step [34/172], Loss: 62.7246\n",
      "Epoch [9/300], Step [35/172], Loss: 83.7410\n",
      "Epoch [9/300], Step [36/172], Loss: 52.7545\n",
      "Epoch [9/300], Step [37/172], Loss: 41.3312\n",
      "Epoch [9/300], Step [38/172], Loss: 55.9836\n",
      "Epoch [9/300], Step [39/172], Loss: 65.8034\n",
      "Epoch [9/300], Step [40/172], Loss: 54.7520\n",
      "Epoch [9/300], Step [41/172], Loss: 57.4922\n",
      "Epoch [9/300], Step [42/172], Loss: 57.3913\n",
      "Epoch [9/300], Step [43/172], Loss: 51.1116\n",
      "Epoch [9/300], Step [44/172], Loss: 61.0683\n",
      "Epoch [9/300], Step [45/172], Loss: 49.8758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/300], Step [46/172], Loss: 73.6208\n",
      "Epoch [9/300], Step [47/172], Loss: 81.2304\n",
      "Epoch [9/300], Step [48/172], Loss: 98.8628\n",
      "Epoch [9/300], Step [49/172], Loss: 47.7900\n",
      "Epoch [9/300], Step [50/172], Loss: 69.6973\n",
      "Epoch [9/300], Step [51/172], Loss: 30.1373\n",
      "Epoch [9/300], Step [52/172], Loss: 40.6326\n",
      "Epoch [9/300], Step [53/172], Loss: 49.1219\n",
      "Epoch [9/300], Step [54/172], Loss: 44.9489\n",
      "Epoch [9/300], Step [55/172], Loss: 42.4851\n",
      "Epoch [9/300], Step [56/172], Loss: 34.4635\n",
      "Epoch [9/300], Step [57/172], Loss: 70.4641\n",
      "Epoch [9/300], Step [58/172], Loss: 39.2397\n",
      "Epoch [9/300], Step [59/172], Loss: 58.2250\n",
      "Epoch [9/300], Step [60/172], Loss: 93.0947\n",
      "Epoch [9/300], Step [61/172], Loss: 34.8846\n",
      "Epoch [9/300], Step [62/172], Loss: 44.4679\n",
      "Epoch [9/300], Step [63/172], Loss: 28.0019\n",
      "Epoch [9/300], Step [64/172], Loss: 22.4972\n",
      "Epoch [9/300], Step [65/172], Loss: 42.0463\n",
      "Epoch [9/300], Step [66/172], Loss: 24.4349\n",
      "Epoch [9/300], Step [67/172], Loss: 47.4313\n",
      "Epoch [9/300], Step [68/172], Loss: 49.1272\n",
      "Epoch [9/300], Step [69/172], Loss: 74.1976\n",
      "Epoch [9/300], Step [70/172], Loss: 66.7398\n",
      "Epoch [9/300], Step [71/172], Loss: 65.8483\n",
      "Epoch [9/300], Step [72/172], Loss: 63.5202\n",
      "Epoch [9/300], Step [73/172], Loss: 66.7578\n",
      "Epoch [9/300], Step [74/172], Loss: 60.7342\n",
      "Epoch [9/300], Step [75/172], Loss: 47.8473\n",
      "Epoch [9/300], Step [76/172], Loss: 57.9954\n",
      "Epoch [9/300], Step [77/172], Loss: 61.1238\n",
      "Epoch [9/300], Step [78/172], Loss: 62.5699\n",
      "Epoch [9/300], Step [79/172], Loss: 59.2395\n",
      "Epoch [9/300], Step [80/172], Loss: 59.8232\n",
      "Epoch [9/300], Step [81/172], Loss: 55.5976\n",
      "Epoch [9/300], Step [82/172], Loss: 55.2180\n",
      "Epoch [9/300], Step [83/172], Loss: 55.9645\n",
      "Epoch [9/300], Step [84/172], Loss: 56.5531\n",
      "Epoch [9/300], Step [85/172], Loss: 58.4731\n",
      "Epoch [9/300], Step [86/172], Loss: 49.1727\n",
      "Epoch [9/300], Step [87/172], Loss: 46.1609\n",
      "Epoch [9/300], Step [88/172], Loss: 48.4951\n",
      "Epoch [9/300], Step [89/172], Loss: 49.3315\n",
      "Epoch [9/300], Step [90/172], Loss: 44.5609\n",
      "Epoch [9/300], Step [91/172], Loss: 44.6102\n",
      "Epoch [9/300], Step [92/172], Loss: 41.2987\n",
      "Epoch [9/300], Step [93/172], Loss: 42.2231\n",
      "Epoch [9/300], Step [94/172], Loss: 43.5531\n",
      "Epoch [9/300], Step [95/172], Loss: 41.4819\n",
      "Epoch [9/300], Step [96/172], Loss: 40.9361\n",
      "Epoch [9/300], Step [97/172], Loss: 42.5125\n",
      "Epoch [9/300], Step [98/172], Loss: 40.6498\n",
      "Epoch [9/300], Step [99/172], Loss: 39.4582\n",
      "Epoch [9/300], Step [100/172], Loss: 39.2784\n",
      "Epoch [9/300], Step [101/172], Loss: 37.5418\n",
      "Epoch [9/300], Step [102/172], Loss: 38.2681\n",
      "Epoch [9/300], Step [103/172], Loss: 37.8307\n",
      "Epoch [9/300], Step [104/172], Loss: 36.1568\n",
      "Epoch [9/300], Step [105/172], Loss: 36.4990\n",
      "Epoch [9/300], Step [106/172], Loss: 35.1077\n",
      "Epoch [9/300], Step [107/172], Loss: 32.6990\n",
      "Epoch [9/300], Step [108/172], Loss: 36.0124\n",
      "Epoch [9/300], Step [109/172], Loss: 33.9040\n",
      "Epoch [9/300], Step [110/172], Loss: 34.2297\n",
      "Epoch [9/300], Step [111/172], Loss: 32.8361\n",
      "Epoch [9/300], Step [112/172], Loss: 35.7661\n",
      "Epoch [9/300], Step [113/172], Loss: 31.6666\n",
      "Epoch [9/300], Step [114/172], Loss: 32.3176\n",
      "Epoch [9/300], Step [115/172], Loss: 33.2992\n",
      "Epoch [9/300], Step [116/172], Loss: 31.4770\n",
      "Epoch [9/300], Step [117/172], Loss: 29.7124\n",
      "Epoch [9/300], Step [118/172], Loss: 30.7888\n",
      "Epoch [9/300], Step [119/172], Loss: 28.9878\n",
      "Epoch [9/300], Step [120/172], Loss: 29.0527\n",
      "Epoch [9/300], Step [121/172], Loss: 29.1061\n",
      "Epoch [9/300], Step [122/172], Loss: 26.3688\n",
      "Epoch [9/300], Step [123/172], Loss: 26.9068\n",
      "Epoch [9/300], Step [124/172], Loss: 26.8429\n",
      "Epoch [9/300], Step [125/172], Loss: 27.6962\n",
      "Epoch [9/300], Step [126/172], Loss: 27.4716\n",
      "Epoch [9/300], Step [127/172], Loss: 28.0308\n",
      "Epoch [9/300], Step [128/172], Loss: 27.9811\n",
      "Epoch [9/300], Step [129/172], Loss: 25.2531\n",
      "Epoch [9/300], Step [130/172], Loss: 26.4102\n",
      "Epoch [9/300], Step [131/172], Loss: 24.9393\n",
      "Epoch [9/300], Step [132/172], Loss: 24.8341\n",
      "Epoch [9/300], Step [133/172], Loss: 24.5743\n",
      "Epoch [9/300], Step [134/172], Loss: 24.5621\n",
      "Epoch [9/300], Step [135/172], Loss: 22.5556\n",
      "Epoch [9/300], Step [136/172], Loss: 23.1317\n",
      "Epoch [9/300], Step [137/172], Loss: 23.4786\n",
      "Epoch [9/300], Step [138/172], Loss: 22.3941\n",
      "Epoch [9/300], Step [139/172], Loss: 23.3951\n",
      "Epoch [9/300], Step [140/172], Loss: 22.6929\n",
      "Epoch [9/300], Step [141/172], Loss: 23.2370\n",
      "Epoch [9/300], Step [142/172], Loss: 23.0243\n",
      "Epoch [9/300], Step [143/172], Loss: 21.4298\n",
      "Epoch [9/300], Step [144/172], Loss: 20.6832\n",
      "Epoch [9/300], Step [145/172], Loss: 20.7015\n",
      "Epoch [9/300], Step [146/172], Loss: 20.2670\n",
      "Epoch [9/300], Step [147/172], Loss: 21.3420\n",
      "Epoch [9/300], Step [148/172], Loss: 20.4713\n",
      "Epoch [9/300], Step [149/172], Loss: 20.2239\n",
      "Epoch [9/300], Step [150/172], Loss: 20.7040\n",
      "Epoch [9/300], Step [151/172], Loss: 18.9286\n",
      "Epoch [9/300], Step [152/172], Loss: 18.8416\n",
      "Epoch [9/300], Step [153/172], Loss: 19.2284\n",
      "Epoch [9/300], Step [154/172], Loss: 19.2430\n",
      "Epoch [9/300], Step [155/172], Loss: 18.4356\n",
      "Epoch [9/300], Step [156/172], Loss: 18.4733\n",
      "Epoch [9/300], Step [157/172], Loss: 18.8791\n",
      "Epoch [9/300], Step [158/172], Loss: 18.4052\n",
      "Epoch [9/300], Step [159/172], Loss: 18.0802\n",
      "Epoch [9/300], Step [160/172], Loss: 18.1858\n",
      "Epoch [9/300], Step [161/172], Loss: 17.3112\n",
      "Epoch [9/300], Step [162/172], Loss: 17.0675\n",
      "Epoch [9/300], Step [163/172], Loss: 16.9448\n",
      "Epoch [9/300], Step [164/172], Loss: 17.1481\n",
      "Epoch [9/300], Step [165/172], Loss: 15.8403\n",
      "Epoch [9/300], Step [166/172], Loss: 17.0275\n",
      "Epoch [9/300], Step [167/172], Loss: 16.2147\n",
      "Epoch [9/300], Step [168/172], Loss: 16.8591\n",
      "Epoch [9/300], Step [169/172], Loss: 15.5562\n",
      "Epoch [9/300], Step [170/172], Loss: 15.5264\n",
      "Epoch [9/300], Step [171/172], Loss: 14.1402\n",
      "Epoch [9/300], Step [172/172], Loss: 10.6364\n",
      "Epoch [10/300], Step [1/172], Loss: 100.1723\n",
      "Epoch [10/300], Step [2/172], Loss: 101.4470\n",
      "Epoch [10/300], Step [3/172], Loss: 144.9634\n",
      "Epoch [10/300], Step [4/172], Loss: 104.3701\n",
      "Epoch [10/300], Step [5/172], Loss: 124.5902\n",
      "Epoch [10/300], Step [6/172], Loss: 71.3873\n",
      "Epoch [10/300], Step [7/172], Loss: 85.2643\n",
      "Epoch [10/300], Step [8/172], Loss: 70.5959\n",
      "Epoch [10/300], Step [9/172], Loss: 84.2331\n",
      "Epoch [10/300], Step [10/172], Loss: 91.3623\n",
      "Epoch [10/300], Step [11/172], Loss: 91.1269\n",
      "Epoch [10/300], Step [12/172], Loss: 87.0250\n",
      "Epoch [10/300], Step [13/172], Loss: 71.8251\n",
      "Epoch [10/300], Step [14/172], Loss: 103.4048\n",
      "Epoch [10/300], Step [15/172], Loss: 97.4908\n",
      "Epoch [10/300], Step [16/172], Loss: 77.1755\n",
      "Epoch [10/300], Step [17/172], Loss: 72.0863\n",
      "Epoch [10/300], Step [18/172], Loss: 88.3848\n",
      "Epoch [10/300], Step [19/172], Loss: 79.5568\n",
      "Epoch [10/300], Step [20/172], Loss: 149.5194\n",
      "Epoch [10/300], Step [21/172], Loss: 96.6257\n",
      "Epoch [10/300], Step [22/172], Loss: 97.8916\n",
      "Epoch [10/300], Step [23/172], Loss: 78.9989\n",
      "Epoch [10/300], Step [24/172], Loss: 89.9879\n",
      "Epoch [10/300], Step [25/172], Loss: 70.3822\n",
      "Epoch [10/300], Step [26/172], Loss: 74.8157\n",
      "Epoch [10/300], Step [27/172], Loss: 92.2832\n",
      "Epoch [10/300], Step [28/172], Loss: 77.6430\n",
      "Epoch [10/300], Step [29/172], Loss: 96.2449\n",
      "Epoch [10/300], Step [30/172], Loss: 84.9616\n",
      "Epoch [10/300], Step [31/172], Loss: 67.6652\n",
      "Epoch [10/300], Step [32/172], Loss: 61.3593\n",
      "Epoch [10/300], Step [33/172], Loss: 84.4934\n",
      "Epoch [10/300], Step [34/172], Loss: 59.9918\n",
      "Epoch [10/300], Step [35/172], Loss: 81.2843\n",
      "Epoch [10/300], Step [36/172], Loss: 48.4458\n",
      "Epoch [10/300], Step [37/172], Loss: 36.6327\n",
      "Epoch [10/300], Step [38/172], Loss: 52.8454\n",
      "Epoch [10/300], Step [39/172], Loss: 63.6185\n",
      "Epoch [10/300], Step [40/172], Loss: 52.2981\n",
      "Epoch [10/300], Step [41/172], Loss: 55.2198\n",
      "Epoch [10/300], Step [42/172], Loss: 55.2756\n",
      "Epoch [10/300], Step [43/172], Loss: 48.4530\n",
      "Epoch [10/300], Step [44/172], Loss: 58.5232\n",
      "Epoch [10/300], Step [45/172], Loss: 46.5079\n",
      "Epoch [10/300], Step [46/172], Loss: 70.2867\n",
      "Epoch [10/300], Step [47/172], Loss: 79.6379\n",
      "Epoch [10/300], Step [48/172], Loss: 97.1155\n",
      "Epoch [10/300], Step [49/172], Loss: 43.4831\n",
      "Epoch [10/300], Step [50/172], Loss: 67.2192\n",
      "Epoch [10/300], Step [51/172], Loss: 24.4020\n",
      "Epoch [10/300], Step [52/172], Loss: 36.1659\n",
      "Epoch [10/300], Step [53/172], Loss: 44.6701\n",
      "Epoch [10/300], Step [54/172], Loss: 39.4578\n",
      "Epoch [10/300], Step [55/172], Loss: 36.9005\n",
      "Epoch [10/300], Step [56/172], Loss: 27.4077\n",
      "Epoch [10/300], Step [57/172], Loss: 64.3509\n",
      "Epoch [10/300], Step [58/172], Loss: 33.2565\n",
      "Epoch [10/300], Step [59/172], Loss: 52.4196\n",
      "Epoch [10/300], Step [60/172], Loss: 87.0762\n",
      "Epoch [10/300], Step [61/172], Loss: 26.8493\n",
      "Epoch [10/300], Step [62/172], Loss: 36.1444\n",
      "Epoch [10/300], Step [63/172], Loss: 19.6143\n",
      "Epoch [10/300], Step [64/172], Loss: 14.8387\n",
      "Epoch [10/300], Step [65/172], Loss: 34.6067\n",
      "Epoch [10/300], Step [66/172], Loss: 16.7642\n",
      "Epoch [10/300], Step [67/172], Loss: 37.7558\n",
      "Epoch [10/300], Step [68/172], Loss: 40.1896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Step [69/172], Loss: 76.4653\n",
      "Epoch [10/300], Step [70/172], Loss: 86.1936\n",
      "Epoch [10/300], Step [71/172], Loss: 85.6548\n",
      "Epoch [10/300], Step [72/172], Loss: 82.1338\n",
      "Epoch [10/300], Step [73/172], Loss: 86.0988\n",
      "Epoch [10/300], Step [74/172], Loss: 75.9572\n",
      "Epoch [10/300], Step [75/172], Loss: 56.9396\n",
      "Epoch [10/300], Step [76/172], Loss: 70.3991\n",
      "Epoch [10/300], Step [77/172], Loss: 74.7633\n",
      "Epoch [10/300], Step [78/172], Loss: 75.1978\n",
      "Epoch [10/300], Step [79/172], Loss: 69.5830\n",
      "Epoch [10/300], Step [80/172], Loss: 68.1976\n",
      "Epoch [10/300], Step [81/172], Loss: 62.2362\n",
      "Epoch [10/300], Step [82/172], Loss: 60.3623\n",
      "Epoch [10/300], Step [83/172], Loss: 59.4793\n",
      "Epoch [10/300], Step [84/172], Loss: 59.0000\n",
      "Epoch [10/300], Step [85/172], Loss: 60.1900\n",
      "Epoch [10/300], Step [86/172], Loss: 49.4807\n",
      "Epoch [10/300], Step [87/172], Loss: 45.1400\n",
      "Epoch [10/300], Step [88/172], Loss: 46.6190\n",
      "Epoch [10/300], Step [89/172], Loss: 47.7193\n",
      "Epoch [10/300], Step [90/172], Loss: 42.7013\n",
      "Epoch [10/300], Step [91/172], Loss: 42.7072\n",
      "Epoch [10/300], Step [92/172], Loss: 38.8022\n",
      "Epoch [10/300], Step [93/172], Loss: 40.0283\n",
      "Epoch [10/300], Step [94/172], Loss: 41.2279\n",
      "Epoch [10/300], Step [95/172], Loss: 39.2933\n",
      "Epoch [10/300], Step [96/172], Loss: 38.2297\n",
      "Epoch [10/300], Step [97/172], Loss: 39.6815\n",
      "Epoch [10/300], Step [98/172], Loss: 37.8572\n",
      "Epoch [10/300], Step [99/172], Loss: 36.5858\n",
      "Epoch [10/300], Step [100/172], Loss: 36.7985\n",
      "Epoch [10/300], Step [101/172], Loss: 35.0469\n",
      "Epoch [10/300], Step [102/172], Loss: 35.6456\n",
      "Epoch [10/300], Step [103/172], Loss: 35.5092\n",
      "Epoch [10/300], Step [104/172], Loss: 34.0868\n",
      "Epoch [10/300], Step [105/172], Loss: 34.2477\n",
      "Epoch [10/300], Step [106/172], Loss: 33.1921\n",
      "Epoch [10/300], Step [107/172], Loss: 30.5769\n",
      "Epoch [10/300], Step [108/172], Loss: 34.0726\n",
      "Epoch [10/300], Step [109/172], Loss: 32.1897\n",
      "Epoch [10/300], Step [110/172], Loss: 32.4449\n",
      "Epoch [10/300], Step [111/172], Loss: 31.1745\n",
      "Epoch [10/300], Step [112/172], Loss: 34.4964\n",
      "Epoch [10/300], Step [113/172], Loss: 30.0881\n",
      "Epoch [10/300], Step [114/172], Loss: 30.8544\n",
      "Epoch [10/300], Step [115/172], Loss: 32.1183\n",
      "Epoch [10/300], Step [116/172], Loss: 30.1814\n",
      "Epoch [10/300], Step [117/172], Loss: 28.2557\n",
      "Epoch [10/300], Step [118/172], Loss: 29.4114\n",
      "Epoch [10/300], Step [119/172], Loss: 27.6599\n",
      "Epoch [10/300], Step [120/172], Loss: 27.6138\n",
      "Epoch [10/300], Step [121/172], Loss: 27.7328\n",
      "Epoch [10/300], Step [122/172], Loss: 25.0478\n",
      "Epoch [10/300], Step [123/172], Loss: 25.8051\n",
      "Epoch [10/300], Step [124/172], Loss: 25.5511\n",
      "Epoch [10/300], Step [125/172], Loss: 26.5314\n",
      "Epoch [10/300], Step [126/172], Loss: 26.4247\n",
      "Epoch [10/300], Step [127/172], Loss: 27.1210\n",
      "Epoch [10/300], Step [128/172], Loss: 27.1357\n",
      "Epoch [10/300], Step [129/172], Loss: 24.1294\n",
      "Epoch [10/300], Step [130/172], Loss: 25.3647\n",
      "Epoch [10/300], Step [131/172], Loss: 23.8641\n",
      "Epoch [10/300], Step [132/172], Loss: 23.8217\n",
      "Epoch [10/300], Step [133/172], Loss: 23.5115\n",
      "Epoch [10/300], Step [134/172], Loss: 23.5379\n",
      "Epoch [10/300], Step [135/172], Loss: 21.5505\n",
      "Epoch [10/300], Step [136/172], Loss: 22.2604\n",
      "Epoch [10/300], Step [137/172], Loss: 22.6432\n",
      "Epoch [10/300], Step [138/172], Loss: 21.5539\n",
      "Epoch [10/300], Step [139/172], Loss: 22.5327\n",
      "Epoch [10/300], Step [140/172], Loss: 21.8826\n",
      "Epoch [10/300], Step [141/172], Loss: 22.5449\n",
      "Epoch [10/300], Step [142/172], Loss: 22.2393\n",
      "Epoch [10/300], Step [143/172], Loss: 20.6169\n",
      "Epoch [10/300], Step [144/172], Loss: 19.8137\n",
      "Epoch [10/300], Step [145/172], Loss: 19.8800\n",
      "Epoch [10/300], Step [146/172], Loss: 19.5013\n",
      "Epoch [10/300], Step [147/172], Loss: 20.4198\n",
      "Epoch [10/300], Step [148/172], Loss: 19.6211\n",
      "Epoch [10/300], Step [149/172], Loss: 19.4445\n",
      "Epoch [10/300], Step [150/172], Loss: 19.9551\n",
      "Epoch [10/300], Step [151/172], Loss: 18.1319\n",
      "Epoch [10/300], Step [152/172], Loss: 18.1108\n",
      "Epoch [10/300], Step [153/172], Loss: 18.4660\n",
      "Epoch [10/300], Step [154/172], Loss: 18.6220\n",
      "Epoch [10/300], Step [155/172], Loss: 17.7180\n",
      "Epoch [10/300], Step [156/172], Loss: 17.7537\n",
      "Epoch [10/300], Step [157/172], Loss: 18.2668\n",
      "Epoch [10/300], Step [158/172], Loss: 17.7315\n",
      "Epoch [10/300], Step [159/172], Loss: 17.3847\n",
      "Epoch [10/300], Step [160/172], Loss: 17.5103\n",
      "Epoch [10/300], Step [161/172], Loss: 16.6254\n",
      "Epoch [10/300], Step [162/172], Loss: 16.3931\n",
      "Epoch [10/300], Step [163/172], Loss: 16.2699\n",
      "Epoch [10/300], Step [164/172], Loss: 16.4712\n",
      "Epoch [10/300], Step [165/172], Loss: 15.1895\n",
      "Epoch [10/300], Step [166/172], Loss: 16.3751\n",
      "Epoch [10/300], Step [167/172], Loss: 15.6261\n",
      "Epoch [10/300], Step [168/172], Loss: 16.2169\n",
      "Epoch [10/300], Step [169/172], Loss: 14.9151\n",
      "Epoch [10/300], Step [170/172], Loss: 14.8927\n",
      "Epoch [10/300], Step [171/172], Loss: 13.4970\n",
      "Epoch [10/300], Step [172/172], Loss: 10.2149\n",
      "Epoch [11/300], Step [1/172], Loss: 101.1736\n",
      "Epoch [11/300], Step [2/172], Loss: 102.6901\n",
      "Epoch [11/300], Step [3/172], Loss: 144.7274\n",
      "Epoch [11/300], Step [4/172], Loss: 103.7714\n",
      "Epoch [11/300], Step [5/172], Loss: 124.1028\n",
      "Epoch [11/300], Step [6/172], Loss: 70.4068\n",
      "Epoch [11/300], Step [7/172], Loss: 84.8192\n",
      "Epoch [11/300], Step [8/172], Loss: 69.4823\n",
      "Epoch [11/300], Step [9/172], Loss: 84.3016\n",
      "Epoch [11/300], Step [10/172], Loss: 90.9332\n",
      "Epoch [11/300], Step [11/172], Loss: 91.5754\n",
      "Epoch [11/300], Step [12/172], Loss: 88.1119\n",
      "Epoch [11/300], Step [13/172], Loss: 71.4094\n",
      "Epoch [11/300], Step [14/172], Loss: 104.3236\n",
      "Epoch [11/300], Step [15/172], Loss: 98.0990\n",
      "Epoch [11/300], Step [16/172], Loss: 77.5964\n",
      "Epoch [11/300], Step [17/172], Loss: 71.9502\n",
      "Epoch [11/300], Step [18/172], Loss: 89.2378\n",
      "Epoch [11/300], Step [19/172], Loss: 80.2918\n",
      "Epoch [11/300], Step [20/172], Loss: 147.9391\n",
      "Epoch [11/300], Step [21/172], Loss: 97.6570\n",
      "Epoch [11/300], Step [22/172], Loss: 98.6128\n",
      "Epoch [11/300], Step [23/172], Loss: 78.8614\n",
      "Epoch [11/300], Step [24/172], Loss: 90.7996\n",
      "Epoch [11/300], Step [25/172], Loss: 70.6582\n",
      "Epoch [11/300], Step [26/172], Loss: 75.1274\n",
      "Epoch [11/300], Step [27/172], Loss: 92.8733\n",
      "Epoch [11/300], Step [28/172], Loss: 79.3647\n",
      "Epoch [11/300], Step [29/172], Loss: 97.9317\n",
      "Epoch [11/300], Step [30/172], Loss: 85.7222\n",
      "Epoch [11/300], Step [31/172], Loss: 67.9633\n",
      "Epoch [11/300], Step [32/172], Loss: 61.1212\n",
      "Epoch [11/300], Step [33/172], Loss: 85.1834\n",
      "Epoch [11/300], Step [34/172], Loss: 59.4181\n",
      "Epoch [11/300], Step [35/172], Loss: 80.6949\n",
      "Epoch [11/300], Step [36/172], Loss: 48.0038\n",
      "Epoch [11/300], Step [37/172], Loss: 36.5561\n",
      "Epoch [11/300], Step [38/172], Loss: 53.9243\n",
      "Epoch [11/300], Step [39/172], Loss: 63.9434\n",
      "Epoch [11/300], Step [40/172], Loss: 52.6991\n",
      "Epoch [11/300], Step [41/172], Loss: 55.5133\n",
      "Epoch [11/300], Step [42/172], Loss: 55.9182\n",
      "Epoch [11/300], Step [43/172], Loss: 49.3061\n",
      "Epoch [11/300], Step [44/172], Loss: 58.6267\n",
      "Epoch [11/300], Step [45/172], Loss: 47.1028\n",
      "Epoch [11/300], Step [46/172], Loss: 70.2426\n",
      "Epoch [11/300], Step [47/172], Loss: 80.6663\n",
      "Epoch [11/300], Step [48/172], Loss: 96.3097\n",
      "Epoch [11/300], Step [49/172], Loss: 44.6012\n",
      "Epoch [11/300], Step [50/172], Loss: 69.7429\n",
      "Epoch [11/300], Step [51/172], Loss: 25.7444\n",
      "Epoch [11/300], Step [52/172], Loss: 38.3030\n",
      "Epoch [11/300], Step [53/172], Loss: 47.1213\n",
      "Epoch [11/300], Step [54/172], Loss: 41.9898\n",
      "Epoch [11/300], Step [55/172], Loss: 39.4718\n",
      "Epoch [11/300], Step [56/172], Loss: 31.1536\n",
      "Epoch [11/300], Step [57/172], Loss: 68.1688\n",
      "Epoch [11/300], Step [58/172], Loss: 35.5382\n",
      "Epoch [11/300], Step [59/172], Loss: 56.2993\n",
      "Epoch [11/300], Step [60/172], Loss: 91.6205\n",
      "Epoch [11/300], Step [61/172], Loss: 31.2778\n",
      "Epoch [11/300], Step [62/172], Loss: 39.5267\n",
      "Epoch [11/300], Step [63/172], Loss: 25.4685\n",
      "Epoch [11/300], Step [64/172], Loss: 19.0772\n",
      "Epoch [11/300], Step [65/172], Loss: 39.7705\n",
      "Epoch [11/300], Step [66/172], Loss: 22.0644\n",
      "Epoch [11/300], Step [67/172], Loss: 45.9396\n",
      "Epoch [11/300], Step [68/172], Loss: 48.2052\n",
      "Epoch [11/300], Step [69/172], Loss: 76.2879\n",
      "Epoch [11/300], Step [70/172], Loss: 66.7883\n",
      "Epoch [11/300], Step [71/172], Loss: 65.5862\n",
      "Epoch [11/300], Step [72/172], Loss: 62.9494\n",
      "Epoch [11/300], Step [73/172], Loss: 66.2100\n",
      "Epoch [11/300], Step [74/172], Loss: 59.0630\n",
      "Epoch [11/300], Step [75/172], Loss: 43.6202\n",
      "Epoch [11/300], Step [76/172], Loss: 56.2846\n",
      "Epoch [11/300], Step [77/172], Loss: 60.1710\n",
      "Epoch [11/300], Step [78/172], Loss: 62.0474\n",
      "Epoch [11/300], Step [79/172], Loss: 57.6841\n",
      "Epoch [11/300], Step [80/172], Loss: 59.6104\n",
      "Epoch [11/300], Step [81/172], Loss: 54.6159\n",
      "Epoch [11/300], Step [82/172], Loss: 54.4674\n",
      "Epoch [11/300], Step [83/172], Loss: 55.4470\n",
      "Epoch [11/300], Step [84/172], Loss: 55.9578\n",
      "Epoch [11/300], Step [85/172], Loss: 59.2699\n",
      "Epoch [11/300], Step [86/172], Loss: 48.6280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/300], Step [87/172], Loss: 44.7016\n",
      "Epoch [11/300], Step [88/172], Loss: 47.4868\n",
      "Epoch [11/300], Step [89/172], Loss: 48.8386\n",
      "Epoch [11/300], Step [90/172], Loss: 43.3901\n",
      "Epoch [11/300], Step [91/172], Loss: 43.5272\n",
      "Epoch [11/300], Step [92/172], Loss: 39.5279\n",
      "Epoch [11/300], Step [93/172], Loss: 41.0029\n",
      "Epoch [11/300], Step [94/172], Loss: 42.6871\n",
      "Epoch [11/300], Step [95/172], Loss: 40.4355\n",
      "Epoch [11/300], Step [96/172], Loss: 39.5145\n",
      "Epoch [11/300], Step [97/172], Loss: 41.5108\n",
      "Epoch [11/300], Step [98/172], Loss: 39.3443\n",
      "Epoch [11/300], Step [99/172], Loss: 38.0017\n",
      "Epoch [11/300], Step [100/172], Loss: 37.9362\n",
      "Epoch [11/300], Step [101/172], Loss: 36.2340\n",
      "Epoch [11/300], Step [102/172], Loss: 36.7530\n",
      "Epoch [11/300], Step [103/172], Loss: 36.5870\n",
      "Epoch [11/300], Step [104/172], Loss: 34.9640\n",
      "Epoch [11/300], Step [105/172], Loss: 35.1470\n",
      "Epoch [11/300], Step [106/172], Loss: 33.8886\n",
      "Epoch [11/300], Step [107/172], Loss: 31.0615\n",
      "Epoch [11/300], Step [108/172], Loss: 34.9980\n",
      "Epoch [11/300], Step [109/172], Loss: 33.0462\n",
      "Epoch [11/300], Step [110/172], Loss: 33.1347\n",
      "Epoch [11/300], Step [111/172], Loss: 31.6250\n",
      "Epoch [11/300], Step [112/172], Loss: 35.2861\n",
      "Epoch [11/300], Step [113/172], Loss: 30.5252\n",
      "Epoch [11/300], Step [114/172], Loss: 31.4676\n",
      "Epoch [11/300], Step [115/172], Loss: 33.0742\n",
      "Epoch [11/300], Step [116/172], Loss: 30.6619\n",
      "Epoch [11/300], Step [117/172], Loss: 28.5493\n",
      "Epoch [11/300], Step [118/172], Loss: 29.8633\n",
      "Epoch [11/300], Step [119/172], Loss: 27.9337\n",
      "Epoch [11/300], Step [120/172], Loss: 27.9590\n",
      "Epoch [11/300], Step [121/172], Loss: 28.2668\n",
      "Epoch [11/300], Step [122/172], Loss: 25.1579\n",
      "Epoch [11/300], Step [123/172], Loss: 25.9256\n",
      "Epoch [11/300], Step [124/172], Loss: 25.6661\n",
      "Epoch [11/300], Step [125/172], Loss: 26.9741\n",
      "Epoch [11/300], Step [126/172], Loss: 26.7154\n",
      "Epoch [11/300], Step [127/172], Loss: 27.4727\n",
      "Epoch [11/300], Step [128/172], Loss: 27.5459\n",
      "Epoch [11/300], Step [129/172], Loss: 24.2878\n",
      "Epoch [11/300], Step [130/172], Loss: 25.5984\n",
      "Epoch [11/300], Step [131/172], Loss: 23.9660\n",
      "Epoch [11/300], Step [132/172], Loss: 23.9406\n",
      "Epoch [11/300], Step [133/172], Loss: 23.6476\n",
      "Epoch [11/300], Step [134/172], Loss: 23.6562\n",
      "Epoch [11/300], Step [135/172], Loss: 21.5342\n",
      "Epoch [11/300], Step [136/172], Loss: 22.4926\n",
      "Epoch [11/300], Step [137/172], Loss: 22.7557\n",
      "Epoch [11/300], Step [138/172], Loss: 21.5874\n",
      "Epoch [11/300], Step [139/172], Loss: 22.4913\n",
      "Epoch [11/300], Step [140/172], Loss: 21.8039\n",
      "Epoch [11/300], Step [141/172], Loss: 22.6433\n",
      "Epoch [11/300], Step [142/172], Loss: 22.1253\n",
      "Epoch [11/300], Step [143/172], Loss: 20.3563\n",
      "Epoch [11/300], Step [144/172], Loss: 19.4565\n",
      "Epoch [11/300], Step [145/172], Loss: 19.4993\n",
      "Epoch [11/300], Step [146/172], Loss: 19.1014\n",
      "Epoch [11/300], Step [147/172], Loss: 19.9309\n",
      "Epoch [11/300], Step [148/172], Loss: 19.0812\n",
      "Epoch [11/300], Step [149/172], Loss: 18.8532\n",
      "Epoch [11/300], Step [150/172], Loss: 19.3715\n",
      "Epoch [11/300], Step [151/172], Loss: 17.3913\n",
      "Epoch [11/300], Step [152/172], Loss: 17.3539\n",
      "Epoch [11/300], Step [153/172], Loss: 17.6902\n",
      "Epoch [11/300], Step [154/172], Loss: 17.8877\n",
      "Epoch [11/300], Step [155/172], Loss: 16.8071\n",
      "Epoch [11/300], Step [156/172], Loss: 16.7530\n",
      "Epoch [11/300], Step [157/172], Loss: 17.2861\n",
      "Epoch [11/300], Step [158/172], Loss: 16.7068\n",
      "Epoch [11/300], Step [159/172], Loss: 16.2044\n",
      "Epoch [11/300], Step [160/172], Loss: 16.2784\n",
      "Epoch [11/300], Step [161/172], Loss: 15.4481\n",
      "Epoch [11/300], Step [162/172], Loss: 15.1112\n",
      "Epoch [11/300], Step [163/172], Loss: 14.9370\n",
      "Epoch [11/300], Step [164/172], Loss: 15.0306\n",
      "Epoch [11/300], Step [165/172], Loss: 13.7189\n",
      "Epoch [11/300], Step [166/172], Loss: 14.8294\n",
      "Epoch [11/300], Step [167/172], Loss: 14.1378\n",
      "Epoch [11/300], Step [168/172], Loss: 14.5573\n",
      "Epoch [11/300], Step [169/172], Loss: 13.2753\n",
      "Epoch [11/300], Step [170/172], Loss: 13.1977\n",
      "Epoch [11/300], Step [171/172], Loss: 11.7672\n",
      "Epoch [11/300], Step [172/172], Loss: 8.9307\n",
      "Epoch [12/300], Step [1/172], Loss: 107.4941\n",
      "Epoch [12/300], Step [2/172], Loss: 110.1047\n",
      "Epoch [12/300], Step [3/172], Loss: 145.6680\n",
      "Epoch [12/300], Step [4/172], Loss: 106.6411\n",
      "Epoch [12/300], Step [5/172], Loss: 126.2771\n",
      "Epoch [12/300], Step [6/172], Loss: 72.7341\n",
      "Epoch [12/300], Step [7/172], Loss: 89.5328\n",
      "Epoch [12/300], Step [8/172], Loss: 72.8681\n",
      "Epoch [12/300], Step [9/172], Loss: 88.8741\n",
      "Epoch [12/300], Step [10/172], Loss: 92.6434\n",
      "Epoch [12/300], Step [11/172], Loss: 94.1653\n",
      "Epoch [12/300], Step [12/172], Loss: 91.3221\n",
      "Epoch [12/300], Step [13/172], Loss: 73.5682\n",
      "Epoch [12/300], Step [14/172], Loss: 107.8235\n",
      "Epoch [12/300], Step [15/172], Loss: 100.6071\n",
      "Epoch [12/300], Step [16/172], Loss: 80.1756\n",
      "Epoch [12/300], Step [17/172], Loss: 72.5871\n",
      "Epoch [12/300], Step [18/172], Loss: 91.6966\n",
      "Epoch [12/300], Step [19/172], Loss: 82.0605\n",
      "Epoch [12/300], Step [20/172], Loss: 146.0717\n",
      "Epoch [12/300], Step [21/172], Loss: 100.2682\n",
      "Epoch [12/300], Step [22/172], Loss: 99.7158\n",
      "Epoch [12/300], Step [23/172], Loss: 79.6656\n",
      "Epoch [12/300], Step [24/172], Loss: 91.9199\n",
      "Epoch [12/300], Step [25/172], Loss: 70.3910\n",
      "Epoch [12/300], Step [26/172], Loss: 74.7600\n",
      "Epoch [12/300], Step [27/172], Loss: 93.8212\n",
      "Epoch [12/300], Step [28/172], Loss: 80.6398\n",
      "Epoch [12/300], Step [29/172], Loss: 99.6647\n",
      "Epoch [12/300], Step [30/172], Loss: 86.0820\n",
      "Epoch [12/300], Step [31/172], Loss: 67.2291\n",
      "Epoch [12/300], Step [32/172], Loss: 59.3220\n",
      "Epoch [12/300], Step [33/172], Loss: 85.5194\n",
      "Epoch [12/300], Step [34/172], Loss: 57.2937\n",
      "Epoch [12/300], Step [35/172], Loss: 78.3984\n",
      "Epoch [12/300], Step [36/172], Loss: 44.7521\n",
      "Epoch [12/300], Step [37/172], Loss: 32.9588\n",
      "Epoch [12/300], Step [38/172], Loss: 50.7390\n",
      "Epoch [12/300], Step [39/172], Loss: 62.0508\n",
      "Epoch [12/300], Step [40/172], Loss: 50.5745\n",
      "Epoch [12/300], Step [41/172], Loss: 53.4369\n",
      "Epoch [12/300], Step [42/172], Loss: 53.9057\n",
      "Epoch [12/300], Step [43/172], Loss: 46.7195\n",
      "Epoch [12/300], Step [44/172], Loss: 55.8694\n",
      "Epoch [12/300], Step [45/172], Loss: 43.5041\n",
      "Epoch [12/300], Step [46/172], Loss: 66.6905\n",
      "Epoch [12/300], Step [47/172], Loss: 78.9119\n",
      "Epoch [12/300], Step [48/172], Loss: 94.8659\n",
      "Epoch [12/300], Step [49/172], Loss: 40.0245\n",
      "Epoch [12/300], Step [50/172], Loss: 67.4705\n",
      "Epoch [12/300], Step [51/172], Loss: 20.5467\n",
      "Epoch [12/300], Step [52/172], Loss: 34.5969\n",
      "Epoch [12/300], Step [53/172], Loss: 42.7449\n",
      "Epoch [12/300], Step [54/172], Loss: 35.8682\n",
      "Epoch [12/300], Step [55/172], Loss: 33.7364\n",
      "Epoch [12/300], Step [56/172], Loss: 24.1965\n",
      "Epoch [12/300], Step [57/172], Loss: 62.7587\n",
      "Epoch [12/300], Step [58/172], Loss: 30.2262\n",
      "Epoch [12/300], Step [59/172], Loss: 50.3671\n",
      "Epoch [12/300], Step [60/172], Loss: 86.2106\n",
      "Epoch [12/300], Step [61/172], Loss: 23.8829\n",
      "Epoch [12/300], Step [62/172], Loss: 31.9631\n",
      "Epoch [12/300], Step [63/172], Loss: 18.0214\n",
      "Epoch [12/300], Step [64/172], Loss: 12.3942\n",
      "Epoch [12/300], Step [65/172], Loss: 33.6871\n",
      "Epoch [12/300], Step [66/172], Loss: 16.0837\n",
      "Epoch [12/300], Step [67/172], Loss: 37.6167\n",
      "Epoch [12/300], Step [68/172], Loss: 39.9495\n",
      "Epoch [12/300], Step [69/172], Loss: 79.1851\n",
      "Epoch [12/300], Step [70/172], Loss: 84.4845\n",
      "Epoch [12/300], Step [71/172], Loss: 82.1656\n",
      "Epoch [12/300], Step [72/172], Loss: 78.1084\n",
      "Epoch [12/300], Step [73/172], Loss: 83.0256\n",
      "Epoch [12/300], Step [74/172], Loss: 71.2871\n",
      "Epoch [12/300], Step [75/172], Loss: 48.1600\n",
      "Epoch [12/300], Step [76/172], Loss: 65.8752\n",
      "Epoch [12/300], Step [77/172], Loss: 71.4165\n",
      "Epoch [12/300], Step [78/172], Loss: 73.1241\n",
      "Epoch [12/300], Step [79/172], Loss: 65.9013\n",
      "Epoch [12/300], Step [80/172], Loss: 66.2315\n",
      "Epoch [12/300], Step [81/172], Loss: 59.3862\n",
      "Epoch [12/300], Step [82/172], Loss: 58.1689\n",
      "Epoch [12/300], Step [83/172], Loss: 58.5029\n",
      "Epoch [12/300], Step [84/172], Loss: 58.1108\n",
      "Epoch [12/300], Step [85/172], Loss: 61.7351\n",
      "Epoch [12/300], Step [86/172], Loss: 48.7508\n",
      "Epoch [12/300], Step [87/172], Loss: 43.3864\n",
      "Epoch [12/300], Step [88/172], Loss: 45.8492\n",
      "Epoch [12/300], Step [89/172], Loss: 47.4828\n",
      "Epoch [12/300], Step [90/172], Loss: 41.7581\n",
      "Epoch [12/300], Step [91/172], Loss: 42.0614\n",
      "Epoch [12/300], Step [92/172], Loss: 37.5112\n",
      "Epoch [12/300], Step [93/172], Loss: 39.4226\n",
      "Epoch [12/300], Step [94/172], Loss: 41.0257\n",
      "Epoch [12/300], Step [95/172], Loss: 38.7967\n",
      "Epoch [12/300], Step [96/172], Loss: 37.4168\n",
      "Epoch [12/300], Step [97/172], Loss: 39.3243\n",
      "Epoch [12/300], Step [98/172], Loss: 37.1609\n",
      "Epoch [12/300], Step [99/172], Loss: 35.6538\n",
      "Epoch [12/300], Step [100/172], Loss: 35.9447\n",
      "Epoch [12/300], Step [101/172], Loss: 34.1154\n",
      "Epoch [12/300], Step [102/172], Loss: 34.5283\n",
      "Epoch [12/300], Step [103/172], Loss: 34.6118\n",
      "Epoch [12/300], Step [104/172], Loss: 33.2105\n",
      "Epoch [12/300], Step [105/172], Loss: 33.1893\n",
      "Epoch [12/300], Step [106/172], Loss: 32.2953\n",
      "Epoch [12/300], Step [107/172], Loss: 29.3266\n",
      "Epoch [12/300], Step [108/172], Loss: 33.2914\n",
      "Epoch [12/300], Step [109/172], Loss: 31.6280\n",
      "Epoch [12/300], Step [110/172], Loss: 31.6398\n",
      "Epoch [12/300], Step [111/172], Loss: 30.2695\n",
      "Epoch [12/300], Step [112/172], Loss: 34.2376\n",
      "Epoch [12/300], Step [113/172], Loss: 29.2494\n",
      "Epoch [12/300], Step [114/172], Loss: 30.2114\n",
      "Epoch [12/300], Step [115/172], Loss: 32.1341\n",
      "Epoch [12/300], Step [116/172], Loss: 29.6584\n",
      "Epoch [12/300], Step [117/172], Loss: 27.3878\n",
      "Epoch [12/300], Step [118/172], Loss: 28.7731\n",
      "Epoch [12/300], Step [119/172], Loss: 26.8740\n",
      "Epoch [12/300], Step [120/172], Loss: 26.7849\n",
      "Epoch [12/300], Step [121/172], Loss: 27.1388\n",
      "Epoch [12/300], Step [122/172], Loss: 24.1359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/300], Step [123/172], Loss: 25.0419\n",
      "Epoch [12/300], Step [124/172], Loss: 24.6875\n",
      "Epoch [12/300], Step [125/172], Loss: 26.0809\n",
      "Epoch [12/300], Step [126/172], Loss: 25.9565\n",
      "Epoch [12/300], Step [127/172], Loss: 26.8343\n",
      "Epoch [12/300], Step [128/172], Loss: 27.0062\n",
      "Epoch [12/300], Step [129/172], Loss: 23.4728\n",
      "Epoch [12/300], Step [130/172], Loss: 24.8421\n",
      "Epoch [12/300], Step [131/172], Loss: 23.2252\n",
      "Epoch [12/300], Step [132/172], Loss: 23.2567\n",
      "Epoch [12/300], Step [133/172], Loss: 22.9078\n",
      "Epoch [12/300], Step [134/172], Loss: 22.9776\n",
      "Epoch [12/300], Step [135/172], Loss: 20.9187\n",
      "Epoch [12/300], Step [136/172], Loss: 21.9443\n",
      "Epoch [12/300], Step [137/172], Loss: 22.3259\n",
      "Epoch [12/300], Step [138/172], Loss: 21.1539\n",
      "Epoch [12/300], Step [139/172], Loss: 22.1459\n",
      "Epoch [12/300], Step [140/172], Loss: 21.5662\n",
      "Epoch [12/300], Step [141/172], Loss: 22.5078\n",
      "Epoch [12/300], Step [142/172], Loss: 22.0111\n",
      "Epoch [12/300], Step [143/172], Loss: 20.2534\n",
      "Epoch [12/300], Step [144/172], Loss: 19.3193\n",
      "Epoch [12/300], Step [145/172], Loss: 19.4812\n",
      "Epoch [12/300], Step [146/172], Loss: 19.1894\n",
      "Epoch [12/300], Step [147/172], Loss: 19.8604\n",
      "Epoch [12/300], Step [148/172], Loss: 19.1866\n",
      "Epoch [12/300], Step [149/172], Loss: 19.1149\n",
      "Epoch [12/300], Step [150/172], Loss: 19.6646\n",
      "Epoch [12/300], Step [151/172], Loss: 17.6696\n",
      "Epoch [12/300], Step [152/172], Loss: 17.7817\n",
      "Epoch [12/300], Step [153/172], Loss: 18.1017\n",
      "Epoch [12/300], Step [154/172], Loss: 18.5399\n",
      "Epoch [12/300], Step [155/172], Loss: 17.3212\n",
      "Epoch [12/300], Step [156/172], Loss: 17.3794\n",
      "Epoch [12/300], Step [157/172], Loss: 18.1248\n",
      "Epoch [12/300], Step [158/172], Loss: 17.4959\n",
      "Epoch [12/300], Step [159/172], Loss: 17.0451\n",
      "Epoch [12/300], Step [160/172], Loss: 17.1767\n",
      "Epoch [12/300], Step [161/172], Loss: 16.3325\n",
      "Epoch [12/300], Step [162/172], Loss: 16.0074\n",
      "Epoch [12/300], Step [163/172], Loss: 15.9022\n",
      "Epoch [12/300], Step [164/172], Loss: 16.1677\n",
      "Epoch [12/300], Step [165/172], Loss: 14.8217\n",
      "Epoch [12/300], Step [166/172], Loss: 15.9957\n",
      "Epoch [12/300], Step [167/172], Loss: 15.4418\n",
      "Epoch [12/300], Step [168/172], Loss: 15.8619\n",
      "Epoch [12/300], Step [169/172], Loss: 14.4607\n",
      "Epoch [12/300], Step [170/172], Loss: 14.5183\n",
      "Epoch [12/300], Step [171/172], Loss: 13.0614\n",
      "Epoch [12/300], Step [172/172], Loss: 9.9884\n",
      "Epoch [13/300], Step [1/172], Loss: 99.5048\n",
      "Epoch [13/300], Step [2/172], Loss: 102.1155\n",
      "Epoch [13/300], Step [3/172], Loss: 139.8311\n",
      "Epoch [13/300], Step [4/172], Loss: 99.0959\n",
      "Epoch [13/300], Step [5/172], Loss: 119.4152\n",
      "Epoch [13/300], Step [6/172], Loss: 66.0685\n",
      "Epoch [13/300], Step [7/172], Loss: 81.9007\n",
      "Epoch [13/300], Step [8/172], Loss: 64.9140\n",
      "Epoch [13/300], Step [9/172], Loss: 81.8072\n",
      "Epoch [13/300], Step [10/172], Loss: 86.5586\n",
      "Epoch [13/300], Step [11/172], Loss: 90.2546\n",
      "Epoch [13/300], Step [12/172], Loss: 87.4975\n",
      "Epoch [13/300], Step [13/172], Loss: 68.6450\n",
      "Epoch [13/300], Step [14/172], Loss: 103.5194\n",
      "Epoch [13/300], Step [15/172], Loss: 96.5671\n",
      "Epoch [13/300], Step [16/172], Loss: 76.3206\n",
      "Epoch [13/300], Step [17/172], Loss: 69.1600\n",
      "Epoch [13/300], Step [18/172], Loss: 88.9668\n",
      "Epoch [13/300], Step [19/172], Loss: 79.3794\n",
      "Epoch [13/300], Step [20/172], Loss: 143.6746\n",
      "Epoch [13/300], Step [21/172], Loss: 97.7009\n",
      "Epoch [13/300], Step [22/172], Loss: 97.6562\n",
      "Epoch [13/300], Step [23/172], Loss: 76.1066\n",
      "Epoch [13/300], Step [24/172], Loss: 90.2550\n",
      "Epoch [13/300], Step [25/172], Loss: 68.4831\n",
      "Epoch [13/300], Step [26/172], Loss: 73.5305\n",
      "Epoch [13/300], Step [27/172], Loss: 92.4647\n",
      "Epoch [13/300], Step [28/172], Loss: 80.3276\n",
      "Epoch [13/300], Step [29/172], Loss: 98.8210\n",
      "Epoch [13/300], Step [30/172], Loss: 85.5204\n",
      "Epoch [13/300], Step [31/172], Loss: 66.1901\n",
      "Epoch [13/300], Step [32/172], Loss: 58.4742\n",
      "Epoch [13/300], Step [33/172], Loss: 85.1048\n",
      "Epoch [13/300], Step [34/172], Loss: 55.8815\n",
      "Epoch [13/300], Step [35/172], Loss: 79.4048\n",
      "Epoch [13/300], Step [36/172], Loss: 45.2735\n",
      "Epoch [13/300], Step [37/172], Loss: 33.8148\n",
      "Epoch [13/300], Step [38/172], Loss: 52.8917\n",
      "Epoch [13/300], Step [39/172], Loss: 62.7992\n",
      "Epoch [13/300], Step [40/172], Loss: 51.1831\n",
      "Epoch [13/300], Step [41/172], Loss: 54.0380\n",
      "Epoch [13/300], Step [42/172], Loss: 54.4164\n",
      "Epoch [13/300], Step [43/172], Loss: 48.2056\n",
      "Epoch [13/300], Step [44/172], Loss: 55.8519\n",
      "Epoch [13/300], Step [45/172], Loss: 44.6748\n",
      "Epoch [13/300], Step [46/172], Loss: 67.7553\n",
      "Epoch [13/300], Step [47/172], Loss: 80.4059\n",
      "Epoch [13/300], Step [48/172], Loss: 94.5568\n",
      "Epoch [13/300], Step [49/172], Loss: 42.2264\n",
      "Epoch [13/300], Step [50/172], Loss: 70.2211\n",
      "Epoch [13/300], Step [51/172], Loss: 22.9416\n",
      "Epoch [13/300], Step [52/172], Loss: 37.0201\n",
      "Epoch [13/300], Step [53/172], Loss: 46.3492\n",
      "Epoch [13/300], Step [54/172], Loss: 40.2915\n",
      "Epoch [13/300], Step [55/172], Loss: 37.2274\n",
      "Epoch [13/300], Step [56/172], Loss: 29.1257\n",
      "Epoch [13/300], Step [57/172], Loss: 67.2947\n",
      "Epoch [13/300], Step [58/172], Loss: 33.5501\n",
      "Epoch [13/300], Step [59/172], Loss: 55.0378\n",
      "Epoch [13/300], Step [60/172], Loss: 90.3753\n",
      "Epoch [13/300], Step [61/172], Loss: 28.6932\n",
      "Epoch [13/300], Step [62/172], Loss: 35.9633\n",
      "Epoch [13/300], Step [63/172], Loss: 23.8642\n",
      "Epoch [13/300], Step [64/172], Loss: 16.9362\n",
      "Epoch [13/300], Step [65/172], Loss: 38.7517\n",
      "Epoch [13/300], Step [66/172], Loss: 20.9880\n",
      "Epoch [13/300], Step [67/172], Loss: 45.4392\n",
      "Epoch [13/300], Step [68/172], Loss: 48.2988\n",
      "Epoch [13/300], Step [69/172], Loss: 78.2120\n",
      "Epoch [13/300], Step [70/172], Loss: 65.6346\n",
      "Epoch [13/300], Step [71/172], Loss: 64.3802\n",
      "Epoch [13/300], Step [72/172], Loss: 61.9733\n",
      "Epoch [13/300], Step [73/172], Loss: 64.5612\n",
      "Epoch [13/300], Step [74/172], Loss: 57.0190\n",
      "Epoch [13/300], Step [75/172], Loss: 40.5785\n",
      "Epoch [13/300], Step [76/172], Loss: 54.1026\n",
      "Epoch [13/300], Step [77/172], Loss: 58.5221\n",
      "Epoch [13/300], Step [78/172], Loss: 60.7997\n",
      "Epoch [13/300], Step [79/172], Loss: 55.5021\n",
      "Epoch [13/300], Step [80/172], Loss: 58.3242\n",
      "Epoch [13/300], Step [81/172], Loss: 52.9757\n",
      "Epoch [13/300], Step [82/172], Loss: 52.6500\n",
      "Epoch [13/300], Step [83/172], Loss: 54.4228\n",
      "Epoch [13/300], Step [84/172], Loss: 54.2996\n",
      "Epoch [13/300], Step [85/172], Loss: 59.0732\n",
      "Epoch [13/300], Step [86/172], Loss: 47.6388\n",
      "Epoch [13/300], Step [87/172], Loss: 43.0067\n",
      "Epoch [13/300], Step [88/172], Loss: 46.0842\n",
      "Epoch [13/300], Step [89/172], Loss: 47.8683\n",
      "Epoch [13/300], Step [90/172], Loss: 42.2226\n",
      "Epoch [13/300], Step [91/172], Loss: 42.5504\n",
      "Epoch [13/300], Step [92/172], Loss: 37.9966\n",
      "Epoch [13/300], Step [93/172], Loss: 40.0256\n",
      "Epoch [13/300], Step [94/172], Loss: 41.8942\n",
      "Epoch [13/300], Step [95/172], Loss: 39.6710\n",
      "Epoch [13/300], Step [96/172], Loss: 38.3570\n",
      "Epoch [13/300], Step [97/172], Loss: 40.7260\n",
      "Epoch [13/300], Step [98/172], Loss: 38.3376\n",
      "Epoch [13/300], Step [99/172], Loss: 36.8778\n",
      "Epoch [13/300], Step [100/172], Loss: 36.8972\n",
      "Epoch [13/300], Step [101/172], Loss: 35.3076\n",
      "Epoch [13/300], Step [102/172], Loss: 35.5767\n",
      "Epoch [13/300], Step [103/172], Loss: 35.6748\n",
      "Epoch [13/300], Step [104/172], Loss: 34.1365\n",
      "Epoch [13/300], Step [105/172], Loss: 34.1501\n",
      "Epoch [13/300], Step [106/172], Loss: 33.0575\n",
      "Epoch [13/300], Step [107/172], Loss: 29.9176\n",
      "Epoch [13/300], Step [108/172], Loss: 34.3311\n",
      "Epoch [13/300], Step [109/172], Loss: 32.5301\n",
      "Epoch [13/300], Step [110/172], Loss: 32.4274\n",
      "Epoch [13/300], Step [111/172], Loss: 30.8200\n",
      "Epoch [13/300], Step [112/172], Loss: 34.9973\n",
      "Epoch [13/300], Step [113/172], Loss: 29.7847\n",
      "Epoch [13/300], Step [114/172], Loss: 30.9250\n",
      "Epoch [13/300], Step [115/172], Loss: 33.2358\n",
      "Epoch [13/300], Step [116/172], Loss: 30.2200\n",
      "Epoch [13/300], Step [117/172], Loss: 27.8051\n",
      "Epoch [13/300], Step [118/172], Loss: 29.3160\n",
      "Epoch [13/300], Step [119/172], Loss: 27.2606\n",
      "Epoch [13/300], Step [120/172], Loss: 27.2004\n",
      "Epoch [13/300], Step [121/172], Loss: 27.7485\n",
      "Epoch [13/300], Step [122/172], Loss: 24.3544\n",
      "Epoch [13/300], Step [123/172], Loss: 25.2657\n",
      "Epoch [13/300], Step [124/172], Loss: 24.8604\n",
      "Epoch [13/300], Step [125/172], Loss: 26.5588\n",
      "Epoch [13/300], Step [126/172], Loss: 26.2798\n",
      "Epoch [13/300], Step [127/172], Loss: 27.1948\n",
      "Epoch [13/300], Step [128/172], Loss: 27.4070\n",
      "Epoch [13/300], Step [129/172], Loss: 23.7024\n",
      "Epoch [13/300], Step [130/172], Loss: 25.1625\n",
      "Epoch [13/300], Step [131/172], Loss: 23.4026\n",
      "Epoch [13/300], Step [132/172], Loss: 23.4630\n",
      "Epoch [13/300], Step [133/172], Loss: 23.1666\n",
      "Epoch [13/300], Step [134/172], Loss: 23.2193\n",
      "Epoch [13/300], Step [135/172], Loss: 21.0732\n",
      "Epoch [13/300], Step [136/172], Loss: 22.2724\n",
      "Epoch [13/300], Step [137/172], Loss: 22.5997\n",
      "Epoch [13/300], Step [138/172], Loss: 21.3687\n",
      "Epoch [13/300], Step [139/172], Loss: 22.3763\n",
      "Epoch [13/300], Step [140/172], Loss: 21.8147\n",
      "Epoch [13/300], Step [141/172], Loss: 22.9320\n",
      "Epoch [13/300], Step [142/172], Loss: 22.3248\n",
      "Epoch [13/300], Step [143/172], Loss: 20.4374\n",
      "Epoch [13/300], Step [144/172], Loss: 19.5084\n",
      "Epoch [13/300], Step [145/172], Loss: 19.6867\n",
      "Epoch [13/300], Step [146/172], Loss: 19.4243\n",
      "Epoch [13/300], Step [147/172], Loss: 20.1003\n",
      "Epoch [13/300], Step [148/172], Loss: 19.4698\n",
      "Epoch [13/300], Step [149/172], Loss: 19.3583\n",
      "Epoch [13/300], Step [150/172], Loss: 19.9556\n",
      "Epoch [13/300], Step [151/172], Loss: 17.8321\n",
      "Epoch [13/300], Step [152/172], Loss: 17.9732\n",
      "Epoch [13/300], Step [153/172], Loss: 18.3520\n",
      "Epoch [13/300], Step [154/172], Loss: 18.8780\n",
      "Epoch [13/300], Step [155/172], Loss: 17.5200\n",
      "Epoch [13/300], Step [156/172], Loss: 17.6381\n",
      "Epoch [13/300], Step [157/172], Loss: 18.4551\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/300], Step [158/172], Loss: 17.8174\n",
      "Epoch [13/300], Step [159/172], Loss: 17.3863\n",
      "Epoch [13/300], Step [160/172], Loss: 17.4921\n",
      "Epoch [13/300], Step [161/172], Loss: 16.6816\n",
      "Epoch [13/300], Step [162/172], Loss: 16.2776\n",
      "Epoch [13/300], Step [163/172], Loss: 16.2015\n",
      "Epoch [13/300], Step [164/172], Loss: 16.5431\n",
      "Epoch [13/300], Step [165/172], Loss: 15.0884\n",
      "Epoch [13/300], Step [166/172], Loss: 16.3192\n",
      "Epoch [13/300], Step [167/172], Loss: 15.8805\n",
      "Epoch [13/300], Step [168/172], Loss: 16.2427\n",
      "Epoch [13/300], Step [169/172], Loss: 14.7211\n",
      "Epoch [13/300], Step [170/172], Loss: 14.8562\n",
      "Epoch [13/300], Step [171/172], Loss: 13.3303\n",
      "Epoch [13/300], Step [172/172], Loss: 10.2456\n",
      "Epoch [14/300], Step [1/172], Loss: 96.9180\n",
      "Epoch [14/300], Step [2/172], Loss: 99.7097\n",
      "Epoch [14/300], Step [3/172], Loss: 136.3992\n",
      "Epoch [14/300], Step [4/172], Loss: 95.4869\n",
      "Epoch [14/300], Step [5/172], Loss: 115.1875\n",
      "Epoch [14/300], Step [6/172], Loss: 62.7454\n",
      "Epoch [14/300], Step [7/172], Loss: 79.1397\n",
      "Epoch [14/300], Step [8/172], Loss: 61.7748\n",
      "Epoch [14/300], Step [9/172], Loss: 79.0926\n",
      "Epoch [14/300], Step [10/172], Loss: 83.6372\n",
      "Epoch [14/300], Step [11/172], Loss: 88.4393\n",
      "Epoch [14/300], Step [12/172], Loss: 85.3670\n",
      "Epoch [14/300], Step [13/172], Loss: 66.2148\n",
      "Epoch [14/300], Step [14/172], Loss: 102.0465\n",
      "Epoch [14/300], Step [15/172], Loss: 94.8419\n",
      "Epoch [14/300], Step [16/172], Loss: 74.8650\n",
      "Epoch [14/300], Step [17/172], Loss: 67.1225\n",
      "Epoch [14/300], Step [18/172], Loss: 87.9211\n",
      "Epoch [14/300], Step [19/172], Loss: 78.1160\n",
      "Epoch [14/300], Step [20/172], Loss: 141.0684\n",
      "Epoch [14/300], Step [21/172], Loss: 97.2096\n",
      "Epoch [14/300], Step [22/172], Loss: 96.7600\n",
      "Epoch [14/300], Step [23/172], Loss: 74.3112\n",
      "Epoch [14/300], Step [24/172], Loss: 89.7319\n",
      "Epoch [14/300], Step [25/172], Loss: 67.0197\n",
      "Epoch [14/300], Step [26/172], Loss: 72.0679\n",
      "Epoch [14/300], Step [27/172], Loss: 91.9647\n",
      "Epoch [14/300], Step [28/172], Loss: 80.3382\n",
      "Epoch [14/300], Step [29/172], Loss: 99.1395\n",
      "Epoch [14/300], Step [30/172], Loss: 84.9386\n",
      "Epoch [14/300], Step [31/172], Loss: 64.7943\n",
      "Epoch [14/300], Step [32/172], Loss: 56.5682\n",
      "Epoch [14/300], Step [33/172], Loss: 84.8475\n",
      "Epoch [14/300], Step [34/172], Loss: 53.6031\n",
      "Epoch [14/300], Step [35/172], Loss: 78.0414\n",
      "Epoch [14/300], Step [36/172], Loss: 43.5939\n",
      "Epoch [14/300], Step [37/172], Loss: 31.9358\n",
      "Epoch [14/300], Step [38/172], Loss: 51.1079\n",
      "Epoch [14/300], Step [39/172], Loss: 61.7467\n",
      "Epoch [14/300], Step [40/172], Loss: 49.9866\n",
      "Epoch [14/300], Step [41/172], Loss: 52.9282\n",
      "Epoch [14/300], Step [42/172], Loss: 53.2252\n",
      "Epoch [14/300], Step [43/172], Loss: 46.9669\n",
      "Epoch [14/300], Step [44/172], Loss: 53.9199\n",
      "Epoch [14/300], Step [45/172], Loss: 42.6043\n",
      "Epoch [14/300], Step [46/172], Loss: 65.4328\n",
      "Epoch [14/300], Step [47/172], Loss: 79.4915\n",
      "Epoch [14/300], Step [48/172], Loss: 93.6342\n",
      "Epoch [14/300], Step [49/172], Loss: 40.0937\n",
      "Epoch [14/300], Step [50/172], Loss: 69.5280\n",
      "Epoch [14/300], Step [51/172], Loss: 20.3746\n",
      "Epoch [14/300], Step [52/172], Loss: 35.4364\n",
      "Epoch [14/300], Step [53/172], Loss: 44.7009\n",
      "Epoch [14/300], Step [54/172], Loss: 37.6516\n",
      "Epoch [14/300], Step [55/172], Loss: 34.5913\n",
      "Epoch [14/300], Step [56/172], Loss: 25.9330\n",
      "Epoch [14/300], Step [57/172], Loss: 64.8889\n",
      "Epoch [14/300], Step [58/172], Loss: 31.2410\n",
      "Epoch [14/300], Step [59/172], Loss: 52.6750\n",
      "Epoch [14/300], Step [60/172], Loss: 87.6553\n",
      "Epoch [14/300], Step [61/172], Loss: 24.9792\n",
      "Epoch [14/300], Step [62/172], Loss: 31.9428\n",
      "Epoch [14/300], Step [63/172], Loss: 20.8453\n",
      "Epoch [14/300], Step [64/172], Loss: 13.7831\n",
      "Epoch [14/300], Step [65/172], Loss: 36.3789\n",
      "Epoch [14/300], Step [66/172], Loss: 18.4962\n",
      "Epoch [14/300], Step [67/172], Loss: 41.5207\n",
      "Epoch [14/300], Step [68/172], Loss: 46.2776\n",
      "Epoch [14/300], Step [69/172], Loss: 79.9335\n",
      "Epoch [14/300], Step [70/172], Loss: 71.5306\n",
      "Epoch [14/300], Step [71/172], Loss: 69.4698\n",
      "Epoch [14/300], Step [72/172], Loss: 66.7975\n",
      "Epoch [14/300], Step [73/172], Loss: 70.1305\n",
      "Epoch [14/300], Step [74/172], Loss: 61.0469\n",
      "Epoch [14/300], Step [75/172], Loss: 40.4444\n",
      "Epoch [14/300], Step [76/172], Loss: 56.5760\n",
      "Epoch [14/300], Step [77/172], Loss: 61.5804\n",
      "Epoch [14/300], Step [78/172], Loss: 64.7522\n",
      "Epoch [14/300], Step [79/172], Loss: 57.7989\n",
      "Epoch [14/300], Step [80/172], Loss: 59.8918\n",
      "Epoch [14/300], Step [81/172], Loss: 54.0952\n",
      "Epoch [14/300], Step [82/172], Loss: 53.1396\n",
      "Epoch [14/300], Step [83/172], Loss: 54.9392\n",
      "Epoch [14/300], Step [84/172], Loss: 54.3862\n",
      "Epoch [14/300], Step [85/172], Loss: 60.0371\n",
      "Epoch [14/300], Step [86/172], Loss: 47.2634\n",
      "Epoch [14/300], Step [87/172], Loss: 41.7955\n",
      "Epoch [14/300], Step [88/172], Loss: 44.9157\n",
      "Epoch [14/300], Step [89/172], Loss: 46.7758\n",
      "Epoch [14/300], Step [90/172], Loss: 41.1279\n",
      "Epoch [14/300], Step [91/172], Loss: 41.4813\n",
      "Epoch [14/300], Step [92/172], Loss: 36.6384\n",
      "Epoch [14/300], Step [93/172], Loss: 38.8883\n",
      "Epoch [14/300], Step [94/172], Loss: 40.7902\n",
      "Epoch [14/300], Step [95/172], Loss: 38.5862\n",
      "Epoch [14/300], Step [96/172], Loss: 37.0062\n",
      "Epoch [14/300], Step [97/172], Loss: 39.3643\n",
      "Epoch [14/300], Step [98/172], Loss: 36.9712\n",
      "Epoch [14/300], Step [99/172], Loss: 35.4558\n",
      "Epoch [14/300], Step [100/172], Loss: 35.5768\n",
      "Epoch [14/300], Step [101/172], Loss: 34.0828\n",
      "Epoch [14/300], Step [102/172], Loss: 34.1789\n",
      "Epoch [14/300], Step [103/172], Loss: 34.4250\n",
      "Epoch [14/300], Step [104/172], Loss: 33.0367\n",
      "Epoch [14/300], Step [105/172], Loss: 32.9315\n",
      "Epoch [14/300], Step [106/172], Loss: 32.0483\n",
      "Epoch [14/300], Step [107/172], Loss: 28.7861\n",
      "Epoch [14/300], Step [108/172], Loss: 33.2568\n",
      "Epoch [14/300], Step [109/172], Loss: 31.6466\n",
      "Epoch [14/300], Step [110/172], Loss: 31.4172\n",
      "Epoch [14/300], Step [111/172], Loss: 29.9053\n",
      "Epoch [14/300], Step [112/172], Loss: 34.3680\n",
      "Epoch [14/300], Step [113/172], Loss: 28.9262\n",
      "Epoch [14/300], Step [114/172], Loss: 30.0888\n",
      "Epoch [14/300], Step [115/172], Loss: 32.7616\n",
      "Epoch [14/300], Step [116/172], Loss: 29.5377\n",
      "Epoch [14/300], Step [117/172], Loss: 26.9863\n",
      "Epoch [14/300], Step [118/172], Loss: 28.5435\n",
      "Epoch [14/300], Step [119/172], Loss: 26.4852\n",
      "Epoch [14/300], Step [120/172], Loss: 26.3212\n",
      "Epoch [14/300], Step [121/172], Loss: 26.9023\n",
      "Epoch [14/300], Step [122/172], Loss: 23.5439\n",
      "Epoch [14/300], Step [123/172], Loss: 24.5300\n",
      "Epoch [14/300], Step [124/172], Loss: 24.0326\n",
      "Epoch [14/300], Step [125/172], Loss: 25.8484\n",
      "Epoch [14/300], Step [126/172], Loss: 25.6333\n",
      "Epoch [14/300], Step [127/172], Loss: 26.6403\n",
      "Epoch [14/300], Step [128/172], Loss: 26.9384\n",
      "Epoch [14/300], Step [129/172], Loss: 23.0110\n",
      "Epoch [14/300], Step [130/172], Loss: 24.5099\n",
      "Epoch [14/300], Step [131/172], Loss: 22.7392\n",
      "Epoch [14/300], Step [132/172], Loss: 22.8229\n",
      "Epoch [14/300], Step [133/172], Loss: 22.4798\n",
      "Epoch [14/300], Step [134/172], Loss: 22.5647\n",
      "Epoch [14/300], Step [135/172], Loss: 20.4408\n",
      "Epoch [14/300], Step [136/172], Loss: 21.6784\n",
      "Epoch [14/300], Step [137/172], Loss: 22.0455\n",
      "Epoch [14/300], Step [138/172], Loss: 20.7709\n",
      "Epoch [14/300], Step [139/172], Loss: 21.8247\n",
      "Epoch [14/300], Step [140/172], Loss: 21.2963\n",
      "Epoch [14/300], Step [141/172], Loss: 22.5286\n",
      "Epoch [14/300], Step [142/172], Loss: 21.8431\n",
      "Epoch [14/300], Step [143/172], Loss: 19.8998\n",
      "Epoch [14/300], Step [144/172], Loss: 18.9341\n",
      "Epoch [14/300], Step [145/172], Loss: 19.1380\n",
      "Epoch [14/300], Step [146/172], Loss: 18.9413\n",
      "Epoch [14/300], Step [147/172], Loss: 19.4244\n",
      "Epoch [14/300], Step [148/172], Loss: 18.8903\n",
      "Epoch [14/300], Step [149/172], Loss: 18.8567\n",
      "Epoch [14/300], Step [150/172], Loss: 19.4380\n",
      "Epoch [14/300], Step [151/172], Loss: 17.2903\n",
      "Epoch [14/300], Step [152/172], Loss: 17.4847\n",
      "Epoch [14/300], Step [153/172], Loss: 17.8263\n",
      "Epoch [14/300], Step [154/172], Loss: 18.4519\n",
      "Epoch [14/300], Step [155/172], Loss: 17.0171\n",
      "Epoch [14/300], Step [156/172], Loss: 17.1548\n",
      "Epoch [14/300], Step [157/172], Loss: 18.0568\n",
      "Epoch [14/300], Step [158/172], Loss: 17.3616\n",
      "Epoch [14/300], Step [159/172], Loss: 16.9221\n",
      "Epoch [14/300], Step [160/172], Loss: 17.0188\n",
      "Epoch [14/300], Step [161/172], Loss: 16.2061\n",
      "Epoch [14/300], Step [162/172], Loss: 15.7935\n",
      "Epoch [14/300], Step [163/172], Loss: 15.7262\n",
      "Epoch [14/300], Step [164/172], Loss: 16.0940\n",
      "Epoch [14/300], Step [165/172], Loss: 14.6453\n",
      "Epoch [14/300], Step [166/172], Loss: 15.8445\n",
      "Epoch [14/300], Step [167/172], Loss: 15.4688\n",
      "Epoch [14/300], Step [168/172], Loss: 15.7700\n",
      "Epoch [14/300], Step [169/172], Loss: 14.2388\n",
      "Epoch [14/300], Step [170/172], Loss: 14.4114\n",
      "Epoch [14/300], Step [171/172], Loss: 12.8706\n",
      "Epoch [14/300], Step [172/172], Loss: 9.9552\n",
      "Epoch [15/300], Step [1/172], Loss: 97.4774\n",
      "Epoch [15/300], Step [2/172], Loss: 100.4406\n",
      "Epoch [15/300], Step [3/172], Loss: 135.5026\n",
      "Epoch [15/300], Step [4/172], Loss: 94.4257\n",
      "Epoch [15/300], Step [5/172], Loss: 113.8934\n",
      "Epoch [15/300], Step [6/172], Loss: 62.0004\n",
      "Epoch [15/300], Step [7/172], Loss: 78.5472\n",
      "Epoch [15/300], Step [8/172], Loss: 60.2143\n",
      "Epoch [15/300], Step [9/172], Loss: 78.5308\n",
      "Epoch [15/300], Step [10/172], Loss: 82.7931\n",
      "Epoch [15/300], Step [11/172], Loss: 88.9449\n",
      "Epoch [15/300], Step [12/172], Loss: 86.0360\n",
      "Epoch [15/300], Step [13/172], Loss: 65.6953\n",
      "Epoch [15/300], Step [14/172], Loss: 102.4521\n",
      "Epoch [15/300], Step [15/172], Loss: 94.8528\n",
      "Epoch [15/300], Step [16/172], Loss: 75.0474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/300], Step [17/172], Loss: 66.8533\n",
      "Epoch [15/300], Step [18/172], Loss: 88.3797\n",
      "Epoch [15/300], Step [19/172], Loss: 78.4103\n",
      "Epoch [15/300], Step [20/172], Loss: 139.7222\n",
      "Epoch [15/300], Step [21/172], Loss: 97.6813\n",
      "Epoch [15/300], Step [22/172], Loss: 96.9004\n",
      "Epoch [15/300], Step [23/172], Loss: 73.2619\n",
      "Epoch [15/300], Step [24/172], Loss: 90.0342\n",
      "Epoch [15/300], Step [25/172], Loss: 66.6796\n",
      "Epoch [15/300], Step [26/172], Loss: 71.8390\n",
      "Epoch [15/300], Step [27/172], Loss: 92.0643\n",
      "Epoch [15/300], Step [28/172], Loss: 81.3446\n",
      "Epoch [15/300], Step [29/172], Loss: 99.8832\n",
      "Epoch [15/300], Step [30/172], Loss: 85.1232\n",
      "Epoch [15/300], Step [31/172], Loss: 64.4534\n",
      "Epoch [15/300], Step [32/172], Loss: 55.7283\n",
      "Epoch [15/300], Step [33/172], Loss: 84.8796\n",
      "Epoch [15/300], Step [34/172], Loss: 52.0423\n",
      "Epoch [15/300], Step [35/172], Loss: 77.9631\n",
      "Epoch [15/300], Step [36/172], Loss: 43.1252\n",
      "Epoch [15/300], Step [37/172], Loss: 31.5539\n",
      "Epoch [15/300], Step [38/172], Loss: 51.1912\n",
      "Epoch [15/300], Step [39/172], Loss: 61.5986\n",
      "Epoch [15/300], Step [40/172], Loss: 49.6164\n",
      "Epoch [15/300], Step [41/172], Loss: 52.6167\n",
      "Epoch [15/300], Step [42/172], Loss: 52.7755\n",
      "Epoch [15/300], Step [43/172], Loss: 46.7753\n",
      "Epoch [15/300], Step [44/172], Loss: 52.8000\n",
      "Epoch [15/300], Step [45/172], Loss: 41.8137\n",
      "Epoch [15/300], Step [46/172], Loss: 64.4785\n",
      "Epoch [15/300], Step [47/172], Loss: 79.3689\n",
      "Epoch [15/300], Step [48/172], Loss: 92.5891\n",
      "Epoch [15/300], Step [49/172], Loss: 39.3276\n",
      "Epoch [15/300], Step [50/172], Loss: 69.8381\n",
      "Epoch [15/300], Step [51/172], Loss: 19.8034\n",
      "Epoch [15/300], Step [52/172], Loss: 35.2004\n",
      "Epoch [15/300], Step [53/172], Loss: 44.5278\n",
      "Epoch [15/300], Step [54/172], Loss: 37.2774\n",
      "Epoch [15/300], Step [55/172], Loss: 33.9396\n",
      "Epoch [15/300], Step [56/172], Loss: 25.9317\n",
      "Epoch [15/300], Step [57/172], Loss: 65.0187\n",
      "Epoch [15/300], Step [58/172], Loss: 30.8845\n",
      "Epoch [15/300], Step [59/172], Loss: 52.5956\n",
      "Epoch [15/300], Step [60/172], Loss: 87.6547\n",
      "Epoch [15/300], Step [61/172], Loss: 24.6264\n",
      "Epoch [15/300], Step [62/172], Loss: 31.2914\n",
      "Epoch [15/300], Step [63/172], Loss: 20.9278\n",
      "Epoch [15/300], Step [64/172], Loss: 13.7804\n",
      "Epoch [15/300], Step [65/172], Loss: 36.6345\n",
      "Epoch [15/300], Step [66/172], Loss: 18.6746\n",
      "Epoch [15/300], Step [67/172], Loss: 42.2178\n",
      "Epoch [15/300], Step [68/172], Loss: 45.4058\n",
      "Epoch [15/300], Step [69/172], Loss: 80.3949\n",
      "Epoch [15/300], Step [70/172], Loss: 68.9599\n",
      "Epoch [15/300], Step [71/172], Loss: 67.2928\n",
      "Epoch [15/300], Step [72/172], Loss: 64.7259\n",
      "Epoch [15/300], Step [73/172], Loss: 67.5924\n",
      "Epoch [15/300], Step [74/172], Loss: 58.5317\n",
      "Epoch [15/300], Step [75/172], Loss: 39.2730\n",
      "Epoch [15/300], Step [76/172], Loss: 54.7746\n",
      "Epoch [15/300], Step [77/172], Loss: 59.6464\n",
      "Epoch [15/300], Step [78/172], Loss: 62.6061\n",
      "Epoch [15/300], Step [79/172], Loss: 55.5848\n",
      "Epoch [15/300], Step [80/172], Loss: 58.6587\n",
      "Epoch [15/300], Step [81/172], Loss: 52.9287\n",
      "Epoch [15/300], Step [82/172], Loss: 52.0926\n",
      "Epoch [15/300], Step [83/172], Loss: 54.1008\n",
      "Epoch [15/300], Step [84/172], Loss: 53.3207\n",
      "Epoch [15/300], Step [85/172], Loss: 59.4936\n",
      "Epoch [15/300], Step [86/172], Loss: 46.8419\n",
      "Epoch [15/300], Step [87/172], Loss: 41.4183\n",
      "Epoch [15/300], Step [88/172], Loss: 44.6348\n",
      "Epoch [15/300], Step [89/172], Loss: 46.5407\n",
      "Epoch [15/300], Step [90/172], Loss: 40.8862\n",
      "Epoch [15/300], Step [91/172], Loss: 41.2353\n",
      "Epoch [15/300], Step [92/172], Loss: 36.3130\n",
      "Epoch [15/300], Step [93/172], Loss: 38.7275\n",
      "Epoch [15/300], Step [94/172], Loss: 40.6881\n",
      "Epoch [15/300], Step [95/172], Loss: 38.5039\n",
      "Epoch [15/300], Step [96/172], Loss: 36.7763\n",
      "Epoch [15/300], Step [97/172], Loss: 39.2924\n",
      "Epoch [15/300], Step [98/172], Loss: 36.8038\n",
      "Epoch [15/300], Step [99/172], Loss: 35.2298\n",
      "Epoch [15/300], Step [100/172], Loss: 35.2954\n",
      "Epoch [15/300], Step [101/172], Loss: 33.9475\n",
      "Epoch [15/300], Step [102/172], Loss: 33.8726\n",
      "Epoch [15/300], Step [103/172], Loss: 34.2056\n",
      "Epoch [15/300], Step [104/172], Loss: 32.8473\n",
      "Epoch [15/300], Step [105/172], Loss: 32.6552\n",
      "Epoch [15/300], Step [106/172], Loss: 31.8347\n",
      "Epoch [15/300], Step [107/172], Loss: 28.4504\n",
      "Epoch [15/300], Step [108/172], Loss: 33.0937\n",
      "Epoch [15/300], Step [109/172], Loss: 31.5563\n",
      "Epoch [15/300], Step [110/172], Loss: 31.2208\n",
      "Epoch [15/300], Step [111/172], Loss: 29.6667\n",
      "Epoch [15/300], Step [112/172], Loss: 34.3086\n",
      "Epoch [15/300], Step [113/172], Loss: 28.7060\n",
      "Epoch [15/300], Step [114/172], Loss: 29.9139\n",
      "Epoch [15/300], Step [115/172], Loss: 32.9710\n",
      "Epoch [15/300], Step [116/172], Loss: 29.4143\n",
      "Epoch [15/300], Step [117/172], Loss: 26.6974\n",
      "Epoch [15/300], Step [118/172], Loss: 28.3355\n",
      "Epoch [15/300], Step [119/172], Loss: 26.2215\n",
      "Epoch [15/300], Step [120/172], Loss: 26.0124\n",
      "Epoch [15/300], Step [121/172], Loss: 26.6961\n",
      "Epoch [15/300], Step [122/172], Loss: 23.2034\n",
      "Epoch [15/300], Step [123/172], Loss: 24.2420\n",
      "Epoch [15/300], Step [124/172], Loss: 23.6538\n",
      "Epoch [15/300], Step [125/172], Loss: 25.6580\n",
      "Epoch [15/300], Step [126/172], Loss: 25.4267\n",
      "Epoch [15/300], Step [127/172], Loss: 26.5000\n",
      "Epoch [15/300], Step [128/172], Loss: 26.8850\n",
      "Epoch [15/300], Step [129/172], Loss: 22.7288\n",
      "Epoch [15/300], Step [130/172], Loss: 24.2762\n",
      "Epoch [15/300], Step [131/172], Loss: 22.4542\n",
      "Epoch [15/300], Step [132/172], Loss: 22.5637\n",
      "Epoch [15/300], Step [133/172], Loss: 22.1874\n",
      "Epoch [15/300], Step [134/172], Loss: 22.3030\n",
      "Epoch [15/300], Step [135/172], Loss: 20.1467\n",
      "Epoch [15/300], Step [136/172], Loss: 21.5119\n",
      "Epoch [15/300], Step [137/172], Loss: 21.8584\n",
      "Epoch [15/300], Step [138/172], Loss: 20.5303\n",
      "Epoch [15/300], Step [139/172], Loss: 21.6106\n",
      "Epoch [15/300], Step [140/172], Loss: 21.1038\n",
      "Epoch [15/300], Step [141/172], Loss: 22.4404\n",
      "Epoch [15/300], Step [142/172], Loss: 21.6764\n",
      "Epoch [15/300], Step [143/172], Loss: 19.6293\n",
      "Epoch [15/300], Step [144/172], Loss: 18.5955\n",
      "Epoch [15/300], Step [145/172], Loss: 18.7966\n",
      "Epoch [15/300], Step [146/172], Loss: 18.6229\n",
      "Epoch [15/300], Step [147/172], Loss: 18.8491\n",
      "Epoch [15/300], Step [148/172], Loss: 18.3775\n",
      "Epoch [15/300], Step [149/172], Loss: 18.3669\n",
      "Epoch [15/300], Step [150/172], Loss: 18.8931\n",
      "Epoch [15/300], Step [151/172], Loss: 16.6499\n",
      "Epoch [15/300], Step [152/172], Loss: 16.8462\n",
      "Epoch [15/300], Step [153/172], Loss: 17.0994\n",
      "Epoch [15/300], Step [154/172], Loss: 17.7745\n",
      "Epoch [15/300], Step [155/172], Loss: 16.1913\n",
      "Epoch [15/300], Step [156/172], Loss: 16.2748\n",
      "Epoch [15/300], Step [157/172], Loss: 17.2021\n",
      "Epoch [15/300], Step [158/172], Loss: 16.3810\n",
      "Epoch [15/300], Step [159/172], Loss: 15.8814\n",
      "Epoch [15/300], Step [160/172], Loss: 15.8813\n",
      "Epoch [15/300], Step [161/172], Loss: 15.0417\n",
      "Epoch [15/300], Step [162/172], Loss: 14.5846\n",
      "Epoch [15/300], Step [163/172], Loss: 14.4741\n",
      "Epoch [15/300], Step [164/172], Loss: 14.7160\n",
      "Epoch [15/300], Step [165/172], Loss: 13.3352\n",
      "Epoch [15/300], Step [166/172], Loss: 14.3963\n",
      "Epoch [15/300], Step [167/172], Loss: 13.9628\n",
      "Epoch [15/300], Step [168/172], Loss: 14.1722\n",
      "Epoch [15/300], Step [169/172], Loss: 12.6904\n",
      "Epoch [15/300], Step [170/172], Loss: 12.8041\n",
      "Epoch [15/300], Step [171/172], Loss: 11.2430\n",
      "Epoch [15/300], Step [172/172], Loss: 8.7456\n",
      "Epoch [16/300], Step [1/172], Loss: 104.1593\n",
      "Epoch [16/300], Step [2/172], Loss: 107.1339\n",
      "Epoch [16/300], Step [3/172], Loss: 137.7127\n",
      "Epoch [16/300], Step [4/172], Loss: 97.9921\n",
      "Epoch [16/300], Step [5/172], Loss: 116.1210\n",
      "Epoch [16/300], Step [6/172], Loss: 65.3739\n",
      "Epoch [16/300], Step [7/172], Loss: 82.8449\n",
      "Epoch [16/300], Step [8/172], Loss: 62.9074\n",
      "Epoch [16/300], Step [9/172], Loss: 82.3880\n",
      "Epoch [16/300], Step [10/172], Loss: 85.4376\n",
      "Epoch [16/300], Step [11/172], Loss: 92.3798\n",
      "Epoch [16/300], Step [12/172], Loss: 89.8864\n",
      "Epoch [16/300], Step [13/172], Loss: 68.4521\n",
      "Epoch [16/300], Step [14/172], Loss: 105.7203\n",
      "Epoch [16/300], Step [15/172], Loss: 96.9895\n",
      "Epoch [16/300], Step [16/172], Loss: 77.5816\n",
      "Epoch [16/300], Step [17/172], Loss: 68.2514\n",
      "Epoch [16/300], Step [18/172], Loss: 90.4232\n",
      "Epoch [16/300], Step [19/172], Loss: 80.0422\n",
      "Epoch [16/300], Step [20/172], Loss: 136.6616\n",
      "Epoch [16/300], Step [21/172], Loss: 98.2230\n",
      "Epoch [16/300], Step [22/172], Loss: 97.2941\n",
      "Epoch [16/300], Step [23/172], Loss: 71.7810\n",
      "Epoch [16/300], Step [24/172], Loss: 89.0841\n",
      "Epoch [16/300], Step [25/172], Loss: 65.7602\n",
      "Epoch [16/300], Step [26/172], Loss: 70.4688\n",
      "Epoch [16/300], Step [27/172], Loss: 90.7352\n",
      "Epoch [16/300], Step [28/172], Loss: 79.7764\n",
      "Epoch [16/300], Step [29/172], Loss: 97.1657\n",
      "Epoch [16/300], Step [30/172], Loss: 82.5214\n",
      "Epoch [16/300], Step [31/172], Loss: 61.1654\n",
      "Epoch [16/300], Step [32/172], Loss: 51.9524\n",
      "Epoch [16/300], Step [33/172], Loss: 81.1030\n",
      "Epoch [16/300], Step [34/172], Loss: 47.5866\n",
      "Epoch [16/300], Step [35/172], Loss: 75.5042\n",
      "Epoch [16/300], Step [36/172], Loss: 39.7290\n",
      "Epoch [16/300], Step [37/172], Loss: 28.1780\n",
      "Epoch [16/300], Step [38/172], Loss: 45.3893\n",
      "Epoch [16/300], Step [39/172], Loss: 57.1204\n",
      "Epoch [16/300], Step [40/172], Loss: 44.5352\n",
      "Epoch [16/300], Step [41/172], Loss: 47.7178\n",
      "Epoch [16/300], Step [42/172], Loss: 47.1054\n",
      "Epoch [16/300], Step [43/172], Loss: 41.1367\n",
      "Epoch [16/300], Step [44/172], Loss: 46.8519\n",
      "Epoch [16/300], Step [45/172], Loss: 36.2552\n",
      "Epoch [16/300], Step [46/172], Loss: 59.6422\n",
      "Epoch [16/300], Step [47/172], Loss: 74.0287\n",
      "Epoch [16/300], Step [48/172], Loss: 88.2102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/300], Step [49/172], Loss: 32.7368\n",
      "Epoch [16/300], Step [50/172], Loss: 64.3021\n",
      "Epoch [16/300], Step [51/172], Loss: 15.0182\n",
      "Epoch [16/300], Step [52/172], Loss: 29.0181\n",
      "Epoch [16/300], Step [53/172], Loss: 37.9288\n",
      "Epoch [16/300], Step [54/172], Loss: 30.1558\n",
      "Epoch [16/300], Step [55/172], Loss: 27.2206\n",
      "Epoch [16/300], Step [56/172], Loss: 19.0971\n",
      "Epoch [16/300], Step [57/172], Loss: 59.5671\n",
      "Epoch [16/300], Step [58/172], Loss: 26.4715\n",
      "Epoch [16/300], Step [59/172], Loss: 46.2640\n",
      "Epoch [16/300], Step [60/172], Loss: 83.7784\n",
      "Epoch [16/300], Step [61/172], Loss: 19.4585\n",
      "Epoch [16/300], Step [62/172], Loss: 26.0536\n",
      "Epoch [16/300], Step [63/172], Loss: 14.8708\n",
      "Epoch [16/300], Step [64/172], Loss: 9.5060\n",
      "Epoch [16/300], Step [65/172], Loss: 31.9994\n",
      "Epoch [16/300], Step [66/172], Loss: 13.5021\n",
      "Epoch [16/300], Step [67/172], Loss: 35.4646\n",
      "Epoch [16/300], Step [68/172], Loss: 36.3195\n",
      "Epoch [16/300], Step [69/172], Loss: 82.4662\n",
      "Epoch [16/300], Step [70/172], Loss: 83.5430\n",
      "Epoch [16/300], Step [71/172], Loss: 82.7830\n",
      "Epoch [16/300], Step [72/172], Loss: 79.9229\n",
      "Epoch [16/300], Step [73/172], Loss: 81.6727\n",
      "Epoch [16/300], Step [74/172], Loss: 70.9094\n",
      "Epoch [16/300], Step [75/172], Loss: 49.0537\n",
      "Epoch [16/300], Step [76/172], Loss: 64.6688\n",
      "Epoch [16/300], Step [77/172], Loss: 69.2403\n",
      "Epoch [16/300], Step [78/172], Loss: 71.4136\n",
      "Epoch [16/300], Step [79/172], Loss: 62.8395\n",
      "Epoch [16/300], Step [80/172], Loss: 65.1635\n",
      "Epoch [16/300], Step [81/172], Loss: 58.7474\n",
      "Epoch [16/300], Step [82/172], Loss: 56.6142\n",
      "Epoch [16/300], Step [83/172], Loss: 57.9264\n",
      "Epoch [16/300], Step [84/172], Loss: 56.0472\n",
      "Epoch [16/300], Step [85/172], Loss: 61.8020\n",
      "Epoch [16/300], Step [86/172], Loss: 49.6361\n",
      "Epoch [16/300], Step [87/172], Loss: 43.9555\n",
      "Epoch [16/300], Step [88/172], Loss: 46.3243\n",
      "Epoch [16/300], Step [89/172], Loss: 48.1612\n",
      "Epoch [16/300], Step [90/172], Loss: 42.6954\n",
      "Epoch [16/300], Step [91/172], Loss: 42.5930\n",
      "Epoch [16/300], Step [92/172], Loss: 37.8387\n",
      "Epoch [16/300], Step [93/172], Loss: 40.1093\n",
      "Epoch [16/300], Step [94/172], Loss: 41.5888\n",
      "Epoch [16/300], Step [95/172], Loss: 39.5284\n",
      "Epoch [16/300], Step [96/172], Loss: 37.3316\n",
      "Epoch [16/300], Step [97/172], Loss: 39.4854\n",
      "Epoch [16/300], Step [98/172], Loss: 37.0727\n",
      "Epoch [16/300], Step [99/172], Loss: 35.2936\n",
      "Epoch [16/300], Step [100/172], Loss: 35.6187\n",
      "Epoch [16/300], Step [101/172], Loss: 34.0307\n",
      "Epoch [16/300], Step [102/172], Loss: 33.8083\n",
      "Epoch [16/300], Step [103/172], Loss: 34.3511\n",
      "Epoch [16/300], Step [104/172], Loss: 33.0227\n",
      "Epoch [16/300], Step [105/172], Loss: 32.5024\n",
      "Epoch [16/300], Step [106/172], Loss: 32.0134\n",
      "Epoch [16/300], Step [107/172], Loss: 28.6459\n",
      "Epoch [16/300], Step [108/172], Loss: 32.8384\n",
      "Epoch [16/300], Step [109/172], Loss: 31.7004\n",
      "Epoch [16/300], Step [110/172], Loss: 31.1565\n",
      "Epoch [16/300], Step [111/172], Loss: 29.7432\n",
      "Epoch [16/300], Step [112/172], Loss: 34.5315\n",
      "Epoch [16/300], Step [113/172], Loss: 28.9193\n",
      "Epoch [16/300], Step [114/172], Loss: 29.7255\n",
      "Epoch [16/300], Step [115/172], Loss: 32.9178\n",
      "Epoch [16/300], Step [116/172], Loss: 29.3906\n",
      "Epoch [16/300], Step [117/172], Loss: 26.5045\n",
      "Epoch [16/300], Step [118/172], Loss: 27.9777\n",
      "Epoch [16/300], Step [119/172], Loss: 26.0096\n",
      "Epoch [16/300], Step [120/172], Loss: 25.6672\n",
      "Epoch [16/300], Step [121/172], Loss: 26.1841\n",
      "Epoch [16/300], Step [122/172], Loss: 22.8655\n",
      "Epoch [16/300], Step [123/172], Loss: 24.1463\n",
      "Epoch [16/300], Step [124/172], Loss: 23.3167\n",
      "Epoch [16/300], Step [125/172], Loss: 25.1843\n",
      "Epoch [16/300], Step [126/172], Loss: 25.0150\n",
      "Epoch [16/300], Step [127/172], Loss: 26.2531\n",
      "Epoch [16/300], Step [128/172], Loss: 26.7499\n",
      "Epoch [16/300], Step [129/172], Loss: 22.1627\n",
      "Epoch [16/300], Step [130/172], Loss: 23.5999\n",
      "Epoch [16/300], Step [131/172], Loss: 21.9166\n",
      "Epoch [16/300], Step [132/172], Loss: 21.9800\n",
      "Epoch [16/300], Step [133/172], Loss: 21.3888\n",
      "Epoch [16/300], Step [134/172], Loss: 21.6368\n",
      "Epoch [16/300], Step [135/172], Loss: 19.4602\n",
      "Epoch [16/300], Step [136/172], Loss: 20.8619\n",
      "Epoch [16/300], Step [137/172], Loss: 21.2200\n",
      "Epoch [16/300], Step [138/172], Loss: 19.8983\n",
      "Epoch [16/300], Step [139/172], Loss: 20.8606\n",
      "Epoch [16/300], Step [140/172], Loss: 20.3553\n",
      "Epoch [16/300], Step [141/172], Loss: 21.7223\n",
      "Epoch [16/300], Step [142/172], Loss: 20.9383\n",
      "Epoch [16/300], Step [143/172], Loss: 18.8252\n",
      "Epoch [16/300], Step [144/172], Loss: 17.7380\n",
      "Epoch [16/300], Step [145/172], Loss: 17.9469\n",
      "Epoch [16/300], Step [146/172], Loss: 17.8599\n",
      "Epoch [16/300], Step [147/172], Loss: 17.7752\n",
      "Epoch [16/300], Step [148/172], Loss: 17.4422\n",
      "Epoch [16/300], Step [149/172], Loss: 17.5379\n",
      "Epoch [16/300], Step [150/172], Loss: 18.0616\n",
      "Epoch [16/300], Step [151/172], Loss: 15.8263\n",
      "Epoch [16/300], Step [152/172], Loss: 16.1215\n",
      "Epoch [16/300], Step [153/172], Loss: 16.3357\n",
      "Epoch [16/300], Step [154/172], Loss: 17.1043\n",
      "Epoch [16/300], Step [155/172], Loss: 15.5099\n",
      "Epoch [16/300], Step [156/172], Loss: 15.6662\n",
      "Epoch [16/300], Step [157/172], Loss: 16.7008\n",
      "Epoch [16/300], Step [158/172], Loss: 15.8630\n",
      "Epoch [16/300], Step [159/172], Loss: 15.3558\n",
      "Epoch [16/300], Step [160/172], Loss: 15.3885\n",
      "Epoch [16/300], Step [161/172], Loss: 14.5797\n",
      "Epoch [16/300], Step [162/172], Loss: 14.1425\n",
      "Epoch [16/300], Step [163/172], Loss: 14.0772\n",
      "Epoch [16/300], Step [164/172], Loss: 14.4866\n",
      "Epoch [16/300], Step [165/172], Loss: 13.0395\n",
      "Epoch [16/300], Step [166/172], Loss: 14.1138\n",
      "Epoch [16/300], Step [167/172], Loss: 13.7990\n",
      "Epoch [16/300], Step [168/172], Loss: 14.0245\n",
      "Epoch [16/300], Step [169/172], Loss: 12.4682\n",
      "Epoch [16/300], Step [170/172], Loss: 12.6803\n",
      "Epoch [16/300], Step [171/172], Loss: 11.1436\n",
      "Epoch [16/300], Step [172/172], Loss: 8.7326\n",
      "Epoch [17/300], Step [1/172], Loss: 103.1278\n",
      "Epoch [17/300], Step [2/172], Loss: 106.5685\n",
      "Epoch [17/300], Step [3/172], Loss: 136.4803\n",
      "Epoch [17/300], Step [4/172], Loss: 95.4639\n",
      "Epoch [17/300], Step [5/172], Loss: 114.6540\n",
      "Epoch [17/300], Step [6/172], Loss: 63.4056\n",
      "Epoch [17/300], Step [7/172], Loss: 80.5062\n",
      "Epoch [17/300], Step [8/172], Loss: 58.7140\n",
      "Epoch [17/300], Step [9/172], Loss: 79.7910\n",
      "Epoch [17/300], Step [10/172], Loss: 83.4802\n",
      "Epoch [17/300], Step [11/172], Loss: 92.2109\n",
      "Epoch [17/300], Step [12/172], Loss: 90.0700\n",
      "Epoch [17/300], Step [13/172], Loss: 66.5756\n",
      "Epoch [17/300], Step [14/172], Loss: 105.8124\n",
      "Epoch [17/300], Step [15/172], Loss: 96.6291\n",
      "Epoch [17/300], Step [16/172], Loss: 76.6100\n",
      "Epoch [17/300], Step [17/172], Loss: 67.3936\n",
      "Epoch [17/300], Step [18/172], Loss: 90.5353\n",
      "Epoch [17/300], Step [19/172], Loss: 79.9733\n",
      "Epoch [17/300], Step [20/172], Loss: 137.3530\n",
      "Epoch [17/300], Step [21/172], Loss: 99.1511\n",
      "Epoch [17/300], Step [22/172], Loss: 97.0602\n",
      "Epoch [17/300], Step [23/172], Loss: 71.3617\n",
      "Epoch [17/300], Step [24/172], Loss: 91.1782\n",
      "Epoch [17/300], Step [25/172], Loss: 65.4571\n",
      "Epoch [17/300], Step [26/172], Loss: 70.3482\n",
      "Epoch [17/300], Step [27/172], Loss: 91.1631\n",
      "Epoch [17/300], Step [28/172], Loss: 82.4243\n",
      "Epoch [17/300], Step [29/172], Loss: 100.8401\n",
      "Epoch [17/300], Step [30/172], Loss: 84.0136\n",
      "Epoch [17/300], Step [31/172], Loss: 61.8390\n",
      "Epoch [17/300], Step [32/172], Loss: 52.4798\n",
      "Epoch [17/300], Step [33/172], Loss: 83.0218\n",
      "Epoch [17/300], Step [34/172], Loss: 46.5758\n",
      "Epoch [17/300], Step [35/172], Loss: 74.9638\n",
      "Epoch [17/300], Step [36/172], Loss: 40.3810\n",
      "Epoch [17/300], Step [37/172], Loss: 28.9001\n",
      "Epoch [17/300], Step [38/172], Loss: 47.1882\n",
      "Epoch [17/300], Step [39/172], Loss: 57.4647\n",
      "Epoch [17/300], Step [40/172], Loss: 44.9463\n",
      "Epoch [17/300], Step [41/172], Loss: 47.9445\n",
      "Epoch [17/300], Step [42/172], Loss: 47.4989\n",
      "Epoch [17/300], Step [43/172], Loss: 41.8052\n",
      "Epoch [17/300], Step [44/172], Loss: 45.7931\n",
      "Epoch [17/300], Step [45/172], Loss: 35.2578\n",
      "Epoch [17/300], Step [46/172], Loss: 59.0877\n",
      "Epoch [17/300], Step [47/172], Loss: 73.9608\n",
      "Epoch [17/300], Step [48/172], Loss: 86.4609\n",
      "Epoch [17/300], Step [49/172], Loss: 33.0347\n",
      "Epoch [17/300], Step [50/172], Loss: 64.3134\n",
      "Epoch [17/300], Step [51/172], Loss: 15.3748\n",
      "Epoch [17/300], Step [52/172], Loss: 29.0717\n",
      "Epoch [17/300], Step [53/172], Loss: 37.5478\n",
      "Epoch [17/300], Step [54/172], Loss: 29.3630\n",
      "Epoch [17/300], Step [55/172], Loss: 26.4811\n",
      "Epoch [17/300], Step [56/172], Loss: 20.3916\n",
      "Epoch [17/300], Step [57/172], Loss: 58.6736\n",
      "Epoch [17/300], Step [58/172], Loss: 26.0150\n",
      "Epoch [17/300], Step [59/172], Loss: 45.3607\n",
      "Epoch [17/300], Step [60/172], Loss: 81.6233\n",
      "Epoch [17/300], Step [61/172], Loss: 20.1462\n",
      "Epoch [17/300], Step [62/172], Loss: 25.7084\n",
      "Epoch [17/300], Step [63/172], Loss: 15.6574\n",
      "Epoch [17/300], Step [64/172], Loss: 10.5177\n",
      "Epoch [17/300], Step [65/172], Loss: 31.6162\n",
      "Epoch [17/300], Step [66/172], Loss: 14.3885\n",
      "Epoch [17/300], Step [67/172], Loss: 35.7078\n",
      "Epoch [17/300], Step [68/172], Loss: 32.0082\n",
      "Epoch [17/300], Step [69/172], Loss: 80.8092\n",
      "Epoch [17/300], Step [70/172], Loss: 81.5780\n",
      "Epoch [17/300], Step [71/172], Loss: 79.2008\n",
      "Epoch [17/300], Step [72/172], Loss: 77.8210\n",
      "Epoch [17/300], Step [73/172], Loss: 80.1795\n",
      "Epoch [17/300], Step [74/172], Loss: 70.8329\n",
      "Epoch [17/300], Step [75/172], Loss: 49.5398\n",
      "Epoch [17/300], Step [76/172], Loss: 64.9990\n",
      "Epoch [17/300], Step [77/172], Loss: 69.1649\n",
      "Epoch [17/300], Step [78/172], Loss: 71.6834\n",
      "Epoch [17/300], Step [79/172], Loss: 62.5828\n",
      "Epoch [17/300], Step [80/172], Loss: 66.8931\n",
      "Epoch [17/300], Step [81/172], Loss: 60.2782\n",
      "Epoch [17/300], Step [82/172], Loss: 59.6914\n",
      "Epoch [17/300], Step [83/172], Loss: 59.8439\n",
      "Epoch [17/300], Step [84/172], Loss: 58.1203\n",
      "Epoch [17/300], Step [85/172], Loss: 64.9643\n",
      "Epoch [17/300], Step [86/172], Loss: 51.9973\n",
      "Epoch [17/300], Step [87/172], Loss: 45.2938\n",
      "Epoch [17/300], Step [88/172], Loss: 48.6422\n",
      "Epoch [17/300], Step [89/172], Loss: 50.3269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/300], Step [90/172], Loss: 44.1917\n",
      "Epoch [17/300], Step [91/172], Loss: 44.0577\n",
      "Epoch [17/300], Step [92/172], Loss: 39.2922\n",
      "Epoch [17/300], Step [93/172], Loss: 42.0422\n",
      "Epoch [17/300], Step [94/172], Loss: 43.6330\n",
      "Epoch [17/300], Step [95/172], Loss: 40.8917\n",
      "Epoch [17/300], Step [96/172], Loss: 38.3568\n",
      "Epoch [17/300], Step [97/172], Loss: 40.4608\n",
      "Epoch [17/300], Step [98/172], Loss: 37.8715\n",
      "Epoch [17/300], Step [99/172], Loss: 35.6969\n",
      "Epoch [17/300], Step [100/172], Loss: 35.6657\n",
      "Epoch [17/300], Step [101/172], Loss: 34.3992\n",
      "Epoch [17/300], Step [102/172], Loss: 33.8521\n",
      "Epoch [17/300], Step [103/172], Loss: 34.2487\n",
      "Epoch [17/300], Step [104/172], Loss: 32.9666\n",
      "Epoch [17/300], Step [105/172], Loss: 32.4405\n",
      "Epoch [17/300], Step [106/172], Loss: 31.9228\n",
      "Epoch [17/300], Step [107/172], Loss: 28.0150\n",
      "Epoch [17/300], Step [108/172], Loss: 32.7159\n",
      "Epoch [17/300], Step [109/172], Loss: 31.2184\n",
      "Epoch [17/300], Step [110/172], Loss: 30.6098\n",
      "Epoch [17/300], Step [111/172], Loss: 29.1094\n",
      "Epoch [17/300], Step [112/172], Loss: 34.2734\n",
      "Epoch [17/300], Step [113/172], Loss: 27.9700\n",
      "Epoch [17/300], Step [114/172], Loss: 29.0898\n",
      "Epoch [17/300], Step [115/172], Loss: 32.6641\n",
      "Epoch [17/300], Step [116/172], Loss: 28.6260\n",
      "Epoch [17/300], Step [117/172], Loss: 25.4863\n",
      "Epoch [17/300], Step [118/172], Loss: 27.2141\n",
      "Epoch [17/300], Step [119/172], Loss: 24.9078\n",
      "Epoch [17/300], Step [120/172], Loss: 24.4700\n",
      "Epoch [17/300], Step [121/172], Loss: 25.1550\n",
      "Epoch [17/300], Step [122/172], Loss: 21.6225\n",
      "Epoch [17/300], Step [123/172], Loss: 22.6005\n",
      "Epoch [17/300], Step [124/172], Loss: 21.8756\n",
      "Epoch [17/300], Step [125/172], Loss: 24.0273\n",
      "Epoch [17/300], Step [126/172], Loss: 23.8261\n",
      "Epoch [17/300], Step [127/172], Loss: 24.9999\n",
      "Epoch [17/300], Step [128/172], Loss: 25.4847\n",
      "Epoch [17/300], Step [129/172], Loss: 20.9070\n",
      "Epoch [17/300], Step [130/172], Loss: 22.4879\n",
      "Epoch [17/300], Step [131/172], Loss: 20.6475\n",
      "Epoch [17/300], Step [132/172], Loss: 20.7423\n",
      "Epoch [17/300], Step [133/172], Loss: 20.2148\n",
      "Epoch [17/300], Step [134/172], Loss: 20.4196\n",
      "Epoch [17/300], Step [135/172], Loss: 18.2592\n",
      "Epoch [17/300], Step [136/172], Loss: 19.5985\n",
      "Epoch [17/300], Step [137/172], Loss: 20.0236\n",
      "Epoch [17/300], Step [138/172], Loss: 18.6377\n",
      "Epoch [17/300], Step [139/172], Loss: 19.7191\n",
      "Epoch [17/300], Step [140/172], Loss: 19.2326\n",
      "Epoch [17/300], Step [141/172], Loss: 20.7855\n",
      "Epoch [17/300], Step [142/172], Loss: 19.8857\n",
      "Epoch [17/300], Step [143/172], Loss: 17.7778\n",
      "Epoch [17/300], Step [144/172], Loss: 16.7615\n",
      "Epoch [17/300], Step [145/172], Loss: 16.9575\n",
      "Epoch [17/300], Step [146/172], Loss: 16.9128\n",
      "Epoch [17/300], Step [147/172], Loss: 16.8804\n",
      "Epoch [17/300], Step [148/172], Loss: 16.5945\n",
      "Epoch [17/300], Step [149/172], Loss: 16.7451\n",
      "Epoch [17/300], Step [150/172], Loss: 17.2604\n",
      "Epoch [17/300], Step [151/172], Loss: 15.0900\n",
      "Epoch [17/300], Step [152/172], Loss: 15.4016\n",
      "Epoch [17/300], Step [153/172], Loss: 15.6364\n",
      "Epoch [17/300], Step [154/172], Loss: 16.4715\n",
      "Epoch [17/300], Step [155/172], Loss: 14.9038\n",
      "Epoch [17/300], Step [156/172], Loss: 15.0663\n",
      "Epoch [17/300], Step [157/172], Loss: 16.1378\n",
      "Epoch [17/300], Step [158/172], Loss: 15.3160\n",
      "Epoch [17/300], Step [159/172], Loss: 14.8468\n",
      "Epoch [17/300], Step [160/172], Loss: 14.9046\n",
      "Epoch [17/300], Step [161/172], Loss: 14.1468\n",
      "Epoch [17/300], Step [162/172], Loss: 13.7211\n",
      "Epoch [17/300], Step [163/172], Loss: 13.7076\n",
      "Epoch [17/300], Step [164/172], Loss: 14.1036\n",
      "Epoch [17/300], Step [165/172], Loss: 12.6857\n",
      "Epoch [17/300], Step [166/172], Loss: 13.7853\n",
      "Epoch [17/300], Step [167/172], Loss: 13.5472\n",
      "Epoch [17/300], Step [168/172], Loss: 13.7388\n",
      "Epoch [17/300], Step [169/172], Loss: 12.2380\n",
      "Epoch [17/300], Step [170/172], Loss: 12.4961\n",
      "Epoch [17/300], Step [171/172], Loss: 10.9502\n",
      "Epoch [17/300], Step [172/172], Loss: 8.6345\n",
      "Epoch [18/300], Step [1/172], Loss: 102.4602\n",
      "Epoch [18/300], Step [2/172], Loss: 105.1902\n",
      "Epoch [18/300], Step [3/172], Loss: 137.6750\n",
      "Epoch [18/300], Step [4/172], Loss: 94.2886\n",
      "Epoch [18/300], Step [5/172], Loss: 114.5834\n",
      "Epoch [18/300], Step [6/172], Loss: 61.8677\n",
      "Epoch [18/300], Step [7/172], Loss: 78.8842\n",
      "Epoch [18/300], Step [8/172], Loss: 57.2009\n",
      "Epoch [18/300], Step [9/172], Loss: 78.9215\n",
      "Epoch [18/300], Step [10/172], Loss: 82.7451\n",
      "Epoch [18/300], Step [11/172], Loss: 92.4527\n",
      "Epoch [18/300], Step [12/172], Loss: 89.5784\n",
      "Epoch [18/300], Step [13/172], Loss: 66.3051\n",
      "Epoch [18/300], Step [14/172], Loss: 105.6794\n",
      "Epoch [18/300], Step [15/172], Loss: 96.6603\n",
      "Epoch [18/300], Step [16/172], Loss: 77.0320\n",
      "Epoch [18/300], Step [17/172], Loss: 67.4610\n",
      "Epoch [18/300], Step [18/172], Loss: 91.0701\n",
      "Epoch [18/300], Step [19/172], Loss: 80.6620\n",
      "Epoch [18/300], Step [20/172], Loss: 137.4956\n",
      "Epoch [18/300], Step [21/172], Loss: 100.0664\n",
      "Epoch [18/300], Step [22/172], Loss: 97.8282\n",
      "Epoch [18/300], Step [23/172], Loss: 70.4958\n",
      "Epoch [18/300], Step [24/172], Loss: 91.9547\n",
      "Epoch [18/300], Step [25/172], Loss: 66.0901\n",
      "Epoch [18/300], Step [26/172], Loss: 71.0144\n",
      "Epoch [18/300], Step [27/172], Loss: 92.8604\n",
      "Epoch [18/300], Step [28/172], Loss: 84.7734\n",
      "Epoch [18/300], Step [29/172], Loss: 103.5130\n",
      "Epoch [18/300], Step [30/172], Loss: 85.9254\n",
      "Epoch [18/300], Step [31/172], Loss: 63.2133\n",
      "Epoch [18/300], Step [32/172], Loss: 53.1407\n",
      "Epoch [18/300], Step [33/172], Loss: 85.8245\n",
      "Epoch [18/300], Step [34/172], Loss: 47.2027\n",
      "Epoch [18/300], Step [35/172], Loss: 75.3195\n",
      "Epoch [18/300], Step [36/172], Loss: 40.7063\n",
      "Epoch [18/300], Step [37/172], Loss: 29.3495\n",
      "Epoch [18/300], Step [38/172], Loss: 48.6013\n",
      "Epoch [18/300], Step [39/172], Loss: 59.6947\n",
      "Epoch [18/300], Step [40/172], Loss: 47.2935\n",
      "Epoch [18/300], Step [41/172], Loss: 50.5511\n",
      "Epoch [18/300], Step [42/172], Loss: 50.3785\n",
      "Epoch [18/300], Step [43/172], Loss: 44.7003\n",
      "Epoch [18/300], Step [44/172], Loss: 47.8668\n",
      "Epoch [18/300], Step [45/172], Loss: 37.3078\n",
      "Epoch [18/300], Step [46/172], Loss: 59.5413\n",
      "Epoch [18/300], Step [47/172], Loss: 76.2732\n",
      "Epoch [18/300], Step [48/172], Loss: 87.3175\n",
      "Epoch [18/300], Step [49/172], Loss: 33.7033\n",
      "Epoch [18/300], Step [50/172], Loss: 66.6114\n",
      "Epoch [18/300], Step [51/172], Loss: 15.1763\n",
      "Epoch [18/300], Step [52/172], Loss: 31.5454\n",
      "Epoch [18/300], Step [53/172], Loss: 39.3075\n",
      "Epoch [18/300], Step [54/172], Loss: 30.8524\n",
      "Epoch [18/300], Step [55/172], Loss: 27.4909\n",
      "Epoch [18/300], Step [56/172], Loss: 19.6784\n",
      "Epoch [18/300], Step [57/172], Loss: 57.7238\n",
      "Epoch [18/300], Step [58/172], Loss: 25.5792\n",
      "Epoch [18/300], Step [59/172], Loss: 45.1407\n",
      "Epoch [18/300], Step [60/172], Loss: 79.0495\n",
      "Epoch [18/300], Step [61/172], Loss: 18.8251\n",
      "Epoch [18/300], Step [62/172], Loss: 24.4685\n",
      "Epoch [18/300], Step [63/172], Loss: 14.7255\n",
      "Epoch [18/300], Step [64/172], Loss: 9.2746\n",
      "Epoch [18/300], Step [65/172], Loss: 31.0936\n",
      "Epoch [18/300], Step [66/172], Loss: 14.6296\n",
      "Epoch [18/300], Step [67/172], Loss: 33.8143\n",
      "Epoch [18/300], Step [68/172], Loss: 34.5918\n",
      "Epoch [18/300], Step [69/172], Loss: 81.6410\n",
      "Epoch [18/300], Step [70/172], Loss: 83.3833\n",
      "Epoch [18/300], Step [71/172], Loss: 80.4594\n",
      "Epoch [18/300], Step [72/172], Loss: 77.1092\n",
      "Epoch [18/300], Step [73/172], Loss: 81.5054\n",
      "Epoch [18/300], Step [74/172], Loss: 68.9477\n",
      "Epoch [18/300], Step [75/172], Loss: 43.1605\n",
      "Epoch [18/300], Step [76/172], Loss: 62.5602\n",
      "Epoch [18/300], Step [77/172], Loss: 68.2219\n",
      "Epoch [18/300], Step [78/172], Loss: 71.1452\n",
      "Epoch [18/300], Step [79/172], Loss: 60.8551\n",
      "Epoch [18/300], Step [80/172], Loss: 64.1145\n",
      "Epoch [18/300], Step [81/172], Loss: 57.2408\n",
      "Epoch [18/300], Step [82/172], Loss: 55.2273\n",
      "Epoch [18/300], Step [83/172], Loss: 56.3603\n",
      "Epoch [18/300], Step [84/172], Loss: 54.5922\n",
      "Epoch [18/300], Step [85/172], Loss: 62.0592\n",
      "Epoch [18/300], Step [86/172], Loss: 47.7848\n",
      "Epoch [18/300], Step [87/172], Loss: 40.9611\n",
      "Epoch [18/300], Step [88/172], Loss: 44.4587\n",
      "Epoch [18/300], Step [89/172], Loss: 46.0443\n",
      "Epoch [18/300], Step [90/172], Loss: 40.3788\n",
      "Epoch [18/300], Step [91/172], Loss: 40.5425\n",
      "Epoch [18/300], Step [92/172], Loss: 35.6955\n",
      "Epoch [18/300], Step [93/172], Loss: 38.5623\n",
      "Epoch [18/300], Step [94/172], Loss: 40.2232\n",
      "Epoch [18/300], Step [95/172], Loss: 37.7996\n",
      "Epoch [18/300], Step [96/172], Loss: 35.2924\n",
      "Epoch [18/300], Step [97/172], Loss: 37.5765\n",
      "Epoch [18/300], Step [98/172], Loss: 35.0278\n",
      "Epoch [18/300], Step [99/172], Loss: 32.9424\n",
      "Epoch [18/300], Step [100/172], Loss: 33.1401\n",
      "Epoch [18/300], Step [101/172], Loss: 31.9809\n",
      "Epoch [18/300], Step [102/172], Loss: 31.3406\n",
      "Epoch [18/300], Step [103/172], Loss: 31.9267\n",
      "Epoch [18/300], Step [104/172], Loss: 30.8441\n",
      "Epoch [18/300], Step [105/172], Loss: 30.3524\n",
      "Epoch [18/300], Step [106/172], Loss: 30.0135\n",
      "Epoch [18/300], Step [107/172], Loss: 26.1973\n",
      "Epoch [18/300], Step [108/172], Loss: 30.8193\n",
      "Epoch [18/300], Step [109/172], Loss: 29.5552\n",
      "Epoch [18/300], Step [110/172], Loss: 28.8614\n",
      "Epoch [18/300], Step [111/172], Loss: 27.5024\n",
      "Epoch [18/300], Step [112/172], Loss: 32.5935\n",
      "Epoch [18/300], Step [113/172], Loss: 26.5450\n",
      "Epoch [18/300], Step [114/172], Loss: 27.6313\n",
      "Epoch [18/300], Step [115/172], Loss: 31.4093\n",
      "Epoch [18/300], Step [116/172], Loss: 27.3283\n",
      "Epoch [18/300], Step [117/172], Loss: 24.2367\n",
      "Epoch [18/300], Step [118/172], Loss: 26.0287\n",
      "Epoch [18/300], Step [119/172], Loss: 23.7704\n",
      "Epoch [18/300], Step [120/172], Loss: 23.2850\n",
      "Epoch [18/300], Step [121/172], Loss: 23.9659\n",
      "Epoch [18/300], Step [122/172], Loss: 20.6166\n",
      "Epoch [18/300], Step [123/172], Loss: 21.6529\n",
      "Epoch [18/300], Step [124/172], Loss: 20.9250\n",
      "Epoch [18/300], Step [125/172], Loss: 23.1573\n",
      "Epoch [18/300], Step [126/172], Loss: 22.9924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/300], Step [127/172], Loss: 24.2506\n",
      "Epoch [18/300], Step [128/172], Loss: 24.7966\n",
      "Epoch [18/300], Step [129/172], Loss: 20.1712\n",
      "Epoch [18/300], Step [130/172], Loss: 21.8147\n",
      "Epoch [18/300], Step [131/172], Loss: 20.0080\n",
      "Epoch [18/300], Step [132/172], Loss: 20.1363\n",
      "Epoch [18/300], Step [133/172], Loss: 19.6061\n",
      "Epoch [18/300], Step [134/172], Loss: 19.8637\n",
      "Epoch [18/300], Step [135/172], Loss: 17.7374\n",
      "Epoch [18/300], Step [136/172], Loss: 19.0438\n",
      "Epoch [18/300], Step [137/172], Loss: 19.5536\n",
      "Epoch [18/300], Step [138/172], Loss: 18.1513\n",
      "Epoch [18/300], Step [139/172], Loss: 19.3072\n",
      "Epoch [18/300], Step [140/172], Loss: 18.8609\n",
      "Epoch [18/300], Step [141/172], Loss: 20.4676\n",
      "Epoch [18/300], Step [142/172], Loss: 19.5670\n",
      "Epoch [18/300], Step [143/172], Loss: 17.4540\n",
      "Epoch [18/300], Step [144/172], Loss: 16.4358\n",
      "Epoch [18/300], Step [145/172], Loss: 16.6401\n",
      "Epoch [18/300], Step [146/172], Loss: 16.6502\n",
      "Epoch [18/300], Step [147/172], Loss: 16.4979\n",
      "Epoch [18/300], Step [148/172], Loss: 16.2945\n",
      "Epoch [18/300], Step [149/172], Loss: 16.5363\n",
      "Epoch [18/300], Step [150/172], Loss: 17.0151\n",
      "Epoch [18/300], Step [151/172], Loss: 14.8627\n",
      "Epoch [18/300], Step [152/172], Loss: 15.2127\n",
      "Epoch [18/300], Step [153/172], Loss: 15.4189\n",
      "Epoch [18/300], Step [154/172], Loss: 16.3332\n",
      "Epoch [18/300], Step [155/172], Loss: 14.7125\n",
      "Epoch [18/300], Step [156/172], Loss: 14.9268\n",
      "Epoch [18/300], Step [157/172], Loss: 16.0542\n",
      "Epoch [18/300], Step [158/172], Loss: 15.1831\n",
      "Epoch [18/300], Step [159/172], Loss: 14.7189\n",
      "Epoch [18/300], Step [160/172], Loss: 14.7986\n",
      "Epoch [18/300], Step [161/172], Loss: 14.0090\n",
      "Epoch [18/300], Step [162/172], Loss: 13.6057\n",
      "Epoch [18/300], Step [163/172], Loss: 13.5948\n",
      "Epoch [18/300], Step [164/172], Loss: 14.0579\n",
      "Epoch [18/300], Step [165/172], Loss: 12.6092\n",
      "Epoch [18/300], Step [166/172], Loss: 13.7140\n",
      "Epoch [18/300], Step [167/172], Loss: 13.4911\n",
      "Epoch [18/300], Step [168/172], Loss: 13.6801\n",
      "Epoch [18/300], Step [169/172], Loss: 12.1460\n",
      "Epoch [18/300], Step [170/172], Loss: 12.4529\n",
      "Epoch [18/300], Step [171/172], Loss: 10.8789\n",
      "Epoch [18/300], Step [172/172], Loss: 8.6231\n",
      "Epoch [19/300], Step [1/172], Loss: 101.9811\n",
      "Epoch [19/300], Step [2/172], Loss: 104.7491\n",
      "Epoch [19/300], Step [3/172], Loss: 135.3948\n",
      "Epoch [19/300], Step [4/172], Loss: 92.2941\n",
      "Epoch [19/300], Step [5/172], Loss: 113.0255\n",
      "Epoch [19/300], Step [6/172], Loss: 60.5968\n",
      "Epoch [19/300], Step [7/172], Loss: 77.3618\n",
      "Epoch [19/300], Step [8/172], Loss: 54.5346\n",
      "Epoch [19/300], Step [9/172], Loss: 77.1719\n",
      "Epoch [19/300], Step [10/172], Loss: 81.2317\n",
      "Epoch [19/300], Step [11/172], Loss: 92.3121\n",
      "Epoch [19/300], Step [12/172], Loss: 89.4489\n",
      "Epoch [19/300], Step [13/172], Loss: 65.1365\n",
      "Epoch [19/300], Step [14/172], Loss: 105.2781\n",
      "Epoch [19/300], Step [15/172], Loss: 96.3142\n",
      "Epoch [19/300], Step [16/172], Loss: 76.6592\n",
      "Epoch [19/300], Step [17/172], Loss: 67.0737\n",
      "Epoch [19/300], Step [18/172], Loss: 90.9866\n",
      "Epoch [19/300], Step [19/172], Loss: 80.6609\n",
      "Epoch [19/300], Step [20/172], Loss: 136.4130\n",
      "Epoch [19/300], Step [21/172], Loss: 100.4228\n",
      "Epoch [19/300], Step [22/172], Loss: 98.0723\n",
      "Epoch [19/300], Step [23/172], Loss: 69.3825\n",
      "Epoch [19/300], Step [24/172], Loss: 92.2838\n",
      "Epoch [19/300], Step [25/172], Loss: 65.9598\n",
      "Epoch [19/300], Step [26/172], Loss: 70.9498\n",
      "Epoch [19/300], Step [27/172], Loss: 93.1905\n",
      "Epoch [19/300], Step [28/172], Loss: 85.7121\n",
      "Epoch [19/300], Step [29/172], Loss: 104.2829\n",
      "Epoch [19/300], Step [30/172], Loss: 86.5213\n",
      "Epoch [19/300], Step [31/172], Loss: 63.3703\n",
      "Epoch [19/300], Step [32/172], Loss: 53.2333\n",
      "Epoch [19/300], Step [33/172], Loss: 86.1357\n",
      "Epoch [19/300], Step [34/172], Loss: 45.5009\n",
      "Epoch [19/300], Step [35/172], Loss: 76.1838\n",
      "Epoch [19/300], Step [36/172], Loss: 41.2151\n",
      "Epoch [19/300], Step [37/172], Loss: 29.7073\n",
      "Epoch [19/300], Step [38/172], Loss: 49.6483\n",
      "Epoch [19/300], Step [39/172], Loss: 60.5640\n",
      "Epoch [19/300], Step [40/172], Loss: 47.5835\n",
      "Epoch [19/300], Step [41/172], Loss: 50.9860\n",
      "Epoch [19/300], Step [42/172], Loss: 50.6564\n",
      "Epoch [19/300], Step [43/172], Loss: 45.3487\n",
      "Epoch [19/300], Step [44/172], Loss: 47.8493\n",
      "Epoch [19/300], Step [45/172], Loss: 37.6637\n",
      "Epoch [19/300], Step [46/172], Loss: 60.2645\n",
      "Epoch [19/300], Step [47/172], Loss: 77.9212\n",
      "Epoch [19/300], Step [48/172], Loss: 88.9746\n",
      "Epoch [19/300], Step [49/172], Loss: 35.2766\n",
      "Epoch [19/300], Step [50/172], Loss: 68.8059\n",
      "Epoch [19/300], Step [51/172], Loss: 16.3344\n",
      "Epoch [19/300], Step [52/172], Loss: 33.2065\n",
      "Epoch [19/300], Step [53/172], Loss: 41.7635\n",
      "Epoch [19/300], Step [54/172], Loss: 33.2333\n",
      "Epoch [19/300], Step [55/172], Loss: 29.3524\n",
      "Epoch [19/300], Step [56/172], Loss: 22.4792\n",
      "Epoch [19/300], Step [57/172], Loss: 61.5911\n",
      "Epoch [19/300], Step [58/172], Loss: 27.5594\n",
      "Epoch [19/300], Step [59/172], Loss: 48.3884\n",
      "Epoch [19/300], Step [60/172], Loss: 82.3318\n",
      "Epoch [19/300], Step [61/172], Loss: 20.8446\n",
      "Epoch [19/300], Step [62/172], Loss: 26.9405\n",
      "Epoch [19/300], Step [63/172], Loss: 17.7320\n",
      "Epoch [19/300], Step [64/172], Loss: 11.1975\n",
      "Epoch [19/300], Step [65/172], Loss: 34.0981\n",
      "Epoch [19/300], Step [66/172], Loss: 16.7703\n",
      "Epoch [19/300], Step [67/172], Loss: 37.9695\n",
      "Epoch [19/300], Step [68/172], Loss: 39.6534\n",
      "Epoch [19/300], Step [69/172], Loss: 81.6367\n",
      "Epoch [19/300], Step [70/172], Loss: 71.7102\n",
      "Epoch [19/300], Step [71/172], Loss: 69.4737\n",
      "Epoch [19/300], Step [72/172], Loss: 66.4489\n",
      "Epoch [19/300], Step [73/172], Loss: 70.2877\n",
      "Epoch [19/300], Step [74/172], Loss: 59.2530\n",
      "Epoch [19/300], Step [75/172], Loss: 37.9912\n",
      "Epoch [19/300], Step [76/172], Loss: 55.1872\n",
      "Epoch [19/300], Step [77/172], Loss: 60.3397\n",
      "Epoch [19/300], Step [78/172], Loss: 63.6742\n",
      "Epoch [19/300], Step [79/172], Loss: 54.9325\n",
      "Epoch [19/300], Step [80/172], Loss: 59.3730\n",
      "Epoch [19/300], Step [81/172], Loss: 53.2557\n",
      "Epoch [19/300], Step [82/172], Loss: 51.8822\n",
      "Epoch [19/300], Step [83/172], Loss: 53.8734\n",
      "Epoch [19/300], Step [84/172], Loss: 51.9362\n",
      "Epoch [19/300], Step [85/172], Loss: 60.2678\n",
      "Epoch [19/300], Step [86/172], Loss: 46.2806\n",
      "Epoch [19/300], Step [87/172], Loss: 39.8313\n",
      "Epoch [19/300], Step [88/172], Loss: 43.4226\n",
      "Epoch [19/300], Step [89/172], Loss: 45.2232\n",
      "Epoch [19/300], Step [90/172], Loss: 39.6239\n",
      "Epoch [19/300], Step [91/172], Loss: 39.8438\n",
      "Epoch [19/300], Step [92/172], Loss: 34.7539\n",
      "Epoch [19/300], Step [93/172], Loss: 37.8397\n",
      "Epoch [19/300], Step [94/172], Loss: 39.7549\n",
      "Epoch [19/300], Step [95/172], Loss: 37.3865\n",
      "Epoch [19/300], Step [96/172], Loss: 34.9615\n",
      "Epoch [19/300], Step [97/172], Loss: 37.8200\n",
      "Epoch [19/300], Step [98/172], Loss: 35.0979\n",
      "Epoch [19/300], Step [99/172], Loss: 33.0549\n",
      "Epoch [19/300], Step [100/172], Loss: 33.0638\n",
      "Epoch [19/300], Step [101/172], Loss: 32.3236\n",
      "Epoch [19/300], Step [102/172], Loss: 31.4083\n",
      "Epoch [19/300], Step [103/172], Loss: 32.0780\n",
      "Epoch [19/300], Step [104/172], Loss: 30.9704\n",
      "Epoch [19/300], Step [105/172], Loss: 30.5199\n",
      "Epoch [19/300], Step [106/172], Loss: 30.0723\n",
      "Epoch [19/300], Step [107/172], Loss: 26.2712\n",
      "Epoch [19/300], Step [108/172], Loss: 31.1962\n",
      "Epoch [19/300], Step [109/172], Loss: 29.9610\n",
      "Epoch [19/300], Step [110/172], Loss: 29.1305\n",
      "Epoch [19/300], Step [111/172], Loss: 27.7037\n",
      "Epoch [19/300], Step [112/172], Loss: 32.8005\n",
      "Epoch [19/300], Step [113/172], Loss: 26.7910\n",
      "Epoch [19/300], Step [114/172], Loss: 27.9849\n",
      "Epoch [19/300], Step [115/172], Loss: 32.2553\n",
      "Epoch [19/300], Step [116/172], Loss: 27.6607\n",
      "Epoch [19/300], Step [117/172], Loss: 24.5266\n",
      "Epoch [19/300], Step [118/172], Loss: 26.4998\n",
      "Epoch [19/300], Step [119/172], Loss: 24.1083\n",
      "Epoch [19/300], Step [120/172], Loss: 23.6396\n",
      "Epoch [19/300], Step [121/172], Loss: 24.5315\n",
      "Epoch [19/300], Step [122/172], Loss: 20.9407\n",
      "Epoch [19/300], Step [123/172], Loss: 21.9894\n",
      "Epoch [19/300], Step [124/172], Loss: 21.1786\n",
      "Epoch [19/300], Step [125/172], Loss: 23.7090\n",
      "Epoch [19/300], Step [126/172], Loss: 23.4633\n",
      "Epoch [19/300], Step [127/172], Loss: 24.7620\n",
      "Epoch [19/300], Step [128/172], Loss: 25.3801\n",
      "Epoch [19/300], Step [129/172], Loss: 20.5824\n",
      "Epoch [19/300], Step [130/172], Loss: 22.3100\n",
      "Epoch [19/300], Step [131/172], Loss: 20.3817\n",
      "Epoch [19/300], Step [132/172], Loss: 20.5278\n",
      "Epoch [19/300], Step [133/172], Loss: 20.0295\n",
      "Epoch [19/300], Step [134/172], Loss: 20.2734\n",
      "Epoch [19/300], Step [135/172], Loss: 18.0906\n",
      "Epoch [19/300], Step [136/172], Loss: 19.5328\n",
      "Epoch [19/300], Step [137/172], Loss: 20.0283\n",
      "Epoch [19/300], Step [138/172], Loss: 18.5539\n",
      "Epoch [19/300], Step [139/172], Loss: 19.7476\n",
      "Epoch [19/300], Step [140/172], Loss: 19.3243\n",
      "Epoch [19/300], Step [141/172], Loss: 21.0764\n",
      "Epoch [19/300], Step [142/172], Loss: 20.0913\n",
      "Epoch [19/300], Step [143/172], Loss: 17.8336\n",
      "Epoch [19/300], Step [144/172], Loss: 16.8014\n",
      "Epoch [19/300], Step [145/172], Loss: 17.0245\n",
      "Epoch [19/300], Step [146/172], Loss: 17.0927\n",
      "Epoch [19/300], Step [147/172], Loss: 16.7866\n",
      "Epoch [19/300], Step [148/172], Loss: 16.6417\n",
      "Epoch [19/300], Step [149/172], Loss: 16.9557\n",
      "Epoch [19/300], Step [150/172], Loss: 17.4404\n",
      "Epoch [19/300], Step [151/172], Loss: 15.1674\n",
      "Epoch [19/300], Step [152/172], Loss: 15.5714\n",
      "Epoch [19/300], Step [153/172], Loss: 15.7573\n",
      "Epoch [19/300], Step [154/172], Loss: 16.7628\n",
      "Epoch [19/300], Step [155/172], Loss: 15.0254\n",
      "Epoch [19/300], Step [156/172], Loss: 15.3096\n",
      "Epoch [19/300], Step [157/172], Loss: 16.5184\n",
      "Epoch [19/300], Step [158/172], Loss: 15.5636\n",
      "Epoch [19/300], Step [159/172], Loss: 15.1093\n",
      "Epoch [19/300], Step [160/172], Loss: 15.1664\n",
      "Epoch [19/300], Step [161/172], Loss: 14.3767\n",
      "Epoch [19/300], Step [162/172], Loss: 13.9202\n",
      "Epoch [19/300], Step [163/172], Loss: 13.9121\n",
      "Epoch [19/300], Step [164/172], Loss: 14.4635\n",
      "Epoch [19/300], Step [165/172], Loss: 12.9142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/300], Step [166/172], Loss: 14.0098\n",
      "Epoch [19/300], Step [167/172], Loss: 13.8830\n",
      "Epoch [19/300], Step [168/172], Loss: 13.9955\n",
      "Epoch [19/300], Step [169/172], Loss: 12.3864\n",
      "Epoch [19/300], Step [170/172], Loss: 12.7403\n",
      "Epoch [19/300], Step [171/172], Loss: 11.1477\n",
      "Epoch [19/300], Step [172/172], Loss: 8.8454\n",
      "Epoch [20/300], Step [1/172], Loss: 99.8276\n",
      "Epoch [20/300], Step [2/172], Loss: 102.3758\n",
      "Epoch [20/300], Step [3/172], Loss: 131.5480\n",
      "Epoch [20/300], Step [4/172], Loss: 88.9073\n",
      "Epoch [20/300], Step [5/172], Loss: 109.5454\n",
      "Epoch [20/300], Step [6/172], Loss: 57.4252\n",
      "Epoch [20/300], Step [7/172], Loss: 74.5789\n",
      "Epoch [20/300], Step [8/172], Loss: 51.1405\n",
      "Epoch [20/300], Step [9/172], Loss: 74.3669\n",
      "Epoch [20/300], Step [10/172], Loss: 78.2798\n",
      "Epoch [20/300], Step [11/172], Loss: 91.0732\n",
      "Epoch [20/300], Step [12/172], Loss: 87.8209\n",
      "Epoch [20/300], Step [13/172], Loss: 62.8353\n",
      "Epoch [20/300], Step [14/172], Loss: 103.5505\n",
      "Epoch [20/300], Step [15/172], Loss: 94.4631\n",
      "Epoch [20/300], Step [16/172], Loss: 75.1361\n",
      "Epoch [20/300], Step [17/172], Loss: 65.1525\n",
      "Epoch [20/300], Step [18/172], Loss: 89.7840\n",
      "Epoch [20/300], Step [19/172], Loss: 79.2880\n",
      "Epoch [20/300], Step [20/172], Loss: 132.9727\n",
      "Epoch [20/300], Step [21/172], Loss: 99.3692\n",
      "Epoch [20/300], Step [22/172], Loss: 96.5897\n",
      "Epoch [20/300], Step [23/172], Loss: 67.3593\n",
      "Epoch [20/300], Step [24/172], Loss: 91.1942\n",
      "Epoch [20/300], Step [25/172], Loss: 64.3422\n",
      "Epoch [20/300], Step [26/172], Loss: 69.7814\n",
      "Epoch [20/300], Step [27/172], Loss: 92.4286\n",
      "Epoch [20/300], Step [28/172], Loss: 85.1673\n",
      "Epoch [20/300], Step [29/172], Loss: 103.7787\n",
      "Epoch [20/300], Step [30/172], Loss: 85.6067\n",
      "Epoch [20/300], Step [31/172], Loss: 61.8255\n",
      "Epoch [20/300], Step [32/172], Loss: 51.0564\n",
      "Epoch [20/300], Step [33/172], Loss: 85.3881\n",
      "Epoch [20/300], Step [34/172], Loss: 42.8997\n",
      "Epoch [20/300], Step [35/172], Loss: 75.1134\n",
      "Epoch [20/300], Step [36/172], Loss: 40.1381\n",
      "Epoch [20/300], Step [37/172], Loss: 28.2814\n",
      "Epoch [20/300], Step [38/172], Loss: 47.2948\n",
      "Epoch [20/300], Step [39/172], Loss: 59.6901\n",
      "Epoch [20/300], Step [40/172], Loss: 46.1857\n",
      "Epoch [20/300], Step [41/172], Loss: 49.9173\n",
      "Epoch [20/300], Step [42/172], Loss: 49.7200\n",
      "Epoch [20/300], Step [43/172], Loss: 44.0348\n",
      "Epoch [20/300], Step [44/172], Loss: 46.2869\n",
      "Epoch [20/300], Step [45/172], Loss: 35.6107\n",
      "Epoch [20/300], Step [46/172], Loss: 58.7384\n",
      "Epoch [20/300], Step [47/172], Loss: 77.3085\n",
      "Epoch [20/300], Step [48/172], Loss: 88.3623\n",
      "Epoch [20/300], Step [49/172], Loss: 32.6704\n",
      "Epoch [20/300], Step [50/172], Loss: 68.2264\n",
      "Epoch [20/300], Step [51/172], Loss: 14.4050\n",
      "Epoch [20/300], Step [52/172], Loss: 32.0335\n",
      "Epoch [20/300], Step [53/172], Loss: 40.0359\n",
      "Epoch [20/300], Step [54/172], Loss: 31.0047\n",
      "Epoch [20/300], Step [55/172], Loss: 27.1536\n",
      "Epoch [20/300], Step [56/172], Loss: 19.5471\n",
      "Epoch [20/300], Step [57/172], Loss: 59.6919\n",
      "Epoch [20/300], Step [58/172], Loss: 25.9917\n",
      "Epoch [20/300], Step [59/172], Loss: 46.3635\n",
      "Epoch [20/300], Step [60/172], Loss: 81.2090\n",
      "Epoch [20/300], Step [61/172], Loss: 18.8849\n",
      "Epoch [20/300], Step [62/172], Loss: 24.4515\n",
      "Epoch [20/300], Step [63/172], Loss: 15.4220\n",
      "Epoch [20/300], Step [64/172], Loss: 9.4861\n",
      "Epoch [20/300], Step [65/172], Loss: 32.8175\n",
      "Epoch [20/300], Step [66/172], Loss: 15.5681\n",
      "Epoch [20/300], Step [67/172], Loss: 35.7186\n",
      "Epoch [20/300], Step [68/172], Loss: 36.9496\n",
      "Epoch [20/300], Step [69/172], Loss: 82.9198\n",
      "Epoch [20/300], Step [70/172], Loss: 75.3648\n",
      "Epoch [20/300], Step [71/172], Loss: 73.0623\n",
      "Epoch [20/300], Step [72/172], Loss: 69.1824\n",
      "Epoch [20/300], Step [73/172], Loss: 73.5361\n",
      "Epoch [20/300], Step [74/172], Loss: 60.5384\n",
      "Epoch [20/300], Step [75/172], Loss: 37.4228\n",
      "Epoch [20/300], Step [76/172], Loss: 56.4435\n",
      "Epoch [20/300], Step [77/172], Loss: 62.3140\n",
      "Epoch [20/300], Step [78/172], Loss: 65.2859\n",
      "Epoch [20/300], Step [79/172], Loss: 55.7003\n",
      "Epoch [20/300], Step [80/172], Loss: 59.5090\n",
      "Epoch [20/300], Step [81/172], Loss: 53.2920\n",
      "Epoch [20/300], Step [82/172], Loss: 51.0172\n",
      "Epoch [20/300], Step [83/172], Loss: 53.1806\n",
      "Epoch [20/300], Step [84/172], Loss: 51.0545\n",
      "Epoch [20/300], Step [85/172], Loss: 59.6401\n",
      "Epoch [20/300], Step [86/172], Loss: 44.9267\n",
      "Epoch [20/300], Step [87/172], Loss: 38.2977\n",
      "Epoch [20/300], Step [88/172], Loss: 41.7892\n",
      "Epoch [20/300], Step [89/172], Loss: 43.4993\n",
      "Epoch [20/300], Step [90/172], Loss: 38.2288\n",
      "Epoch [20/300], Step [91/172], Loss: 38.3019\n",
      "Epoch [20/300], Step [92/172], Loss: 33.2438\n",
      "Epoch [20/300], Step [93/172], Loss: 36.2898\n",
      "Epoch [20/300], Step [94/172], Loss: 38.1982\n",
      "Epoch [20/300], Step [95/172], Loss: 35.9579\n",
      "Epoch [20/300], Step [96/172], Loss: 33.4747\n",
      "Epoch [20/300], Step [97/172], Loss: 36.3923\n",
      "Epoch [20/300], Step [98/172], Loss: 33.6662\n",
      "Epoch [20/300], Step [99/172], Loss: 31.6728\n",
      "Epoch [20/300], Step [100/172], Loss: 31.7874\n",
      "Epoch [20/300], Step [101/172], Loss: 31.1427\n",
      "Epoch [20/300], Step [102/172], Loss: 30.1250\n",
      "Epoch [20/300], Step [103/172], Loss: 30.9160\n",
      "Epoch [20/300], Step [104/172], Loss: 29.9246\n",
      "Epoch [20/300], Step [105/172], Loss: 29.4484\n",
      "Epoch [20/300], Step [106/172], Loss: 29.1437\n",
      "Epoch [20/300], Step [107/172], Loss: 25.4130\n",
      "Epoch [20/300], Step [108/172], Loss: 30.2247\n",
      "Epoch [20/300], Step [109/172], Loss: 29.2143\n",
      "Epoch [20/300], Step [110/172], Loss: 28.2611\n",
      "Epoch [20/300], Step [111/172], Loss: 26.9415\n",
      "Epoch [20/300], Step [112/172], Loss: 32.0700\n",
      "Epoch [20/300], Step [113/172], Loss: 26.1790\n",
      "Epoch [20/300], Step [114/172], Loss: 27.2903\n",
      "Epoch [20/300], Step [115/172], Loss: 31.7891\n",
      "Epoch [20/300], Step [116/172], Loss: 27.0544\n",
      "Epoch [20/300], Step [117/172], Loss: 23.9532\n",
      "Epoch [20/300], Step [118/172], Loss: 25.9525\n",
      "Epoch [20/300], Step [119/172], Loss: 23.5823\n",
      "Epoch [20/300], Step [120/172], Loss: 23.0670\n",
      "Epoch [20/300], Step [121/172], Loss: 23.9365\n",
      "Epoch [20/300], Step [122/172], Loss: 20.4787\n",
      "Epoch [20/300], Step [123/172], Loss: 21.5786\n",
      "Epoch [20/300], Step [124/172], Loss: 20.7036\n",
      "Epoch [20/300], Step [125/172], Loss: 23.3029\n",
      "Epoch [20/300], Step [126/172], Loss: 23.0479\n",
      "Epoch [20/300], Step [127/172], Loss: 24.4156\n",
      "Epoch [20/300], Step [128/172], Loss: 25.0536\n",
      "Epoch [20/300], Step [129/172], Loss: 20.2262\n",
      "Epoch [20/300], Step [130/172], Loss: 21.9810\n",
      "Epoch [20/300], Step [131/172], Loss: 20.0722\n",
      "Epoch [20/300], Step [132/172], Loss: 20.2284\n",
      "Epoch [20/300], Step [133/172], Loss: 19.7308\n",
      "Epoch [20/300], Step [134/172], Loss: 20.0089\n",
      "Epoch [20/300], Step [135/172], Loss: 17.8520\n",
      "Epoch [20/300], Step [136/172], Loss: 19.2682\n",
      "Epoch [20/300], Step [137/172], Loss: 19.8291\n",
      "Epoch [20/300], Step [138/172], Loss: 18.3243\n",
      "Epoch [20/300], Step [139/172], Loss: 19.5926\n",
      "Epoch [20/300], Step [140/172], Loss: 19.1989\n",
      "Epoch [20/300], Step [141/172], Loss: 21.0287\n",
      "Epoch [20/300], Step [142/172], Loss: 20.0127\n",
      "Epoch [20/300], Step [143/172], Loss: 17.7312\n",
      "Epoch [20/300], Step [144/172], Loss: 16.6925\n",
      "Epoch [20/300], Step [145/172], Loss: 16.9444\n",
      "Epoch [20/300], Step [146/172], Loss: 17.0482\n",
      "Epoch [20/300], Step [147/172], Loss: 16.6200\n",
      "Epoch [20/300], Step [148/172], Loss: 16.5352\n",
      "Epoch [20/300], Step [149/172], Loss: 16.9431\n",
      "Epoch [20/300], Step [150/172], Loss: 17.4021\n",
      "Epoch [20/300], Step [151/172], Loss: 15.1460\n",
      "Epoch [20/300], Step [152/172], Loss: 15.5691\n",
      "Epoch [20/300], Step [153/172], Loss: 15.7317\n",
      "Epoch [20/300], Step [154/172], Loss: 16.7926\n",
      "Epoch [20/300], Step [155/172], Loss: 15.0334\n",
      "Epoch [20/300], Step [156/172], Loss: 15.3663\n",
      "Epoch [20/300], Step [157/172], Loss: 16.6098\n",
      "Epoch [20/300], Step [158/172], Loss: 15.6049\n",
      "Epoch [20/300], Step [159/172], Loss: 15.1634\n",
      "Epoch [20/300], Step [160/172], Loss: 15.2315\n",
      "Epoch [20/300], Step [161/172], Loss: 14.4060\n",
      "Epoch [20/300], Step [162/172], Loss: 13.9690\n",
      "Epoch [20/300], Step [163/172], Loss: 13.9739\n",
      "Epoch [20/300], Step [164/172], Loss: 14.5804\n",
      "Epoch [20/300], Step [165/172], Loss: 12.9909\n",
      "Epoch [20/300], Step [166/172], Loss: 14.0909\n",
      "Epoch [20/300], Step [167/172], Loss: 13.9692\n",
      "Epoch [20/300], Step [168/172], Loss: 14.0716\n",
      "Epoch [20/300], Step [169/172], Loss: 12.4579\n",
      "Epoch [20/300], Step [170/172], Loss: 12.8449\n",
      "Epoch [20/300], Step [171/172], Loss: 11.2221\n",
      "Epoch [20/300], Step [172/172], Loss: 8.9340\n",
      "Epoch [21/300], Step [1/172], Loss: 99.2357\n",
      "Epoch [21/300], Step [2/172], Loss: 101.7541\n",
      "Epoch [21/300], Step [3/172], Loss: 129.4450\n",
      "Epoch [21/300], Step [4/172], Loss: 87.1590\n",
      "Epoch [21/300], Step [5/172], Loss: 108.1841\n",
      "Epoch [21/300], Step [6/172], Loss: 56.1961\n",
      "Epoch [21/300], Step [7/172], Loss: 73.2819\n",
      "Epoch [21/300], Step [8/172], Loss: 48.9103\n",
      "Epoch [21/300], Step [9/172], Loss: 72.6792\n",
      "Epoch [21/300], Step [10/172], Loss: 76.6569\n",
      "Epoch [21/300], Step [11/172], Loss: 90.5981\n",
      "Epoch [21/300], Step [12/172], Loss: 87.1911\n",
      "Epoch [21/300], Step [13/172], Loss: 61.8281\n",
      "Epoch [21/300], Step [14/172], Loss: 102.7346\n",
      "Epoch [21/300], Step [15/172], Loss: 93.6753\n",
      "Epoch [21/300], Step [16/172], Loss: 74.7108\n",
      "Epoch [21/300], Step [17/172], Loss: 64.5392\n",
      "Epoch [21/300], Step [18/172], Loss: 89.1466\n",
      "Epoch [21/300], Step [19/172], Loss: 78.9751\n",
      "Epoch [21/300], Step [20/172], Loss: 130.7707\n",
      "Epoch [21/300], Step [21/172], Loss: 98.9886\n",
      "Epoch [21/300], Step [22/172], Loss: 96.0890\n",
      "Epoch [21/300], Step [23/172], Loss: 66.2525\n",
      "Epoch [21/300], Step [24/172], Loss: 90.9300\n",
      "Epoch [21/300], Step [25/172], Loss: 63.8994\n",
      "Epoch [21/300], Step [26/172], Loss: 69.3546\n",
      "Epoch [21/300], Step [27/172], Loss: 92.1399\n",
      "Epoch [21/300], Step [28/172], Loss: 85.4703\n",
      "Epoch [21/300], Step [29/172], Loss: 103.5876\n",
      "Epoch [21/300], Step [30/172], Loss: 85.3911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/300], Step [31/172], Loss: 61.4609\n",
      "Epoch [21/300], Step [32/172], Loss: 50.5692\n",
      "Epoch [21/300], Step [33/172], Loss: 85.1937\n",
      "Epoch [21/300], Step [34/172], Loss: 41.0557\n",
      "Epoch [21/300], Step [35/172], Loss: 75.2843\n",
      "Epoch [21/300], Step [36/172], Loss: 40.2619\n",
      "Epoch [21/300], Step [37/172], Loss: 28.2924\n",
      "Epoch [21/300], Step [38/172], Loss: 47.4642\n",
      "Epoch [21/300], Step [39/172], Loss: 60.1254\n",
      "Epoch [21/300], Step [40/172], Loss: 46.1085\n",
      "Epoch [21/300], Step [41/172], Loss: 50.0346\n",
      "Epoch [21/300], Step [42/172], Loss: 49.6310\n",
      "Epoch [21/300], Step [43/172], Loss: 44.0886\n",
      "Epoch [21/300], Step [44/172], Loss: 45.5206\n",
      "Epoch [21/300], Step [45/172], Loss: 35.2455\n",
      "Epoch [21/300], Step [46/172], Loss: 58.8072\n",
      "Epoch [21/300], Step [47/172], Loss: 77.8549\n",
      "Epoch [21/300], Step [48/172], Loss: 88.7061\n",
      "Epoch [21/300], Step [49/172], Loss: 32.6865\n",
      "Epoch [21/300], Step [50/172], Loss: 68.7611\n",
      "Epoch [21/300], Step [51/172], Loss: 14.3985\n",
      "Epoch [21/300], Step [52/172], Loss: 32.2773\n",
      "Epoch [21/300], Step [53/172], Loss: 40.5896\n",
      "Epoch [21/300], Step [54/172], Loss: 31.2931\n",
      "Epoch [21/300], Step [55/172], Loss: 27.1639\n",
      "Epoch [21/300], Step [56/172], Loss: 19.8359\n",
      "Epoch [21/300], Step [57/172], Loss: 60.7435\n",
      "Epoch [21/300], Step [58/172], Loss: 26.3725\n",
      "Epoch [21/300], Step [59/172], Loss: 47.0225\n",
      "Epoch [21/300], Step [60/172], Loss: 81.8594\n",
      "Epoch [21/300], Step [61/172], Loss: 18.9800\n",
      "Epoch [21/300], Step [62/172], Loss: 24.5230\n",
      "Epoch [21/300], Step [63/172], Loss: 15.8523\n",
      "Epoch [21/300], Step [64/172], Loss: 9.6988\n",
      "Epoch [21/300], Step [65/172], Loss: 33.5665\n",
      "Epoch [21/300], Step [66/172], Loss: 15.8757\n",
      "Epoch [21/300], Step [67/172], Loss: 36.3323\n",
      "Epoch [21/300], Step [68/172], Loss: 37.1375\n",
      "Epoch [21/300], Step [69/172], Loss: 83.3918\n",
      "Epoch [21/300], Step [70/172], Loss: 72.2955\n",
      "Epoch [21/300], Step [71/172], Loss: 70.5716\n",
      "Epoch [21/300], Step [72/172], Loss: 66.9010\n",
      "Epoch [21/300], Step [73/172], Loss: 70.9520\n",
      "Epoch [21/300], Step [74/172], Loss: 58.2232\n",
      "Epoch [21/300], Step [75/172], Loss: 36.1826\n",
      "Epoch [21/300], Step [76/172], Loss: 54.4187\n",
      "Epoch [21/300], Step [77/172], Loss: 60.2806\n",
      "Epoch [21/300], Step [78/172], Loss: 63.7371\n",
      "Epoch [21/300], Step [79/172], Loss: 54.1339\n",
      "Epoch [21/300], Step [80/172], Loss: 58.3567\n",
      "Epoch [21/300], Step [81/172], Loss: 52.4134\n",
      "Epoch [21/300], Step [82/172], Loss: 50.1602\n",
      "Epoch [21/300], Step [83/172], Loss: 52.5928\n",
      "Epoch [21/300], Step [84/172], Loss: 50.1431\n",
      "Epoch [21/300], Step [85/172], Loss: 59.2133\n",
      "Epoch [21/300], Step [86/172], Loss: 44.4879\n",
      "Epoch [21/300], Step [87/172], Loss: 37.8723\n",
      "Epoch [21/300], Step [88/172], Loss: 41.3492\n",
      "Epoch [21/300], Step [89/172], Loss: 43.0284\n",
      "Epoch [21/300], Step [90/172], Loss: 37.9595\n",
      "Epoch [21/300], Step [91/172], Loss: 37.8800\n",
      "Epoch [21/300], Step [92/172], Loss: 32.8282\n",
      "Epoch [21/300], Step [93/172], Loss: 35.9779\n",
      "Epoch [21/300], Step [94/172], Loss: 37.8692\n",
      "Epoch [21/300], Step [95/172], Loss: 35.6614\n",
      "Epoch [21/300], Step [96/172], Loss: 33.0460\n",
      "Epoch [21/300], Step [97/172], Loss: 36.1462\n",
      "Epoch [21/300], Step [98/172], Loss: 33.3333\n",
      "Epoch [21/300], Step [99/172], Loss: 31.3284\n",
      "Epoch [21/300], Step [100/172], Loss: 31.3511\n",
      "Epoch [21/300], Step [101/172], Loss: 30.9488\n",
      "Epoch [21/300], Step [102/172], Loss: 29.7083\n",
      "Epoch [21/300], Step [103/172], Loss: 30.5683\n",
      "Epoch [21/300], Step [104/172], Loss: 29.6188\n",
      "Epoch [21/300], Step [105/172], Loss: 29.0936\n",
      "Epoch [21/300], Step [106/172], Loss: 28.8502\n",
      "Epoch [21/300], Step [107/172], Loss: 25.1344\n",
      "Epoch [21/300], Step [108/172], Loss: 30.0010\n",
      "Epoch [21/300], Step [109/172], Loss: 29.1012\n",
      "Epoch [21/300], Step [110/172], Loss: 27.9976\n",
      "Epoch [21/300], Step [111/172], Loss: 26.7385\n",
      "Epoch [21/300], Step [112/172], Loss: 31.9910\n",
      "Epoch [21/300], Step [113/172], Loss: 26.0532\n",
      "Epoch [21/300], Step [114/172], Loss: 27.1505\n",
      "Epoch [21/300], Step [115/172], Loss: 32.0029\n",
      "Epoch [21/300], Step [116/172], Loss: 26.9861\n",
      "Epoch [21/300], Step [117/172], Loss: 23.8287\n",
      "Epoch [21/300], Step [118/172], Loss: 25.9225\n",
      "Epoch [21/300], Step [119/172], Loss: 23.5048\n",
      "Epoch [21/300], Step [120/172], Loss: 22.9392\n",
      "Epoch [21/300], Step [121/172], Loss: 23.8597\n",
      "Epoch [21/300], Step [122/172], Loss: 20.3860\n",
      "Epoch [21/300], Step [123/172], Loss: 21.4830\n",
      "Epoch [21/300], Step [124/172], Loss: 20.5527\n",
      "Epoch [21/300], Step [125/172], Loss: 23.2777\n",
      "Epoch [21/300], Step [126/172], Loss: 23.0387\n",
      "Epoch [21/300], Step [127/172], Loss: 24.4601\n",
      "Epoch [21/300], Step [128/172], Loss: 25.1385\n",
      "Epoch [21/300], Step [129/172], Loss: 20.2111\n",
      "Epoch [21/300], Step [130/172], Loss: 22.0044\n",
      "Epoch [21/300], Step [131/172], Loss: 20.0567\n",
      "Epoch [21/300], Step [132/172], Loss: 20.2311\n",
      "Epoch [21/300], Step [133/172], Loss: 19.7228\n",
      "Epoch [21/300], Step [134/172], Loss: 20.0223\n",
      "Epoch [21/300], Step [135/172], Loss: 17.8457\n",
      "Epoch [21/300], Step [136/172], Loss: 19.2750\n",
      "Epoch [21/300], Step [137/172], Loss: 19.8952\n",
      "Epoch [21/300], Step [138/172], Loss: 18.3305\n",
      "Epoch [21/300], Step [139/172], Loss: 19.6364\n",
      "Epoch [21/300], Step [140/172], Loss: 19.2864\n",
      "Epoch [21/300], Step [141/172], Loss: 21.1843\n",
      "Epoch [21/300], Step [142/172], Loss: 20.1075\n",
      "Epoch [21/300], Step [143/172], Loss: 17.7680\n",
      "Epoch [21/300], Step [144/172], Loss: 16.6988\n",
      "Epoch [21/300], Step [145/172], Loss: 16.9798\n",
      "Epoch [21/300], Step [146/172], Loss: 17.1133\n",
      "Epoch [21/300], Step [147/172], Loss: 16.5002\n",
      "Epoch [21/300], Step [148/172], Loss: 16.4980\n",
      "Epoch [21/300], Step [149/172], Loss: 17.0007\n",
      "Epoch [21/300], Step [150/172], Loss: 17.4082\n",
      "Epoch [21/300], Step [151/172], Loss: 15.1286\n",
      "Epoch [21/300], Step [152/172], Loss: 15.5863\n",
      "Epoch [21/300], Step [153/172], Loss: 15.7126\n",
      "Epoch [21/300], Step [154/172], Loss: 16.8314\n",
      "Epoch [21/300], Step [155/172], Loss: 15.0112\n",
      "Epoch [21/300], Step [156/172], Loss: 15.4108\n",
      "Epoch [21/300], Step [157/172], Loss: 16.6781\n",
      "Epoch [21/300], Step [158/172], Loss: 15.6156\n",
      "Epoch [21/300], Step [159/172], Loss: 15.1831\n",
      "Epoch [21/300], Step [160/172], Loss: 15.2168\n",
      "Epoch [21/300], Step [161/172], Loss: 14.3841\n",
      "Epoch [21/300], Step [162/172], Loss: 13.9590\n",
      "Epoch [21/300], Step [163/172], Loss: 13.9333\n",
      "Epoch [21/300], Step [164/172], Loss: 14.6102\n",
      "Epoch [21/300], Step [165/172], Loss: 12.9846\n",
      "Epoch [21/300], Step [166/172], Loss: 14.0308\n",
      "Epoch [21/300], Step [167/172], Loss: 13.9520\n",
      "Epoch [21/300], Step [168/172], Loss: 14.0196\n",
      "Epoch [21/300], Step [169/172], Loss: 12.3695\n",
      "Epoch [21/300], Step [170/172], Loss: 12.7772\n",
      "Epoch [21/300], Step [171/172], Loss: 11.1739\n",
      "Epoch [21/300], Step [172/172], Loss: 8.9203\n",
      "Epoch [22/300], Step [1/172], Loss: 98.4359\n",
      "Epoch [22/300], Step [2/172], Loss: 100.9804\n",
      "Epoch [22/300], Step [3/172], Loss: 127.7919\n",
      "Epoch [22/300], Step [4/172], Loss: 85.5003\n",
      "Epoch [22/300], Step [5/172], Loss: 106.4786\n",
      "Epoch [22/300], Step [6/172], Loss: 54.5986\n",
      "Epoch [22/300], Step [7/172], Loss: 71.6762\n",
      "Epoch [22/300], Step [8/172], Loss: 46.5777\n",
      "Epoch [22/300], Step [9/172], Loss: 71.0471\n",
      "Epoch [22/300], Step [10/172], Loss: 75.1087\n",
      "Epoch [22/300], Step [11/172], Loss: 90.6139\n",
      "Epoch [22/300], Step [12/172], Loss: 87.0015\n",
      "Epoch [22/300], Step [13/172], Loss: 60.7190\n",
      "Epoch [22/300], Step [14/172], Loss: 101.8581\n",
      "Epoch [22/300], Step [15/172], Loss: 92.7030\n",
      "Epoch [22/300], Step [16/172], Loss: 74.2171\n",
      "Epoch [22/300], Step [17/172], Loss: 63.7282\n",
      "Epoch [22/300], Step [18/172], Loss: 88.6039\n",
      "Epoch [22/300], Step [19/172], Loss: 78.3428\n",
      "Epoch [22/300], Step [20/172], Loss: 129.0069\n",
      "Epoch [22/300], Step [21/172], Loss: 98.2106\n",
      "Epoch [22/300], Step [22/172], Loss: 95.1891\n",
      "Epoch [22/300], Step [23/172], Loss: 64.5228\n",
      "Epoch [22/300], Step [24/172], Loss: 90.3006\n",
      "Epoch [22/300], Step [25/172], Loss: 62.8949\n",
      "Epoch [22/300], Step [26/172], Loss: 68.5966\n",
      "Epoch [22/300], Step [27/172], Loss: 91.4258\n",
      "Epoch [22/300], Step [28/172], Loss: 85.0652\n",
      "Epoch [22/300], Step [29/172], Loss: 102.6383\n",
      "Epoch [22/300], Step [30/172], Loss: 84.5233\n",
      "Epoch [22/300], Step [31/172], Loss: 60.4717\n",
      "Epoch [22/300], Step [32/172], Loss: 49.3185\n",
      "Epoch [22/300], Step [33/172], Loss: 84.3271\n",
      "Epoch [22/300], Step [34/172], Loss: 38.9533\n",
      "Epoch [22/300], Step [35/172], Loss: 74.3340\n",
      "Epoch [22/300], Step [36/172], Loss: 39.6906\n",
      "Epoch [22/300], Step [37/172], Loss: 27.7664\n",
      "Epoch [22/300], Step [38/172], Loss: 46.4649\n",
      "Epoch [22/300], Step [39/172], Loss: 59.5900\n",
      "Epoch [22/300], Step [40/172], Loss: 45.2725\n",
      "Epoch [22/300], Step [41/172], Loss: 49.5042\n",
      "Epoch [22/300], Step [42/172], Loss: 49.0914\n",
      "Epoch [22/300], Step [43/172], Loss: 43.5010\n",
      "Epoch [22/300], Step [44/172], Loss: 44.4018\n",
      "Epoch [22/300], Step [45/172], Loss: 34.2873\n",
      "Epoch [22/300], Step [46/172], Loss: 57.9323\n",
      "Epoch [22/300], Step [47/172], Loss: 77.3227\n",
      "Epoch [22/300], Step [48/172], Loss: 87.2461\n",
      "Epoch [22/300], Step [49/172], Loss: 31.5464\n",
      "Epoch [22/300], Step [50/172], Loss: 68.0130\n",
      "Epoch [22/300], Step [51/172], Loss: 13.6113\n",
      "Epoch [22/300], Step [52/172], Loss: 31.7375\n",
      "Epoch [22/300], Step [53/172], Loss: 39.6920\n",
      "Epoch [22/300], Step [54/172], Loss: 30.5525\n",
      "Epoch [22/300], Step [55/172], Loss: 26.2427\n",
      "Epoch [22/300], Step [56/172], Loss: 18.7970\n",
      "Epoch [22/300], Step [57/172], Loss: 59.8186\n",
      "Epoch [22/300], Step [58/172], Loss: 25.7107\n",
      "Epoch [22/300], Step [59/172], Loss: 45.9738\n",
      "Epoch [22/300], Step [60/172], Loss: 80.2669\n",
      "Epoch [22/300], Step [61/172], Loss: 18.1079\n",
      "Epoch [22/300], Step [62/172], Loss: 23.7660\n",
      "Epoch [22/300], Step [63/172], Loss: 14.8967\n",
      "Epoch [22/300], Step [64/172], Loss: 9.1594\n",
      "Epoch [22/300], Step [65/172], Loss: 33.0487\n",
      "Epoch [22/300], Step [66/172], Loss: 15.4050\n",
      "Epoch [22/300], Step [67/172], Loss: 35.2924\n",
      "Epoch [22/300], Step [68/172], Loss: 35.6492\n",
      "Epoch [22/300], Step [69/172], Loss: 83.4237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/300], Step [70/172], Loss: 73.2491\n",
      "Epoch [22/300], Step [71/172], Loss: 71.4660\n",
      "Epoch [22/300], Step [72/172], Loss: 67.6229\n",
      "Epoch [22/300], Step [73/172], Loss: 71.8043\n",
      "Epoch [22/300], Step [74/172], Loss: 58.1093\n",
      "Epoch [22/300], Step [75/172], Loss: 35.6668\n",
      "Epoch [22/300], Step [76/172], Loss: 54.4265\n",
      "Epoch [22/300], Step [77/172], Loss: 60.4956\n",
      "Epoch [22/300], Step [78/172], Loss: 63.7412\n",
      "Epoch [22/300], Step [79/172], Loss: 53.8548\n",
      "Epoch [22/300], Step [80/172], Loss: 57.9997\n",
      "Epoch [22/300], Step [81/172], Loss: 52.1485\n",
      "Epoch [22/300], Step [82/172], Loss: 49.5477\n",
      "Epoch [22/300], Step [83/172], Loss: 51.9818\n",
      "Epoch [22/300], Step [84/172], Loss: 49.3111\n",
      "Epoch [22/300], Step [85/172], Loss: 58.3623\n",
      "Epoch [22/300], Step [86/172], Loss: 43.7794\n",
      "Epoch [22/300], Step [87/172], Loss: 37.1359\n",
      "Epoch [22/300], Step [88/172], Loss: 40.5201\n",
      "Epoch [22/300], Step [89/172], Loss: 42.1222\n",
      "Epoch [22/300], Step [90/172], Loss: 37.3544\n",
      "Epoch [22/300], Step [91/172], Loss: 37.1508\n",
      "Epoch [22/300], Step [92/172], Loss: 32.2099\n",
      "Epoch [22/300], Step [93/172], Loss: 35.3553\n",
      "Epoch [22/300], Step [94/172], Loss: 37.3004\n",
      "Epoch [22/300], Step [95/172], Loss: 35.1154\n",
      "Epoch [22/300], Step [96/172], Loss: 32.4333\n",
      "Epoch [22/300], Step [97/172], Loss: 35.6072\n",
      "Epoch [22/300], Step [98/172], Loss: 32.7875\n",
      "Epoch [22/300], Step [99/172], Loss: 30.7917\n",
      "Epoch [22/300], Step [100/172], Loss: 30.8185\n",
      "Epoch [22/300], Step [101/172], Loss: 30.5508\n",
      "Epoch [22/300], Step [102/172], Loss: 29.2674\n",
      "Epoch [22/300], Step [103/172], Loss: 30.1216\n",
      "Epoch [22/300], Step [104/172], Loss: 29.2331\n",
      "Epoch [22/300], Step [105/172], Loss: 28.7463\n",
      "Epoch [22/300], Step [106/172], Loss: 28.5152\n",
      "Epoch [22/300], Step [107/172], Loss: 24.8259\n",
      "Epoch [22/300], Step [108/172], Loss: 29.6403\n",
      "Epoch [22/300], Step [109/172], Loss: 28.8804\n",
      "Epoch [22/300], Step [110/172], Loss: 27.6272\n",
      "Epoch [22/300], Step [111/172], Loss: 26.4352\n",
      "Epoch [22/300], Step [112/172], Loss: 31.7083\n",
      "Epoch [22/300], Step [113/172], Loss: 25.7859\n",
      "Epoch [22/300], Step [114/172], Loss: 26.8581\n",
      "Epoch [22/300], Step [115/172], Loss: 31.9303\n",
      "Epoch [22/300], Step [116/172], Loss: 26.7088\n",
      "Epoch [22/300], Step [117/172], Loss: 23.5749\n",
      "Epoch [22/300], Step [118/172], Loss: 25.7166\n",
      "Epoch [22/300], Step [119/172], Loss: 23.2582\n",
      "Epoch [22/300], Step [120/172], Loss: 22.6502\n",
      "Epoch [22/300], Step [121/172], Loss: 23.5726\n",
      "Epoch [22/300], Step [122/172], Loss: 20.1484\n",
      "Epoch [22/300], Step [123/172], Loss: 21.2416\n",
      "Epoch [22/300], Step [124/172], Loss: 20.2658\n",
      "Epoch [22/300], Step [125/172], Loss: 23.0839\n",
      "Epoch [22/300], Step [126/172], Loss: 22.7822\n",
      "Epoch [22/300], Step [127/172], Loss: 24.2334\n",
      "Epoch [22/300], Step [128/172], Loss: 24.9348\n",
      "Epoch [22/300], Step [129/172], Loss: 19.9709\n",
      "Epoch [22/300], Step [130/172], Loss: 21.8032\n",
      "Epoch [22/300], Step [131/172], Loss: 19.8346\n",
      "Epoch [22/300], Step [132/172], Loss: 20.0138\n",
      "Epoch [22/300], Step [133/172], Loss: 19.5222\n",
      "Epoch [22/300], Step [134/172], Loss: 19.8280\n",
      "Epoch [22/300], Step [135/172], Loss: 17.6599\n",
      "Epoch [22/300], Step [136/172], Loss: 19.0497\n",
      "Epoch [22/300], Step [137/172], Loss: 19.7183\n",
      "Epoch [22/300], Step [138/172], Loss: 18.1134\n",
      "Epoch [22/300], Step [139/172], Loss: 19.4400\n",
      "Epoch [22/300], Step [140/172], Loss: 19.1091\n",
      "Epoch [22/300], Step [141/172], Loss: 21.0857\n",
      "Epoch [22/300], Step [142/172], Loss: 19.9576\n",
      "Epoch [22/300], Step [143/172], Loss: 17.5799\n",
      "Epoch [22/300], Step [144/172], Loss: 16.5448\n",
      "Epoch [22/300], Step [145/172], Loss: 16.8277\n",
      "Epoch [22/300], Step [146/172], Loss: 16.9743\n",
      "Epoch [22/300], Step [147/172], Loss: 16.2988\n",
      "Epoch [22/300], Step [148/172], Loss: 16.3071\n",
      "Epoch [22/300], Step [149/172], Loss: 16.8813\n",
      "Epoch [22/300], Step [150/172], Loss: 17.2593\n",
      "Epoch [22/300], Step [151/172], Loss: 14.9976\n",
      "Epoch [22/300], Step [152/172], Loss: 15.4406\n",
      "Epoch [22/300], Step [153/172], Loss: 15.5638\n",
      "Epoch [22/300], Step [154/172], Loss: 16.7087\n",
      "Epoch [22/300], Step [155/172], Loss: 14.8812\n",
      "Epoch [22/300], Step [156/172], Loss: 15.3130\n",
      "Epoch [22/300], Step [157/172], Loss: 16.5799\n",
      "Epoch [22/300], Step [158/172], Loss: 15.4911\n",
      "Epoch [22/300], Step [159/172], Loss: 15.1037\n",
      "Epoch [22/300], Step [160/172], Loss: 15.1131\n",
      "Epoch [22/300], Step [161/172], Loss: 14.2623\n",
      "Epoch [22/300], Step [162/172], Loss: 13.8604\n",
      "Epoch [22/300], Step [163/172], Loss: 13.8498\n",
      "Epoch [22/300], Step [164/172], Loss: 14.5642\n",
      "Epoch [22/300], Step [165/172], Loss: 12.8859\n",
      "Epoch [22/300], Step [166/172], Loss: 13.9495\n",
      "Epoch [22/300], Step [167/172], Loss: 13.8792\n",
      "Epoch [22/300], Step [168/172], Loss: 13.9322\n",
      "Epoch [22/300], Step [169/172], Loss: 12.2999\n",
      "Epoch [22/300], Step [170/172], Loss: 12.7382\n",
      "Epoch [22/300], Step [171/172], Loss: 11.0964\n",
      "Epoch [22/300], Step [172/172], Loss: 8.8960\n",
      "Epoch [23/300], Step [1/172], Loss: 98.5639\n",
      "Epoch [23/300], Step [2/172], Loss: 100.7759\n",
      "Epoch [23/300], Step [3/172], Loss: 127.2356\n",
      "Epoch [23/300], Step [4/172], Loss: 84.7799\n",
      "Epoch [23/300], Step [5/172], Loss: 105.7696\n",
      "Epoch [23/300], Step [6/172], Loss: 53.6315\n",
      "Epoch [23/300], Step [7/172], Loss: 70.8653\n",
      "Epoch [23/300], Step [8/172], Loss: 44.8344\n",
      "Epoch [23/300], Step [9/172], Loss: 69.9127\n",
      "Epoch [23/300], Step [10/172], Loss: 74.2223\n",
      "Epoch [23/300], Step [11/172], Loss: 90.5813\n",
      "Epoch [23/300], Step [12/172], Loss: 86.4676\n",
      "Epoch [23/300], Step [13/172], Loss: 60.1784\n",
      "Epoch [23/300], Step [14/172], Loss: 101.6623\n",
      "Epoch [23/300], Step [15/172], Loss: 92.4497\n",
      "Epoch [23/300], Step [16/172], Loss: 74.2036\n",
      "Epoch [23/300], Step [17/172], Loss: 63.3673\n",
      "Epoch [23/300], Step [18/172], Loss: 88.1963\n",
      "Epoch [23/300], Step [19/172], Loss: 78.4005\n",
      "Epoch [23/300], Step [20/172], Loss: 128.1335\n",
      "Epoch [23/300], Step [21/172], Loss: 98.2039\n",
      "Epoch [23/300], Step [22/172], Loss: 95.0608\n",
      "Epoch [23/300], Step [23/172], Loss: 63.4597\n",
      "Epoch [23/300], Step [24/172], Loss: 90.2108\n",
      "Epoch [23/300], Step [25/172], Loss: 62.5483\n",
      "Epoch [23/300], Step [26/172], Loss: 68.3115\n",
      "Epoch [23/300], Step [27/172], Loss: 91.3383\n",
      "Epoch [23/300], Step [28/172], Loss: 85.1531\n",
      "Epoch [23/300], Step [29/172], Loss: 102.3124\n",
      "Epoch [23/300], Step [30/172], Loss: 84.2516\n",
      "Epoch [23/300], Step [31/172], Loss: 59.9923\n",
      "Epoch [23/300], Step [32/172], Loss: 48.7042\n",
      "Epoch [23/300], Step [33/172], Loss: 84.0400\n",
      "Epoch [23/300], Step [34/172], Loss: 37.1364\n",
      "Epoch [23/300], Step [35/172], Loss: 73.9125\n",
      "Epoch [23/300], Step [36/172], Loss: 39.3484\n",
      "Epoch [23/300], Step [37/172], Loss: 27.4302\n",
      "Epoch [23/300], Step [38/172], Loss: 45.9521\n",
      "Epoch [23/300], Step [39/172], Loss: 59.4922\n",
      "Epoch [23/300], Step [40/172], Loss: 44.7236\n",
      "Epoch [23/300], Step [41/172], Loss: 49.2082\n",
      "Epoch [23/300], Step [42/172], Loss: 48.6626\n",
      "Epoch [23/300], Step [43/172], Loss: 43.0818\n",
      "Epoch [23/300], Step [44/172], Loss: 43.5242\n",
      "Epoch [23/300], Step [45/172], Loss: 33.5528\n",
      "Epoch [23/300], Step [46/172], Loss: 57.4604\n",
      "Epoch [23/300], Step [47/172], Loss: 77.1034\n",
      "Epoch [23/300], Step [48/172], Loss: 86.7714\n",
      "Epoch [23/300], Step [49/172], Loss: 30.9784\n",
      "Epoch [23/300], Step [50/172], Loss: 67.7369\n",
      "Epoch [23/300], Step [51/172], Loss: 13.1291\n",
      "Epoch [23/300], Step [52/172], Loss: 31.3566\n",
      "Epoch [23/300], Step [53/172], Loss: 39.1243\n",
      "Epoch [23/300], Step [54/172], Loss: 29.8818\n",
      "Epoch [23/300], Step [55/172], Loss: 25.5300\n",
      "Epoch [23/300], Step [56/172], Loss: 18.1056\n",
      "Epoch [23/300], Step [57/172], Loss: 59.6426\n",
      "Epoch [23/300], Step [58/172], Loss: 25.4727\n",
      "Epoch [23/300], Step [59/172], Loss: 45.6007\n",
      "Epoch [23/300], Step [60/172], Loss: 79.8996\n",
      "Epoch [23/300], Step [61/172], Loss: 17.6577\n",
      "Epoch [23/300], Step [62/172], Loss: 23.3700\n",
      "Epoch [23/300], Step [63/172], Loss: 14.3617\n",
      "Epoch [23/300], Step [64/172], Loss: 8.8402\n",
      "Epoch [23/300], Step [65/172], Loss: 32.9568\n",
      "Epoch [23/300], Step [66/172], Loss: 15.0497\n",
      "Epoch [23/300], Step [67/172], Loss: 34.8228\n",
      "Epoch [23/300], Step [68/172], Loss: 34.2275\n",
      "Epoch [23/300], Step [69/172], Loss: 83.8513\n",
      "Epoch [23/300], Step [70/172], Loss: 72.9323\n",
      "Epoch [23/300], Step [71/172], Loss: 71.4771\n",
      "Epoch [23/300], Step [72/172], Loss: 67.6433\n",
      "Epoch [23/300], Step [73/172], Loss: 71.9439\n",
      "Epoch [23/300], Step [74/172], Loss: 57.8267\n",
      "Epoch [23/300], Step [75/172], Loss: 35.5251\n",
      "Epoch [23/300], Step [76/172], Loss: 54.3036\n",
      "Epoch [23/300], Step [77/172], Loss: 60.6153\n",
      "Epoch [23/300], Step [78/172], Loss: 63.9591\n",
      "Epoch [23/300], Step [79/172], Loss: 53.9366\n",
      "Epoch [23/300], Step [80/172], Loss: 58.1961\n",
      "Epoch [23/300], Step [81/172], Loss: 52.3817\n",
      "Epoch [23/300], Step [82/172], Loss: 49.5389\n",
      "Epoch [23/300], Step [83/172], Loss: 52.0314\n",
      "Epoch [23/300], Step [84/172], Loss: 49.0664\n",
      "Epoch [23/300], Step [85/172], Loss: 58.3191\n",
      "Epoch [23/300], Step [86/172], Loss: 43.5584\n",
      "Epoch [23/300], Step [87/172], Loss: 36.8802\n",
      "Epoch [23/300], Step [88/172], Loss: 40.2071\n",
      "Epoch [23/300], Step [89/172], Loss: 41.7079\n",
      "Epoch [23/300], Step [90/172], Loss: 37.1416\n",
      "Epoch [23/300], Step [91/172], Loss: 36.8089\n",
      "Epoch [23/300], Step [92/172], Loss: 31.8678\n",
      "Epoch [23/300], Step [93/172], Loss: 35.0560\n",
      "Epoch [23/300], Step [94/172], Loss: 36.9739\n",
      "Epoch [23/300], Step [95/172], Loss: 34.7915\n",
      "Epoch [23/300], Step [96/172], Loss: 32.0334\n",
      "Epoch [23/300], Step [97/172], Loss: 35.2898\n",
      "Epoch [23/300], Step [98/172], Loss: 32.4081\n",
      "Epoch [23/300], Step [99/172], Loss: 30.3634\n",
      "Epoch [23/300], Step [100/172], Loss: 30.3229\n",
      "Epoch [23/300], Step [101/172], Loss: 30.2565\n",
      "Epoch [23/300], Step [102/172], Loss: 28.7778\n",
      "Epoch [23/300], Step [103/172], Loss: 29.6565\n",
      "Epoch [23/300], Step [104/172], Loss: 28.8304\n",
      "Epoch [23/300], Step [105/172], Loss: 28.3190\n",
      "Epoch [23/300], Step [106/172], Loss: 28.1492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/300], Step [107/172], Loss: 24.4614\n",
      "Epoch [23/300], Step [108/172], Loss: 29.2417\n",
      "Epoch [23/300], Step [109/172], Loss: 28.6011\n",
      "Epoch [23/300], Step [110/172], Loss: 27.1858\n",
      "Epoch [23/300], Step [111/172], Loss: 26.0596\n",
      "Epoch [23/300], Step [112/172], Loss: 31.3562\n",
      "Epoch [23/300], Step [113/172], Loss: 25.4398\n",
      "Epoch [23/300], Step [114/172], Loss: 26.5001\n",
      "Epoch [23/300], Step [115/172], Loss: 31.8149\n",
      "Epoch [23/300], Step [116/172], Loss: 26.3633\n",
      "Epoch [23/300], Step [117/172], Loss: 23.1913\n",
      "Epoch [23/300], Step [118/172], Loss: 25.4123\n",
      "Epoch [23/300], Step [119/172], Loss: 22.9035\n",
      "Epoch [23/300], Step [120/172], Loss: 22.2291\n",
      "Epoch [23/300], Step [121/172], Loss: 23.1711\n",
      "Epoch [23/300], Step [122/172], Loss: 19.7895\n",
      "Epoch [23/300], Step [123/172], Loss: 20.8694\n",
      "Epoch [23/300], Step [124/172], Loss: 19.8319\n",
      "Epoch [23/300], Step [125/172], Loss: 22.7319\n",
      "Epoch [23/300], Step [126/172], Loss: 22.4180\n",
      "Epoch [23/300], Step [127/172], Loss: 23.9201\n",
      "Epoch [23/300], Step [128/172], Loss: 24.6445\n",
      "Epoch [23/300], Step [129/172], Loss: 19.6100\n",
      "Epoch [23/300], Step [130/172], Loss: 21.4678\n",
      "Epoch [23/300], Step [131/172], Loss: 19.4864\n",
      "Epoch [23/300], Step [132/172], Loss: 19.6729\n",
      "Epoch [23/300], Step [133/172], Loss: 19.1731\n",
      "Epoch [23/300], Step [134/172], Loss: 19.4964\n",
      "Epoch [23/300], Step [135/172], Loss: 17.3364\n",
      "Epoch [23/300], Step [136/172], Loss: 18.7018\n",
      "Epoch [23/300], Step [137/172], Loss: 19.4136\n",
      "Epoch [23/300], Step [138/172], Loss: 17.7646\n",
      "Epoch [23/300], Step [139/172], Loss: 19.1338\n",
      "Epoch [23/300], Step [140/172], Loss: 18.8268\n",
      "Epoch [23/300], Step [141/172], Loss: 20.8783\n",
      "Epoch [23/300], Step [142/172], Loss: 19.6946\n",
      "Epoch [23/300], Step [143/172], Loss: 17.2965\n",
      "Epoch [23/300], Step [144/172], Loss: 16.2605\n",
      "Epoch [23/300], Step [145/172], Loss: 16.5443\n",
      "Epoch [23/300], Step [146/172], Loss: 16.7298\n",
      "Epoch [23/300], Step [147/172], Loss: 15.8832\n",
      "Epoch [23/300], Step [148/172], Loss: 15.9577\n",
      "Epoch [23/300], Step [149/172], Loss: 16.6284\n",
      "Epoch [23/300], Step [150/172], Loss: 16.9551\n",
      "Epoch [23/300], Step [151/172], Loss: 14.7184\n",
      "Epoch [23/300], Step [152/172], Loss: 15.1733\n",
      "Epoch [23/300], Step [153/172], Loss: 15.2670\n",
      "Epoch [23/300], Step [154/172], Loss: 16.4431\n",
      "Epoch [23/300], Step [155/172], Loss: 14.5990\n",
      "Epoch [23/300], Step [156/172], Loss: 15.0785\n",
      "Epoch [23/300], Step [157/172], Loss: 16.3620\n",
      "Epoch [23/300], Step [158/172], Loss: 15.2233\n",
      "Epoch [23/300], Step [159/172], Loss: 14.8476\n",
      "Epoch [23/300], Step [160/172], Loss: 14.8321\n",
      "Epoch [23/300], Step [161/172], Loss: 13.9836\n",
      "Epoch [23/300], Step [162/172], Loss: 13.6077\n",
      "Epoch [23/300], Step [163/172], Loss: 13.5845\n",
      "Epoch [23/300], Step [164/172], Loss: 14.3479\n",
      "Epoch [23/300], Step [165/172], Loss: 12.6552\n",
      "Epoch [23/300], Step [166/172], Loss: 13.6662\n",
      "Epoch [23/300], Step [167/172], Loss: 13.6281\n",
      "Epoch [23/300], Step [168/172], Loss: 13.6491\n",
      "Epoch [23/300], Step [169/172], Loss: 12.0353\n",
      "Epoch [23/300], Step [170/172], Loss: 12.4774\n",
      "Epoch [23/300], Step [171/172], Loss: 10.8763\n",
      "Epoch [23/300], Step [172/172], Loss: 8.7438\n",
      "Epoch [24/300], Step [1/172], Loss: 98.6254\n",
      "Epoch [24/300], Step [2/172], Loss: 100.7351\n",
      "Epoch [24/300], Step [3/172], Loss: 127.0443\n",
      "Epoch [24/300], Step [4/172], Loss: 84.0738\n",
      "Epoch [24/300], Step [5/172], Loss: 104.9800\n",
      "Epoch [24/300], Step [6/172], Loss: 52.7618\n",
      "Epoch [24/300], Step [7/172], Loss: 69.7905\n",
      "Epoch [24/300], Step [8/172], Loss: 43.0055\n",
      "Epoch [24/300], Step [9/172], Loss: 68.8255\n",
      "Epoch [24/300], Step [10/172], Loss: 73.4102\n",
      "Epoch [24/300], Step [11/172], Loss: 91.1330\n",
      "Epoch [24/300], Step [12/172], Loss: 86.7223\n",
      "Epoch [24/300], Step [13/172], Loss: 59.5900\n",
      "Epoch [24/300], Step [14/172], Loss: 101.4565\n",
      "Epoch [24/300], Step [15/172], Loss: 92.0742\n",
      "Epoch [24/300], Step [16/172], Loss: 74.0844\n",
      "Epoch [24/300], Step [17/172], Loss: 63.1019\n",
      "Epoch [24/300], Step [18/172], Loss: 87.9379\n",
      "Epoch [24/300], Step [19/172], Loss: 78.3176\n",
      "Epoch [24/300], Step [20/172], Loss: 127.1565\n",
      "Epoch [24/300], Step [21/172], Loss: 98.0105\n",
      "Epoch [24/300], Step [22/172], Loss: 94.8613\n",
      "Epoch [24/300], Step [23/172], Loss: 61.8751\n",
      "Epoch [24/300], Step [24/172], Loss: 90.0182\n",
      "Epoch [24/300], Step [25/172], Loss: 62.0031\n",
      "Epoch [24/300], Step [26/172], Loss: 67.9604\n",
      "Epoch [24/300], Step [27/172], Loss: 91.1220\n",
      "Epoch [24/300], Step [28/172], Loss: 85.0025\n",
      "Epoch [24/300], Step [29/172], Loss: 101.8931\n",
      "Epoch [24/300], Step [30/172], Loss: 83.8287\n",
      "Epoch [24/300], Step [31/172], Loss: 59.3203\n",
      "Epoch [24/300], Step [32/172], Loss: 47.9947\n",
      "Epoch [24/300], Step [33/172], Loss: 83.6480\n",
      "Epoch [24/300], Step [34/172], Loss: 35.3222\n",
      "Epoch [24/300], Step [35/172], Loss: 73.6078\n",
      "Epoch [24/300], Step [36/172], Loss: 39.0251\n",
      "Epoch [24/300], Step [37/172], Loss: 27.1192\n",
      "Epoch [24/300], Step [38/172], Loss: 45.3465\n",
      "Epoch [24/300], Step [39/172], Loss: 59.2861\n",
      "Epoch [24/300], Step [40/172], Loss: 44.0003\n",
      "Epoch [24/300], Step [41/172], Loss: 48.8551\n",
      "Epoch [24/300], Step [42/172], Loss: 48.2064\n",
      "Epoch [24/300], Step [43/172], Loss: 42.5280\n",
      "Epoch [24/300], Step [44/172], Loss: 42.5039\n",
      "Epoch [24/300], Step [45/172], Loss: 32.8235\n",
      "Epoch [24/300], Step [46/172], Loss: 56.9306\n",
      "Epoch [24/300], Step [47/172], Loss: 76.7649\n",
      "Epoch [24/300], Step [48/172], Loss: 85.9927\n",
      "Epoch [24/300], Step [49/172], Loss: 30.1934\n",
      "Epoch [24/300], Step [50/172], Loss: 67.0814\n",
      "Epoch [24/300], Step [51/172], Loss: 12.7121\n",
      "Epoch [24/300], Step [52/172], Loss: 30.9050\n",
      "Epoch [24/300], Step [53/172], Loss: 38.5516\n",
      "Epoch [24/300], Step [54/172], Loss: 29.4109\n",
      "Epoch [24/300], Step [55/172], Loss: 24.9509\n",
      "Epoch [24/300], Step [56/172], Loss: 17.6356\n",
      "Epoch [24/300], Step [57/172], Loss: 59.0457\n",
      "Epoch [24/300], Step [58/172], Loss: 25.0821\n",
      "Epoch [24/300], Step [59/172], Loss: 45.0631\n",
      "Epoch [24/300], Step [60/172], Loss: 78.9372\n",
      "Epoch [24/300], Step [61/172], Loss: 17.2801\n",
      "Epoch [24/300], Step [62/172], Loss: 23.0243\n",
      "Epoch [24/300], Step [63/172], Loss: 13.9562\n",
      "Epoch [24/300], Step [64/172], Loss: 8.6660\n",
      "Epoch [24/300], Step [65/172], Loss: 32.8155\n",
      "Epoch [24/300], Step [66/172], Loss: 14.8683\n",
      "Epoch [24/300], Step [67/172], Loss: 34.4738\n",
      "Epoch [24/300], Step [68/172], Loss: 33.5543\n",
      "Epoch [24/300], Step [69/172], Loss: 83.8316\n",
      "Epoch [24/300], Step [70/172], Loss: 72.1939\n",
      "Epoch [24/300], Step [71/172], Loss: 70.8242\n",
      "Epoch [24/300], Step [72/172], Loss: 67.0647\n",
      "Epoch [24/300], Step [73/172], Loss: 71.4460\n",
      "Epoch [24/300], Step [74/172], Loss: 56.8945\n",
      "Epoch [24/300], Step [75/172], Loss: 34.8598\n",
      "Epoch [24/300], Step [76/172], Loss: 53.6076\n",
      "Epoch [24/300], Step [77/172], Loss: 59.9801\n",
      "Epoch [24/300], Step [78/172], Loss: 63.4446\n",
      "Epoch [24/300], Step [79/172], Loss: 53.3684\n",
      "Epoch [24/300], Step [80/172], Loss: 57.7213\n",
      "Epoch [24/300], Step [81/172], Loss: 52.1339\n",
      "Epoch [24/300], Step [82/172], Loss: 49.1498\n",
      "Epoch [24/300], Step [83/172], Loss: 51.7704\n",
      "Epoch [24/300], Step [84/172], Loss: 48.5802\n",
      "Epoch [24/300], Step [85/172], Loss: 57.9446\n",
      "Epoch [24/300], Step [86/172], Loss: 43.2548\n",
      "Epoch [24/300], Step [87/172], Loss: 36.5680\n",
      "Epoch [24/300], Step [88/172], Loss: 39.8700\n",
      "Epoch [24/300], Step [89/172], Loss: 41.2341\n",
      "Epoch [24/300], Step [90/172], Loss: 36.9113\n",
      "Epoch [24/300], Step [91/172], Loss: 36.4516\n",
      "Epoch [24/300], Step [92/172], Loss: 31.5405\n",
      "Epoch [24/300], Step [93/172], Loss: 34.7414\n",
      "Epoch [24/300], Step [94/172], Loss: 36.7007\n",
      "Epoch [24/300], Step [95/172], Loss: 34.4871\n",
      "Epoch [24/300], Step [96/172], Loss: 31.6692\n",
      "Epoch [24/300], Step [97/172], Loss: 34.9860\n",
      "Epoch [24/300], Step [98/172], Loss: 32.0613\n",
      "Epoch [24/300], Step [99/172], Loss: 29.9901\n",
      "Epoch [24/300], Step [100/172], Loss: 29.8993\n",
      "Epoch [24/300], Step [101/172], Loss: 29.9934\n",
      "Epoch [24/300], Step [102/172], Loss: 28.4166\n",
      "Epoch [24/300], Step [103/172], Loss: 29.2759\n",
      "Epoch [24/300], Step [104/172], Loss: 28.4927\n",
      "Epoch [24/300], Step [105/172], Loss: 28.0056\n",
      "Epoch [24/300], Step [106/172], Loss: 27.8443\n",
      "Epoch [24/300], Step [107/172], Loss: 24.1629\n",
      "Epoch [24/300], Step [108/172], Loss: 28.9055\n",
      "Epoch [24/300], Step [109/172], Loss: 28.3921\n",
      "Epoch [24/300], Step [110/172], Loss: 26.8171\n",
      "Epoch [24/300], Step [111/172], Loss: 25.7462\n",
      "Epoch [24/300], Step [112/172], Loss: 31.0965\n",
      "Epoch [24/300], Step [113/172], Loss: 25.1805\n",
      "Epoch [24/300], Step [114/172], Loss: 26.2174\n",
      "Epoch [24/300], Step [115/172], Loss: 31.7515\n",
      "Epoch [24/300], Step [116/172], Loss: 26.0835\n",
      "Epoch [24/300], Step [117/172], Loss: 22.9065\n",
      "Epoch [24/300], Step [118/172], Loss: 25.1820\n",
      "Epoch [24/300], Step [119/172], Loss: 22.6341\n",
      "Epoch [24/300], Step [120/172], Loss: 21.9064\n",
      "Epoch [24/300], Step [121/172], Loss: 22.8569\n",
      "Epoch [24/300], Step [122/172], Loss: 19.5227\n",
      "Epoch [24/300], Step [123/172], Loss: 20.6068\n",
      "Epoch [24/300], Step [124/172], Loss: 19.5060\n",
      "Epoch [24/300], Step [125/172], Loss: 22.5068\n",
      "Epoch [24/300], Step [126/172], Loss: 22.1281\n",
      "Epoch [24/300], Step [127/172], Loss: 23.6736\n",
      "Epoch [24/300], Step [128/172], Loss: 24.4198\n",
      "Epoch [24/300], Step [129/172], Loss: 19.3284\n",
      "Epoch [24/300], Step [130/172], Loss: 21.2198\n",
      "Epoch [24/300], Step [131/172], Loss: 19.2197\n",
      "Epoch [24/300], Step [132/172], Loss: 19.4156\n",
      "Epoch [24/300], Step [133/172], Loss: 18.9266\n",
      "Epoch [24/300], Step [134/172], Loss: 19.2606\n",
      "Epoch [24/300], Step [135/172], Loss: 17.1027\n",
      "Epoch [24/300], Step [136/172], Loss: 18.4387\n",
      "Epoch [24/300], Step [137/172], Loss: 19.1893\n",
      "Epoch [24/300], Step [138/172], Loss: 17.5095\n",
      "Epoch [24/300], Step [139/172], Loss: 18.8937\n",
      "Epoch [24/300], Step [140/172], Loss: 18.5991\n",
      "Epoch [24/300], Step [141/172], Loss: 20.7365\n",
      "Epoch [24/300], Step [142/172], Loss: 19.5081\n",
      "Epoch [24/300], Step [143/172], Loss: 17.0625\n",
      "Epoch [24/300], Step [144/172], Loss: 16.0542\n",
      "Epoch [24/300], Step [145/172], Loss: 16.3339\n",
      "Epoch [24/300], Step [146/172], Loss: 16.5457\n",
      "Epoch [24/300], Step [147/172], Loss: 15.6112\n",
      "Epoch [24/300], Step [148/172], Loss: 15.6980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/300], Step [149/172], Loss: 16.4544\n",
      "Epoch [24/300], Step [150/172], Loss: 16.7515\n",
      "Epoch [24/300], Step [151/172], Loss: 14.5220\n",
      "Epoch [24/300], Step [152/172], Loss: 14.9747\n",
      "Epoch [24/300], Step [153/172], Loss: 15.0608\n",
      "Epoch [24/300], Step [154/172], Loss: 16.2542\n",
      "Epoch [24/300], Step [155/172], Loss: 14.4101\n",
      "Epoch [24/300], Step [156/172], Loss: 14.9292\n",
      "Epoch [24/300], Step [157/172], Loss: 16.2173\n",
      "Epoch [24/300], Step [158/172], Loss: 15.0343\n",
      "Epoch [24/300], Step [159/172], Loss: 14.7098\n",
      "Epoch [24/300], Step [160/172], Loss: 14.6638\n",
      "Epoch [24/300], Step [161/172], Loss: 13.8043\n",
      "Epoch [24/300], Step [162/172], Loss: 13.4437\n",
      "Epoch [24/300], Step [163/172], Loss: 13.4245\n",
      "Epoch [24/300], Step [164/172], Loss: 14.2340\n",
      "Epoch [24/300], Step [165/172], Loss: 12.4863\n",
      "Epoch [24/300], Step [166/172], Loss: 13.5039\n",
      "Epoch [24/300], Step [167/172], Loss: 13.4894\n",
      "Epoch [24/300], Step [168/172], Loss: 13.4734\n",
      "Epoch [24/300], Step [169/172], Loss: 11.8869\n",
      "Epoch [24/300], Step [170/172], Loss: 12.3495\n",
      "Epoch [24/300], Step [171/172], Loss: 10.7281\n",
      "Epoch [24/300], Step [172/172], Loss: 8.6563\n",
      "Epoch [25/300], Step [1/172], Loss: 98.8865\n",
      "Epoch [25/300], Step [2/172], Loss: 100.6593\n",
      "Epoch [25/300], Step [3/172], Loss: 126.6966\n",
      "Epoch [25/300], Step [4/172], Loss: 83.4930\n",
      "Epoch [25/300], Step [5/172], Loss: 104.1540\n",
      "Epoch [25/300], Step [6/172], Loss: 51.8715\n",
      "Epoch [25/300], Step [7/172], Loss: 68.9185\n",
      "Epoch [25/300], Step [8/172], Loss: 41.2804\n",
      "Epoch [25/300], Step [9/172], Loss: 67.8504\n",
      "Epoch [25/300], Step [10/172], Loss: 72.7235\n",
      "Epoch [25/300], Step [11/172], Loss: 91.3563\n",
      "Epoch [25/300], Step [12/172], Loss: 86.4038\n",
      "Epoch [25/300], Step [13/172], Loss: 59.1218\n",
      "Epoch [25/300], Step [14/172], Loss: 101.3369\n",
      "Epoch [25/300], Step [15/172], Loss: 91.9227\n",
      "Epoch [25/300], Step [16/172], Loss: 74.0811\n",
      "Epoch [25/300], Step [17/172], Loss: 62.8050\n",
      "Epoch [25/300], Step [18/172], Loss: 87.5804\n",
      "Epoch [25/300], Step [19/172], Loss: 78.3853\n",
      "Epoch [25/300], Step [20/172], Loss: 126.2487\n",
      "Epoch [25/300], Step [21/172], Loss: 98.0147\n",
      "Epoch [25/300], Step [22/172], Loss: 94.8146\n",
      "Epoch [25/300], Step [23/172], Loss: 60.6012\n",
      "Epoch [25/300], Step [24/172], Loss: 89.8132\n",
      "Epoch [25/300], Step [25/172], Loss: 61.6285\n",
      "Epoch [25/300], Step [26/172], Loss: 67.6981\n",
      "Epoch [25/300], Step [27/172], Loss: 90.9739\n",
      "Epoch [25/300], Step [28/172], Loss: 84.9558\n",
      "Epoch [25/300], Step [29/172], Loss: 101.4672\n",
      "Epoch [25/300], Step [30/172], Loss: 83.4600\n",
      "Epoch [25/300], Step [31/172], Loss: 58.7655\n",
      "Epoch [25/300], Step [32/172], Loss: 47.3759\n",
      "Epoch [25/300], Step [33/172], Loss: 83.2318\n",
      "Epoch [25/300], Step [34/172], Loss: 33.5274\n",
      "Epoch [25/300], Step [35/172], Loss: 73.0987\n",
      "Epoch [25/300], Step [36/172], Loss: 38.5700\n",
      "Epoch [25/300], Step [37/172], Loss: 26.7336\n",
      "Epoch [25/300], Step [38/172], Loss: 44.7371\n",
      "Epoch [25/300], Step [39/172], Loss: 59.0628\n",
      "Epoch [25/300], Step [40/172], Loss: 43.3088\n",
      "Epoch [25/300], Step [41/172], Loss: 48.5045\n",
      "Epoch [25/300], Step [42/172], Loss: 47.7510\n",
      "Epoch [25/300], Step [43/172], Loss: 41.9451\n",
      "Epoch [25/300], Step [44/172], Loss: 41.5089\n",
      "Epoch [25/300], Step [45/172], Loss: 32.0124\n",
      "Epoch [25/300], Step [46/172], Loss: 56.2974\n",
      "Epoch [25/300], Step [47/172], Loss: 76.3574\n",
      "Epoch [25/300], Step [48/172], Loss: 85.1972\n",
      "Epoch [25/300], Step [49/172], Loss: 29.5372\n",
      "Epoch [25/300], Step [50/172], Loss: 66.5458\n",
      "Epoch [25/300], Step [51/172], Loss: 12.2165\n",
      "Epoch [25/300], Step [52/172], Loss: 30.4251\n",
      "Epoch [25/300], Step [53/172], Loss: 37.7535\n",
      "Epoch [25/300], Step [54/172], Loss: 28.6626\n",
      "Epoch [25/300], Step [55/172], Loss: 24.1988\n",
      "Epoch [25/300], Step [56/172], Loss: 16.9126\n",
      "Epoch [25/300], Step [57/172], Loss: 58.4965\n",
      "Epoch [25/300], Step [58/172], Loss: 24.7526\n",
      "Epoch [25/300], Step [59/172], Loss: 44.4313\n",
      "Epoch [25/300], Step [60/172], Loss: 78.1400\n",
      "Epoch [25/300], Step [61/172], Loss: 16.7361\n",
      "Epoch [25/300], Step [62/172], Loss: 22.5233\n",
      "Epoch [25/300], Step [63/172], Loss: 13.3926\n",
      "Epoch [25/300], Step [64/172], Loss: 8.3672\n",
      "Epoch [25/300], Step [65/172], Loss: 32.5795\n",
      "Epoch [25/300], Step [66/172], Loss: 14.4246\n",
      "Epoch [25/300], Step [67/172], Loss: 33.8666\n",
      "Epoch [25/300], Step [68/172], Loss: 32.2027\n",
      "Epoch [25/300], Step [69/172], Loss: 84.0116\n",
      "Epoch [25/300], Step [70/172], Loss: 72.2539\n",
      "Epoch [25/300], Step [71/172], Loss: 71.0064\n",
      "Epoch [25/300], Step [72/172], Loss: 67.2495\n",
      "Epoch [25/300], Step [73/172], Loss: 71.7505\n",
      "Epoch [25/300], Step [74/172], Loss: 56.5876\n",
      "Epoch [25/300], Step [75/172], Loss: 34.6347\n",
      "Epoch [25/300], Step [76/172], Loss: 53.4272\n",
      "Epoch [25/300], Step [77/172], Loss: 59.9593\n",
      "Epoch [25/300], Step [78/172], Loss: 63.4308\n",
      "Epoch [25/300], Step [79/172], Loss: 53.2572\n",
      "Epoch [25/300], Step [80/172], Loss: 57.6585\n",
      "Epoch [25/300], Step [81/172], Loss: 52.2339\n",
      "Epoch [25/300], Step [82/172], Loss: 48.9324\n",
      "Epoch [25/300], Step [83/172], Loss: 51.7147\n",
      "Epoch [25/300], Step [84/172], Loss: 48.2699\n",
      "Epoch [25/300], Step [85/172], Loss: 57.7047\n",
      "Epoch [25/300], Step [86/172], Loss: 43.0267\n",
      "Epoch [25/300], Step [87/172], Loss: 36.3221\n",
      "Epoch [25/300], Step [88/172], Loss: 39.5559\n",
      "Epoch [25/300], Step [89/172], Loss: 40.8281\n",
      "Epoch [25/300], Step [90/172], Loss: 36.7436\n",
      "Epoch [25/300], Step [91/172], Loss: 36.1407\n",
      "Epoch [25/300], Step [92/172], Loss: 31.2593\n",
      "Epoch [25/300], Step [93/172], Loss: 34.4877\n",
      "Epoch [25/300], Step [94/172], Loss: 36.4315\n",
      "Epoch [25/300], Step [95/172], Loss: 34.2101\n",
      "Epoch [25/300], Step [96/172], Loss: 31.3403\n",
      "Epoch [25/300], Step [97/172], Loss: 34.7092\n",
      "Epoch [25/300], Step [98/172], Loss: 31.7331\n",
      "Epoch [25/300], Step [99/172], Loss: 29.6143\n",
      "Epoch [25/300], Step [100/172], Loss: 29.4724\n",
      "Epoch [25/300], Step [101/172], Loss: 29.7531\n",
      "Epoch [25/300], Step [102/172], Loss: 28.0135\n",
      "Epoch [25/300], Step [103/172], Loss: 28.8771\n",
      "Epoch [25/300], Step [104/172], Loss: 28.1693\n",
      "Epoch [25/300], Step [105/172], Loss: 27.6672\n",
      "Epoch [25/300], Step [106/172], Loss: 27.5642\n",
      "Epoch [25/300], Step [107/172], Loss: 23.8839\n",
      "Epoch [25/300], Step [108/172], Loss: 28.5836\n",
      "Epoch [25/300], Step [109/172], Loss: 28.1876\n",
      "Epoch [25/300], Step [110/172], Loss: 26.4583\n",
      "Epoch [25/300], Step [111/172], Loss: 25.4555\n",
      "Epoch [25/300], Step [112/172], Loss: 30.8297\n",
      "Epoch [25/300], Step [113/172], Loss: 24.9232\n",
      "Epoch [25/300], Step [114/172], Loss: 25.9499\n",
      "Epoch [25/300], Step [115/172], Loss: 31.6825\n",
      "Epoch [25/300], Step [116/172], Loss: 25.8074\n",
      "Epoch [25/300], Step [117/172], Loss: 22.6093\n",
      "Epoch [25/300], Step [118/172], Loss: 24.9521\n",
      "Epoch [25/300], Step [119/172], Loss: 22.3699\n",
      "Epoch [25/300], Step [120/172], Loss: 21.5833\n",
      "Epoch [25/300], Step [121/172], Loss: 22.5301\n",
      "Epoch [25/300], Step [122/172], Loss: 19.2518\n",
      "Epoch [25/300], Step [123/172], Loss: 20.3375\n",
      "Epoch [25/300], Step [124/172], Loss: 19.1652\n",
      "Epoch [25/300], Step [125/172], Loss: 22.2372\n",
      "Epoch [25/300], Step [126/172], Loss: 21.8445\n",
      "Epoch [25/300], Step [127/172], Loss: 23.4592\n",
      "Epoch [25/300], Step [128/172], Loss: 24.2206\n",
      "Epoch [25/300], Step [129/172], Loss: 19.0476\n",
      "Epoch [25/300], Step [130/172], Loss: 20.9651\n",
      "Epoch [25/300], Step [131/172], Loss: 18.9588\n",
      "Epoch [25/300], Step [132/172], Loss: 19.1512\n",
      "Epoch [25/300], Step [133/172], Loss: 18.6545\n",
      "Epoch [25/300], Step [134/172], Loss: 19.0124\n",
      "Epoch [25/300], Step [135/172], Loss: 16.8584\n",
      "Epoch [25/300], Step [136/172], Loss: 18.1767\n",
      "Epoch [25/300], Step [137/172], Loss: 18.9778\n",
      "Epoch [25/300], Step [138/172], Loss: 17.2656\n",
      "Epoch [25/300], Step [139/172], Loss: 18.6653\n",
      "Epoch [25/300], Step [140/172], Loss: 18.3945\n",
      "Epoch [25/300], Step [141/172], Loss: 20.5986\n",
      "Epoch [25/300], Step [142/172], Loss: 19.3223\n",
      "Epoch [25/300], Step [143/172], Loss: 16.8524\n",
      "Epoch [25/300], Step [144/172], Loss: 15.8393\n",
      "Epoch [25/300], Step [145/172], Loss: 16.1223\n",
      "Epoch [25/300], Step [146/172], Loss: 16.3657\n",
      "Epoch [25/300], Step [147/172], Loss: 15.2710\n",
      "Epoch [25/300], Step [148/172], Loss: 15.4087\n",
      "Epoch [25/300], Step [149/172], Loss: 16.2750\n",
      "Epoch [25/300], Step [150/172], Loss: 16.5306\n",
      "Epoch [25/300], Step [151/172], Loss: 14.3071\n",
      "Epoch [25/300], Step [152/172], Loss: 14.7763\n",
      "Epoch [25/300], Step [153/172], Loss: 14.8399\n",
      "Epoch [25/300], Step [154/172], Loss: 16.0589\n",
      "Epoch [25/300], Step [155/172], Loss: 14.2047\n",
      "Epoch [25/300], Step [156/172], Loss: 14.7693\n",
      "Epoch [25/300], Step [157/172], Loss: 16.0661\n",
      "Epoch [25/300], Step [158/172], Loss: 14.8388\n",
      "Epoch [25/300], Step [159/172], Loss: 14.5208\n",
      "Epoch [25/300], Step [160/172], Loss: 14.4540\n",
      "Epoch [25/300], Step [161/172], Loss: 13.5835\n",
      "Epoch [25/300], Step [162/172], Loss: 13.2557\n",
      "Epoch [25/300], Step [163/172], Loss: 13.2162\n",
      "Epoch [25/300], Step [164/172], Loss: 14.0693\n",
      "Epoch [25/300], Step [165/172], Loss: 12.3089\n",
      "Epoch [25/300], Step [166/172], Loss: 13.2780\n",
      "Epoch [25/300], Step [167/172], Loss: 13.2867\n",
      "Epoch [25/300], Step [168/172], Loss: 13.2333\n",
      "Epoch [25/300], Step [169/172], Loss: 11.6756\n",
      "Epoch [25/300], Step [170/172], Loss: 12.1204\n",
      "Epoch [25/300], Step [171/172], Loss: 10.5235\n",
      "Epoch [25/300], Step [172/172], Loss: 8.5259\n",
      "Epoch [26/300], Step [1/172], Loss: 98.8832\n",
      "Epoch [26/300], Step [2/172], Loss: 100.5571\n",
      "Epoch [26/300], Step [3/172], Loss: 126.4538\n",
      "Epoch [26/300], Step [4/172], Loss: 82.8483\n",
      "Epoch [26/300], Step [5/172], Loss: 103.3518\n",
      "Epoch [26/300], Step [6/172], Loss: 50.9883\n",
      "Epoch [26/300], Step [7/172], Loss: 67.8017\n",
      "Epoch [26/300], Step [8/172], Loss: 39.5941\n",
      "Epoch [26/300], Step [9/172], Loss: 66.8778\n",
      "Epoch [26/300], Step [10/172], Loss: 71.9978\n",
      "Epoch [26/300], Step [11/172], Loss: 91.9051\n",
      "Epoch [26/300], Step [12/172], Loss: 86.5996\n",
      "Epoch [26/300], Step [13/172], Loss: 58.6781\n",
      "Epoch [26/300], Step [14/172], Loss: 101.2466\n",
      "Epoch [26/300], Step [15/172], Loss: 91.7166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/300], Step [16/172], Loss: 74.1661\n",
      "Epoch [26/300], Step [17/172], Loss: 62.7245\n",
      "Epoch [26/300], Step [18/172], Loss: 87.3431\n",
      "Epoch [26/300], Step [19/172], Loss: 78.5237\n",
      "Epoch [26/300], Step [20/172], Loss: 125.6054\n",
      "Epoch [26/300], Step [21/172], Loss: 98.0132\n",
      "Epoch [26/300], Step [22/172], Loss: 94.8909\n",
      "Epoch [26/300], Step [23/172], Loss: 59.1463\n",
      "Epoch [26/300], Step [24/172], Loss: 89.6349\n",
      "Epoch [26/300], Step [25/172], Loss: 61.3751\n",
      "Epoch [26/300], Step [26/172], Loss: 67.5864\n",
      "Epoch [26/300], Step [27/172], Loss: 90.9742\n",
      "Epoch [26/300], Step [28/172], Loss: 84.8275\n",
      "Epoch [26/300], Step [29/172], Loss: 100.9895\n",
      "Epoch [26/300], Step [30/172], Loss: 83.2385\n",
      "Epoch [26/300], Step [31/172], Loss: 58.2243\n",
      "Epoch [26/300], Step [32/172], Loss: 46.9093\n",
      "Epoch [26/300], Step [33/172], Loss: 82.9946\n",
      "Epoch [26/300], Step [34/172], Loss: 32.0013\n",
      "Epoch [26/300], Step [35/172], Loss: 72.6420\n",
      "Epoch [26/300], Step [36/172], Loss: 38.2042\n",
      "Epoch [26/300], Step [37/172], Loss: 26.4768\n",
      "Epoch [26/300], Step [38/172], Loss: 44.1077\n",
      "Epoch [26/300], Step [39/172], Loss: 58.9745\n",
      "Epoch [26/300], Step [40/172], Loss: 42.6680\n",
      "Epoch [26/300], Step [41/172], Loss: 48.3781\n",
      "Epoch [26/300], Step [42/172], Loss: 47.4574\n",
      "Epoch [26/300], Step [43/172], Loss: 41.4576\n",
      "Epoch [26/300], Step [44/172], Loss: 40.7006\n",
      "Epoch [26/300], Step [45/172], Loss: 31.3789\n",
      "Epoch [26/300], Step [46/172], Loss: 56.0798\n",
      "Epoch [26/300], Step [47/172], Loss: 76.1840\n",
      "Epoch [26/300], Step [48/172], Loss: 84.6518\n",
      "Epoch [26/300], Step [49/172], Loss: 28.8106\n",
      "Epoch [26/300], Step [50/172], Loss: 65.7936\n",
      "Epoch [26/300], Step [51/172], Loss: 11.7918\n",
      "Epoch [26/300], Step [52/172], Loss: 29.9731\n",
      "Epoch [26/300], Step [53/172], Loss: 37.1040\n",
      "Epoch [26/300], Step [54/172], Loss: 27.9608\n",
      "Epoch [26/300], Step [55/172], Loss: 23.4934\n",
      "Epoch [26/300], Step [56/172], Loss: 16.2743\n",
      "Epoch [26/300], Step [57/172], Loss: 58.0089\n",
      "Epoch [26/300], Step [58/172], Loss: 24.3953\n",
      "Epoch [26/300], Step [59/172], Loss: 43.8198\n",
      "Epoch [26/300], Step [60/172], Loss: 77.4362\n",
      "Epoch [26/300], Step [61/172], Loss: 16.2174\n",
      "Epoch [26/300], Step [62/172], Loss: 22.0412\n",
      "Epoch [26/300], Step [63/172], Loss: 12.8329\n",
      "Epoch [26/300], Step [64/172], Loss: 8.0916\n",
      "Epoch [26/300], Step [65/172], Loss: 32.3469\n",
      "Epoch [26/300], Step [66/172], Loss: 13.9558\n",
      "Epoch [26/300], Step [67/172], Loss: 33.2622\n",
      "Epoch [26/300], Step [68/172], Loss: 30.2953\n",
      "Epoch [26/300], Step [69/172], Loss: 84.0145\n",
      "Epoch [26/300], Step [70/172], Loss: 72.6292\n",
      "Epoch [26/300], Step [71/172], Loss: 71.3923\n",
      "Epoch [26/300], Step [72/172], Loss: 67.5375\n",
      "Epoch [26/300], Step [73/172], Loss: 72.1793\n",
      "Epoch [26/300], Step [74/172], Loss: 56.2058\n",
      "Epoch [26/300], Step [75/172], Loss: 34.3658\n",
      "Epoch [26/300], Step [76/172], Loss: 53.2270\n",
      "Epoch [26/300], Step [77/172], Loss: 59.8911\n",
      "Epoch [26/300], Step [78/172], Loss: 63.1928\n",
      "Epoch [26/300], Step [79/172], Loss: 52.9051\n",
      "Epoch [26/300], Step [80/172], Loss: 57.2525\n",
      "Epoch [26/300], Step [81/172], Loss: 52.0171\n",
      "Epoch [26/300], Step [82/172], Loss: 48.6323\n",
      "Epoch [26/300], Step [83/172], Loss: 51.2744\n",
      "Epoch [26/300], Step [84/172], Loss: 47.5652\n",
      "Epoch [26/300], Step [85/172], Loss: 56.9608\n",
      "Epoch [26/300], Step [86/172], Loss: 42.5355\n",
      "Epoch [26/300], Step [87/172], Loss: 35.8373\n",
      "Epoch [26/300], Step [88/172], Loss: 38.9865\n",
      "Epoch [26/300], Step [89/172], Loss: 40.0823\n",
      "Epoch [26/300], Step [90/172], Loss: 36.3689\n",
      "Epoch [26/300], Step [91/172], Loss: 35.6126\n",
      "Epoch [26/300], Step [92/172], Loss: 30.8313\n",
      "Epoch [26/300], Step [93/172], Loss: 34.0232\n",
      "Epoch [26/300], Step [94/172], Loss: 35.9974\n",
      "Epoch [26/300], Step [95/172], Loss: 33.7577\n",
      "Epoch [26/300], Step [96/172], Loss: 30.8348\n",
      "Epoch [26/300], Step [97/172], Loss: 34.2211\n",
      "Epoch [26/300], Step [98/172], Loss: 31.2159\n",
      "Epoch [26/300], Step [99/172], Loss: 29.0918\n",
      "Epoch [26/300], Step [100/172], Loss: 28.9038\n",
      "Epoch [26/300], Step [101/172], Loss: 29.3361\n",
      "Epoch [26/300], Step [102/172], Loss: 27.5747\n",
      "Epoch [26/300], Step [103/172], Loss: 28.3446\n",
      "Epoch [26/300], Step [104/172], Loss: 27.7027\n",
      "Epoch [26/300], Step [105/172], Loss: 27.2851\n",
      "Epoch [26/300], Step [106/172], Loss: 27.1404\n",
      "Epoch [26/300], Step [107/172], Loss: 23.5142\n",
      "Epoch [26/300], Step [108/172], Loss: 28.1132\n",
      "Epoch [26/300], Step [109/172], Loss: 27.8735\n",
      "Epoch [26/300], Step [110/172], Loss: 25.9791\n",
      "Epoch [26/300], Step [111/172], Loss: 25.0391\n",
      "Epoch [26/300], Step [112/172], Loss: 30.4464\n",
      "Epoch [26/300], Step [113/172], Loss: 24.5777\n",
      "Epoch [26/300], Step [114/172], Loss: 25.5606\n",
      "Epoch [26/300], Step [115/172], Loss: 31.4717\n",
      "Epoch [26/300], Step [116/172], Loss: 25.4179\n",
      "Epoch [26/300], Step [117/172], Loss: 22.2402\n",
      "Epoch [26/300], Step [118/172], Loss: 24.6385\n",
      "Epoch [26/300], Step [119/172], Loss: 22.0311\n",
      "Epoch [26/300], Step [120/172], Loss: 21.2039\n",
      "Epoch [26/300], Step [121/172], Loss: 22.1232\n",
      "Epoch [26/300], Step [122/172], Loss: 18.9234\n",
      "Epoch [26/300], Step [123/172], Loss: 20.0124\n",
      "Epoch [26/300], Step [124/172], Loss: 18.7908\n",
      "Epoch [26/300], Step [125/172], Loss: 21.9120\n",
      "Epoch [26/300], Step [126/172], Loss: 21.4703\n",
      "Epoch [26/300], Step [127/172], Loss: 23.1261\n",
      "Epoch [26/300], Step [128/172], Loss: 23.8983\n",
      "Epoch [26/300], Step [129/172], Loss: 18.7068\n",
      "Epoch [26/300], Step [130/172], Loss: 20.6484\n",
      "Epoch [26/300], Step [131/172], Loss: 18.6383\n",
      "Epoch [26/300], Step [132/172], Loss: 18.8336\n",
      "Epoch [26/300], Step [133/172], Loss: 18.3479\n",
      "Epoch [26/300], Step [134/172], Loss: 18.7227\n",
      "Epoch [26/300], Step [135/172], Loss: 16.5767\n",
      "Epoch [26/300], Step [136/172], Loss: 17.8549\n",
      "Epoch [26/300], Step [137/172], Loss: 18.6998\n",
      "Epoch [26/300], Step [138/172], Loss: 16.9600\n",
      "Epoch [26/300], Step [139/172], Loss: 18.3666\n",
      "Epoch [26/300], Step [140/172], Loss: 18.1060\n",
      "Epoch [26/300], Step [141/172], Loss: 20.3932\n",
      "Epoch [26/300], Step [142/172], Loss: 19.0659\n",
      "Epoch [26/300], Step [143/172], Loss: 16.5819\n",
      "Epoch [26/300], Step [144/172], Loss: 15.5980\n",
      "Epoch [26/300], Step [145/172], Loss: 15.8751\n",
      "Epoch [26/300], Step [146/172], Loss: 16.1341\n",
      "Epoch [26/300], Step [147/172], Loss: 14.9585\n",
      "Epoch [26/300], Step [148/172], Loss: 15.1175\n",
      "Epoch [26/300], Step [149/172], Loss: 16.0527\n",
      "Epoch [26/300], Step [150/172], Loss: 16.2771\n",
      "Epoch [26/300], Step [151/172], Loss: 14.0949\n",
      "Epoch [26/300], Step [152/172], Loss: 14.5436\n",
      "Epoch [26/300], Step [153/172], Loss: 14.6005\n",
      "Epoch [26/300], Step [154/172], Loss: 15.8397\n",
      "Epoch [26/300], Step [155/172], Loss: 13.9889\n",
      "Epoch [26/300], Step [156/172], Loss: 14.5888\n",
      "Epoch [26/300], Step [157/172], Loss: 15.8717\n",
      "Epoch [26/300], Step [158/172], Loss: 14.6254\n",
      "Epoch [26/300], Step [159/172], Loss: 14.3458\n",
      "Epoch [26/300], Step [160/172], Loss: 14.2458\n",
      "Epoch [26/300], Step [161/172], Loss: 13.3790\n",
      "Epoch [26/300], Step [162/172], Loss: 13.0701\n",
      "Epoch [26/300], Step [163/172], Loss: 13.0483\n",
      "Epoch [26/300], Step [164/172], Loss: 13.9277\n",
      "Epoch [26/300], Step [165/172], Loss: 12.1372\n",
      "Epoch [26/300], Step [166/172], Loss: 13.1078\n",
      "Epoch [26/300], Step [167/172], Loss: 13.1178\n",
      "Epoch [26/300], Step [168/172], Loss: 13.0494\n",
      "Epoch [26/300], Step [169/172], Loss: 11.5319\n",
      "Epoch [26/300], Step [170/172], Loss: 11.9988\n",
      "Epoch [26/300], Step [171/172], Loss: 10.3901\n",
      "Epoch [26/300], Step [172/172], Loss: 8.4509\n",
      "Epoch [27/300], Step [1/172], Loss: 99.0165\n",
      "Epoch [27/300], Step [2/172], Loss: 100.4523\n",
      "Epoch [27/300], Step [3/172], Loss: 126.2851\n",
      "Epoch [27/300], Step [4/172], Loss: 82.2939\n",
      "Epoch [27/300], Step [5/172], Loss: 102.7161\n",
      "Epoch [27/300], Step [6/172], Loss: 50.3588\n",
      "Epoch [27/300], Step [7/172], Loss: 66.9786\n",
      "Epoch [27/300], Step [8/172], Loss: 38.4174\n",
      "Epoch [27/300], Step [9/172], Loss: 66.2026\n",
      "Epoch [27/300], Step [10/172], Loss: 71.3085\n",
      "Epoch [27/300], Step [11/172], Loss: 92.1157\n",
      "Epoch [27/300], Step [12/172], Loss: 86.3328\n",
      "Epoch [27/300], Step [13/172], Loss: 58.1077\n",
      "Epoch [27/300], Step [14/172], Loss: 100.9721\n",
      "Epoch [27/300], Step [15/172], Loss: 91.3672\n",
      "Epoch [27/300], Step [16/172], Loss: 73.9205\n",
      "Epoch [27/300], Step [17/172], Loss: 62.3298\n",
      "Epoch [27/300], Step [18/172], Loss: 86.7531\n",
      "Epoch [27/300], Step [19/172], Loss: 78.3280\n",
      "Epoch [27/300], Step [20/172], Loss: 124.8019\n",
      "Epoch [27/300], Step [21/172], Loss: 97.7266\n",
      "Epoch [27/300], Step [22/172], Loss: 94.6823\n",
      "Epoch [27/300], Step [23/172], Loss: 57.3749\n",
      "Epoch [27/300], Step [24/172], Loss: 89.1912\n",
      "Epoch [27/300], Step [25/172], Loss: 60.7262\n",
      "Epoch [27/300], Step [26/172], Loss: 67.1279\n",
      "Epoch [27/300], Step [27/172], Loss: 90.5671\n",
      "Epoch [27/300], Step [28/172], Loss: 84.3704\n",
      "Epoch [27/300], Step [29/172], Loss: 100.2237\n",
      "Epoch [27/300], Step [30/172], Loss: 82.7546\n",
      "Epoch [27/300], Step [31/172], Loss: 57.3197\n",
      "Epoch [27/300], Step [32/172], Loss: 46.2402\n",
      "Epoch [27/300], Step [33/172], Loss: 82.4833\n",
      "Epoch [27/300], Step [34/172], Loss: 30.1181\n",
      "Epoch [27/300], Step [35/172], Loss: 72.0804\n",
      "Epoch [27/300], Step [36/172], Loss: 37.7981\n",
      "Epoch [27/300], Step [37/172], Loss: 26.0478\n",
      "Epoch [27/300], Step [38/172], Loss: 43.3256\n",
      "Epoch [27/300], Step [39/172], Loss: 58.6311\n",
      "Epoch [27/300], Step [40/172], Loss: 41.7400\n",
      "Epoch [27/300], Step [41/172], Loss: 47.8547\n",
      "Epoch [27/300], Step [42/172], Loss: 46.8224\n",
      "Epoch [27/300], Step [43/172], Loss: 40.6865\n",
      "Epoch [27/300], Step [44/172], Loss: 39.6985\n",
      "Epoch [27/300], Step [45/172], Loss: 30.4969\n",
      "Epoch [27/300], Step [46/172], Loss: 55.4534\n",
      "Epoch [27/300], Step [47/172], Loss: 75.7160\n",
      "Epoch [27/300], Step [48/172], Loss: 83.8073\n",
      "Epoch [27/300], Step [49/172], Loss: 28.1716\n",
      "Epoch [27/300], Step [50/172], Loss: 65.1285\n",
      "Epoch [27/300], Step [51/172], Loss: 11.3439\n",
      "Epoch [27/300], Step [52/172], Loss: 29.5054\n",
      "Epoch [27/300], Step [53/172], Loss: 36.3037\n",
      "Epoch [27/300], Step [54/172], Loss: 27.2637\n",
      "Epoch [27/300], Step [55/172], Loss: 22.8627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/300], Step [56/172], Loss: 15.6490\n",
      "Epoch [27/300], Step [57/172], Loss: 57.3301\n",
      "Epoch [27/300], Step [58/172], Loss: 23.9940\n",
      "Epoch [27/300], Step [59/172], Loss: 43.2090\n",
      "Epoch [27/300], Step [60/172], Loss: 76.5320\n",
      "Epoch [27/300], Step [61/172], Loss: 15.7654\n",
      "Epoch [27/300], Step [62/172], Loss: 21.7120\n",
      "Epoch [27/300], Step [63/172], Loss: 12.2457\n",
      "Epoch [27/300], Step [64/172], Loss: 7.7890\n",
      "Epoch [27/300], Step [65/172], Loss: 32.0581\n",
      "Epoch [27/300], Step [66/172], Loss: 13.6706\n",
      "Epoch [27/300], Step [67/172], Loss: 32.6118\n",
      "Epoch [27/300], Step [68/172], Loss: 29.2778\n",
      "Epoch [27/300], Step [69/172], Loss: 84.1814\n",
      "Epoch [27/300], Step [70/172], Loss: 72.7153\n",
      "Epoch [27/300], Step [71/172], Loss: 71.4659\n",
      "Epoch [27/300], Step [72/172], Loss: 67.7513\n",
      "Epoch [27/300], Step [73/172], Loss: 72.4507\n",
      "Epoch [27/300], Step [74/172], Loss: 55.9824\n",
      "Epoch [27/300], Step [75/172], Loss: 34.1378\n",
      "Epoch [27/300], Step [76/172], Loss: 53.0492\n",
      "Epoch [27/300], Step [77/172], Loss: 59.9168\n",
      "Epoch [27/300], Step [78/172], Loss: 63.2211\n",
      "Epoch [27/300], Step [79/172], Loss: 52.8630\n",
      "Epoch [27/300], Step [80/172], Loss: 57.2387\n",
      "Epoch [27/300], Step [81/172], Loss: 52.0632\n",
      "Epoch [27/300], Step [82/172], Loss: 48.3721\n",
      "Epoch [27/300], Step [83/172], Loss: 51.2207\n",
      "Epoch [27/300], Step [84/172], Loss: 47.2480\n",
      "Epoch [27/300], Step [85/172], Loss: 56.6403\n",
      "Epoch [27/300], Step [86/172], Loss: 42.2262\n",
      "Epoch [27/300], Step [87/172], Loss: 35.5267\n",
      "Epoch [27/300], Step [88/172], Loss: 38.6469\n",
      "Epoch [27/300], Step [89/172], Loss: 39.5993\n",
      "Epoch [27/300], Step [90/172], Loss: 36.1593\n",
      "Epoch [27/300], Step [91/172], Loss: 35.2795\n",
      "Epoch [27/300], Step [92/172], Loss: 30.5633\n",
      "Epoch [27/300], Step [93/172], Loss: 33.7262\n",
      "Epoch [27/300], Step [94/172], Loss: 35.7010\n",
      "Epoch [27/300], Step [95/172], Loss: 33.4609\n",
      "Epoch [27/300], Step [96/172], Loss: 30.4675\n",
      "Epoch [27/300], Step [97/172], Loss: 33.9096\n",
      "Epoch [27/300], Step [98/172], Loss: 30.8403\n",
      "Epoch [27/300], Step [99/172], Loss: 28.6879\n",
      "Epoch [27/300], Step [100/172], Loss: 28.4748\n",
      "Epoch [27/300], Step [101/172], Loss: 29.0438\n",
      "Epoch [27/300], Step [102/172], Loss: 27.1798\n",
      "Epoch [27/300], Step [103/172], Loss: 27.9378\n",
      "Epoch [27/300], Step [104/172], Loss: 27.3333\n",
      "Epoch [27/300], Step [105/172], Loss: 26.9333\n",
      "Epoch [27/300], Step [106/172], Loss: 26.8400\n",
      "Epoch [27/300], Step [107/172], Loss: 23.2224\n",
      "Epoch [27/300], Step [108/172], Loss: 27.7674\n",
      "Epoch [27/300], Step [109/172], Loss: 27.6378\n",
      "Epoch [27/300], Step [110/172], Loss: 25.6088\n",
      "Epoch [27/300], Step [111/172], Loss: 24.7294\n",
      "Epoch [27/300], Step [112/172], Loss: 30.2071\n",
      "Epoch [27/300], Step [113/172], Loss: 24.3152\n",
      "Epoch [27/300], Step [114/172], Loss: 25.2779\n",
      "Epoch [27/300], Step [115/172], Loss: 31.3710\n",
      "Epoch [27/300], Step [116/172], Loss: 25.1354\n",
      "Epoch [27/300], Step [117/172], Loss: 21.9484\n",
      "Epoch [27/300], Step [118/172], Loss: 24.4131\n",
      "Epoch [27/300], Step [119/172], Loss: 21.7658\n",
      "Epoch [27/300], Step [120/172], Loss: 20.8839\n",
      "Epoch [27/300], Step [121/172], Loss: 21.7911\n",
      "Epoch [27/300], Step [122/172], Loss: 18.6544\n",
      "Epoch [27/300], Step [123/172], Loss: 19.7615\n",
      "Epoch [27/300], Step [124/172], Loss: 18.4694\n",
      "Epoch [27/300], Step [125/172], Loss: 21.6559\n",
      "Epoch [27/300], Step [126/172], Loss: 21.1760\n",
      "Epoch [27/300], Step [127/172], Loss: 22.8874\n",
      "Epoch [27/300], Step [128/172], Loss: 23.6720\n",
      "Epoch [27/300], Step [129/172], Loss: 18.4230\n",
      "Epoch [27/300], Step [130/172], Loss: 20.3959\n",
      "Epoch [27/300], Step [131/172], Loss: 18.3749\n",
      "Epoch [27/300], Step [132/172], Loss: 18.5735\n",
      "Epoch [27/300], Step [133/172], Loss: 18.0951\n",
      "Epoch [27/300], Step [134/172], Loss: 18.4910\n",
      "Epoch [27/300], Step [135/172], Loss: 16.3418\n",
      "Epoch [27/300], Step [136/172], Loss: 17.5980\n",
      "Epoch [27/300], Step [137/172], Loss: 18.4718\n",
      "Epoch [27/300], Step [138/172], Loss: 16.6972\n",
      "Epoch [27/300], Step [139/172], Loss: 18.1284\n",
      "Epoch [27/300], Step [140/172], Loss: 17.8795\n",
      "Epoch [27/300], Step [141/172], Loss: 20.2454\n",
      "Epoch [27/300], Step [142/172], Loss: 18.8808\n",
      "Epoch [27/300], Step [143/172], Loss: 16.3689\n",
      "Epoch [27/300], Step [144/172], Loss: 15.4073\n",
      "Epoch [27/300], Step [145/172], Loss: 15.6749\n",
      "Epoch [27/300], Step [146/172], Loss: 15.9591\n",
      "Epoch [27/300], Step [147/172], Loss: 14.6713\n",
      "Epoch [27/300], Step [148/172], Loss: 14.8565\n",
      "Epoch [27/300], Step [149/172], Loss: 15.8835\n",
      "Epoch [27/300], Step [150/172], Loss: 16.0665\n",
      "Epoch [27/300], Step [151/172], Loss: 13.9106\n",
      "Epoch [27/300], Step [152/172], Loss: 14.3498\n",
      "Epoch [27/300], Step [153/172], Loss: 14.3963\n",
      "Epoch [27/300], Step [154/172], Loss: 15.6440\n",
      "Epoch [27/300], Step [155/172], Loss: 13.7988\n",
      "Epoch [27/300], Step [156/172], Loss: 14.4428\n",
      "Epoch [27/300], Step [157/172], Loss: 15.7313\n",
      "Epoch [27/300], Step [158/172], Loss: 14.4512\n",
      "Epoch [27/300], Step [159/172], Loss: 14.1998\n",
      "Epoch [27/300], Step [160/172], Loss: 14.0839\n",
      "Epoch [27/300], Step [161/172], Loss: 13.1945\n",
      "Epoch [27/300], Step [162/172], Loss: 12.9167\n",
      "Epoch [27/300], Step [163/172], Loss: 12.8880\n",
      "Epoch [27/300], Step [164/172], Loss: 13.8105\n",
      "Epoch [27/300], Step [165/172], Loss: 11.9816\n",
      "Epoch [27/300], Step [166/172], Loss: 12.9399\n",
      "Epoch [27/300], Step [167/172], Loss: 12.9607\n",
      "Epoch [27/300], Step [168/172], Loss: 12.8642\n",
      "Epoch [27/300], Step [169/172], Loss: 11.3922\n",
      "Epoch [27/300], Step [170/172], Loss: 11.8614\n",
      "Epoch [27/300], Step [171/172], Loss: 10.2519\n",
      "Epoch [27/300], Step [172/172], Loss: 8.3612\n",
      "Epoch [28/300], Step [1/172], Loss: 99.2586\n",
      "Epoch [28/300], Step [2/172], Loss: 100.3954\n",
      "Epoch [28/300], Step [3/172], Loss: 126.0715\n",
      "Epoch [28/300], Step [4/172], Loss: 81.7166\n",
      "Epoch [28/300], Step [5/172], Loss: 101.9829\n",
      "Epoch [28/300], Step [6/172], Loss: 49.6693\n",
      "Epoch [28/300], Step [7/172], Loss: 66.0498\n",
      "Epoch [28/300], Step [8/172], Loss: 36.7229\n",
      "Epoch [28/300], Step [9/172], Loss: 65.2250\n",
      "Epoch [28/300], Step [10/172], Loss: 70.6360\n",
      "Epoch [28/300], Step [11/172], Loss: 92.4154\n",
      "Epoch [28/300], Step [12/172], Loss: 86.2803\n",
      "Epoch [28/300], Step [13/172], Loss: 57.5732\n",
      "Epoch [28/300], Step [14/172], Loss: 100.8264\n",
      "Epoch [28/300], Step [15/172], Loss: 91.2239\n",
      "Epoch [28/300], Step [16/172], Loss: 73.8284\n",
      "Epoch [28/300], Step [17/172], Loss: 62.0988\n",
      "Epoch [28/300], Step [18/172], Loss: 86.4145\n",
      "Epoch [28/300], Step [19/172], Loss: 78.3755\n",
      "Epoch [28/300], Step [20/172], Loss: 123.9636\n",
      "Epoch [28/300], Step [21/172], Loss: 97.6956\n",
      "Epoch [28/300], Step [22/172], Loss: 94.5868\n",
      "Epoch [28/300], Step [23/172], Loss: 55.8870\n",
      "Epoch [28/300], Step [24/172], Loss: 88.8910\n",
      "Epoch [28/300], Step [25/172], Loss: 60.4456\n",
      "Epoch [28/300], Step [26/172], Loss: 66.8800\n",
      "Epoch [28/300], Step [27/172], Loss: 90.4407\n",
      "Epoch [28/300], Step [28/172], Loss: 83.8821\n",
      "Epoch [28/300], Step [29/172], Loss: 99.2371\n",
      "Epoch [28/300], Step [30/172], Loss: 82.4812\n",
      "Epoch [28/300], Step [31/172], Loss: 56.6811\n",
      "Epoch [28/300], Step [32/172], Loss: 45.8427\n",
      "Epoch [28/300], Step [33/172], Loss: 82.1750\n",
      "Epoch [28/300], Step [34/172], Loss: 28.6048\n",
      "Epoch [28/300], Step [35/172], Loss: 71.4061\n",
      "Epoch [28/300], Step [36/172], Loss: 37.3554\n",
      "Epoch [28/300], Step [37/172], Loss: 25.8075\n",
      "Epoch [28/300], Step [38/172], Loss: 42.9116\n",
      "Epoch [28/300], Step [39/172], Loss: 58.3963\n",
      "Epoch [28/300], Step [40/172], Loss: 41.0128\n",
      "Epoch [28/300], Step [41/172], Loss: 47.5073\n",
      "Epoch [28/300], Step [42/172], Loss: 46.4101\n",
      "Epoch [28/300], Step [43/172], Loss: 40.1247\n",
      "Epoch [28/300], Step [44/172], Loss: 38.6472\n",
      "Epoch [28/300], Step [45/172], Loss: 29.7335\n",
      "Epoch [28/300], Step [46/172], Loss: 54.9586\n",
      "Epoch [28/300], Step [47/172], Loss: 75.3937\n",
      "Epoch [28/300], Step [48/172], Loss: 82.9140\n",
      "Epoch [28/300], Step [49/172], Loss: 27.5479\n",
      "Epoch [28/300], Step [50/172], Loss: 64.3705\n",
      "Epoch [28/300], Step [51/172], Loss: 11.0399\n",
      "Epoch [28/300], Step [52/172], Loss: 29.0182\n",
      "Epoch [28/300], Step [53/172], Loss: 35.6917\n",
      "Epoch [28/300], Step [54/172], Loss: 26.4414\n",
      "Epoch [28/300], Step [55/172], Loss: 22.1298\n",
      "Epoch [28/300], Step [56/172], Loss: 15.2108\n",
      "Epoch [28/300], Step [57/172], Loss: 56.7685\n",
      "Epoch [28/300], Step [58/172], Loss: 23.5418\n",
      "Epoch [28/300], Step [59/172], Loss: 42.3778\n",
      "Epoch [28/300], Step [60/172], Loss: 75.5606\n",
      "Epoch [28/300], Step [61/172], Loss: 15.2770\n",
      "Epoch [28/300], Step [62/172], Loss: 21.0820\n",
      "Epoch [28/300], Step [63/172], Loss: 11.7811\n",
      "Epoch [28/300], Step [64/172], Loss: 7.5074\n",
      "Epoch [28/300], Step [65/172], Loss: 31.4276\n",
      "Epoch [28/300], Step [66/172], Loss: 13.0280\n",
      "Epoch [28/300], Step [67/172], Loss: 31.5966\n",
      "Epoch [28/300], Step [68/172], Loss: 26.4335\n",
      "Epoch [28/300], Step [69/172], Loss: 84.0831\n",
      "Epoch [28/300], Step [70/172], Loss: 75.6979\n",
      "Epoch [28/300], Step [71/172], Loss: 73.4516\n",
      "Epoch [28/300], Step [72/172], Loss: 69.6098\n",
      "Epoch [28/300], Step [73/172], Loss: 74.5244\n",
      "Epoch [28/300], Step [74/172], Loss: 56.7532\n",
      "Epoch [28/300], Step [75/172], Loss: 34.4264\n",
      "Epoch [28/300], Step [76/172], Loss: 53.6657\n",
      "Epoch [28/300], Step [77/172], Loss: 60.4759\n",
      "Epoch [28/300], Step [78/172], Loss: 63.4339\n",
      "Epoch [28/300], Step [79/172], Loss: 52.8400\n",
      "Epoch [28/300], Step [80/172], Loss: 57.0812\n",
      "Epoch [28/300], Step [81/172], Loss: 52.1098\n",
      "Epoch [28/300], Step [82/172], Loss: 48.1566\n",
      "Epoch [28/300], Step [83/172], Loss: 51.0490\n",
      "Epoch [28/300], Step [84/172], Loss: 46.7981\n",
      "Epoch [28/300], Step [85/172], Loss: 56.1811\n",
      "Epoch [28/300], Step [86/172], Loss: 41.9153\n",
      "Epoch [28/300], Step [87/172], Loss: 35.1553\n",
      "Epoch [28/300], Step [88/172], Loss: 38.2477\n",
      "Epoch [28/300], Step [89/172], Loss: 38.9864\n",
      "Epoch [28/300], Step [90/172], Loss: 35.8616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/300], Step [91/172], Loss: 34.8849\n",
      "Epoch [28/300], Step [92/172], Loss: 30.2239\n",
      "Epoch [28/300], Step [93/172], Loss: 33.3511\n",
      "Epoch [28/300], Step [94/172], Loss: 35.3740\n",
      "Epoch [28/300], Step [95/172], Loss: 33.0328\n",
      "Epoch [28/300], Step [96/172], Loss: 29.9935\n",
      "Epoch [28/300], Step [97/172], Loss: 33.4743\n",
      "Epoch [28/300], Step [98/172], Loss: 30.3481\n",
      "Epoch [28/300], Step [99/172], Loss: 28.1983\n",
      "Epoch [28/300], Step [100/172], Loss: 27.9092\n",
      "Epoch [28/300], Step [101/172], Loss: 28.6378\n",
      "Epoch [28/300], Step [102/172], Loss: 26.6892\n",
      "Epoch [28/300], Step [103/172], Loss: 27.3910\n",
      "Epoch [28/300], Step [104/172], Loss: 26.8639\n",
      "Epoch [28/300], Step [105/172], Loss: 26.4882\n",
      "Epoch [28/300], Step [106/172], Loss: 26.4138\n",
      "Epoch [28/300], Step [107/172], Loss: 22.8232\n",
      "Epoch [28/300], Step [108/172], Loss: 27.2834\n",
      "Epoch [28/300], Step [109/172], Loss: 27.2892\n",
      "Epoch [28/300], Step [110/172], Loss: 25.1313\n",
      "Epoch [28/300], Step [111/172], Loss: 24.2716\n",
      "Epoch [28/300], Step [112/172], Loss: 29.7742\n",
      "Epoch [28/300], Step [113/172], Loss: 23.9218\n",
      "Epoch [28/300], Step [114/172], Loss: 24.8513\n",
      "Epoch [28/300], Step [115/172], Loss: 31.1042\n",
      "Epoch [28/300], Step [116/172], Loss: 24.7164\n",
      "Epoch [28/300], Step [117/172], Loss: 21.5204\n",
      "Epoch [28/300], Step [118/172], Loss: 24.0797\n",
      "Epoch [28/300], Step [119/172], Loss: 21.3742\n",
      "Epoch [28/300], Step [120/172], Loss: 20.4423\n",
      "Epoch [28/300], Step [121/172], Loss: 21.3398\n",
      "Epoch [28/300], Step [122/172], Loss: 18.2663\n",
      "Epoch [28/300], Step [123/172], Loss: 19.3627\n",
      "Epoch [28/300], Step [124/172], Loss: 18.0172\n",
      "Epoch [28/300], Step [125/172], Loss: 21.2600\n",
      "Epoch [28/300], Step [126/172], Loss: 20.7631\n",
      "Epoch [28/300], Step [127/172], Loss: 22.4942\n",
      "Epoch [28/300], Step [128/172], Loss: 23.3034\n",
      "Epoch [28/300], Step [129/172], Loss: 18.0231\n",
      "Epoch [28/300], Step [130/172], Loss: 20.0144\n",
      "Epoch [28/300], Step [131/172], Loss: 17.9821\n",
      "Epoch [28/300], Step [132/172], Loss: 18.1868\n",
      "Epoch [28/300], Step [133/172], Loss: 17.7023\n",
      "Epoch [28/300], Step [134/172], Loss: 18.1119\n",
      "Epoch [28/300], Step [135/172], Loss: 15.9754\n",
      "Epoch [28/300], Step [136/172], Loss: 17.1832\n",
      "Epoch [28/300], Step [137/172], Loss: 18.1241\n",
      "Epoch [28/300], Step [138/172], Loss: 16.3324\n",
      "Epoch [28/300], Step [139/172], Loss: 17.7793\n",
      "Epoch [28/300], Step [140/172], Loss: 17.5419\n",
      "Epoch [28/300], Step [141/172], Loss: 19.9581\n",
      "Epoch [28/300], Step [142/172], Loss: 18.5657\n",
      "Epoch [28/300], Step [143/172], Loss: 16.0532\n",
      "Epoch [28/300], Step [144/172], Loss: 15.0753\n",
      "Epoch [28/300], Step [145/172], Loss: 15.3357\n",
      "Epoch [28/300], Step [146/172], Loss: 15.6550\n",
      "Epoch [28/300], Step [147/172], Loss: 14.2143\n",
      "Epoch [28/300], Step [148/172], Loss: 14.4520\n",
      "Epoch [28/300], Step [149/172], Loss: 15.5832\n",
      "Epoch [28/300], Step [150/172], Loss: 15.7218\n",
      "Epoch [28/300], Step [151/172], Loss: 13.5815\n",
      "Epoch [28/300], Step [152/172], Loss: 14.0447\n",
      "Epoch [28/300], Step [153/172], Loss: 14.0602\n",
      "Epoch [28/300], Step [154/172], Loss: 15.3317\n",
      "Epoch [28/300], Step [155/172], Loss: 13.4840\n",
      "Epoch [28/300], Step [156/172], Loss: 14.1682\n",
      "Epoch [28/300], Step [157/172], Loss: 15.4647\n",
      "Epoch [28/300], Step [158/172], Loss: 14.1357\n",
      "Epoch [28/300], Step [159/172], Loss: 13.8854\n",
      "Epoch [28/300], Step [160/172], Loss: 13.7496\n",
      "Epoch [28/300], Step [161/172], Loss: 12.8497\n",
      "Epoch [28/300], Step [162/172], Loss: 12.6086\n",
      "Epoch [28/300], Step [163/172], Loss: 12.5553\n",
      "Epoch [28/300], Step [164/172], Loss: 13.5289\n",
      "Epoch [28/300], Step [165/172], Loss: 11.6617\n",
      "Epoch [28/300], Step [166/172], Loss: 12.5748\n",
      "Epoch [28/300], Step [167/172], Loss: 12.6196\n",
      "Epoch [28/300], Step [168/172], Loss: 12.4688\n",
      "Epoch [28/300], Step [169/172], Loss: 11.0483\n",
      "Epoch [28/300], Step [170/172], Loss: 11.4801\n",
      "Epoch [28/300], Step [171/172], Loss: 9.9092\n",
      "Epoch [28/300], Step [172/172], Loss: 8.1216\n",
      "Epoch [29/300], Step [1/172], Loss: 99.9029\n",
      "Epoch [29/300], Step [2/172], Loss: 100.9776\n",
      "Epoch [29/300], Step [3/172], Loss: 126.4660\n",
      "Epoch [29/300], Step [4/172], Loss: 81.4085\n",
      "Epoch [29/300], Step [5/172], Loss: 101.5657\n",
      "Epoch [29/300], Step [6/172], Loss: 49.0501\n",
      "Epoch [29/300], Step [7/172], Loss: 65.0838\n",
      "Epoch [29/300], Step [8/172], Loss: 35.0986\n",
      "Epoch [29/300], Step [9/172], Loss: 64.5983\n",
      "Epoch [29/300], Step [10/172], Loss: 70.0638\n",
      "Epoch [29/300], Step [11/172], Loss: 93.3370\n",
      "Epoch [29/300], Step [12/172], Loss: 86.8780\n",
      "Epoch [29/300], Step [13/172], Loss: 57.2108\n",
      "Epoch [29/300], Step [14/172], Loss: 100.8926\n",
      "Epoch [29/300], Step [15/172], Loss: 91.1797\n",
      "Epoch [29/300], Step [16/172], Loss: 73.7153\n",
      "Epoch [29/300], Step [17/172], Loss: 62.0549\n",
      "Epoch [29/300], Step [18/172], Loss: 85.9670\n",
      "Epoch [29/300], Step [19/172], Loss: 78.4269\n",
      "Epoch [29/300], Step [20/172], Loss: 123.4747\n",
      "Epoch [29/300], Step [21/172], Loss: 97.6855\n",
      "Epoch [29/300], Step [22/172], Loss: 94.5926\n",
      "Epoch [29/300], Step [23/172], Loss: 54.3915\n",
      "Epoch [29/300], Step [24/172], Loss: 88.6334\n",
      "Epoch [29/300], Step [25/172], Loss: 60.0972\n",
      "Epoch [29/300], Step [26/172], Loss: 66.6263\n",
      "Epoch [29/300], Step [27/172], Loss: 90.2366\n",
      "Epoch [29/300], Step [28/172], Loss: 83.2313\n",
      "Epoch [29/300], Step [29/172], Loss: 98.3411\n",
      "Epoch [29/300], Step [30/172], Loss: 82.1092\n",
      "Epoch [29/300], Step [31/172], Loss: 55.9427\n",
      "Epoch [29/300], Step [32/172], Loss: 45.3807\n",
      "Epoch [29/300], Step [33/172], Loss: 81.8411\n",
      "Epoch [29/300], Step [34/172], Loss: 26.9860\n",
      "Epoch [29/300], Step [35/172], Loss: 70.9243\n",
      "Epoch [29/300], Step [36/172], Loss: 36.9955\n",
      "Epoch [29/300], Step [37/172], Loss: 25.4193\n",
      "Epoch [29/300], Step [38/172], Loss: 41.9743\n",
      "Epoch [29/300], Step [39/172], Loss: 58.0397\n",
      "Epoch [29/300], Step [40/172], Loss: 40.0929\n",
      "Epoch [29/300], Step [41/172], Loss: 47.0340\n",
      "Epoch [29/300], Step [42/172], Loss: 45.8762\n",
      "Epoch [29/300], Step [43/172], Loss: 39.4145\n",
      "Epoch [29/300], Step [44/172], Loss: 38.0392\n",
      "Epoch [29/300], Step [45/172], Loss: 29.1299\n",
      "Epoch [29/300], Step [46/172], Loss: 54.5135\n",
      "Epoch [29/300], Step [47/172], Loss: 75.0109\n",
      "Epoch [29/300], Step [48/172], Loss: 82.5589\n",
      "Epoch [29/300], Step [49/172], Loss: 27.0453\n",
      "Epoch [29/300], Step [50/172], Loss: 63.6414\n",
      "Epoch [29/300], Step [51/172], Loss: 10.6902\n",
      "Epoch [29/300], Step [52/172], Loss: 28.6100\n",
      "Epoch [29/300], Step [53/172], Loss: 35.0515\n",
      "Epoch [29/300], Step [54/172], Loss: 26.1552\n",
      "Epoch [29/300], Step [55/172], Loss: 21.7483\n",
      "Epoch [29/300], Step [56/172], Loss: 14.7655\n",
      "Epoch [29/300], Step [57/172], Loss: 56.1876\n",
      "Epoch [29/300], Step [58/172], Loss: 23.3800\n",
      "Epoch [29/300], Step [59/172], Loss: 42.1827\n",
      "Epoch [29/300], Step [60/172], Loss: 75.0068\n",
      "Epoch [29/300], Step [61/172], Loss: 15.0614\n",
      "Epoch [29/300], Step [62/172], Loss: 21.2228\n",
      "Epoch [29/300], Step [63/172], Loss: 11.3936\n",
      "Epoch [29/300], Step [64/172], Loss: 7.4025\n",
      "Epoch [29/300], Step [65/172], Loss: 31.5846\n",
      "Epoch [29/300], Step [66/172], Loss: 13.0922\n",
      "Epoch [29/300], Step [67/172], Loss: 31.7139\n",
      "Epoch [29/300], Step [68/172], Loss: 27.5916\n",
      "Epoch [29/300], Step [69/172], Loss: 84.0680\n",
      "Epoch [29/300], Step [70/172], Loss: 72.9206\n",
      "Epoch [29/300], Step [71/172], Loss: 71.4902\n",
      "Epoch [29/300], Step [72/172], Loss: 68.0046\n",
      "Epoch [29/300], Step [73/172], Loss: 72.9007\n",
      "Epoch [29/300], Step [74/172], Loss: 55.4025\n",
      "Epoch [29/300], Step [75/172], Loss: 33.8955\n",
      "Epoch [29/300], Step [76/172], Loss: 52.8885\n",
      "Epoch [29/300], Step [77/172], Loss: 60.1586\n",
      "Epoch [29/300], Step [78/172], Loss: 63.2870\n",
      "Epoch [29/300], Step [79/172], Loss: 53.0116\n",
      "Epoch [29/300], Step [80/172], Loss: 57.4147\n",
      "Epoch [29/300], Step [81/172], Loss: 52.4981\n",
      "Epoch [29/300], Step [82/172], Loss: 48.3272\n",
      "Epoch [29/300], Step [83/172], Loss: 51.3225\n",
      "Epoch [29/300], Step [84/172], Loss: 46.9690\n",
      "Epoch [29/300], Step [85/172], Loss: 56.3395\n",
      "Epoch [29/300], Step [86/172], Loss: 41.8514\n",
      "Epoch [29/300], Step [87/172], Loss: 35.0791\n",
      "Epoch [29/300], Step [88/172], Loss: 38.1334\n",
      "Epoch [29/300], Step [89/172], Loss: 38.7849\n",
      "Epoch [29/300], Step [90/172], Loss: 35.8288\n",
      "Epoch [29/300], Step [91/172], Loss: 34.7379\n",
      "Epoch [29/300], Step [92/172], Loss: 30.0642\n",
      "Epoch [29/300], Step [93/172], Loss: 33.2031\n",
      "Epoch [29/300], Step [94/172], Loss: 35.2668\n",
      "Epoch [29/300], Step [95/172], Loss: 32.8910\n",
      "Epoch [29/300], Step [96/172], Loss: 29.7821\n",
      "Epoch [29/300], Step [97/172], Loss: 33.3693\n",
      "Epoch [29/300], Step [98/172], Loss: 30.1606\n",
      "Epoch [29/300], Step [99/172], Loss: 27.9468\n",
      "Epoch [29/300], Step [100/172], Loss: 27.6302\n",
      "Epoch [29/300], Step [101/172], Loss: 28.4812\n",
      "Epoch [29/300], Step [102/172], Loss: 26.4560\n",
      "Epoch [29/300], Step [103/172], Loss: 27.1230\n",
      "Epoch [29/300], Step [104/172], Loss: 26.6284\n",
      "Epoch [29/300], Step [105/172], Loss: 26.2754\n",
      "Epoch [29/300], Step [106/172], Loss: 26.2402\n",
      "Epoch [29/300], Step [107/172], Loss: 22.6508\n",
      "Epoch [29/300], Step [108/172], Loss: 27.0639\n",
      "Epoch [29/300], Step [109/172], Loss: 27.1513\n",
      "Epoch [29/300], Step [110/172], Loss: 24.8713\n",
      "Epoch [29/300], Step [111/172], Loss: 24.0798\n",
      "Epoch [29/300], Step [112/172], Loss: 29.6905\n",
      "Epoch [29/300], Step [113/172], Loss: 23.7552\n",
      "Epoch [29/300], Step [114/172], Loss: 24.6903\n",
      "Epoch [29/300], Step [115/172], Loss: 31.1033\n",
      "Epoch [29/300], Step [116/172], Loss: 24.5303\n",
      "Epoch [29/300], Step [117/172], Loss: 21.3412\n",
      "Epoch [29/300], Step [118/172], Loss: 23.9808\n",
      "Epoch [29/300], Step [119/172], Loss: 21.2091\n",
      "Epoch [29/300], Step [120/172], Loss: 20.2367\n",
      "Epoch [29/300], Step [121/172], Loss: 21.1248\n",
      "Epoch [29/300], Step [122/172], Loss: 18.0911\n",
      "Epoch [29/300], Step [123/172], Loss: 19.2149\n",
      "Epoch [29/300], Step [124/172], Loss: 17.8096\n",
      "Epoch [29/300], Step [125/172], Loss: 21.1012\n",
      "Epoch [29/300], Step [126/172], Loss: 20.5633\n",
      "Epoch [29/300], Step [127/172], Loss: 22.3639\n",
      "Epoch [29/300], Step [128/172], Loss: 23.1709\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/300], Step [129/172], Loss: 17.8311\n",
      "Epoch [29/300], Step [130/172], Loss: 19.8664\n",
      "Epoch [29/300], Step [131/172], Loss: 17.8220\n",
      "Epoch [29/300], Step [132/172], Loss: 18.0156\n",
      "Epoch [29/300], Step [133/172], Loss: 17.5573\n",
      "Epoch [29/300], Step [134/172], Loss: 17.9807\n",
      "Epoch [29/300], Step [135/172], Loss: 15.8368\n",
      "Epoch [29/300], Step [136/172], Loss: 17.0368\n",
      "Epoch [29/300], Step [137/172], Loss: 17.9940\n",
      "Epoch [29/300], Step [138/172], Loss: 16.1656\n",
      "Epoch [29/300], Step [139/172], Loss: 17.6250\n",
      "Epoch [29/300], Step [140/172], Loss: 17.4041\n",
      "Epoch [29/300], Step [141/172], Loss: 19.8805\n",
      "Epoch [29/300], Step [142/172], Loss: 18.4621\n",
      "Epoch [29/300], Step [143/172], Loss: 15.9261\n",
      "Epoch [29/300], Step [144/172], Loss: 14.9767\n",
      "Epoch [29/300], Step [145/172], Loss: 15.2232\n",
      "Epoch [29/300], Step [146/172], Loss: 15.5626\n",
      "Epoch [29/300], Step [147/172], Loss: 14.0492\n",
      "Epoch [29/300], Step [148/172], Loss: 14.2990\n",
      "Epoch [29/300], Step [149/172], Loss: 15.5072\n",
      "Epoch [29/300], Step [150/172], Loss: 15.6140\n",
      "Epoch [29/300], Step [151/172], Loss: 13.4998\n",
      "Epoch [29/300], Step [152/172], Loss: 13.9309\n",
      "Epoch [29/300], Step [153/172], Loss: 13.9563\n",
      "Epoch [29/300], Step [154/172], Loss: 15.2292\n",
      "Epoch [29/300], Step [155/172], Loss: 13.3924\n",
      "Epoch [29/300], Step [156/172], Loss: 14.1218\n",
      "Epoch [29/300], Step [157/172], Loss: 15.4138\n",
      "Epoch [29/300], Step [158/172], Loss: 14.0539\n",
      "Epoch [29/300], Step [159/172], Loss: 13.8310\n",
      "Epoch [29/300], Step [160/172], Loss: 13.6908\n",
      "Epoch [29/300], Step [161/172], Loss: 12.7802\n",
      "Epoch [29/300], Step [162/172], Loss: 12.5518\n",
      "Epoch [29/300], Step [163/172], Loss: 12.5163\n",
      "Epoch [29/300], Step [164/172], Loss: 13.5304\n",
      "Epoch [29/300], Step [165/172], Loss: 11.6280\n",
      "Epoch [29/300], Step [166/172], Loss: 12.5412\n",
      "Epoch [29/300], Step [167/172], Loss: 12.5791\n",
      "Epoch [29/300], Step [168/172], Loss: 12.4310\n",
      "Epoch [29/300], Step [169/172], Loss: 11.0295\n",
      "Epoch [29/300], Step [170/172], Loss: 11.4993\n",
      "Epoch [29/300], Step [171/172], Loss: 9.8959\n",
      "Epoch [29/300], Step [172/172], Loss: 8.1445\n",
      "Epoch [30/300], Step [1/172], Loss: 99.7287\n",
      "Epoch [30/300], Step [2/172], Loss: 100.3783\n",
      "Epoch [30/300], Step [3/172], Loss: 125.7303\n",
      "Epoch [30/300], Step [4/172], Loss: 80.5487\n",
      "Epoch [30/300], Step [5/172], Loss: 100.9875\n",
      "Epoch [30/300], Step [6/172], Loss: 48.2690\n",
      "Epoch [30/300], Step [7/172], Loss: 64.0828\n",
      "Epoch [30/300], Step [8/172], Loss: 33.4836\n",
      "Epoch [30/300], Step [9/172], Loss: 63.4749\n",
      "Epoch [30/300], Step [10/172], Loss: 69.3763\n",
      "Epoch [30/300], Step [11/172], Loss: 93.3741\n",
      "Epoch [30/300], Step [12/172], Loss: 86.4615\n",
      "Epoch [30/300], Step [13/172], Loss: 56.6386\n",
      "Epoch [30/300], Step [14/172], Loss: 100.6259\n",
      "Epoch [30/300], Step [15/172], Loss: 91.0493\n",
      "Epoch [30/300], Step [16/172], Loss: 73.5578\n",
      "Epoch [30/300], Step [17/172], Loss: 61.8513\n",
      "Epoch [30/300], Step [18/172], Loss: 85.5793\n",
      "Epoch [30/300], Step [19/172], Loss: 78.5641\n",
      "Epoch [30/300], Step [20/172], Loss: 122.7063\n",
      "Epoch [30/300], Step [21/172], Loss: 97.7019\n",
      "Epoch [30/300], Step [22/172], Loss: 94.5207\n",
      "Epoch [30/300], Step [23/172], Loss: 52.3946\n",
      "Epoch [30/300], Step [24/172], Loss: 88.1633\n",
      "Epoch [30/300], Step [25/172], Loss: 59.8985\n",
      "Epoch [30/300], Step [26/172], Loss: 66.4192\n",
      "Epoch [30/300], Step [27/172], Loss: 90.1007\n",
      "Epoch [30/300], Step [28/172], Loss: 82.8105\n",
      "Epoch [30/300], Step [29/172], Loss: 97.5754\n",
      "Epoch [30/300], Step [30/172], Loss: 81.8482\n",
      "Epoch [30/300], Step [31/172], Loss: 55.3536\n",
      "Epoch [30/300], Step [32/172], Loss: 45.1051\n",
      "Epoch [30/300], Step [33/172], Loss: 81.4569\n",
      "Epoch [30/300], Step [34/172], Loss: 25.6571\n",
      "Epoch [30/300], Step [35/172], Loss: 69.9541\n",
      "Epoch [30/300], Step [36/172], Loss: 36.7061\n",
      "Epoch [30/300], Step [37/172], Loss: 25.2376\n",
      "Epoch [30/300], Step [38/172], Loss: 41.4785\n",
      "Epoch [30/300], Step [39/172], Loss: 57.8789\n",
      "Epoch [30/300], Step [40/172], Loss: 39.4070\n",
      "Epoch [30/300], Step [41/172], Loss: 46.7816\n",
      "Epoch [30/300], Step [42/172], Loss: 45.5264\n",
      "Epoch [30/300], Step [43/172], Loss: 38.9381\n",
      "Epoch [30/300], Step [44/172], Loss: 37.0928\n",
      "Epoch [30/300], Step [45/172], Loss: 28.4193\n",
      "Epoch [30/300], Step [46/172], Loss: 54.1304\n",
      "Epoch [30/300], Step [47/172], Loss: 74.8005\n",
      "Epoch [30/300], Step [48/172], Loss: 81.9651\n",
      "Epoch [30/300], Step [49/172], Loss: 26.4729\n",
      "Epoch [30/300], Step [50/172], Loss: 62.9914\n",
      "Epoch [30/300], Step [51/172], Loss: 10.4353\n",
      "Epoch [30/300], Step [52/172], Loss: 28.2286\n",
      "Epoch [30/300], Step [53/172], Loss: 34.5286\n",
      "Epoch [30/300], Step [54/172], Loss: 25.4299\n",
      "Epoch [30/300], Step [55/172], Loss: 21.2144\n",
      "Epoch [30/300], Step [56/172], Loss: 14.3303\n",
      "Epoch [30/300], Step [57/172], Loss: 55.5937\n",
      "Epoch [30/300], Step [58/172], Loss: 23.0864\n",
      "Epoch [30/300], Step [59/172], Loss: 41.6137\n",
      "Epoch [30/300], Step [60/172], Loss: 74.2450\n",
      "Epoch [30/300], Step [61/172], Loss: 14.7023\n",
      "Epoch [30/300], Step [62/172], Loss: 20.8380\n",
      "Epoch [30/300], Step [63/172], Loss: 11.0172\n",
      "Epoch [30/300], Step [64/172], Loss: 7.1975\n",
      "Epoch [30/300], Step [65/172], Loss: 31.4224\n",
      "Epoch [30/300], Step [66/172], Loss: 12.5438\n",
      "Epoch [30/300], Step [67/172], Loss: 31.1935\n",
      "Epoch [30/300], Step [68/172], Loss: 25.4130\n",
      "Epoch [30/300], Step [69/172], Loss: 83.9708\n",
      "Epoch [30/300], Step [70/172], Loss: 73.4857\n",
      "Epoch [30/300], Step [71/172], Loss: 71.8502\n",
      "Epoch [30/300], Step [72/172], Loss: 68.3261\n",
      "Epoch [30/300], Step [73/172], Loss: 73.4352\n",
      "Epoch [30/300], Step [74/172], Loss: 55.1770\n",
      "Epoch [30/300], Step [75/172], Loss: 33.9374\n",
      "Epoch [30/300], Step [76/172], Loss: 52.8439\n",
      "Epoch [30/300], Step [77/172], Loss: 60.4478\n",
      "Epoch [30/300], Step [78/172], Loss: 63.2742\n",
      "Epoch [30/300], Step [79/172], Loss: 52.8624\n",
      "Epoch [30/300], Step [80/172], Loss: 57.0614\n",
      "Epoch [30/300], Step [81/172], Loss: 52.4444\n",
      "Epoch [30/300], Step [82/172], Loss: 47.9241\n",
      "Epoch [30/300], Step [83/172], Loss: 51.0293\n",
      "Epoch [30/300], Step [84/172], Loss: 46.4425\n",
      "Epoch [30/300], Step [85/172], Loss: 55.8132\n",
      "Epoch [30/300], Step [86/172], Loss: 41.4950\n",
      "Epoch [30/300], Step [87/172], Loss: 34.7091\n",
      "Epoch [30/300], Step [88/172], Loss: 37.7434\n",
      "Epoch [30/300], Step [89/172], Loss: 38.1798\n",
      "Epoch [30/300], Step [90/172], Loss: 35.5607\n",
      "Epoch [30/300], Step [91/172], Loss: 34.3787\n",
      "Epoch [30/300], Step [92/172], Loss: 29.7616\n",
      "Epoch [30/300], Step [93/172], Loss: 32.8561\n",
      "Epoch [30/300], Step [94/172], Loss: 34.9161\n",
      "Epoch [30/300], Step [95/172], Loss: 32.5375\n",
      "Epoch [30/300], Step [96/172], Loss: 29.3805\n",
      "Epoch [30/300], Step [97/172], Loss: 32.9897\n",
      "Epoch [30/300], Step [98/172], Loss: 29.7590\n",
      "Epoch [30/300], Step [99/172], Loss: 27.5360\n",
      "Epoch [30/300], Step [100/172], Loss: 27.1735\n",
      "Epoch [30/300], Step [101/172], Loss: 28.1644\n",
      "Epoch [30/300], Step [102/172], Loss: 26.0582\n",
      "Epoch [30/300], Step [103/172], Loss: 26.6596\n",
      "Epoch [30/300], Step [104/172], Loss: 26.2354\n",
      "Epoch [30/300], Step [105/172], Loss: 25.9159\n",
      "Epoch [30/300], Step [106/172], Loss: 25.9112\n",
      "Epoch [30/300], Step [107/172], Loss: 22.3539\n",
      "Epoch [30/300], Step [108/172], Loss: 26.6757\n",
      "Epoch [30/300], Step [109/172], Loss: 26.8814\n",
      "Epoch [30/300], Step [110/172], Loss: 24.4708\n",
      "Epoch [30/300], Step [111/172], Loss: 23.7444\n",
      "Epoch [30/300], Step [112/172], Loss: 29.3282\n",
      "Epoch [30/300], Step [113/172], Loss: 23.4605\n",
      "Epoch [30/300], Step [114/172], Loss: 24.3597\n",
      "Epoch [30/300], Step [115/172], Loss: 30.9008\n",
      "Epoch [30/300], Step [116/172], Loss: 24.1931\n",
      "Epoch [30/300], Step [117/172], Loss: 21.0310\n",
      "Epoch [30/300], Step [118/172], Loss: 23.7362\n",
      "Epoch [30/300], Step [119/172], Loss: 20.9264\n",
      "Epoch [30/300], Step [120/172], Loss: 19.9111\n",
      "Epoch [30/300], Step [121/172], Loss: 20.7758\n",
      "Epoch [30/300], Step [122/172], Loss: 17.8201\n",
      "Epoch [30/300], Step [123/172], Loss: 18.9258\n",
      "Epoch [30/300], Step [124/172], Loss: 17.4749\n",
      "Epoch [30/300], Step [125/172], Loss: 20.8216\n",
      "Epoch [30/300], Step [126/172], Loss: 20.2533\n",
      "Epoch [30/300], Step [127/172], Loss: 22.0793\n",
      "Epoch [30/300], Step [128/172], Loss: 22.8909\n",
      "Epoch [30/300], Step [129/172], Loss: 17.5317\n",
      "Epoch [30/300], Step [130/172], Loss: 19.5921\n",
      "Epoch [30/300], Step [131/172], Loss: 17.5375\n",
      "Epoch [30/300], Step [132/172], Loss: 17.7334\n",
      "Epoch [30/300], Step [133/172], Loss: 17.2843\n",
      "Epoch [30/300], Step [134/172], Loss: 17.7206\n",
      "Epoch [30/300], Step [135/172], Loss: 15.5796\n",
      "Epoch [30/300], Step [136/172], Loss: 16.7279\n",
      "Epoch [30/300], Step [137/172], Loss: 17.7421\n",
      "Epoch [30/300], Step [138/172], Loss: 15.8924\n",
      "Epoch [30/300], Step [139/172], Loss: 17.3696\n",
      "Epoch [30/300], Step [140/172], Loss: 17.1563\n",
      "Epoch [30/300], Step [141/172], Loss: 19.6770\n",
      "Epoch [30/300], Step [142/172], Loss: 18.2433\n",
      "Epoch [30/300], Step [143/172], Loss: 15.7011\n",
      "Epoch [30/300], Step [144/172], Loss: 14.7486\n",
      "Epoch [30/300], Step [145/172], Loss: 14.9990\n",
      "Epoch [30/300], Step [146/172], Loss: 15.3536\n",
      "Epoch [30/300], Step [147/172], Loss: 13.7363\n",
      "Epoch [30/300], Step [148/172], Loss: 14.0119\n",
      "Epoch [30/300], Step [149/172], Loss: 15.3162\n",
      "Epoch [30/300], Step [150/172], Loss: 15.3719\n",
      "Epoch [30/300], Step [151/172], Loss: 13.2993\n",
      "Epoch [30/300], Step [152/172], Loss: 13.7233\n",
      "Epoch [30/300], Step [153/172], Loss: 13.7383\n",
      "Epoch [30/300], Step [154/172], Loss: 15.0144\n",
      "Epoch [30/300], Step [155/172], Loss: 13.1858\n",
      "Epoch [30/300], Step [156/172], Loss: 13.9502\n",
      "Epoch [30/300], Step [157/172], Loss: 15.2385\n",
      "Epoch [30/300], Step [158/172], Loss: 13.8629\n",
      "Epoch [30/300], Step [159/172], Loss: 13.6520\n",
      "Epoch [30/300], Step [160/172], Loss: 13.4963\n",
      "Epoch [30/300], Step [161/172], Loss: 12.5676\n",
      "Epoch [30/300], Step [162/172], Loss: 12.3809\n",
      "Epoch [30/300], Step [163/172], Loss: 12.3344\n",
      "Epoch [30/300], Step [164/172], Loss: 13.3811\n",
      "Epoch [30/300], Step [165/172], Loss: 11.4465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/300], Step [166/172], Loss: 12.3506\n",
      "Epoch [30/300], Step [167/172], Loss: 12.3897\n",
      "Epoch [30/300], Step [168/172], Loss: 12.2292\n",
      "Epoch [30/300], Step [169/172], Loss: 10.8755\n",
      "Epoch [30/300], Step [170/172], Loss: 11.3315\n",
      "Epoch [30/300], Step [171/172], Loss: 9.7372\n",
      "Epoch [30/300], Step [172/172], Loss: 8.0420\n",
      "Epoch [31/300], Step [1/172], Loss: 100.0062\n",
      "Epoch [31/300], Step [2/172], Loss: 100.5103\n",
      "Epoch [31/300], Step [3/172], Loss: 125.9800\n",
      "Epoch [31/300], Step [4/172], Loss: 80.1484\n",
      "Epoch [31/300], Step [5/172], Loss: 100.6504\n",
      "Epoch [31/300], Step [6/172], Loss: 47.7542\n",
      "Epoch [31/300], Step [7/172], Loss: 63.1292\n",
      "Epoch [31/300], Step [8/172], Loss: 32.4423\n",
      "Epoch [31/300], Step [9/172], Loss: 63.0015\n",
      "Epoch [31/300], Step [10/172], Loss: 68.8776\n",
      "Epoch [31/300], Step [11/172], Loss: 93.8928\n",
      "Epoch [31/300], Step [12/172], Loss: 86.8287\n",
      "Epoch [31/300], Step [13/172], Loss: 56.4745\n",
      "Epoch [31/300], Step [14/172], Loss: 100.7228\n",
      "Epoch [31/300], Step [15/172], Loss: 91.2630\n",
      "Epoch [31/300], Step [16/172], Loss: 73.8006\n",
      "Epoch [31/300], Step [17/172], Loss: 62.1004\n",
      "Epoch [31/300], Step [18/172], Loss: 85.3785\n",
      "Epoch [31/300], Step [19/172], Loss: 79.0313\n",
      "Epoch [31/300], Step [20/172], Loss: 122.6949\n",
      "Epoch [31/300], Step [21/172], Loss: 97.9650\n",
      "Epoch [31/300], Step [22/172], Loss: 94.8740\n",
      "Epoch [31/300], Step [23/172], Loss: 50.7961\n",
      "Epoch [31/300], Step [24/172], Loss: 88.0314\n",
      "Epoch [31/300], Step [25/172], Loss: 59.9654\n",
      "Epoch [31/300], Step [26/172], Loss: 66.4991\n",
      "Epoch [31/300], Step [27/172], Loss: 90.3129\n",
      "Epoch [31/300], Step [28/172], Loss: 82.3852\n",
      "Epoch [31/300], Step [29/172], Loss: 96.9482\n",
      "Epoch [31/300], Step [30/172], Loss: 81.8765\n",
      "Epoch [31/300], Step [31/172], Loss: 55.0120\n",
      "Epoch [31/300], Step [32/172], Loss: 45.0278\n",
      "Epoch [31/300], Step [33/172], Loss: 81.5161\n",
      "Epoch [31/300], Step [34/172], Loss: 24.6143\n",
      "Epoch [31/300], Step [35/172], Loss: 69.4233\n",
      "Epoch [31/300], Step [36/172], Loss: 36.4882\n",
      "Epoch [31/300], Step [37/172], Loss: 25.2229\n",
      "Epoch [31/300], Step [38/172], Loss: 41.2940\n",
      "Epoch [31/300], Step [39/172], Loss: 58.0421\n",
      "Epoch [31/300], Step [40/172], Loss: 39.1155\n",
      "Epoch [31/300], Step [41/172], Loss: 46.8488\n",
      "Epoch [31/300], Step [42/172], Loss: 45.5585\n",
      "Epoch [31/300], Step [43/172], Loss: 38.8978\n",
      "Epoch [31/300], Step [44/172], Loss: 36.7143\n",
      "Epoch [31/300], Step [45/172], Loss: 28.3584\n",
      "Epoch [31/300], Step [46/172], Loss: 54.0019\n",
      "Epoch [31/300], Step [47/172], Loss: 74.6534\n",
      "Epoch [31/300], Step [48/172], Loss: 81.6393\n",
      "Epoch [31/300], Step [49/172], Loss: 26.4938\n",
      "Epoch [31/300], Step [50/172], Loss: 62.5002\n",
      "Epoch [31/300], Step [51/172], Loss: 10.4487\n",
      "Epoch [31/300], Step [52/172], Loss: 28.1806\n",
      "Epoch [31/300], Step [53/172], Loss: 34.4274\n",
      "Epoch [31/300], Step [54/172], Loss: 25.5474\n",
      "Epoch [31/300], Step [55/172], Loss: 21.1188\n",
      "Epoch [31/300], Step [56/172], Loss: 14.3252\n",
      "Epoch [31/300], Step [57/172], Loss: 55.4356\n",
      "Epoch [31/300], Step [58/172], Loss: 23.1905\n",
      "Epoch [31/300], Step [59/172], Loss: 41.6810\n",
      "Epoch [31/300], Step [60/172], Loss: 73.4184\n",
      "Epoch [31/300], Step [61/172], Loss: 14.6176\n",
      "Epoch [31/300], Step [62/172], Loss: 20.9314\n",
      "Epoch [31/300], Step [63/172], Loss: 10.9924\n",
      "Epoch [31/300], Step [64/172], Loss: 7.2696\n",
      "Epoch [31/300], Step [65/172], Loss: 31.5344\n",
      "Epoch [31/300], Step [66/172], Loss: 12.4818\n",
      "Epoch [31/300], Step [67/172], Loss: 31.2293\n",
      "Epoch [31/300], Step [68/172], Loss: 25.6960\n",
      "Epoch [31/300], Step [69/172], Loss: 83.7377\n",
      "Epoch [31/300], Step [70/172], Loss: 72.3271\n",
      "Epoch [31/300], Step [71/172], Loss: 70.7720\n",
      "Epoch [31/300], Step [72/172], Loss: 67.6455\n",
      "Epoch [31/300], Step [73/172], Loss: 72.7045\n",
      "Epoch [31/300], Step [74/172], Loss: 54.2503\n",
      "Epoch [31/300], Step [75/172], Loss: 33.3386\n",
      "Epoch [31/300], Step [76/172], Loss: 52.3610\n",
      "Epoch [31/300], Step [77/172], Loss: 60.2080\n",
      "Epoch [31/300], Step [78/172], Loss: 63.0799\n",
      "Epoch [31/300], Step [79/172], Loss: 52.8984\n",
      "Epoch [31/300], Step [80/172], Loss: 57.3314\n",
      "Epoch [31/300], Step [81/172], Loss: 52.7663\n",
      "Epoch [31/300], Step [82/172], Loss: 48.2302\n",
      "Epoch [31/300], Step [83/172], Loss: 51.2727\n",
      "Epoch [31/300], Step [84/172], Loss: 46.5361\n",
      "Epoch [31/300], Step [85/172], Loss: 55.7838\n",
      "Epoch [31/300], Step [86/172], Loss: 41.5898\n",
      "Epoch [31/300], Step [87/172], Loss: 34.6608\n",
      "Epoch [31/300], Step [88/172], Loss: 37.6793\n",
      "Epoch [31/300], Step [89/172], Loss: 37.9899\n",
      "Epoch [31/300], Step [90/172], Loss: 35.5424\n",
      "Epoch [31/300], Step [91/172], Loss: 34.2395\n",
      "Epoch [31/300], Step [92/172], Loss: 29.5715\n",
      "Epoch [31/300], Step [93/172], Loss: 32.6039\n",
      "Epoch [31/300], Step [94/172], Loss: 34.7265\n",
      "Epoch [31/300], Step [95/172], Loss: 32.3028\n",
      "Epoch [31/300], Step [96/172], Loss: 29.0727\n",
      "Epoch [31/300], Step [97/172], Loss: 32.7949\n",
      "Epoch [31/300], Step [98/172], Loss: 29.4680\n",
      "Epoch [31/300], Step [99/172], Loss: 27.2139\n",
      "Epoch [31/300], Step [100/172], Loss: 26.8012\n",
      "Epoch [31/300], Step [101/172], Loss: 27.8736\n",
      "Epoch [31/300], Step [102/172], Loss: 25.8686\n",
      "Epoch [31/300], Step [103/172], Loss: 26.2964\n",
      "Epoch [31/300], Step [104/172], Loss: 25.8856\n",
      "Epoch [31/300], Step [105/172], Loss: 25.7035\n",
      "Epoch [31/300], Step [106/172], Loss: 25.5830\n",
      "Epoch [31/300], Step [107/172], Loss: 22.0802\n",
      "Epoch [31/300], Step [108/172], Loss: 26.3493\n",
      "Epoch [31/300], Step [109/172], Loss: 26.6573\n",
      "Epoch [31/300], Step [110/172], Loss: 24.1238\n",
      "Epoch [31/300], Step [111/172], Loss: 23.4034\n",
      "Epoch [31/300], Step [112/172], Loss: 29.0588\n",
      "Epoch [31/300], Step [113/172], Loss: 23.1759\n",
      "Epoch [31/300], Step [114/172], Loss: 24.0488\n",
      "Epoch [31/300], Step [115/172], Loss: 30.7279\n",
      "Epoch [31/300], Step [116/172], Loss: 23.8612\n",
      "Epoch [31/300], Step [117/172], Loss: 20.7150\n",
      "Epoch [31/300], Step [118/172], Loss: 23.4695\n",
      "Epoch [31/300], Step [119/172], Loss: 20.6402\n",
      "Epoch [31/300], Step [120/172], Loss: 19.5927\n",
      "Epoch [31/300], Step [121/172], Loss: 20.4519\n",
      "Epoch [31/300], Step [122/172], Loss: 17.5464\n",
      "Epoch [31/300], Step [123/172], Loss: 18.6454\n",
      "Epoch [31/300], Step [124/172], Loss: 17.1347\n",
      "Epoch [31/300], Step [125/172], Loss: 20.5191\n",
      "Epoch [31/300], Step [126/172], Loss: 19.9038\n",
      "Epoch [31/300], Step [127/172], Loss: 21.7845\n",
      "Epoch [31/300], Step [128/172], Loss: 22.5883\n",
      "Epoch [31/300], Step [129/172], Loss: 17.2288\n",
      "Epoch [31/300], Step [130/172], Loss: 19.2803\n",
      "Epoch [31/300], Step [131/172], Loss: 17.2462\n",
      "Epoch [31/300], Step [132/172], Loss: 17.4204\n",
      "Epoch [31/300], Step [133/172], Loss: 17.0030\n",
      "Epoch [31/300], Step [134/172], Loss: 17.4445\n",
      "Epoch [31/300], Step [135/172], Loss: 15.3033\n",
      "Epoch [31/300], Step [136/172], Loss: 16.4239\n",
      "Epoch [31/300], Step [137/172], Loss: 17.4818\n",
      "Epoch [31/300], Step [138/172], Loss: 15.6148\n",
      "Epoch [31/300], Step [139/172], Loss: 17.0757\n",
      "Epoch [31/300], Step [140/172], Loss: 16.8833\n",
      "Epoch [31/300], Step [141/172], Loss: 19.4596\n",
      "Epoch [31/300], Step [142/172], Loss: 18.0041\n",
      "Epoch [31/300], Step [143/172], Loss: 15.4487\n",
      "Epoch [31/300], Step [144/172], Loss: 14.5129\n",
      "Epoch [31/300], Step [145/172], Loss: 14.7606\n",
      "Epoch [31/300], Step [146/172], Loss: 15.1304\n",
      "Epoch [31/300], Step [147/172], Loss: 13.4102\n",
      "Epoch [31/300], Step [148/172], Loss: 13.7125\n",
      "Epoch [31/300], Step [149/172], Loss: 15.0803\n",
      "Epoch [31/300], Step [150/172], Loss: 15.1253\n",
      "Epoch [31/300], Step [151/172], Loss: 13.0720\n",
      "Epoch [31/300], Step [152/172], Loss: 13.4760\n",
      "Epoch [31/300], Step [153/172], Loss: 13.4826\n",
      "Epoch [31/300], Step [154/172], Loss: 14.7699\n",
      "Epoch [31/300], Step [155/172], Loss: 12.9570\n",
      "Epoch [31/300], Step [156/172], Loss: 13.7733\n",
      "Epoch [31/300], Step [157/172], Loss: 15.0363\n",
      "Epoch [31/300], Step [158/172], Loss: 13.6322\n",
      "Epoch [31/300], Step [159/172], Loss: 13.4280\n",
      "Epoch [31/300], Step [160/172], Loss: 13.2752\n",
      "Epoch [31/300], Step [161/172], Loss: 12.3427\n",
      "Epoch [31/300], Step [162/172], Loss: 12.1642\n",
      "Epoch [31/300], Step [163/172], Loss: 12.1077\n",
      "Epoch [31/300], Step [164/172], Loss: 13.2067\n",
      "Epoch [31/300], Step [165/172], Loss: 11.2380\n",
      "Epoch [31/300], Step [166/172], Loss: 12.1190\n",
      "Epoch [31/300], Step [167/172], Loss: 12.1680\n",
      "Epoch [31/300], Step [168/172], Loss: 11.9796\n",
      "Epoch [31/300], Step [169/172], Loss: 10.6595\n",
      "Epoch [31/300], Step [170/172], Loss: 11.0972\n",
      "Epoch [31/300], Step [171/172], Loss: 9.5265\n",
      "Epoch [31/300], Step [172/172], Loss: 7.9137\n",
      "Epoch [32/300], Step [1/172], Loss: 100.1323\n",
      "Epoch [32/300], Step [2/172], Loss: 100.4618\n",
      "Epoch [32/300], Step [3/172], Loss: 125.8808\n",
      "Epoch [32/300], Step [4/172], Loss: 79.6203\n",
      "Epoch [32/300], Step [5/172], Loss: 100.2631\n",
      "Epoch [32/300], Step [6/172], Loss: 46.9614\n",
      "Epoch [32/300], Step [7/172], Loss: 62.2170\n",
      "Epoch [32/300], Step [8/172], Loss: 31.1643\n",
      "Epoch [32/300], Step [9/172], Loss: 62.4177\n",
      "Epoch [32/300], Step [10/172], Loss: 68.3396\n",
      "Epoch [32/300], Step [11/172], Loss: 94.4912\n",
      "Epoch [32/300], Step [12/172], Loss: 86.8629\n",
      "Epoch [32/300], Step [13/172], Loss: 55.7814\n",
      "Epoch [32/300], Step [14/172], Loss: 100.4825\n",
      "Epoch [32/300], Step [15/172], Loss: 90.9640\n",
      "Epoch [32/300], Step [16/172], Loss: 73.2849\n",
      "Epoch [32/300], Step [17/172], Loss: 61.6586\n",
      "Epoch [32/300], Step [18/172], Loss: 84.6826\n",
      "Epoch [32/300], Step [19/172], Loss: 78.7638\n",
      "Epoch [32/300], Step [20/172], Loss: 121.7341\n",
      "Epoch [32/300], Step [21/172], Loss: 97.6022\n",
      "Epoch [32/300], Step [22/172], Loss: 94.5234\n",
      "Epoch [32/300], Step [23/172], Loss: 48.8057\n",
      "Epoch [32/300], Step [24/172], Loss: 87.3121\n",
      "Epoch [32/300], Step [25/172], Loss: 59.2964\n",
      "Epoch [32/300], Step [26/172], Loss: 66.0370\n",
      "Epoch [32/300], Step [27/172], Loss: 89.8058\n",
      "Epoch [32/300], Step [28/172], Loss: 80.8861\n",
      "Epoch [32/300], Step [29/172], Loss: 95.3196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/300], Step [30/172], Loss: 81.2126\n",
      "Epoch [32/300], Step [31/172], Loss: 53.8827\n",
      "Epoch [32/300], Step [32/172], Loss: 44.3703\n",
      "Epoch [32/300], Step [33/172], Loss: 80.7278\n",
      "Epoch [32/300], Step [34/172], Loss: 22.9813\n",
      "Epoch [32/300], Step [35/172], Loss: 68.7860\n",
      "Epoch [32/300], Step [36/172], Loss: 35.8380\n",
      "Epoch [32/300], Step [37/172], Loss: 24.5800\n",
      "Epoch [32/300], Step [38/172], Loss: 39.9108\n",
      "Epoch [32/300], Step [39/172], Loss: 57.4188\n",
      "Epoch [32/300], Step [40/172], Loss: 37.9416\n",
      "Epoch [32/300], Step [41/172], Loss: 46.0577\n",
      "Epoch [32/300], Step [42/172], Loss: 44.7115\n",
      "Epoch [32/300], Step [43/172], Loss: 37.7319\n",
      "Epoch [32/300], Step [44/172], Loss: 35.5164\n",
      "Epoch [32/300], Step [45/172], Loss: 27.3539\n",
      "Epoch [32/300], Step [46/172], Loss: 53.1605\n",
      "Epoch [32/300], Step [47/172], Loss: 73.9794\n",
      "Epoch [32/300], Step [48/172], Loss: 80.8973\n",
      "Epoch [32/300], Step [49/172], Loss: 25.6485\n",
      "Epoch [32/300], Step [50/172], Loss: 61.5271\n",
      "Epoch [32/300], Step [51/172], Loss: 9.9905\n",
      "Epoch [32/300], Step [52/172], Loss: 27.4548\n",
      "Epoch [32/300], Step [53/172], Loss: 33.5246\n",
      "Epoch [32/300], Step [54/172], Loss: 24.4484\n",
      "Epoch [32/300], Step [55/172], Loss: 20.3497\n",
      "Epoch [32/300], Step [56/172], Loss: 13.6106\n",
      "Epoch [32/300], Step [57/172], Loss: 54.6582\n",
      "Epoch [32/300], Step [58/172], Loss: 22.7498\n",
      "Epoch [32/300], Step [59/172], Loss: 40.8872\n",
      "Epoch [32/300], Step [60/172], Loss: 73.0666\n",
      "Epoch [32/300], Step [61/172], Loss: 14.2482\n",
      "Epoch [32/300], Step [62/172], Loss: 20.5080\n",
      "Epoch [32/300], Step [63/172], Loss: 10.4932\n",
      "Epoch [32/300], Step [64/172], Loss: 6.9985\n",
      "Epoch [32/300], Step [65/172], Loss: 31.2031\n",
      "Epoch [32/300], Step [66/172], Loss: 11.9111\n",
      "Epoch [32/300], Step [67/172], Loss: 30.6237\n",
      "Epoch [32/300], Step [68/172], Loss: 23.5928\n",
      "Epoch [32/300], Step [69/172], Loss: 83.8964\n",
      "Epoch [32/300], Step [70/172], Loss: 72.9713\n",
      "Epoch [32/300], Step [71/172], Loss: 71.3130\n",
      "Epoch [32/300], Step [72/172], Loss: 68.2098\n",
      "Epoch [32/300], Step [73/172], Loss: 73.3733\n",
      "Epoch [32/300], Step [74/172], Loss: 54.1174\n",
      "Epoch [32/300], Step [75/172], Loss: 33.4629\n",
      "Epoch [32/300], Step [76/172], Loss: 52.3781\n",
      "Epoch [32/300], Step [77/172], Loss: 60.5635\n",
      "Epoch [32/300], Step [78/172], Loss: 63.1527\n",
      "Epoch [32/300], Step [79/172], Loss: 52.9511\n",
      "Epoch [32/300], Step [80/172], Loss: 57.0520\n",
      "Epoch [32/300], Step [81/172], Loss: 52.7481\n",
      "Epoch [32/300], Step [82/172], Loss: 47.9685\n",
      "Epoch [32/300], Step [83/172], Loss: 51.1008\n",
      "Epoch [32/300], Step [84/172], Loss: 46.1806\n",
      "Epoch [32/300], Step [85/172], Loss: 55.3017\n",
      "Epoch [32/300], Step [86/172], Loss: 41.2670\n",
      "Epoch [32/300], Step [87/172], Loss: 34.3673\n",
      "Epoch [32/300], Step [88/172], Loss: 37.3420\n",
      "Epoch [32/300], Step [89/172], Loss: 37.4916\n",
      "Epoch [32/300], Step [90/172], Loss: 35.3335\n",
      "Epoch [32/300], Step [91/172], Loss: 33.9622\n",
      "Epoch [32/300], Step [92/172], Loss: 29.3066\n",
      "Epoch [32/300], Step [93/172], Loss: 32.2454\n",
      "Epoch [32/300], Step [94/172], Loss: 34.4386\n",
      "Epoch [32/300], Step [95/172], Loss: 31.9747\n",
      "Epoch [32/300], Step [96/172], Loss: 28.7269\n",
      "Epoch [32/300], Step [97/172], Loss: 32.4931\n",
      "Epoch [32/300], Step [98/172], Loss: 29.1173\n",
      "Epoch [32/300], Step [99/172], Loss: 26.8762\n",
      "Epoch [32/300], Step [100/172], Loss: 26.4033\n",
      "Epoch [32/300], Step [101/172], Loss: 27.5877\n",
      "Epoch [32/300], Step [102/172], Loss: 25.5465\n",
      "Epoch [32/300], Step [103/172], Loss: 25.9057\n",
      "Epoch [32/300], Step [104/172], Loss: 25.5400\n",
      "Epoch [32/300], Step [105/172], Loss: 25.4071\n",
      "Epoch [32/300], Step [106/172], Loss: 25.3008\n",
      "Epoch [32/300], Step [107/172], Loss: 21.8425\n",
      "Epoch [32/300], Step [108/172], Loss: 26.0137\n",
      "Epoch [32/300], Step [109/172], Loss: 26.4448\n",
      "Epoch [32/300], Step [110/172], Loss: 23.8127\n",
      "Epoch [32/300], Step [111/172], Loss: 23.1230\n",
      "Epoch [32/300], Step [112/172], Loss: 28.8203\n",
      "Epoch [32/300], Step [113/172], Loss: 22.9594\n",
      "Epoch [32/300], Step [114/172], Loss: 23.7999\n",
      "Epoch [32/300], Step [115/172], Loss: 30.5814\n",
      "Epoch [32/300], Step [116/172], Loss: 23.6310\n",
      "Epoch [32/300], Step [117/172], Loss: 20.4904\n",
      "Epoch [32/300], Step [118/172], Loss: 23.2969\n",
      "Epoch [32/300], Step [119/172], Loss: 20.4429\n",
      "Epoch [32/300], Step [120/172], Loss: 19.3477\n",
      "Epoch [32/300], Step [121/172], Loss: 20.1761\n",
      "Epoch [32/300], Step [122/172], Loss: 17.3584\n",
      "Epoch [32/300], Step [123/172], Loss: 18.4657\n",
      "Epoch [32/300], Step [124/172], Loss: 16.8920\n",
      "Epoch [32/300], Step [125/172], Loss: 20.3303\n",
      "Epoch [32/300], Step [126/172], Loss: 19.6859\n",
      "Epoch [32/300], Step [127/172], Loss: 21.6012\n",
      "Epoch [32/300], Step [128/172], Loss: 22.4191\n",
      "Epoch [32/300], Step [129/172], Loss: 17.0064\n",
      "Epoch [32/300], Step [130/172], Loss: 19.0810\n",
      "Epoch [32/300], Step [131/172], Loss: 17.0486\n",
      "Epoch [32/300], Step [132/172], Loss: 17.2255\n",
      "Epoch [32/300], Step [133/172], Loss: 16.8336\n",
      "Epoch [32/300], Step [134/172], Loss: 17.2835\n",
      "Epoch [32/300], Step [135/172], Loss: 15.1313\n",
      "Epoch [32/300], Step [136/172], Loss: 16.2007\n",
      "Epoch [32/300], Step [137/172], Loss: 17.3040\n",
      "Epoch [32/300], Step [138/172], Loss: 15.4126\n",
      "Epoch [32/300], Step [139/172], Loss: 16.8914\n",
      "Epoch [32/300], Step [140/172], Loss: 16.6941\n",
      "Epoch [32/300], Step [141/172], Loss: 19.3345\n",
      "Epoch [32/300], Step [142/172], Loss: 17.8695\n",
      "Epoch [32/300], Step [143/172], Loss: 15.2773\n",
      "Epoch [32/300], Step [144/172], Loss: 14.3654\n",
      "Epoch [32/300], Step [145/172], Loss: 14.6048\n",
      "Epoch [32/300], Step [146/172], Loss: 14.9999\n",
      "Epoch [32/300], Step [147/172], Loss: 13.1914\n",
      "Epoch [32/300], Step [148/172], Loss: 13.4997\n",
      "Epoch [32/300], Step [149/172], Loss: 14.9427\n",
      "Epoch [32/300], Step [150/172], Loss: 14.9605\n",
      "Epoch [32/300], Step [151/172], Loss: 12.9412\n",
      "Epoch [32/300], Step [152/172], Loss: 13.3268\n",
      "Epoch [32/300], Step [153/172], Loss: 13.3309\n",
      "Epoch [32/300], Step [154/172], Loss: 14.6106\n",
      "Epoch [32/300], Step [155/172], Loss: 12.8233\n",
      "Epoch [32/300], Step [156/172], Loss: 13.6582\n",
      "Epoch [32/300], Step [157/172], Loss: 14.9225\n",
      "Epoch [32/300], Step [158/172], Loss: 13.4971\n",
      "Epoch [32/300], Step [159/172], Loss: 13.3197\n",
      "Epoch [32/300], Step [160/172], Loss: 13.1479\n",
      "Epoch [32/300], Step [161/172], Loss: 12.2057\n",
      "Epoch [32/300], Step [162/172], Loss: 12.0461\n",
      "Epoch [32/300], Step [163/172], Loss: 12.0096\n",
      "Epoch [32/300], Step [164/172], Loss: 13.1491\n",
      "Epoch [32/300], Step [165/172], Loss: 11.1239\n",
      "Epoch [32/300], Step [166/172], Loss: 12.0078\n",
      "Epoch [32/300], Step [167/172], Loss: 12.0520\n",
      "Epoch [32/300], Step [168/172], Loss: 11.8561\n",
      "Epoch [32/300], Step [169/172], Loss: 10.5877\n",
      "Epoch [32/300], Step [170/172], Loss: 11.0297\n",
      "Epoch [32/300], Step [171/172], Loss: 9.4450\n",
      "Epoch [32/300], Step [172/172], Loss: 7.8776\n",
      "Epoch [33/300], Step [1/172], Loss: 100.3661\n",
      "Epoch [33/300], Step [2/172], Loss: 100.4102\n",
      "Epoch [33/300], Step [3/172], Loss: 125.3175\n",
      "Epoch [33/300], Step [4/172], Loss: 79.0790\n",
      "Epoch [33/300], Step [5/172], Loss: 99.6110\n",
      "Epoch [33/300], Step [6/172], Loss: 46.5146\n",
      "Epoch [33/300], Step [7/172], Loss: 61.2861\n",
      "Epoch [33/300], Step [8/172], Loss: 29.6756\n",
      "Epoch [33/300], Step [9/172], Loss: 61.5255\n",
      "Epoch [33/300], Step [10/172], Loss: 67.6694\n",
      "Epoch [33/300], Step [11/172], Loss: 94.5391\n",
      "Epoch [33/300], Step [12/172], Loss: 86.6494\n",
      "Epoch [33/300], Step [13/172], Loss: 55.4430\n",
      "Epoch [33/300], Step [14/172], Loss: 100.4944\n",
      "Epoch [33/300], Step [15/172], Loss: 91.0373\n",
      "Epoch [33/300], Step [16/172], Loss: 73.1115\n",
      "Epoch [33/300], Step [17/172], Loss: 61.5079\n",
      "Epoch [33/300], Step [18/172], Loss: 83.9658\n",
      "Epoch [33/300], Step [19/172], Loss: 78.8501\n",
      "Epoch [33/300], Step [20/172], Loss: 121.0902\n",
      "Epoch [33/300], Step [21/172], Loss: 97.5173\n",
      "Epoch [33/300], Step [22/172], Loss: 94.3467\n",
      "Epoch [33/300], Step [23/172], Loss: 47.2208\n",
      "Epoch [33/300], Step [24/172], Loss: 86.8898\n",
      "Epoch [33/300], Step [25/172], Loss: 59.0317\n",
      "Epoch [33/300], Step [26/172], Loss: 65.7345\n",
      "Epoch [33/300], Step [27/172], Loss: 89.5951\n",
      "Epoch [33/300], Step [28/172], Loss: 80.0076\n",
      "Epoch [33/300], Step [29/172], Loss: 94.0696\n",
      "Epoch [33/300], Step [30/172], Loss: 80.9106\n",
      "Epoch [33/300], Step [31/172], Loss: 53.2462\n",
      "Epoch [33/300], Step [32/172], Loss: 44.0686\n",
      "Epoch [33/300], Step [33/172], Loss: 80.3523\n",
      "Epoch [33/300], Step [34/172], Loss: 21.7177\n",
      "Epoch [33/300], Step [35/172], Loss: 68.1697\n",
      "Epoch [33/300], Step [36/172], Loss: 35.5088\n",
      "Epoch [33/300], Step [37/172], Loss: 24.3941\n",
      "Epoch [33/300], Step [38/172], Loss: 39.4755\n",
      "Epoch [33/300], Step [39/172], Loss: 57.2943\n",
      "Epoch [33/300], Step [40/172], Loss: 37.2820\n",
      "Epoch [33/300], Step [41/172], Loss: 45.6945\n",
      "Epoch [33/300], Step [42/172], Loss: 44.3164\n",
      "Epoch [33/300], Step [43/172], Loss: 37.3004\n",
      "Epoch [33/300], Step [44/172], Loss: 34.7792\n",
      "Epoch [33/300], Step [45/172], Loss: 26.9897\n",
      "Epoch [33/300], Step [46/172], Loss: 52.6475\n",
      "Epoch [33/300], Step [47/172], Loss: 73.4882\n",
      "Epoch [33/300], Step [48/172], Loss: 80.2378\n",
      "Epoch [33/300], Step [49/172], Loss: 25.4520\n",
      "Epoch [33/300], Step [50/172], Loss: 60.9447\n",
      "Epoch [33/300], Step [51/172], Loss: 9.8997\n",
      "Epoch [33/300], Step [52/172], Loss: 27.1897\n",
      "Epoch [33/300], Step [53/172], Loss: 33.2371\n",
      "Epoch [33/300], Step [54/172], Loss: 24.1946\n",
      "Epoch [33/300], Step [55/172], Loss: 20.0564\n",
      "Epoch [33/300], Step [56/172], Loss: 13.4890\n",
      "Epoch [33/300], Step [57/172], Loss: 54.2772\n",
      "Epoch [33/300], Step [58/172], Loss: 22.6745\n",
      "Epoch [33/300], Step [59/172], Loss: 40.8065\n",
      "Epoch [33/300], Step [60/172], Loss: 72.4024\n",
      "Epoch [33/300], Step [61/172], Loss: 14.0959\n",
      "Epoch [33/300], Step [62/172], Loss: 20.6219\n",
      "Epoch [33/300], Step [63/172], Loss: 10.3640\n",
      "Epoch [33/300], Step [64/172], Loss: 7.0125\n",
      "Epoch [33/300], Step [65/172], Loss: 31.2118\n",
      "Epoch [33/300], Step [66/172], Loss: 11.7510\n",
      "Epoch [33/300], Step [67/172], Loss: 30.5080\n",
      "Epoch [33/300], Step [68/172], Loss: 23.5012\n",
      "Epoch [33/300], Step [69/172], Loss: 83.8852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/300], Step [70/172], Loss: 72.3885\n",
      "Epoch [33/300], Step [71/172], Loss: 70.7399\n",
      "Epoch [33/300], Step [72/172], Loss: 67.8976\n",
      "Epoch [33/300], Step [73/172], Loss: 72.9895\n",
      "Epoch [33/300], Step [74/172], Loss: 53.5652\n",
      "Epoch [33/300], Step [75/172], Loss: 33.1540\n",
      "Epoch [33/300], Step [76/172], Loss: 52.1093\n",
      "Epoch [33/300], Step [77/172], Loss: 60.5728\n",
      "Epoch [33/300], Step [78/172], Loss: 63.0657\n",
      "Epoch [33/300], Step [79/172], Loss: 53.1338\n",
      "Epoch [33/300], Step [80/172], Loss: 57.3020\n",
      "Epoch [33/300], Step [81/172], Loss: 53.1768\n",
      "Epoch [33/300], Step [82/172], Loss: 48.0727\n",
      "Epoch [33/300], Step [83/172], Loss: 51.4840\n",
      "Epoch [33/300], Step [84/172], Loss: 46.3548\n",
      "Epoch [33/300], Step [85/172], Loss: 55.3930\n",
      "Epoch [33/300], Step [86/172], Loss: 41.3253\n",
      "Epoch [33/300], Step [87/172], Loss: 34.3830\n",
      "Epoch [33/300], Step [88/172], Loss: 37.3358\n",
      "Epoch [33/300], Step [89/172], Loss: 37.3796\n",
      "Epoch [33/300], Step [90/172], Loss: 35.3516\n",
      "Epoch [33/300], Step [91/172], Loss: 33.9019\n",
      "Epoch [33/300], Step [92/172], Loss: 29.1572\n",
      "Epoch [33/300], Step [93/172], Loss: 32.0460\n",
      "Epoch [33/300], Step [94/172], Loss: 34.2992\n",
      "Epoch [33/300], Step [95/172], Loss: 31.7865\n",
      "Epoch [33/300], Step [96/172], Loss: 28.4769\n",
      "Epoch [33/300], Step [97/172], Loss: 32.3400\n",
      "Epoch [33/300], Step [98/172], Loss: 28.8767\n",
      "Epoch [33/300], Step [99/172], Loss: 26.5956\n",
      "Epoch [33/300], Step [100/172], Loss: 26.0811\n",
      "Epoch [33/300], Step [101/172], Loss: 27.3553\n",
      "Epoch [33/300], Step [102/172], Loss: 25.2663\n",
      "Epoch [33/300], Step [103/172], Loss: 25.5830\n",
      "Epoch [33/300], Step [104/172], Loss: 25.2183\n",
      "Epoch [33/300], Step [105/172], Loss: 25.1163\n",
      "Epoch [33/300], Step [106/172], Loss: 25.0273\n",
      "Epoch [33/300], Step [107/172], Loss: 21.5820\n",
      "Epoch [33/300], Step [108/172], Loss: 25.7060\n",
      "Epoch [33/300], Step [109/172], Loss: 26.2398\n",
      "Epoch [33/300], Step [110/172], Loss: 23.4838\n",
      "Epoch [33/300], Step [111/172], Loss: 22.7969\n",
      "Epoch [33/300], Step [112/172], Loss: 28.5490\n",
      "Epoch [33/300], Step [113/172], Loss: 22.7066\n",
      "Epoch [33/300], Step [114/172], Loss: 23.5111\n",
      "Epoch [33/300], Step [115/172], Loss: 30.4104\n",
      "Epoch [33/300], Step [116/172], Loss: 23.3322\n",
      "Epoch [33/300], Step [117/172], Loss: 20.1920\n",
      "Epoch [33/300], Step [118/172], Loss: 23.0500\n",
      "Epoch [33/300], Step [119/172], Loss: 20.1697\n",
      "Epoch [33/300], Step [120/172], Loss: 19.0385\n",
      "Epoch [33/300], Step [121/172], Loss: 19.8586\n",
      "Epoch [33/300], Step [122/172], Loss: 17.0912\n",
      "Epoch [33/300], Step [123/172], Loss: 18.2015\n",
      "Epoch [33/300], Step [124/172], Loss: 16.5606\n",
      "Epoch [33/300], Step [125/172], Loss: 20.0509\n",
      "Epoch [33/300], Step [126/172], Loss: 19.3701\n",
      "Epoch [33/300], Step [127/172], Loss: 21.3459\n",
      "Epoch [33/300], Step [128/172], Loss: 22.1653\n",
      "Epoch [33/300], Step [129/172], Loss: 16.6999\n",
      "Epoch [33/300], Step [130/172], Loss: 18.7784\n",
      "Epoch [33/300], Step [131/172], Loss: 16.7636\n",
      "Epoch [33/300], Step [132/172], Loss: 16.9146\n",
      "Epoch [33/300], Step [133/172], Loss: 16.5505\n",
      "Epoch [33/300], Step [134/172], Loss: 17.0086\n",
      "Epoch [33/300], Step [135/172], Loss: 14.8517\n",
      "Epoch [33/300], Step [136/172], Loss: 15.9167\n",
      "Epoch [33/300], Step [137/172], Loss: 17.0485\n",
      "Epoch [33/300], Step [138/172], Loss: 15.1470\n",
      "Epoch [33/300], Step [139/172], Loss: 16.6109\n",
      "Epoch [33/300], Step [140/172], Loss: 16.4280\n",
      "Epoch [33/300], Step [141/172], Loss: 19.1249\n",
      "Epoch [33/300], Step [142/172], Loss: 17.6475\n",
      "Epoch [33/300], Step [143/172], Loss: 15.0246\n",
      "Epoch [33/300], Step [144/172], Loss: 14.1167\n",
      "Epoch [33/300], Step [145/172], Loss: 14.3605\n",
      "Epoch [33/300], Step [146/172], Loss: 14.7709\n",
      "Epoch [33/300], Step [147/172], Loss: 12.8430\n",
      "Epoch [33/300], Step [148/172], Loss: 13.1794\n",
      "Epoch [33/300], Step [149/172], Loss: 14.7052\n",
      "Epoch [33/300], Step [150/172], Loss: 14.7008\n",
      "Epoch [33/300], Step [151/172], Loss: 12.7031\n",
      "Epoch [33/300], Step [152/172], Loss: 13.0695\n",
      "Epoch [33/300], Step [153/172], Loss: 13.0592\n",
      "Epoch [33/300], Step [154/172], Loss: 14.3538\n",
      "Epoch [33/300], Step [155/172], Loss: 12.5669\n",
      "Epoch [33/300], Step [156/172], Loss: 13.4438\n",
      "Epoch [33/300], Step [157/172], Loss: 14.7088\n",
      "Epoch [33/300], Step [158/172], Loss: 13.2484\n",
      "Epoch [33/300], Step [159/172], Loss: 13.0721\n",
      "Epoch [33/300], Step [160/172], Loss: 12.9066\n",
      "Epoch [33/300], Step [161/172], Loss: 11.9599\n",
      "Epoch [33/300], Step [162/172], Loss: 11.8220\n",
      "Epoch [33/300], Step [163/172], Loss: 11.7494\n",
      "Epoch [33/300], Step [164/172], Loss: 12.9394\n",
      "Epoch [33/300], Step [165/172], Loss: 10.8884\n",
      "Epoch [33/300], Step [166/172], Loss: 11.7343\n",
      "Epoch [33/300], Step [167/172], Loss: 11.7966\n",
      "Epoch [33/300], Step [168/172], Loss: 11.5567\n",
      "Epoch [33/300], Step [169/172], Loss: 10.3503\n",
      "Epoch [33/300], Step [170/172], Loss: 10.7562\n",
      "Epoch [33/300], Step [171/172], Loss: 9.2152\n",
      "Epoch [33/300], Step [172/172], Loss: 7.7136\n",
      "Epoch [34/300], Step [1/172], Loss: 100.9582\n",
      "Epoch [34/300], Step [2/172], Loss: 100.8146\n",
      "Epoch [34/300], Step [3/172], Loss: 124.8538\n",
      "Epoch [34/300], Step [4/172], Loss: 78.8532\n",
      "Epoch [34/300], Step [5/172], Loss: 98.9560\n",
      "Epoch [34/300], Step [6/172], Loss: 46.1210\n",
      "Epoch [34/300], Step [7/172], Loss: 60.6068\n",
      "Epoch [34/300], Step [8/172], Loss: 28.4444\n",
      "Epoch [34/300], Step [9/172], Loss: 61.1414\n",
      "Epoch [34/300], Step [10/172], Loss: 67.3512\n",
      "Epoch [34/300], Step [11/172], Loss: 95.2958\n",
      "Epoch [34/300], Step [12/172], Loss: 87.1146\n",
      "Epoch [34/300], Step [13/172], Loss: 55.2342\n",
      "Epoch [34/300], Step [14/172], Loss: 100.6517\n",
      "Epoch [34/300], Step [15/172], Loss: 91.0950\n",
      "Epoch [34/300], Step [16/172], Loss: 73.1444\n",
      "Epoch [34/300], Step [17/172], Loss: 61.5492\n",
      "Epoch [34/300], Step [18/172], Loss: 83.6850\n",
      "Epoch [34/300], Step [19/172], Loss: 79.0284\n",
      "Epoch [34/300], Step [20/172], Loss: 120.1530\n",
      "Epoch [34/300], Step [21/172], Loss: 97.5533\n",
      "Epoch [34/300], Step [22/172], Loss: 94.3463\n",
      "Epoch [34/300], Step [23/172], Loss: 45.1218\n",
      "Epoch [34/300], Step [24/172], Loss: 86.2047\n",
      "Epoch [34/300], Step [25/172], Loss: 58.5825\n",
      "Epoch [34/300], Step [26/172], Loss: 65.5052\n",
      "Epoch [34/300], Step [27/172], Loss: 89.2948\n",
      "Epoch [34/300], Step [28/172], Loss: 78.5701\n",
      "Epoch [34/300], Step [29/172], Loss: 92.5818\n",
      "Epoch [34/300], Step [30/172], Loss: 80.4585\n",
      "Epoch [34/300], Step [31/172], Loss: 52.3682\n",
      "Epoch [34/300], Step [32/172], Loss: 43.6576\n",
      "Epoch [34/300], Step [33/172], Loss: 79.8167\n",
      "Epoch [34/300], Step [34/172], Loss: 20.5208\n",
      "Epoch [34/300], Step [35/172], Loss: 67.5962\n",
      "Epoch [34/300], Step [36/172], Loss: 34.9081\n",
      "Epoch [34/300], Step [37/172], Loss: 23.9353\n",
      "Epoch [34/300], Step [38/172], Loss: 38.4139\n",
      "Epoch [34/300], Step [39/172], Loss: 56.8784\n",
      "Epoch [34/300], Step [40/172], Loss: 36.3605\n",
      "Epoch [34/300], Step [41/172], Loss: 45.1937\n",
      "Epoch [34/300], Step [42/172], Loss: 43.7904\n",
      "Epoch [34/300], Step [43/172], Loss: 36.4918\n",
      "Epoch [34/300], Step [44/172], Loss: 33.9313\n",
      "Epoch [34/300], Step [45/172], Loss: 26.3346\n",
      "Epoch [34/300], Step [46/172], Loss: 52.0130\n",
      "Epoch [34/300], Step [47/172], Loss: 72.9413\n",
      "Epoch [34/300], Step [48/172], Loss: 79.5802\n",
      "Epoch [34/300], Step [49/172], Loss: 24.8124\n",
      "Epoch [34/300], Step [50/172], Loss: 59.9653\n",
      "Epoch [34/300], Step [51/172], Loss: 9.5587\n",
      "Epoch [34/300], Step [52/172], Loss: 26.6010\n",
      "Epoch [34/300], Step [53/172], Loss: 32.4088\n",
      "Epoch [34/300], Step [54/172], Loss: 23.2223\n",
      "Epoch [34/300], Step [55/172], Loss: 19.3476\n",
      "Epoch [34/300], Step [56/172], Loss: 12.9004\n",
      "Epoch [34/300], Step [57/172], Loss: 53.5730\n",
      "Epoch [34/300], Step [58/172], Loss: 22.2560\n",
      "Epoch [34/300], Step [59/172], Loss: 40.1143\n",
      "Epoch [34/300], Step [60/172], Loss: 71.8045\n",
      "Epoch [34/300], Step [61/172], Loss: 13.7142\n",
      "Epoch [34/300], Step [62/172], Loss: 20.2133\n",
      "Epoch [34/300], Step [63/172], Loss: 9.8828\n",
      "Epoch [34/300], Step [64/172], Loss: 6.7405\n",
      "Epoch [34/300], Step [65/172], Loss: 30.8013\n",
      "Epoch [34/300], Step [66/172], Loss: 11.0338\n",
      "Epoch [34/300], Step [67/172], Loss: 29.8474\n",
      "Epoch [34/300], Step [68/172], Loss: 21.5660\n",
      "Epoch [34/300], Step [69/172], Loss: 83.8636\n",
      "Epoch [34/300], Step [70/172], Loss: 73.7014\n",
      "Epoch [34/300], Step [71/172], Loss: 71.7057\n",
      "Epoch [34/300], Step [72/172], Loss: 68.7672\n",
      "Epoch [34/300], Step [73/172], Loss: 74.0923\n",
      "Epoch [34/300], Step [74/172], Loss: 53.6587\n",
      "Epoch [34/300], Step [75/172], Loss: 33.4813\n",
      "Epoch [34/300], Step [76/172], Loss: 52.3987\n",
      "Epoch [34/300], Step [77/172], Loss: 61.0482\n",
      "Epoch [34/300], Step [78/172], Loss: 62.9833\n",
      "Epoch [34/300], Step [79/172], Loss: 53.1240\n",
      "Epoch [34/300], Step [80/172], Loss: 56.8485\n",
      "Epoch [34/300], Step [81/172], Loss: 52.9974\n",
      "Epoch [34/300], Step [82/172], Loss: 47.5859\n",
      "Epoch [34/300], Step [83/172], Loss: 51.0934\n",
      "Epoch [34/300], Step [84/172], Loss: 45.7547\n",
      "Epoch [34/300], Step [85/172], Loss: 54.6344\n",
      "Epoch [34/300], Step [86/172], Loss: 40.8400\n",
      "Epoch [34/300], Step [87/172], Loss: 33.9742\n",
      "Epoch [34/300], Step [88/172], Loss: 36.8830\n",
      "Epoch [34/300], Step [89/172], Loss: 36.7613\n",
      "Epoch [34/300], Step [90/172], Loss: 35.0255\n",
      "Epoch [34/300], Step [91/172], Loss: 33.5937\n",
      "Epoch [34/300], Step [92/172], Loss: 28.7770\n",
      "Epoch [34/300], Step [93/172], Loss: 31.5794\n",
      "Epoch [34/300], Step [94/172], Loss: 33.8995\n",
      "Epoch [34/300], Step [95/172], Loss: 31.3741\n",
      "Epoch [34/300], Step [96/172], Loss: 28.0793\n",
      "Epoch [34/300], Step [97/172], Loss: 32.0012\n",
      "Epoch [34/300], Step [98/172], Loss: 28.4631\n",
      "Epoch [34/300], Step [99/172], Loss: 26.2159\n",
      "Epoch [34/300], Step [100/172], Loss: 25.6420\n",
      "Epoch [34/300], Step [101/172], Loss: 27.0232\n",
      "Epoch [34/300], Step [102/172], Loss: 24.9052\n",
      "Epoch [34/300], Step [103/172], Loss: 25.1240\n",
      "Epoch [34/300], Step [104/172], Loss: 24.8302\n",
      "Epoch [34/300], Step [105/172], Loss: 24.7852\n",
      "Epoch [34/300], Step [106/172], Loss: 24.6826\n",
      "Epoch [34/300], Step [107/172], Loss: 21.3018\n",
      "Epoch [34/300], Step [108/172], Loss: 25.3488\n",
      "Epoch [34/300], Step [109/172], Loss: 26.0098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/300], Step [110/172], Loss: 23.1444\n",
      "Epoch [34/300], Step [111/172], Loss: 22.4411\n",
      "Epoch [34/300], Step [112/172], Loss: 28.0855\n",
      "Epoch [34/300], Step [113/172], Loss: 22.4024\n",
      "Epoch [34/300], Step [114/172], Loss: 23.1817\n",
      "Epoch [34/300], Step [115/172], Loss: 30.1346\n",
      "Epoch [34/300], Step [116/172], Loss: 22.9685\n",
      "Epoch [34/300], Step [117/172], Loss: 19.8813\n",
      "Epoch [34/300], Step [118/172], Loss: 22.7957\n",
      "Epoch [34/300], Step [119/172], Loss: 19.8857\n",
      "Epoch [34/300], Step [120/172], Loss: 18.7399\n",
      "Epoch [34/300], Step [121/172], Loss: 19.5587\n",
      "Epoch [34/300], Step [122/172], Loss: 16.8535\n",
      "Epoch [34/300], Step [123/172], Loss: 17.8779\n",
      "Epoch [34/300], Step [124/172], Loss: 16.2131\n",
      "Epoch [34/300], Step [125/172], Loss: 19.7683\n",
      "Epoch [34/300], Step [126/172], Loss: 19.0760\n",
      "Epoch [34/300], Step [127/172], Loss: 21.0435\n",
      "Epoch [34/300], Step [128/172], Loss: 21.8455\n",
      "Epoch [34/300], Step [129/172], Loss: 16.4203\n",
      "Epoch [34/300], Step [130/172], Loss: 18.5002\n",
      "Epoch [34/300], Step [131/172], Loss: 16.4913\n",
      "Epoch [34/300], Step [132/172], Loss: 16.6448\n",
      "Epoch [34/300], Step [133/172], Loss: 16.3024\n",
      "Epoch [34/300], Step [134/172], Loss: 16.7581\n",
      "Epoch [34/300], Step [135/172], Loss: 14.6040\n",
      "Epoch [34/300], Step [136/172], Loss: 15.6132\n",
      "Epoch [34/300], Step [137/172], Loss: 16.8085\n",
      "Epoch [34/300], Step [138/172], Loss: 14.8924\n",
      "Epoch [34/300], Step [139/172], Loss: 16.3717\n",
      "Epoch [34/300], Step [140/172], Loss: 16.2065\n",
      "Epoch [34/300], Step [141/172], Loss: 18.9349\n",
      "Epoch [34/300], Step [142/172], Loss: 17.4585\n",
      "Epoch [34/300], Step [143/172], Loss: 14.8406\n",
      "Epoch [34/300], Step [144/172], Loss: 13.9177\n",
      "Epoch [34/300], Step [145/172], Loss: 14.1491\n",
      "Epoch [34/300], Step [146/172], Loss: 14.5788\n",
      "Epoch [34/300], Step [147/172], Loss: 12.5369\n",
      "Epoch [34/300], Step [148/172], Loss: 12.9174\n",
      "Epoch [34/300], Step [149/172], Loss: 14.5358\n",
      "Epoch [34/300], Step [150/172], Loss: 14.4755\n",
      "Epoch [34/300], Step [151/172], Loss: 12.5170\n",
      "Epoch [34/300], Step [152/172], Loss: 12.8909\n",
      "Epoch [34/300], Step [153/172], Loss: 12.8667\n",
      "Epoch [34/300], Step [154/172], Loss: 14.1536\n",
      "Epoch [34/300], Step [155/172], Loss: 12.3881\n",
      "Epoch [34/300], Step [156/172], Loss: 13.3129\n",
      "Epoch [34/300], Step [157/172], Loss: 14.5788\n",
      "Epoch [34/300], Step [158/172], Loss: 13.0820\n",
      "Epoch [34/300], Step [159/172], Loss: 12.8917\n",
      "Epoch [34/300], Step [160/172], Loss: 12.7297\n",
      "Epoch [34/300], Step [161/172], Loss: 11.7609\n",
      "Epoch [34/300], Step [162/172], Loss: 11.6813\n",
      "Epoch [34/300], Step [163/172], Loss: 11.5801\n",
      "Epoch [34/300], Step [164/172], Loss: 12.8314\n",
      "Epoch [34/300], Step [165/172], Loss: 10.7191\n",
      "Epoch [34/300], Step [166/172], Loss: 11.5237\n",
      "Epoch [34/300], Step [167/172], Loss: 11.6086\n",
      "Epoch [34/300], Step [168/172], Loss: 11.3523\n",
      "Epoch [34/300], Step [169/172], Loss: 10.1770\n",
      "Epoch [34/300], Step [170/172], Loss: 10.5422\n",
      "Epoch [34/300], Step [171/172], Loss: 9.0060\n",
      "Epoch [34/300], Step [172/172], Loss: 7.5925\n",
      "Epoch [35/300], Step [1/172], Loss: 100.9411\n",
      "Epoch [35/300], Step [2/172], Loss: 100.7108\n",
      "Epoch [35/300], Step [3/172], Loss: 125.7281\n",
      "Epoch [35/300], Step [4/172], Loss: 78.4715\n",
      "Epoch [35/300], Step [5/172], Loss: 98.9132\n",
      "Epoch [35/300], Step [6/172], Loss: 45.4045\n",
      "Epoch [35/300], Step [7/172], Loss: 59.2753\n",
      "Epoch [35/300], Step [8/172], Loss: 26.7309\n",
      "Epoch [35/300], Step [9/172], Loss: 60.2014\n",
      "Epoch [35/300], Step [10/172], Loss: 66.6239\n",
      "Epoch [35/300], Step [11/172], Loss: 95.6942\n",
      "Epoch [35/300], Step [12/172], Loss: 87.3500\n",
      "Epoch [35/300], Step [13/172], Loss: 54.6063\n",
      "Epoch [35/300], Step [14/172], Loss: 100.5371\n",
      "Epoch [35/300], Step [15/172], Loss: 90.9321\n",
      "Epoch [35/300], Step [16/172], Loss: 72.5608\n",
      "Epoch [35/300], Step [17/172], Loss: 61.5927\n",
      "Epoch [35/300], Step [18/172], Loss: 82.9413\n",
      "Epoch [35/300], Step [19/172], Loss: 79.1140\n",
      "Epoch [35/300], Step [20/172], Loss: 120.2933\n",
      "Epoch [35/300], Step [21/172], Loss: 97.4980\n",
      "Epoch [35/300], Step [22/172], Loss: 94.1066\n",
      "Epoch [35/300], Step [23/172], Loss: 43.2989\n",
      "Epoch [35/300], Step [24/172], Loss: 85.7261\n",
      "Epoch [35/300], Step [25/172], Loss: 58.6556\n",
      "Epoch [35/300], Step [26/172], Loss: 65.3294\n",
      "Epoch [35/300], Step [27/172], Loss: 89.2206\n",
      "Epoch [35/300], Step [28/172], Loss: 77.8443\n",
      "Epoch [35/300], Step [29/172], Loss: 91.6459\n",
      "Epoch [35/300], Step [30/172], Loss: 80.2250\n",
      "Epoch [35/300], Step [31/172], Loss: 52.1796\n",
      "Epoch [35/300], Step [32/172], Loss: 43.7643\n",
      "Epoch [35/300], Step [33/172], Loss: 79.8222\n",
      "Epoch [35/300], Step [34/172], Loss: 19.8186\n",
      "Epoch [35/300], Step [35/172], Loss: 66.6438\n",
      "Epoch [35/300], Step [36/172], Loss: 34.6250\n",
      "Epoch [35/300], Step [37/172], Loss: 23.9517\n",
      "Epoch [35/300], Step [38/172], Loss: 38.2719\n",
      "Epoch [35/300], Step [39/172], Loss: 56.9288\n",
      "Epoch [35/300], Step [40/172], Loss: 36.1927\n",
      "Epoch [35/300], Step [41/172], Loss: 45.2615\n",
      "Epoch [35/300], Step [42/172], Loss: 44.0222\n",
      "Epoch [35/300], Step [43/172], Loss: 36.5792\n",
      "Epoch [35/300], Step [44/172], Loss: 33.5905\n",
      "Epoch [35/300], Step [45/172], Loss: 26.3959\n",
      "Epoch [35/300], Step [46/172], Loss: 51.7987\n",
      "Epoch [35/300], Step [47/172], Loss: 72.7469\n",
      "Epoch [35/300], Step [48/172], Loss: 78.5472\n",
      "Epoch [35/300], Step [49/172], Loss: 24.9516\n",
      "Epoch [35/300], Step [50/172], Loss: 59.4729\n",
      "Epoch [35/300], Step [51/172], Loss: 9.6683\n",
      "Epoch [35/300], Step [52/172], Loss: 26.6809\n",
      "Epoch [35/300], Step [53/172], Loss: 32.4418\n",
      "Epoch [35/300], Step [54/172], Loss: 23.2274\n",
      "Epoch [35/300], Step [55/172], Loss: 19.2782\n",
      "Epoch [35/300], Step [56/172], Loss: 13.0440\n",
      "Epoch [35/300], Step [57/172], Loss: 53.2882\n",
      "Epoch [35/300], Step [58/172], Loss: 22.2165\n",
      "Epoch [35/300], Step [59/172], Loss: 40.0929\n",
      "Epoch [35/300], Step [60/172], Loss: 70.9589\n",
      "Epoch [35/300], Step [61/172], Loss: 13.7068\n",
      "Epoch [35/300], Step [62/172], Loss: 20.3464\n",
      "Epoch [35/300], Step [63/172], Loss: 10.0179\n",
      "Epoch [35/300], Step [64/172], Loss: 6.8831\n",
      "Epoch [35/300], Step [65/172], Loss: 30.8873\n",
      "Epoch [35/300], Step [66/172], Loss: 11.0089\n",
      "Epoch [35/300], Step [67/172], Loss: 29.9393\n",
      "Epoch [35/300], Step [68/172], Loss: 21.4042\n",
      "Epoch [35/300], Step [69/172], Loss: 83.6743\n",
      "Epoch [35/300], Step [70/172], Loss: 72.6471\n",
      "Epoch [35/300], Step [71/172], Loss: 70.4056\n",
      "Epoch [35/300], Step [72/172], Loss: 67.8607\n",
      "Epoch [35/300], Step [73/172], Loss: 73.2054\n",
      "Epoch [35/300], Step [74/172], Loss: 52.6661\n",
      "Epoch [35/300], Step [75/172], Loss: 33.0325\n",
      "Epoch [35/300], Step [76/172], Loss: 51.9288\n",
      "Epoch [35/300], Step [77/172], Loss: 60.9008\n",
      "Epoch [35/300], Step [78/172], Loss: 62.8535\n",
      "Epoch [35/300], Step [79/172], Loss: 53.2215\n",
      "Epoch [35/300], Step [80/172], Loss: 57.1147\n",
      "Epoch [35/300], Step [81/172], Loss: 53.3559\n",
      "Epoch [35/300], Step [82/172], Loss: 47.7438\n",
      "Epoch [35/300], Step [83/172], Loss: 51.4810\n",
      "Epoch [35/300], Step [84/172], Loss: 45.9649\n",
      "Epoch [35/300], Step [85/172], Loss: 54.7601\n",
      "Epoch [35/300], Step [86/172], Loss: 40.9669\n",
      "Epoch [35/300], Step [87/172], Loss: 33.9770\n",
      "Epoch [35/300], Step [88/172], Loss: 36.9409\n",
      "Epoch [35/300], Step [89/172], Loss: 36.7361\n",
      "Epoch [35/300], Step [90/172], Loss: 35.0778\n",
      "Epoch [35/300], Step [91/172], Loss: 33.5706\n",
      "Epoch [35/300], Step [92/172], Loss: 28.7134\n",
      "Epoch [35/300], Step [93/172], Loss: 31.4243\n",
      "Epoch [35/300], Step [94/172], Loss: 33.8278\n",
      "Epoch [35/300], Step [95/172], Loss: 31.1853\n",
      "Epoch [35/300], Step [96/172], Loss: 27.8524\n",
      "Epoch [35/300], Step [97/172], Loss: 31.8583\n",
      "Epoch [35/300], Step [98/172], Loss: 28.2301\n",
      "Epoch [35/300], Step [99/172], Loss: 25.9457\n",
      "Epoch [35/300], Step [100/172], Loss: 25.3480\n",
      "Epoch [35/300], Step [101/172], Loss: 26.7680\n",
      "Epoch [35/300], Step [102/172], Loss: 24.6680\n",
      "Epoch [35/300], Step [103/172], Loss: 24.8229\n",
      "Epoch [35/300], Step [104/172], Loss: 24.5008\n",
      "Epoch [35/300], Step [105/172], Loss: 24.5213\n",
      "Epoch [35/300], Step [106/172], Loss: 24.4247\n",
      "Epoch [35/300], Step [107/172], Loss: 21.0615\n",
      "Epoch [35/300], Step [108/172], Loss: 25.0631\n",
      "Epoch [35/300], Step [109/172], Loss: 25.8350\n",
      "Epoch [35/300], Step [110/172], Loss: 22.8376\n",
      "Epoch [35/300], Step [111/172], Loss: 22.1436\n",
      "Epoch [35/300], Step [112/172], Loss: 27.9217\n",
      "Epoch [35/300], Step [113/172], Loss: 22.1717\n",
      "Epoch [35/300], Step [114/172], Loss: 22.9126\n",
      "Epoch [35/300], Step [115/172], Loss: 30.0035\n",
      "Epoch [35/300], Step [116/172], Loss: 22.7138\n",
      "Epoch [35/300], Step [117/172], Loss: 19.6190\n",
      "Epoch [35/300], Step [118/172], Loss: 22.6207\n",
      "Epoch [35/300], Step [119/172], Loss: 19.6572\n",
      "Epoch [35/300], Step [120/172], Loss: 18.4763\n",
      "Epoch [35/300], Step [121/172], Loss: 19.2632\n",
      "Epoch [35/300], Step [122/172], Loss: 16.6253\n",
      "Epoch [35/300], Step [123/172], Loss: 17.6882\n",
      "Epoch [35/300], Step [124/172], Loss: 15.9371\n",
      "Epoch [35/300], Step [125/172], Loss: 19.5513\n",
      "Epoch [35/300], Step [126/172], Loss: 18.7871\n",
      "Epoch [35/300], Step [127/172], Loss: 20.8183\n",
      "Epoch [35/300], Step [128/172], Loss: 21.6381\n",
      "Epoch [35/300], Step [129/172], Loss: 16.1371\n",
      "Epoch [35/300], Step [130/172], Loss: 18.2516\n",
      "Epoch [35/300], Step [131/172], Loss: 16.2375\n",
      "Epoch [35/300], Step [132/172], Loss: 16.3730\n",
      "Epoch [35/300], Step [133/172], Loss: 16.0729\n",
      "Epoch [35/300], Step [134/172], Loss: 16.5525\n",
      "Epoch [35/300], Step [135/172], Loss: 14.3711\n",
      "Epoch [35/300], Step [136/172], Loss: 15.3757\n",
      "Epoch [35/300], Step [137/172], Loss: 16.5792\n",
      "Epoch [35/300], Step [138/172], Loss: 14.6380\n",
      "Epoch [35/300], Step [139/172], Loss: 16.1072\n",
      "Epoch [35/300], Step [140/172], Loss: 15.9470\n",
      "Epoch [35/300], Step [141/172], Loss: 18.7547\n",
      "Epoch [35/300], Step [142/172], Loss: 17.2607\n",
      "Epoch [35/300], Step [143/172], Loss: 14.5926\n",
      "Epoch [35/300], Step [144/172], Loss: 13.7102\n",
      "Epoch [35/300], Step [145/172], Loss: 13.9429\n",
      "Epoch [35/300], Step [146/172], Loss: 14.3811\n",
      "Epoch [35/300], Step [147/172], Loss: 12.2922\n",
      "Epoch [35/300], Step [148/172], Loss: 12.6745\n",
      "Epoch [35/300], Step [149/172], Loss: 14.3215\n",
      "Epoch [35/300], Step [150/172], Loss: 14.2513\n",
      "Epoch [35/300], Step [151/172], Loss: 12.3324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/300], Step [152/172], Loss: 12.6480\n",
      "Epoch [35/300], Step [153/172], Loss: 12.6391\n",
      "Epoch [35/300], Step [154/172], Loss: 13.9411\n",
      "Epoch [35/300], Step [155/172], Loss: 12.1759\n",
      "Epoch [35/300], Step [156/172], Loss: 13.1441\n",
      "Epoch [35/300], Step [157/172], Loss: 14.3856\n",
      "Epoch [35/300], Step [158/172], Loss: 12.8678\n",
      "Epoch [35/300], Step [159/172], Loss: 12.7347\n",
      "Epoch [35/300], Step [160/172], Loss: 12.5550\n",
      "Epoch [35/300], Step [161/172], Loss: 11.5704\n",
      "Epoch [35/300], Step [162/172], Loss: 11.4960\n",
      "Epoch [35/300], Step [163/172], Loss: 11.4152\n",
      "Epoch [35/300], Step [164/172], Loss: 12.7070\n",
      "Epoch [35/300], Step [165/172], Loss: 10.5521\n",
      "Epoch [35/300], Step [166/172], Loss: 11.3746\n",
      "Epoch [35/300], Step [167/172], Loss: 11.4639\n",
      "Epoch [35/300], Step [168/172], Loss: 11.1842\n",
      "Epoch [35/300], Step [169/172], Loss: 10.0613\n",
      "Epoch [35/300], Step [170/172], Loss: 10.4442\n",
      "Epoch [35/300], Step [171/172], Loss: 8.9196\n",
      "Epoch [35/300], Step [172/172], Loss: 7.5382\n",
      "Epoch [36/300], Step [1/172], Loss: 101.3572\n",
      "Epoch [36/300], Step [2/172], Loss: 100.7753\n",
      "Epoch [36/300], Step [3/172], Loss: 124.3406\n",
      "Epoch [36/300], Step [4/172], Loss: 77.8867\n",
      "Epoch [36/300], Step [5/172], Loss: 98.1469\n",
      "Epoch [36/300], Step [6/172], Loss: 44.8974\n",
      "Epoch [36/300], Step [7/172], Loss: 58.5033\n",
      "Epoch [36/300], Step [8/172], Loss: 25.7341\n",
      "Epoch [36/300], Step [9/172], Loss: 59.7762\n",
      "Epoch [36/300], Step [10/172], Loss: 66.3692\n",
      "Epoch [36/300], Step [11/172], Loss: 96.1259\n",
      "Epoch [36/300], Step [12/172], Loss: 87.4420\n",
      "Epoch [36/300], Step [13/172], Loss: 54.6778\n",
      "Epoch [36/300], Step [14/172], Loss: 101.0098\n",
      "Epoch [36/300], Step [15/172], Loss: 91.3491\n",
      "Epoch [36/300], Step [16/172], Loss: 72.8764\n",
      "Epoch [36/300], Step [17/172], Loss: 61.9379\n",
      "Epoch [36/300], Step [18/172], Loss: 82.9315\n",
      "Epoch [36/300], Step [19/172], Loss: 79.7542\n",
      "Epoch [36/300], Step [20/172], Loss: 119.4084\n",
      "Epoch [36/300], Step [21/172], Loss: 98.0378\n",
      "Epoch [36/300], Step [22/172], Loss: 94.7508\n",
      "Epoch [36/300], Step [23/172], Loss: 41.3786\n",
      "Epoch [36/300], Step [24/172], Loss: 85.5957\n",
      "Epoch [36/300], Step [25/172], Loss: 58.5894\n",
      "Epoch [36/300], Step [26/172], Loss: 65.5482\n",
      "Epoch [36/300], Step [27/172], Loss: 89.4978\n",
      "Epoch [36/300], Step [28/172], Loss: 76.8031\n",
      "Epoch [36/300], Step [29/172], Loss: 90.3119\n",
      "Epoch [36/300], Step [30/172], Loss: 80.2902\n",
      "Epoch [36/300], Step [31/172], Loss: 51.5962\n",
      "Epoch [36/300], Step [32/172], Loss: 43.6181\n",
      "Epoch [36/300], Step [33/172], Loss: 79.7927\n",
      "Epoch [36/300], Step [34/172], Loss: 19.0412\n",
      "Epoch [36/300], Step [35/172], Loss: 66.0909\n",
      "Epoch [36/300], Step [36/172], Loss: 34.0324\n",
      "Epoch [36/300], Step [37/172], Loss: 23.6234\n",
      "Epoch [36/300], Step [38/172], Loss: 37.4100\n",
      "Epoch [36/300], Step [39/172], Loss: 56.7423\n",
      "Epoch [36/300], Step [40/172], Loss: 35.4893\n",
      "Epoch [36/300], Step [41/172], Loss: 44.9913\n",
      "Epoch [36/300], Step [42/172], Loss: 43.7189\n",
      "Epoch [36/300], Step [43/172], Loss: 36.0147\n",
      "Epoch [36/300], Step [44/172], Loss: 32.9519\n",
      "Epoch [36/300], Step [45/172], Loss: 25.8498\n",
      "Epoch [36/300], Step [46/172], Loss: 51.3346\n",
      "Epoch [36/300], Step [47/172], Loss: 72.2162\n",
      "Epoch [36/300], Step [48/172], Loss: 78.0588\n",
      "Epoch [36/300], Step [49/172], Loss: 24.4728\n",
      "Epoch [36/300], Step [50/172], Loss: 58.4304\n",
      "Epoch [36/300], Step [51/172], Loss: 9.3561\n",
      "Epoch [36/300], Step [52/172], Loss: 26.0591\n",
      "Epoch [36/300], Step [53/172], Loss: 31.5989\n",
      "Epoch [36/300], Step [54/172], Loss: 22.4829\n",
      "Epoch [36/300], Step [55/172], Loss: 18.6380\n",
      "Epoch [36/300], Step [56/172], Loss: 12.5447\n",
      "Epoch [36/300], Step [57/172], Loss: 52.5737\n",
      "Epoch [36/300], Step [58/172], Loss: 21.9019\n",
      "Epoch [36/300], Step [59/172], Loss: 39.5119\n",
      "Epoch [36/300], Step [60/172], Loss: 70.0702\n",
      "Epoch [36/300], Step [61/172], Loss: 13.3657\n",
      "Epoch [36/300], Step [62/172], Loss: 19.9440\n",
      "Epoch [36/300], Step [63/172], Loss: 9.6606\n",
      "Epoch [36/300], Step [64/172], Loss: 6.6841\n",
      "Epoch [36/300], Step [65/172], Loss: 30.4612\n",
      "Epoch [36/300], Step [66/172], Loss: 10.3247\n",
      "Epoch [36/300], Step [67/172], Loss: 29.3791\n",
      "Epoch [36/300], Step [68/172], Loss: 20.1502\n",
      "Epoch [36/300], Step [69/172], Loss: 83.4150\n",
      "Epoch [36/300], Step [70/172], Loss: 73.6238\n",
      "Epoch [36/300], Step [71/172], Loss: 71.0665\n",
      "Epoch [36/300], Step [72/172], Loss: 68.5158\n",
      "Epoch [36/300], Step [73/172], Loss: 74.1507\n",
      "Epoch [36/300], Step [74/172], Loss: 52.5178\n",
      "Epoch [36/300], Step [75/172], Loss: 33.3506\n",
      "Epoch [36/300], Step [76/172], Loss: 52.2066\n",
      "Epoch [36/300], Step [77/172], Loss: 61.3647\n",
      "Epoch [36/300], Step [78/172], Loss: 62.7709\n",
      "Epoch [36/300], Step [79/172], Loss: 53.2722\n",
      "Epoch [36/300], Step [80/172], Loss: 56.8420\n",
      "Epoch [36/300], Step [81/172], Loss: 53.2619\n",
      "Epoch [36/300], Step [82/172], Loss: 47.5299\n",
      "Epoch [36/300], Step [83/172], Loss: 51.0905\n",
      "Epoch [36/300], Step [84/172], Loss: 45.3809\n",
      "Epoch [36/300], Step [85/172], Loss: 53.9208\n",
      "Epoch [36/300], Step [86/172], Loss: 40.5554\n",
      "Epoch [36/300], Step [87/172], Loss: 33.5306\n",
      "Epoch [36/300], Step [88/172], Loss: 36.4183\n",
      "Epoch [36/300], Step [89/172], Loss: 36.0828\n",
      "Epoch [36/300], Step [90/172], Loss: 34.7011\n",
      "Epoch [36/300], Step [91/172], Loss: 33.2325\n",
      "Epoch [36/300], Step [92/172], Loss: 28.3290\n",
      "Epoch [36/300], Step [93/172], Loss: 30.9987\n",
      "Epoch [36/300], Step [94/172], Loss: 33.5018\n",
      "Epoch [36/300], Step [95/172], Loss: 30.7785\n",
      "Epoch [36/300], Step [96/172], Loss: 27.4215\n",
      "Epoch [36/300], Step [97/172], Loss: 31.4185\n",
      "Epoch [36/300], Step [98/172], Loss: 27.7564\n",
      "Epoch [36/300], Step [99/172], Loss: 25.5016\n",
      "Epoch [36/300], Step [100/172], Loss: 24.8694\n",
      "Epoch [36/300], Step [101/172], Loss: 26.3809\n",
      "Epoch [36/300], Step [102/172], Loss: 24.3756\n",
      "Epoch [36/300], Step [103/172], Loss: 24.3246\n",
      "Epoch [36/300], Step [104/172], Loss: 24.0810\n",
      "Epoch [36/300], Step [105/172], Loss: 24.2643\n",
      "Epoch [36/300], Step [106/172], Loss: 24.0806\n",
      "Epoch [36/300], Step [107/172], Loss: 20.7586\n",
      "Epoch [36/300], Step [108/172], Loss: 24.6873\n",
      "Epoch [36/300], Step [109/172], Loss: 25.5720\n",
      "Epoch [36/300], Step [110/172], Loss: 22.4796\n",
      "Epoch [36/300], Step [111/172], Loss: 21.8030\n",
      "Epoch [36/300], Step [112/172], Loss: 27.5471\n",
      "Epoch [36/300], Step [113/172], Loss: 21.8935\n",
      "Epoch [36/300], Step [114/172], Loss: 22.5922\n",
      "Epoch [36/300], Step [115/172], Loss: 29.7233\n",
      "Epoch [36/300], Step [116/172], Loss: 22.3657\n",
      "Epoch [36/300], Step [117/172], Loss: 19.3408\n",
      "Epoch [36/300], Step [118/172], Loss: 22.4068\n",
      "Epoch [36/300], Step [119/172], Loss: 19.3770\n",
      "Epoch [36/300], Step [120/172], Loss: 18.1849\n",
      "Epoch [36/300], Step [121/172], Loss: 18.9397\n",
      "Epoch [36/300], Step [122/172], Loss: 16.4056\n",
      "Epoch [36/300], Step [123/172], Loss: 17.4198\n",
      "Epoch [36/300], Step [124/172], Loss: 15.6022\n",
      "Epoch [36/300], Step [125/172], Loss: 19.2493\n",
      "Epoch [36/300], Step [126/172], Loss: 18.4748\n",
      "Epoch [36/300], Step [127/172], Loss: 20.5119\n",
      "Epoch [36/300], Step [128/172], Loss: 21.2975\n",
      "Epoch [36/300], Step [129/172], Loss: 15.8486\n",
      "Epoch [36/300], Step [130/172], Loss: 17.9603\n",
      "Epoch [36/300], Step [131/172], Loss: 15.9605\n",
      "Epoch [36/300], Step [132/172], Loss: 16.0912\n",
      "Epoch [36/300], Step [133/172], Loss: 15.8143\n",
      "Epoch [36/300], Step [134/172], Loss: 16.2958\n",
      "Epoch [36/300], Step [135/172], Loss: 14.1224\n",
      "Epoch [36/300], Step [136/172], Loss: 15.0539\n",
      "Epoch [36/300], Step [137/172], Loss: 16.3346\n",
      "Epoch [36/300], Step [138/172], Loss: 14.3894\n",
      "Epoch [36/300], Step [139/172], Loss: 15.8496\n",
      "Epoch [36/300], Step [140/172], Loss: 15.7036\n",
      "Epoch [36/300], Step [141/172], Loss: 18.5740\n",
      "Epoch [36/300], Step [142/172], Loss: 17.0629\n",
      "Epoch [36/300], Step [143/172], Loss: 14.3973\n",
      "Epoch [36/300], Step [144/172], Loss: 13.5068\n",
      "Epoch [36/300], Step [145/172], Loss: 13.7249\n",
      "Epoch [36/300], Step [146/172], Loss: 14.1789\n",
      "Epoch [36/300], Step [147/172], Loss: 11.9858\n",
      "Epoch [36/300], Step [148/172], Loss: 12.3980\n",
      "Epoch [36/300], Step [149/172], Loss: 14.1242\n",
      "Epoch [36/300], Step [150/172], Loss: 14.0121\n",
      "Epoch [36/300], Step [151/172], Loss: 12.1291\n",
      "Epoch [36/300], Step [152/172], Loss: 12.4532\n",
      "Epoch [36/300], Step [153/172], Loss: 12.4280\n",
      "Epoch [36/300], Step [154/172], Loss: 13.7401\n",
      "Epoch [36/300], Step [155/172], Loss: 11.9954\n",
      "Epoch [36/300], Step [156/172], Loss: 12.9943\n",
      "Epoch [36/300], Step [157/172], Loss: 14.2394\n",
      "Epoch [36/300], Step [158/172], Loss: 12.6875\n",
      "Epoch [36/300], Step [159/172], Loss: 12.5483\n",
      "Epoch [36/300], Step [160/172], Loss: 12.3676\n",
      "Epoch [36/300], Step [161/172], Loss: 11.3681\n",
      "Epoch [36/300], Step [162/172], Loss: 11.3309\n",
      "Epoch [36/300], Step [163/172], Loss: 11.2374\n",
      "Epoch [36/300], Step [164/172], Loss: 12.5657\n",
      "Epoch [36/300], Step [165/172], Loss: 10.3580\n",
      "Epoch [36/300], Step [166/172], Loss: 11.1563\n",
      "Epoch [36/300], Step [167/172], Loss: 11.2636\n",
      "Epoch [36/300], Step [168/172], Loss: 10.9514\n",
      "Epoch [36/300], Step [169/172], Loss: 9.8884\n",
      "Epoch [36/300], Step [170/172], Loss: 10.2266\n",
      "Epoch [36/300], Step [171/172], Loss: 8.7129\n",
      "Epoch [36/300], Step [172/172], Loss: 7.4287\n",
      "Epoch [37/300], Step [1/172], Loss: 101.3532\n",
      "Epoch [37/300], Step [2/172], Loss: 100.7248\n",
      "Epoch [37/300], Step [3/172], Loss: 124.8162\n",
      "Epoch [37/300], Step [4/172], Loss: 77.5633\n",
      "Epoch [37/300], Step [5/172], Loss: 98.0009\n",
      "Epoch [37/300], Step [6/172], Loss: 44.3207\n",
      "Epoch [37/300], Step [7/172], Loss: 57.2991\n",
      "Epoch [37/300], Step [8/172], Loss: 24.6316\n",
      "Epoch [37/300], Step [9/172], Loss: 59.3499\n",
      "Epoch [37/300], Step [10/172], Loss: 65.7492\n",
      "Epoch [37/300], Step [11/172], Loss: 96.5244\n",
      "Epoch [37/300], Step [12/172], Loss: 87.6938\n",
      "Epoch [37/300], Step [13/172], Loss: 53.9355\n",
      "Epoch [37/300], Step [14/172], Loss: 100.6798\n",
      "Epoch [37/300], Step [15/172], Loss: 90.9994\n",
      "Epoch [37/300], Step [16/172], Loss: 72.0757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/300], Step [17/172], Loss: 61.7229\n",
      "Epoch [37/300], Step [18/172], Loss: 82.0524\n",
      "Epoch [37/300], Step [19/172], Loss: 79.5495\n",
      "Epoch [37/300], Step [20/172], Loss: 119.1686\n",
      "Epoch [37/300], Step [21/172], Loss: 97.7073\n",
      "Epoch [37/300], Step [22/172], Loss: 94.3017\n",
      "Epoch [37/300], Step [23/172], Loss: 39.3397\n",
      "Epoch [37/300], Step [24/172], Loss: 84.8628\n",
      "Epoch [37/300], Step [25/172], Loss: 58.1667\n",
      "Epoch [37/300], Step [26/172], Loss: 65.1632\n",
      "Epoch [37/300], Step [27/172], Loss: 89.0515\n",
      "Epoch [37/300], Step [28/172], Loss: 75.5687\n",
      "Epoch [37/300], Step [29/172], Loss: 88.8733\n",
      "Epoch [37/300], Step [30/172], Loss: 79.7551\n",
      "Epoch [37/300], Step [31/172], Loss: 51.0580\n",
      "Epoch [37/300], Step [32/172], Loss: 43.4252\n",
      "Epoch [37/300], Step [33/172], Loss: 79.4060\n",
      "Epoch [37/300], Step [34/172], Loss: 18.0702\n",
      "Epoch [37/300], Step [35/172], Loss: 65.3313\n",
      "Epoch [37/300], Step [36/172], Loss: 33.6510\n",
      "Epoch [37/300], Step [37/172], Loss: 23.4095\n",
      "Epoch [37/300], Step [38/172], Loss: 36.8037\n",
      "Epoch [37/300], Step [39/172], Loss: 56.6283\n",
      "Epoch [37/300], Step [40/172], Loss: 35.0004\n",
      "Epoch [37/300], Step [41/172], Loss: 44.7169\n",
      "Epoch [37/300], Step [42/172], Loss: 43.5634\n",
      "Epoch [37/300], Step [43/172], Loss: 35.6846\n",
      "Epoch [37/300], Step [44/172], Loss: 32.3894\n",
      "Epoch [37/300], Step [45/172], Loss: 25.7282\n",
      "Epoch [37/300], Step [46/172], Loss: 50.8656\n",
      "Epoch [37/300], Step [47/172], Loss: 71.7940\n",
      "Epoch [37/300], Step [48/172], Loss: 76.9659\n",
      "Epoch [37/300], Step [49/172], Loss: 24.3864\n",
      "Epoch [37/300], Step [50/172], Loss: 57.7217\n",
      "Epoch [37/300], Step [51/172], Loss: 9.3452\n",
      "Epoch [37/300], Step [52/172], Loss: 25.9927\n",
      "Epoch [37/300], Step [53/172], Loss: 31.5416\n",
      "Epoch [37/300], Step [54/172], Loss: 22.2404\n",
      "Epoch [37/300], Step [55/172], Loss: 18.5290\n",
      "Epoch [37/300], Step [56/172], Loss: 12.5128\n",
      "Epoch [37/300], Step [57/172], Loss: 51.9348\n",
      "Epoch [37/300], Step [58/172], Loss: 21.7480\n",
      "Epoch [37/300], Step [59/172], Loss: 39.3533\n",
      "Epoch [37/300], Step [60/172], Loss: 69.0613\n",
      "Epoch [37/300], Step [61/172], Loss: 13.2485\n",
      "Epoch [37/300], Step [62/172], Loss: 20.0236\n",
      "Epoch [37/300], Step [63/172], Loss: 9.6746\n",
      "Epoch [37/300], Step [64/172], Loss: 6.7630\n",
      "Epoch [37/300], Step [65/172], Loss: 30.4343\n",
      "Epoch [37/300], Step [66/172], Loss: 10.2800\n",
      "Epoch [37/300], Step [67/172], Loss: 29.3123\n",
      "Epoch [37/300], Step [68/172], Loss: 19.9873\n",
      "Epoch [37/300], Step [69/172], Loss: 83.0939\n",
      "Epoch [37/300], Step [70/172], Loss: 72.9850\n",
      "Epoch [37/300], Step [71/172], Loss: 70.0484\n",
      "Epoch [37/300], Step [72/172], Loss: 67.9422\n",
      "Epoch [37/300], Step [73/172], Loss: 73.5901\n",
      "Epoch [37/300], Step [74/172], Loss: 52.0376\n",
      "Epoch [37/300], Step [75/172], Loss: 32.6966\n",
      "Epoch [37/300], Step [76/172], Loss: 51.8581\n",
      "Epoch [37/300], Step [77/172], Loss: 61.4062\n",
      "Epoch [37/300], Step [78/172], Loss: 62.8805\n",
      "Epoch [37/300], Step [79/172], Loss: 53.6007\n",
      "Epoch [37/300], Step [80/172], Loss: 57.1898\n",
      "Epoch [37/300], Step [81/172], Loss: 53.8071\n",
      "Epoch [37/300], Step [82/172], Loss: 47.8187\n",
      "Epoch [37/300], Step [83/172], Loss: 51.6626\n",
      "Epoch [37/300], Step [84/172], Loss: 45.7695\n",
      "Epoch [37/300], Step [85/172], Loss: 54.2293\n",
      "Epoch [37/300], Step [86/172], Loss: 40.7323\n",
      "Epoch [37/300], Step [87/172], Loss: 33.6615\n",
      "Epoch [37/300], Step [88/172], Loss: 36.6138\n",
      "Epoch [37/300], Step [89/172], Loss: 36.2279\n",
      "Epoch [37/300], Step [90/172], Loss: 34.8698\n",
      "Epoch [37/300], Step [91/172], Loss: 33.4048\n",
      "Epoch [37/300], Step [92/172], Loss: 28.3590\n",
      "Epoch [37/300], Step [93/172], Loss: 30.9238\n",
      "Epoch [37/300], Step [94/172], Loss: 33.5006\n",
      "Epoch [37/300], Step [95/172], Loss: 30.7112\n",
      "Epoch [37/300], Step [96/172], Loss: 27.3215\n",
      "Epoch [37/300], Step [97/172], Loss: 31.4605\n",
      "Epoch [37/300], Step [98/172], Loss: 27.6693\n",
      "Epoch [37/300], Step [99/172], Loss: 25.3638\n",
      "Epoch [37/300], Step [100/172], Loss: 24.7135\n",
      "Epoch [37/300], Step [101/172], Loss: 26.2279\n",
      "Epoch [37/300], Step [102/172], Loss: 24.2434\n",
      "Epoch [37/300], Step [103/172], Loss: 24.1824\n",
      "Epoch [37/300], Step [104/172], Loss: 23.8826\n",
      "Epoch [37/300], Step [105/172], Loss: 24.0859\n",
      "Epoch [37/300], Step [106/172], Loss: 23.9350\n",
      "Epoch [37/300], Step [107/172], Loss: 20.6257\n",
      "Epoch [37/300], Step [108/172], Loss: 24.5252\n",
      "Epoch [37/300], Step [109/172], Loss: 25.5234\n",
      "Epoch [37/300], Step [110/172], Loss: 22.3170\n",
      "Epoch [37/300], Step [111/172], Loss: 21.6017\n",
      "Epoch [37/300], Step [112/172], Loss: 27.4633\n",
      "Epoch [37/300], Step [113/172], Loss: 21.7529\n",
      "Epoch [37/300], Step [114/172], Loss: 22.4164\n",
      "Epoch [37/300], Step [115/172], Loss: 29.6867\n",
      "Epoch [37/300], Step [116/172], Loss: 22.1887\n",
      "Epoch [37/300], Step [117/172], Loss: 19.1560\n",
      "Epoch [37/300], Step [118/172], Loss: 22.3037\n",
      "Epoch [37/300], Step [119/172], Loss: 19.2534\n",
      "Epoch [37/300], Step [120/172], Loss: 18.0105\n",
      "Epoch [37/300], Step [121/172], Loss: 18.7619\n",
      "Epoch [37/300], Step [122/172], Loss: 16.2581\n",
      "Epoch [37/300], Step [123/172], Loss: 17.2844\n",
      "Epoch [37/300], Step [124/172], Loss: 15.3895\n",
      "Epoch [37/300], Step [125/172], Loss: 19.1216\n",
      "Epoch [37/300], Step [126/172], Loss: 18.2818\n",
      "Epoch [37/300], Step [127/172], Loss: 20.3512\n",
      "Epoch [37/300], Step [128/172], Loss: 21.1420\n",
      "Epoch [37/300], Step [129/172], Loss: 15.6495\n",
      "Epoch [37/300], Step [130/172], Loss: 17.7930\n",
      "Epoch [37/300], Step [131/172], Loss: 15.7896\n",
      "Epoch [37/300], Step [132/172], Loss: 15.9045\n",
      "Epoch [37/300], Step [133/172], Loss: 15.6653\n",
      "Epoch [37/300], Step [134/172], Loss: 16.1573\n",
      "Epoch [37/300], Step [135/172], Loss: 13.9559\n",
      "Epoch [37/300], Step [136/172], Loss: 14.8838\n",
      "Epoch [37/300], Step [137/172], Loss: 16.1972\n",
      "Epoch [37/300], Step [138/172], Loss: 14.2300\n",
      "Epoch [37/300], Step [139/172], Loss: 15.6811\n",
      "Epoch [37/300], Step [140/172], Loss: 15.5370\n",
      "Epoch [37/300], Step [141/172], Loss: 18.4930\n",
      "Epoch [37/300], Step [142/172], Loss: 16.9632\n",
      "Epoch [37/300], Step [143/172], Loss: 14.2461\n",
      "Epoch [37/300], Step [144/172], Loss: 13.3688\n",
      "Epoch [37/300], Step [145/172], Loss: 13.5976\n",
      "Epoch [37/300], Step [146/172], Loss: 14.0498\n",
      "Epoch [37/300], Step [147/172], Loss: 11.8067\n",
      "Epoch [37/300], Step [148/172], Loss: 12.2234\n",
      "Epoch [37/300], Step [149/172], Loss: 14.0035\n",
      "Epoch [37/300], Step [150/172], Loss: 13.8741\n",
      "Epoch [37/300], Step [151/172], Loss: 12.0231\n",
      "Epoch [37/300], Step [152/172], Loss: 12.3092\n",
      "Epoch [37/300], Step [153/172], Loss: 12.2856\n",
      "Epoch [37/300], Step [154/172], Loss: 13.6148\n",
      "Epoch [37/300], Step [155/172], Loss: 11.8569\n",
      "Epoch [37/300], Step [156/172], Loss: 12.9206\n",
      "Epoch [37/300], Step [157/172], Loss: 14.1468\n",
      "Epoch [37/300], Step [158/172], Loss: 12.5760\n",
      "Epoch [37/300], Step [159/172], Loss: 12.4437\n",
      "Epoch [37/300], Step [160/172], Loss: 12.2760\n",
      "Epoch [37/300], Step [161/172], Loss: 11.2429\n",
      "Epoch [37/300], Step [162/172], Loss: 11.2258\n",
      "Epoch [37/300], Step [163/172], Loss: 11.1322\n",
      "Epoch [37/300], Step [164/172], Loss: 12.5253\n",
      "Epoch [37/300], Step [165/172], Loss: 10.2512\n",
      "Epoch [37/300], Step [166/172], Loss: 11.0284\n",
      "Epoch [37/300], Step [167/172], Loss: 11.1637\n",
      "Epoch [37/300], Step [168/172], Loss: 10.8264\n",
      "Epoch [37/300], Step [169/172], Loss: 9.8113\n",
      "Epoch [37/300], Step [170/172], Loss: 10.1435\n",
      "Epoch [37/300], Step [171/172], Loss: 8.6352\n",
      "Epoch [37/300], Step [172/172], Loss: 7.3560\n",
      "Epoch [38/300], Step [1/172], Loss: 101.4533\n",
      "Epoch [38/300], Step [2/172], Loss: 100.5886\n",
      "Epoch [38/300], Step [3/172], Loss: 123.7549\n",
      "Epoch [38/300], Step [4/172], Loss: 76.9115\n",
      "Epoch [38/300], Step [5/172], Loss: 97.2812\n",
      "Epoch [38/300], Step [6/172], Loss: 43.5857\n",
      "Epoch [38/300], Step [7/172], Loss: 56.1604\n",
      "Epoch [38/300], Step [8/172], Loss: 23.3100\n",
      "Epoch [38/300], Step [9/172], Loss: 58.5880\n",
      "Epoch [38/300], Step [10/172], Loss: 65.2191\n",
      "Epoch [38/300], Step [11/172], Loss: 96.6457\n",
      "Epoch [38/300], Step [12/172], Loss: 87.3462\n",
      "Epoch [38/300], Step [13/172], Loss: 53.5580\n",
      "Epoch [38/300], Step [14/172], Loss: 100.4726\n",
      "Epoch [38/300], Step [15/172], Loss: 90.8464\n",
      "Epoch [38/300], Step [16/172], Loss: 71.7282\n",
      "Epoch [38/300], Step [17/172], Loss: 61.6397\n",
      "Epoch [38/300], Step [18/172], Loss: 81.3625\n",
      "Epoch [38/300], Step [19/172], Loss: 79.7014\n",
      "Epoch [38/300], Step [20/172], Loss: 118.0046\n",
      "Epoch [38/300], Step [21/172], Loss: 97.6518\n",
      "Epoch [38/300], Step [22/172], Loss: 94.2919\n",
      "Epoch [38/300], Step [23/172], Loss: 37.4529\n",
      "Epoch [38/300], Step [24/172], Loss: 84.3749\n",
      "Epoch [38/300], Step [25/172], Loss: 57.9794\n",
      "Epoch [38/300], Step [26/172], Loss: 65.0685\n",
      "Epoch [38/300], Step [27/172], Loss: 88.9581\n",
      "Epoch [38/300], Step [28/172], Loss: 74.3387\n",
      "Epoch [38/300], Step [29/172], Loss: 87.2826\n",
      "Epoch [38/300], Step [30/172], Loss: 79.6383\n",
      "Epoch [38/300], Step [31/172], Loss: 50.4531\n",
      "Epoch [38/300], Step [32/172], Loss: 43.4040\n",
      "Epoch [38/300], Step [33/172], Loss: 79.4226\n",
      "Epoch [38/300], Step [34/172], Loss: 17.3809\n",
      "Epoch [38/300], Step [35/172], Loss: 64.4207\n",
      "Epoch [38/300], Step [36/172], Loss: 33.1548\n",
      "Epoch [38/300], Step [37/172], Loss: 23.2547\n",
      "Epoch [38/300], Step [38/172], Loss: 36.2608\n",
      "Epoch [38/300], Step [39/172], Loss: 56.4172\n",
      "Epoch [38/300], Step [40/172], Loss: 34.5358\n",
      "Epoch [38/300], Step [41/172], Loss: 44.5571\n",
      "Epoch [38/300], Step [42/172], Loss: 43.4768\n",
      "Epoch [38/300], Step [43/172], Loss: 35.3796\n",
      "Epoch [38/300], Step [44/172], Loss: 31.9005\n",
      "Epoch [38/300], Step [45/172], Loss: 25.4739\n",
      "Epoch [38/300], Step [46/172], Loss: 50.3440\n",
      "Epoch [38/300], Step [47/172], Loss: 71.3021\n",
      "Epoch [38/300], Step [48/172], Loss: 76.0757\n",
      "Epoch [38/300], Step [49/172], Loss: 24.1717\n",
      "Epoch [38/300], Step [50/172], Loss: 56.9855\n",
      "Epoch [38/300], Step [51/172], Loss: 9.2065\n",
      "Epoch [38/300], Step [52/172], Loss: 25.6115\n",
      "Epoch [38/300], Step [53/172], Loss: 31.1397\n",
      "Epoch [38/300], Step [54/172], Loss: 21.8671\n",
      "Epoch [38/300], Step [55/172], Loss: 18.1556\n",
      "Epoch [38/300], Step [56/172], Loss: 12.2606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/300], Step [57/172], Loss: 51.3159\n",
      "Epoch [38/300], Step [58/172], Loss: 21.5676\n",
      "Epoch [38/300], Step [59/172], Loss: 38.9178\n",
      "Epoch [38/300], Step [60/172], Loss: 68.1545\n",
      "Epoch [38/300], Step [61/172], Loss: 13.0378\n",
      "Epoch [38/300], Step [62/172], Loss: 19.9040\n",
      "Epoch [38/300], Step [63/172], Loss: 9.4928\n",
      "Epoch [38/300], Step [64/172], Loss: 6.6924\n",
      "Epoch [38/300], Step [65/172], Loss: 30.1903\n",
      "Epoch [38/300], Step [66/172], Loss: 9.8982\n",
      "Epoch [38/300], Step [67/172], Loss: 28.9478\n",
      "Epoch [38/300], Step [68/172], Loss: 19.0668\n",
      "Epoch [38/300], Step [69/172], Loss: 82.9547\n",
      "Epoch [38/300], Step [70/172], Loss: 73.3784\n",
      "Epoch [38/300], Step [71/172], Loss: 70.1247\n",
      "Epoch [38/300], Step [72/172], Loss: 68.0868\n",
      "Epoch [38/300], Step [73/172], Loss: 73.9583\n",
      "Epoch [38/300], Step [74/172], Loss: 51.6150\n",
      "Epoch [38/300], Step [75/172], Loss: 32.7005\n",
      "Epoch [38/300], Step [76/172], Loss: 51.8256\n",
      "Epoch [38/300], Step [77/172], Loss: 61.6627\n",
      "Epoch [38/300], Step [78/172], Loss: 62.7018\n",
      "Epoch [38/300], Step [79/172], Loss: 53.7036\n",
      "Epoch [38/300], Step [80/172], Loss: 57.0184\n",
      "Epoch [38/300], Step [81/172], Loss: 53.8527\n",
      "Epoch [38/300], Step [82/172], Loss: 47.5908\n",
      "Epoch [38/300], Step [83/172], Loss: 51.6011\n",
      "Epoch [38/300], Step [84/172], Loss: 45.4882\n",
      "Epoch [38/300], Step [85/172], Loss: 53.7976\n",
      "Epoch [38/300], Step [86/172], Loss: 40.4153\n",
      "Epoch [38/300], Step [87/172], Loss: 33.3794\n",
      "Epoch [38/300], Step [88/172], Loss: 36.3385\n",
      "Epoch [38/300], Step [89/172], Loss: 35.8538\n",
      "Epoch [38/300], Step [90/172], Loss: 34.6569\n",
      "Epoch [38/300], Step [91/172], Loss: 33.2426\n",
      "Epoch [38/300], Step [92/172], Loss: 28.0865\n",
      "Epoch [38/300], Step [93/172], Loss: 30.5898\n",
      "Epoch [38/300], Step [94/172], Loss: 33.2813\n",
      "Epoch [38/300], Step [95/172], Loss: 30.4074\n",
      "Epoch [38/300], Step [96/172], Loss: 27.0038\n",
      "Epoch [38/300], Step [97/172], Loss: 31.1973\n",
      "Epoch [38/300], Step [98/172], Loss: 27.3290\n",
      "Epoch [38/300], Step [99/172], Loss: 25.0309\n",
      "Epoch [38/300], Step [100/172], Loss: 24.3037\n",
      "Epoch [38/300], Step [101/172], Loss: 25.9231\n",
      "Epoch [38/300], Step [102/172], Loss: 23.8916\n",
      "Epoch [38/300], Step [103/172], Loss: 23.7881\n",
      "Epoch [38/300], Step [104/172], Loss: 23.5165\n",
      "Epoch [38/300], Step [105/172], Loss: 23.7687\n",
      "Epoch [38/300], Step [106/172], Loss: 23.6462\n",
      "Epoch [38/300], Step [107/172], Loss: 20.3669\n",
      "Epoch [38/300], Step [108/172], Loss: 24.2095\n",
      "Epoch [38/300], Step [109/172], Loss: 25.3063\n",
      "Epoch [38/300], Step [110/172], Loss: 22.0300\n",
      "Epoch [38/300], Step [111/172], Loss: 21.3120\n",
      "Epoch [38/300], Step [112/172], Loss: 27.2398\n",
      "Epoch [38/300], Step [113/172], Loss: 21.5099\n",
      "Epoch [38/300], Step [114/172], Loss: 22.1545\n",
      "Epoch [38/300], Step [115/172], Loss: 29.5276\n",
      "Epoch [38/300], Step [116/172], Loss: 21.9265\n",
      "Epoch [38/300], Step [117/172], Loss: 18.9087\n",
      "Epoch [38/300], Step [118/172], Loss: 22.1317\n",
      "Epoch [38/300], Step [119/172], Loss: 19.0186\n",
      "Epoch [38/300], Step [120/172], Loss: 17.7498\n",
      "Epoch [38/300], Step [121/172], Loss: 18.4781\n",
      "Epoch [38/300], Step [122/172], Loss: 16.0633\n",
      "Epoch [38/300], Step [123/172], Loss: 17.0488\n",
      "Epoch [38/300], Step [124/172], Loss: 15.0828\n",
      "Epoch [38/300], Step [125/172], Loss: 18.8565\n",
      "Epoch [38/300], Step [126/172], Loss: 17.9994\n",
      "Epoch [38/300], Step [127/172], Loss: 20.1168\n",
      "Epoch [38/300], Step [128/172], Loss: 20.8842\n",
      "Epoch [38/300], Step [129/172], Loss: 15.3812\n",
      "Epoch [38/300], Step [130/172], Loss: 17.5113\n",
      "Epoch [38/300], Step [131/172], Loss: 15.5224\n",
      "Epoch [38/300], Step [132/172], Loss: 15.6382\n",
      "Epoch [38/300], Step [133/172], Loss: 15.4292\n",
      "Epoch [38/300], Step [134/172], Loss: 15.9202\n",
      "Epoch [38/300], Step [135/172], Loss: 13.7073\n",
      "Epoch [38/300], Step [136/172], Loss: 14.5741\n",
      "Epoch [38/300], Step [137/172], Loss: 15.9480\n",
      "Epoch [38/300], Step [138/172], Loss: 13.9774\n",
      "Epoch [38/300], Step [139/172], Loss: 15.4331\n",
      "Epoch [38/300], Step [140/172], Loss: 15.3007\n",
      "Epoch [38/300], Step [141/172], Loss: 18.3110\n",
      "Epoch [38/300], Step [142/172], Loss: 16.7725\n",
      "Epoch [38/300], Step [143/172], Loss: 14.0403\n",
      "Epoch [38/300], Step [144/172], Loss: 13.1604\n",
      "Epoch [38/300], Step [145/172], Loss: 13.3840\n",
      "Epoch [38/300], Step [146/172], Loss: 13.8471\n",
      "Epoch [38/300], Step [147/172], Loss: 11.4829\n",
      "Epoch [38/300], Step [148/172], Loss: 11.9355\n",
      "Epoch [38/300], Step [149/172], Loss: 13.7828\n",
      "Epoch [38/300], Step [150/172], Loss: 13.6355\n",
      "Epoch [38/300], Step [151/172], Loss: 11.8035\n",
      "Epoch [38/300], Step [152/172], Loss: 12.0836\n",
      "Epoch [38/300], Step [153/172], Loss: 12.0653\n",
      "Epoch [38/300], Step [154/172], Loss: 13.3863\n",
      "Epoch [38/300], Step [155/172], Loss: 11.6423\n",
      "Epoch [38/300], Step [156/172], Loss: 12.7696\n",
      "Epoch [38/300], Step [157/172], Loss: 13.9741\n",
      "Epoch [38/300], Step [158/172], Loss: 12.3601\n",
      "Epoch [38/300], Step [159/172], Loss: 12.2402\n",
      "Epoch [38/300], Step [160/172], Loss: 12.0837\n",
      "Epoch [38/300], Step [161/172], Loss: 11.0054\n",
      "Epoch [38/300], Step [162/172], Loss: 11.0462\n",
      "Epoch [38/300], Step [163/172], Loss: 10.9212\n",
      "Epoch [38/300], Step [164/172], Loss: 12.3781\n",
      "Epoch [38/300], Step [165/172], Loss: 10.0496\n",
      "Epoch [38/300], Step [166/172], Loss: 10.8156\n",
      "Epoch [38/300], Step [167/172], Loss: 10.9664\n",
      "Epoch [38/300], Step [168/172], Loss: 10.6006\n",
      "Epoch [38/300], Step [169/172], Loss: 9.6208\n",
      "Epoch [38/300], Step [170/172], Loss: 9.8951\n",
      "Epoch [38/300], Step [171/172], Loss: 8.4361\n",
      "Epoch [38/300], Step [172/172], Loss: 7.2598\n",
      "Epoch [39/300], Step [1/172], Loss: 101.3872\n",
      "Epoch [39/300], Step [2/172], Loss: 100.4492\n",
      "Epoch [39/300], Step [3/172], Loss: 123.6199\n",
      "Epoch [39/300], Step [4/172], Loss: 76.3902\n",
      "Epoch [39/300], Step [5/172], Loss: 96.7048\n",
      "Epoch [39/300], Step [6/172], Loss: 42.8747\n",
      "Epoch [39/300], Step [7/172], Loss: 55.0183\n",
      "Epoch [39/300], Step [8/172], Loss: 22.0972\n",
      "Epoch [39/300], Step [9/172], Loss: 58.0429\n",
      "Epoch [39/300], Step [10/172], Loss: 64.6764\n",
      "Epoch [39/300], Step [11/172], Loss: 97.1338\n",
      "Epoch [39/300], Step [12/172], Loss: 87.6821\n",
      "Epoch [39/300], Step [13/172], Loss: 53.0786\n",
      "Epoch [39/300], Step [14/172], Loss: 100.4327\n",
      "Epoch [39/300], Step [15/172], Loss: 90.7462\n",
      "Epoch [39/300], Step [16/172], Loss: 71.1079\n",
      "Epoch [39/300], Step [17/172], Loss: 61.6847\n",
      "Epoch [39/300], Step [18/172], Loss: 80.4946\n",
      "Epoch [39/300], Step [19/172], Loss: 79.6500\n",
      "Epoch [39/300], Step [20/172], Loss: 117.2183\n",
      "Epoch [39/300], Step [21/172], Loss: 97.2006\n",
      "Epoch [39/300], Step [22/172], Loss: 93.9011\n",
      "Epoch [39/300], Step [23/172], Loss: 35.4924\n",
      "Epoch [39/300], Step [24/172], Loss: 83.1682\n",
      "Epoch [39/300], Step [25/172], Loss: 57.3459\n",
      "Epoch [39/300], Step [26/172], Loss: 64.4137\n",
      "Epoch [39/300], Step [27/172], Loss: 87.8226\n",
      "Epoch [39/300], Step [28/172], Loss: 71.5099\n",
      "Epoch [39/300], Step [29/172], Loss: 84.1262\n",
      "Epoch [39/300], Step [30/172], Loss: 78.0812\n",
      "Epoch [39/300], Step [31/172], Loss: 48.3513\n",
      "Epoch [39/300], Step [32/172], Loss: 42.4214\n",
      "Epoch [39/300], Step [33/172], Loss: 77.8079\n",
      "Epoch [39/300], Step [34/172], Loss: 16.0011\n",
      "Epoch [39/300], Step [35/172], Loss: 61.6248\n",
      "Epoch [39/300], Step [36/172], Loss: 31.2067\n",
      "Epoch [39/300], Step [37/172], Loss: 21.8283\n",
      "Epoch [39/300], Step [38/172], Loss: 32.5264\n",
      "Epoch [39/300], Step [39/172], Loss: 53.5625\n",
      "Epoch [39/300], Step [40/172], Loss: 31.7261\n",
      "Epoch [39/300], Step [41/172], Loss: 41.7314\n",
      "Epoch [39/300], Step [42/172], Loss: 40.3864\n",
      "Epoch [39/300], Step [43/172], Loss: 31.2924\n",
      "Epoch [39/300], Step [44/172], Loss: 28.6113\n",
      "Epoch [39/300], Step [45/172], Loss: 22.7312\n",
      "Epoch [39/300], Step [46/172], Loss: 45.1280\n",
      "Epoch [39/300], Step [47/172], Loss: 64.8785\n",
      "Epoch [39/300], Step [48/172], Loss: 68.3552\n",
      "Epoch [39/300], Step [49/172], Loss: 20.2359\n",
      "Epoch [39/300], Step [50/172], Loss: 50.0700\n",
      "Epoch [39/300], Step [51/172], Loss: 7.1135\n",
      "Epoch [39/300], Step [52/172], Loss: 20.2252\n",
      "Epoch [39/300], Step [53/172], Loss: 25.1406\n",
      "Epoch [39/300], Step [54/172], Loss: 16.4963\n",
      "Epoch [39/300], Step [55/172], Loss: 14.0124\n",
      "Epoch [39/300], Step [56/172], Loss: 9.1646\n",
      "Epoch [39/300], Step [57/172], Loss: 43.5991\n",
      "Epoch [39/300], Step [58/172], Loss: 17.8779\n",
      "Epoch [39/300], Step [59/172], Loss: 32.2582\n",
      "Epoch [39/300], Step [60/172], Loss: 61.5653\n",
      "Epoch [39/300], Step [61/172], Loss: 10.9104\n",
      "Epoch [39/300], Step [62/172], Loss: 16.0995\n",
      "Epoch [39/300], Step [63/172], Loss: 6.6216\n",
      "Epoch [39/300], Step [64/172], Loss: 4.7800\n",
      "Epoch [39/300], Step [65/172], Loss: 25.1526\n",
      "Epoch [39/300], Step [66/172], Loss: 7.0210\n",
      "Epoch [39/300], Step [67/172], Loss: 23.7861\n",
      "Epoch [39/300], Step [68/172], Loss: 13.5890\n",
      "Epoch [39/300], Step [69/172], Loss: 83.3942\n",
      "Epoch [39/300], Step [70/172], Loss: 92.9432\n",
      "Epoch [39/300], Step [71/172], Loss: 86.7577\n",
      "Epoch [39/300], Step [72/172], Loss: 85.8770\n",
      "Epoch [39/300], Step [73/172], Loss: 91.5075\n",
      "Epoch [39/300], Step [74/172], Loss: 67.5586\n",
      "Epoch [39/300], Step [75/172], Loss: 44.7220\n",
      "Epoch [39/300], Step [76/172], Loss: 63.9083\n",
      "Epoch [39/300], Step [77/172], Loss: 73.0833\n",
      "Epoch [39/300], Step [78/172], Loss: 73.5479\n",
      "Epoch [39/300], Step [79/172], Loss: 62.6052\n",
      "Epoch [39/300], Step [80/172], Loss: 65.3915\n",
      "Epoch [39/300], Step [81/172], Loss: 60.7858\n",
      "Epoch [39/300], Step [82/172], Loss: 54.2588\n",
      "Epoch [39/300], Step [83/172], Loss: 56.1176\n",
      "Epoch [39/300], Step [84/172], Loss: 49.5295\n",
      "Epoch [39/300], Step [85/172], Loss: 56.8428\n",
      "Epoch [39/300], Step [86/172], Loss: 43.7233\n",
      "Epoch [39/300], Step [87/172], Loss: 35.1131\n",
      "Epoch [39/300], Step [88/172], Loss: 38.3576\n",
      "Epoch [39/300], Step [89/172], Loss: 37.2402\n",
      "Epoch [39/300], Step [90/172], Loss: 35.5081\n",
      "Epoch [39/300], Step [91/172], Loss: 34.0912\n",
      "Epoch [39/300], Step [92/172], Loss: 29.2933\n",
      "Epoch [39/300], Step [93/172], Loss: 31.0792\n",
      "Epoch [39/300], Step [94/172], Loss: 33.8237\n",
      "Epoch [39/300], Step [95/172], Loss: 30.4888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/300], Step [96/172], Loss: 26.8167\n",
      "Epoch [39/300], Step [97/172], Loss: 30.2272\n",
      "Epoch [39/300], Step [98/172], Loss: 26.2893\n",
      "Epoch [39/300], Step [99/172], Loss: 24.0282\n",
      "Epoch [39/300], Step [100/172], Loss: 23.3381\n",
      "Epoch [39/300], Step [101/172], Loss: 24.6473\n",
      "Epoch [39/300], Step [102/172], Loss: 22.8314\n",
      "Epoch [39/300], Step [103/172], Loss: 22.6623\n",
      "Epoch [39/300], Step [104/172], Loss: 22.3912\n",
      "Epoch [39/300], Step [105/172], Loss: 22.7248\n",
      "Epoch [39/300], Step [106/172], Loss: 22.9009\n",
      "Epoch [39/300], Step [107/172], Loss: 19.3624\n",
      "Epoch [39/300], Step [108/172], Loss: 23.0405\n",
      "Epoch [39/300], Step [109/172], Loss: 24.1952\n",
      "Epoch [39/300], Step [110/172], Loss: 20.9937\n",
      "Epoch [39/300], Step [111/172], Loss: 20.3563\n",
      "Epoch [39/300], Step [112/172], Loss: 26.4910\n",
      "Epoch [39/300], Step [113/172], Loss: 20.5451\n",
      "Epoch [39/300], Step [114/172], Loss: 21.0560\n",
      "Epoch [39/300], Step [115/172], Loss: 28.4696\n",
      "Epoch [39/300], Step [116/172], Loss: 20.9596\n",
      "Epoch [39/300], Step [117/172], Loss: 17.9627\n",
      "Epoch [39/300], Step [118/172], Loss: 21.2007\n",
      "Epoch [39/300], Step [119/172], Loss: 18.0374\n",
      "Epoch [39/300], Step [120/172], Loss: 16.6936\n",
      "Epoch [39/300], Step [121/172], Loss: 17.1880\n",
      "Epoch [39/300], Step [122/172], Loss: 15.1742\n",
      "Epoch [39/300], Step [123/172], Loss: 16.0960\n",
      "Epoch [39/300], Step [124/172], Loss: 14.1086\n",
      "Epoch [39/300], Step [125/172], Loss: 17.6295\n",
      "Epoch [39/300], Step [126/172], Loss: 16.9747\n",
      "Epoch [39/300], Step [127/172], Loss: 19.0271\n",
      "Epoch [39/300], Step [128/172], Loss: 19.7744\n",
      "Epoch [39/300], Step [129/172], Loss: 14.3306\n",
      "Epoch [39/300], Step [130/172], Loss: 16.4509\n",
      "Epoch [39/300], Step [131/172], Loss: 14.5082\n",
      "Epoch [39/300], Step [132/172], Loss: 14.6224\n",
      "Epoch [39/300], Step [133/172], Loss: 14.3467\n",
      "Epoch [39/300], Step [134/172], Loss: 14.8596\n",
      "Epoch [39/300], Step [135/172], Loss: 12.7024\n",
      "Epoch [39/300], Step [136/172], Loss: 13.5151\n",
      "Epoch [39/300], Step [137/172], Loss: 14.8285\n",
      "Epoch [39/300], Step [138/172], Loss: 12.8984\n",
      "Epoch [39/300], Step [139/172], Loss: 14.3723\n",
      "Epoch [39/300], Step [140/172], Loss: 14.2427\n",
      "Epoch [39/300], Step [141/172], Loss: 17.1867\n",
      "Epoch [39/300], Step [142/172], Loss: 15.6930\n",
      "Epoch [39/300], Step [143/172], Loss: 13.0832\n",
      "Epoch [39/300], Step [144/172], Loss: 12.1753\n",
      "Epoch [39/300], Step [145/172], Loss: 12.3569\n",
      "Epoch [39/300], Step [146/172], Loss: 12.7684\n",
      "Epoch [39/300], Step [147/172], Loss: 10.4612\n",
      "Epoch [39/300], Step [148/172], Loss: 10.9100\n",
      "Epoch [39/300], Step [149/172], Loss: 12.7221\n",
      "Epoch [39/300], Step [150/172], Loss: 12.4930\n",
      "Epoch [39/300], Step [151/172], Loss: 10.8308\n",
      "Epoch [39/300], Step [152/172], Loss: 11.0999\n",
      "Epoch [39/300], Step [153/172], Loss: 11.0699\n",
      "Epoch [39/300], Step [154/172], Loss: 12.2461\n",
      "Epoch [39/300], Step [155/172], Loss: 10.6629\n",
      "Epoch [39/300], Step [156/172], Loss: 11.7062\n",
      "Epoch [39/300], Step [157/172], Loss: 12.8461\n",
      "Epoch [39/300], Step [158/172], Loss: 11.3045\n",
      "Epoch [39/300], Step [159/172], Loss: 11.2816\n",
      "Epoch [39/300], Step [160/172], Loss: 11.0131\n",
      "Epoch [39/300], Step [161/172], Loss: 9.9721\n",
      "Epoch [39/300], Step [162/172], Loss: 10.0921\n",
      "Epoch [39/300], Step [163/172], Loss: 9.9896\n",
      "Epoch [39/300], Step [164/172], Loss: 11.4024\n",
      "Epoch [39/300], Step [165/172], Loss: 9.1357\n",
      "Epoch [39/300], Step [166/172], Loss: 9.8199\n",
      "Epoch [39/300], Step [167/172], Loss: 9.9353\n",
      "Epoch [39/300], Step [168/172], Loss: 9.6015\n",
      "Epoch [39/300], Step [169/172], Loss: 8.7168\n",
      "Epoch [39/300], Step [170/172], Loss: 8.9535\n",
      "Epoch [39/300], Step [171/172], Loss: 7.6234\n",
      "Epoch [39/300], Step [172/172], Loss: 6.5815\n",
      "Epoch [40/300], Step [1/172], Loss: 104.8157\n",
      "Epoch [40/300], Step [2/172], Loss: 103.4322\n",
      "Epoch [40/300], Step [3/172], Loss: 129.9447\n",
      "Epoch [40/300], Step [4/172], Loss: 78.9320\n",
      "Epoch [40/300], Step [5/172], Loss: 100.6343\n",
      "Epoch [40/300], Step [6/172], Loss: 45.2996\n",
      "Epoch [40/300], Step [7/172], Loss: 55.2167\n",
      "Epoch [40/300], Step [8/172], Loss: 21.9155\n",
      "Epoch [40/300], Step [9/172], Loss: 59.1139\n",
      "Epoch [40/300], Step [10/172], Loss: 66.4110\n",
      "Epoch [40/300], Step [11/172], Loss: 100.0165\n",
      "Epoch [40/300], Step [12/172], Loss: 90.8826\n",
      "Epoch [40/300], Step [13/172], Loss: 54.3179\n",
      "Epoch [40/300], Step [14/172], Loss: 102.8553\n",
      "Epoch [40/300], Step [15/172], Loss: 92.2792\n",
      "Epoch [40/300], Step [16/172], Loss: 71.5275\n",
      "Epoch [40/300], Step [17/172], Loss: 63.4119\n",
      "Epoch [40/300], Step [18/172], Loss: 81.6911\n",
      "Epoch [40/300], Step [19/172], Loss: 81.0072\n",
      "Epoch [40/300], Step [20/172], Loss: 122.0111\n",
      "Epoch [40/300], Step [21/172], Loss: 98.3267\n",
      "Epoch [40/300], Step [22/172], Loss: 95.6693\n",
      "Epoch [40/300], Step [23/172], Loss: 34.5971\n",
      "Epoch [40/300], Step [24/172], Loss: 84.8759\n",
      "Epoch [40/300], Step [25/172], Loss: 58.1018\n",
      "Epoch [40/300], Step [26/172], Loss: 65.2120\n",
      "Epoch [40/300], Step [27/172], Loss: 88.9810\n",
      "Epoch [40/300], Step [28/172], Loss: 74.0162\n",
      "Epoch [40/300], Step [29/172], Loss: 87.4192\n",
      "Epoch [40/300], Step [30/172], Loss: 79.3167\n",
      "Epoch [40/300], Step [31/172], Loss: 50.4011\n",
      "Epoch [40/300], Step [32/172], Loss: 44.0379\n",
      "Epoch [40/300], Step [33/172], Loss: 79.8798\n",
      "Epoch [40/300], Step [34/172], Loss: 16.3466\n",
      "Epoch [40/300], Step [35/172], Loss: 63.5111\n",
      "Epoch [40/300], Step [36/172], Loss: 32.9513\n",
      "Epoch [40/300], Step [37/172], Loss: 23.4613\n",
      "Epoch [40/300], Step [38/172], Loss: 36.8344\n",
      "Epoch [40/300], Step [39/172], Loss: 56.4785\n",
      "Epoch [40/300], Step [40/172], Loss: 34.3059\n",
      "Epoch [40/300], Step [41/172], Loss: 44.5562\n",
      "Epoch [40/300], Step [42/172], Loss: 43.8591\n",
      "Epoch [40/300], Step [43/172], Loss: 35.8092\n",
      "Epoch [40/300], Step [44/172], Loss: 31.3930\n",
      "Epoch [40/300], Step [45/172], Loss: 26.0066\n",
      "Epoch [40/300], Step [46/172], Loss: 49.3590\n",
      "Epoch [40/300], Step [47/172], Loss: 69.9866\n",
      "Epoch [40/300], Step [48/172], Loss: 73.7356\n",
      "Epoch [40/300], Step [49/172], Loss: 24.6598\n",
      "Epoch [40/300], Step [50/172], Loss: 55.4231\n",
      "Epoch [40/300], Step [51/172], Loss: 9.7059\n",
      "Epoch [40/300], Step [52/172], Loss: 25.8009\n",
      "Epoch [40/300], Step [53/172], Loss: 31.0668\n",
      "Epoch [40/300], Step [54/172], Loss: 22.1036\n",
      "Epoch [40/300], Step [55/172], Loss: 18.2195\n",
      "Epoch [40/300], Step [56/172], Loss: 13.1067\n",
      "Epoch [40/300], Step [57/172], Loss: 49.0364\n",
      "Epoch [40/300], Step [58/172], Loss: 20.8205\n",
      "Epoch [40/300], Step [59/172], Loss: 38.3656\n",
      "Epoch [40/300], Step [60/172], Loss: 63.5768\n",
      "Epoch [40/300], Step [61/172], Loss: 12.8333\n",
      "Epoch [40/300], Step [62/172], Loss: 19.7936\n",
      "Epoch [40/300], Step [63/172], Loss: 10.0142\n",
      "Epoch [40/300], Step [64/172], Loss: 7.1198\n",
      "Epoch [40/300], Step [65/172], Loss: 29.0341\n",
      "Epoch [40/300], Step [66/172], Loss: 9.9107\n",
      "Epoch [40/300], Step [67/172], Loss: 28.0392\n",
      "Epoch [40/300], Step [68/172], Loss: 18.7112\n",
      "Epoch [40/300], Step [69/172], Loss: 80.4604\n",
      "Epoch [40/300], Step [70/172], Loss: 74.8049\n",
      "Epoch [40/300], Step [71/172], Loss: 69.3760\n",
      "Epoch [40/300], Step [72/172], Loss: 68.5162\n",
      "Epoch [40/300], Step [73/172], Loss: 75.0886\n",
      "Epoch [40/300], Step [74/172], Loss: 52.6991\n",
      "Epoch [40/300], Step [75/172], Loss: 33.5873\n",
      "Epoch [40/300], Step [76/172], Loss: 52.6523\n",
      "Epoch [40/300], Step [77/172], Loss: 62.5640\n",
      "Epoch [40/300], Step [78/172], Loss: 63.5441\n",
      "Epoch [40/300], Step [79/172], Loss: 54.7945\n",
      "Epoch [40/300], Step [80/172], Loss: 58.5175\n",
      "Epoch [40/300], Step [81/172], Loss: 55.0927\n",
      "Epoch [40/300], Step [82/172], Loss: 48.6103\n",
      "Epoch [40/300], Step [83/172], Loss: 52.7565\n",
      "Epoch [40/300], Step [84/172], Loss: 46.4903\n",
      "Epoch [40/300], Step [85/172], Loss: 54.6397\n",
      "Epoch [40/300], Step [86/172], Loss: 41.0492\n",
      "Epoch [40/300], Step [87/172], Loss: 33.7912\n",
      "Epoch [40/300], Step [88/172], Loss: 36.8856\n",
      "Epoch [40/300], Step [89/172], Loss: 36.3060\n",
      "Epoch [40/300], Step [90/172], Loss: 34.9196\n",
      "Epoch [40/300], Step [91/172], Loss: 33.9333\n",
      "Epoch [40/300], Step [92/172], Loss: 28.6179\n",
      "Epoch [40/300], Step [93/172], Loss: 30.8839\n",
      "Epoch [40/300], Step [94/172], Loss: 33.9067\n",
      "Epoch [40/300], Step [95/172], Loss: 30.8421\n",
      "Epoch [40/300], Step [96/172], Loss: 27.2017\n",
      "Epoch [40/300], Step [97/172], Loss: 31.4255\n",
      "Epoch [40/300], Step [98/172], Loss: 27.3515\n",
      "Epoch [40/300], Step [99/172], Loss: 24.8692\n",
      "Epoch [40/300], Step [100/172], Loss: 24.2097\n",
      "Epoch [40/300], Step [101/172], Loss: 25.7816\n",
      "Epoch [40/300], Step [102/172], Loss: 23.6923\n",
      "Epoch [40/300], Step [103/172], Loss: 23.6494\n",
      "Epoch [40/300], Step [104/172], Loss: 23.2803\n",
      "Epoch [40/300], Step [105/172], Loss: 23.5600\n",
      "Epoch [40/300], Step [106/172], Loss: 23.6609\n",
      "Epoch [40/300], Step [107/172], Loss: 20.1973\n",
      "Epoch [40/300], Step [108/172], Loss: 24.0590\n",
      "Epoch [40/300], Step [109/172], Loss: 25.2123\n",
      "Epoch [40/300], Step [110/172], Loss: 21.8979\n",
      "Epoch [40/300], Step [111/172], Loss: 21.1080\n",
      "Epoch [40/300], Step [112/172], Loss: 27.3147\n",
      "Epoch [40/300], Step [113/172], Loss: 21.2447\n",
      "Epoch [40/300], Step [114/172], Loss: 21.9216\n",
      "Epoch [40/300], Step [115/172], Loss: 29.5161\n",
      "Epoch [40/300], Step [116/172], Loss: 21.6823\n",
      "Epoch [40/300], Step [117/172], Loss: 18.6316\n",
      "Epoch [40/300], Step [118/172], Loss: 22.0229\n",
      "Epoch [40/300], Step [119/172], Loss: 18.8270\n",
      "Epoch [40/300], Step [120/172], Loss: 17.4165\n",
      "Epoch [40/300], Step [121/172], Loss: 18.1277\n",
      "Epoch [40/300], Step [122/172], Loss: 15.8084\n",
      "Epoch [40/300], Step [123/172], Loss: 16.7579\n",
      "Epoch [40/300], Step [124/172], Loss: 14.6669\n",
      "Epoch [40/300], Step [125/172], Loss: 18.5822\n",
      "Epoch [40/300], Step [126/172], Loss: 17.6081\n",
      "Epoch [40/300], Step [127/172], Loss: 19.8037\n",
      "Epoch [40/300], Step [128/172], Loss: 20.5409\n",
      "Epoch [40/300], Step [129/172], Loss: 14.9192\n",
      "Epoch [40/300], Step [130/172], Loss: 17.1346\n",
      "Epoch [40/300], Step [131/172], Loss: 15.1202\n",
      "Epoch [40/300], Step [132/172], Loss: 15.2115\n",
      "Epoch [40/300], Step [133/172], Loss: 15.0603\n",
      "Epoch [40/300], Step [134/172], Loss: 15.5985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/300], Step [135/172], Loss: 13.2944\n",
      "Epoch [40/300], Step [136/172], Loss: 14.1222\n",
      "Epoch [40/300], Step [137/172], Loss: 15.5423\n",
      "Epoch [40/300], Step [138/172], Loss: 13.5228\n",
      "Epoch [40/300], Step [139/172], Loss: 14.9650\n",
      "Epoch [40/300], Step [140/172], Loss: 14.8254\n",
      "Epoch [40/300], Step [141/172], Loss: 17.9136\n",
      "Epoch [40/300], Step [142/172], Loss: 16.4424\n",
      "Epoch [40/300], Step [143/172], Loss: 13.6058\n",
      "Epoch [40/300], Step [144/172], Loss: 12.7732\n",
      "Epoch [40/300], Step [145/172], Loss: 12.9794\n",
      "Epoch [40/300], Step [146/172], Loss: 13.4348\n",
      "Epoch [40/300], Step [147/172], Loss: 11.0272\n",
      "Epoch [40/300], Step [148/172], Loss: 11.4636\n",
      "Epoch [40/300], Step [149/172], Loss: 13.3703\n",
      "Epoch [40/300], Step [150/172], Loss: 13.1930\n",
      "Epoch [40/300], Step [151/172], Loss: 11.4313\n",
      "Epoch [40/300], Step [152/172], Loss: 11.6271\n",
      "Epoch [40/300], Step [153/172], Loss: 11.6217\n",
      "Epoch [40/300], Step [154/172], Loss: 12.9343\n",
      "Epoch [40/300], Step [155/172], Loss: 11.2308\n",
      "Epoch [40/300], Step [156/172], Loss: 12.3923\n",
      "Epoch [40/300], Step [157/172], Loss: 13.5644\n",
      "Epoch [40/300], Step [158/172], Loss: 11.9169\n",
      "Epoch [40/300], Step [159/172], Loss: 11.8996\n",
      "Epoch [40/300], Step [160/172], Loss: 11.7190\n",
      "Epoch [40/300], Step [161/172], Loss: 10.5987\n",
      "Epoch [40/300], Step [162/172], Loss: 10.6774\n",
      "Epoch [40/300], Step [163/172], Loss: 10.5844\n",
      "Epoch [40/300], Step [164/172], Loss: 12.0903\n",
      "Epoch [40/300], Step [165/172], Loss: 9.6618\n",
      "Epoch [40/300], Step [166/172], Loss: 10.4545\n",
      "Epoch [40/300], Step [167/172], Loss: 10.6012\n",
      "Epoch [40/300], Step [168/172], Loss: 10.2222\n",
      "Epoch [40/300], Step [169/172], Loss: 9.3396\n",
      "Epoch [40/300], Step [170/172], Loss: 9.6187\n",
      "Epoch [40/300], Step [171/172], Loss: 8.1441\n",
      "Epoch [40/300], Step [172/172], Loss: 7.0398\n",
      "Epoch [41/300], Step [1/172], Loss: 102.8376\n",
      "Epoch [41/300], Step [2/172], Loss: 101.0383\n",
      "Epoch [41/300], Step [3/172], Loss: 123.0479\n",
      "Epoch [41/300], Step [4/172], Loss: 75.6297\n",
      "Epoch [41/300], Step [5/172], Loss: 97.0276\n",
      "Epoch [41/300], Step [6/172], Loss: 42.3101\n",
      "Epoch [41/300], Step [7/172], Loss: 52.9829\n",
      "Epoch [41/300], Step [8/172], Loss: 20.8937\n",
      "Epoch [41/300], Step [9/172], Loss: 57.6309\n",
      "Epoch [41/300], Step [10/172], Loss: 64.4001\n",
      "Epoch [41/300], Step [11/172], Loss: 97.6895\n",
      "Epoch [41/300], Step [12/172], Loss: 88.1796\n",
      "Epoch [41/300], Step [13/172], Loss: 52.7674\n",
      "Epoch [41/300], Step [14/172], Loss: 101.2910\n",
      "Epoch [41/300], Step [15/172], Loss: 91.3434\n",
      "Epoch [41/300], Step [16/172], Loss: 70.3235\n",
      "Epoch [41/300], Step [17/172], Loss: 62.1927\n",
      "Epoch [41/300], Step [18/172], Loss: 79.8632\n",
      "Epoch [41/300], Step [19/172], Loss: 80.6163\n",
      "Epoch [41/300], Step [20/172], Loss: 117.4217\n",
      "Epoch [41/300], Step [21/172], Loss: 98.0964\n",
      "Epoch [41/300], Step [22/172], Loss: 94.8997\n",
      "Epoch [41/300], Step [23/172], Loss: 33.3162\n",
      "Epoch [41/300], Step [24/172], Loss: 84.1257\n",
      "Epoch [41/300], Step [25/172], Loss: 57.7741\n",
      "Epoch [41/300], Step [26/172], Loss: 65.1218\n",
      "Epoch [41/300], Step [27/172], Loss: 88.9482\n",
      "Epoch [41/300], Step [28/172], Loss: 72.3270\n",
      "Epoch [41/300], Step [29/172], Loss: 84.5040\n",
      "Epoch [41/300], Step [30/172], Loss: 79.2059\n",
      "Epoch [41/300], Step [31/172], Loss: 49.6259\n",
      "Epoch [41/300], Step [32/172], Loss: 43.7691\n",
      "Epoch [41/300], Step [33/172], Loss: 79.7287\n",
      "Epoch [41/300], Step [34/172], Loss: 15.5104\n",
      "Epoch [41/300], Step [35/172], Loss: 62.4356\n",
      "Epoch [41/300], Step [36/172], Loss: 32.1924\n",
      "Epoch [41/300], Step [37/172], Loss: 22.9824\n",
      "Epoch [41/300], Step [38/172], Loss: 35.7091\n",
      "Epoch [41/300], Step [39/172], Loss: 56.3860\n",
      "Epoch [41/300], Step [40/172], Loss: 33.6405\n",
      "Epoch [41/300], Step [41/172], Loss: 44.3031\n",
      "Epoch [41/300], Step [42/172], Loss: 43.5500\n",
      "Epoch [41/300], Step [43/172], Loss: 35.1270\n",
      "Epoch [41/300], Step [44/172], Loss: 30.8917\n",
      "Epoch [41/300], Step [45/172], Loss: 25.5114\n",
      "Epoch [41/300], Step [46/172], Loss: 48.5048\n",
      "Epoch [41/300], Step [47/172], Loss: 69.5677\n",
      "Epoch [41/300], Step [48/172], Loss: 73.6143\n",
      "Epoch [41/300], Step [49/172], Loss: 24.2369\n",
      "Epoch [41/300], Step [50/172], Loss: 55.3483\n",
      "Epoch [41/300], Step [51/172], Loss: 9.3170\n",
      "Epoch [41/300], Step [52/172], Loss: 25.2883\n",
      "Epoch [41/300], Step [53/172], Loss: 30.5727\n",
      "Epoch [41/300], Step [54/172], Loss: 21.5016\n",
      "Epoch [41/300], Step [55/172], Loss: 17.7639\n",
      "Epoch [41/300], Step [56/172], Loss: 12.5506\n",
      "Epoch [41/300], Step [57/172], Loss: 48.9799\n",
      "Epoch [41/300], Step [58/172], Loss: 20.9471\n",
      "Epoch [41/300], Step [59/172], Loss: 38.3787\n",
      "Epoch [41/300], Step [60/172], Loss: 64.1298\n",
      "Epoch [41/300], Step [61/172], Loss: 12.6988\n",
      "Epoch [41/300], Step [62/172], Loss: 19.8693\n",
      "Epoch [41/300], Step [63/172], Loss: 9.6416\n",
      "Epoch [41/300], Step [64/172], Loss: 6.9466\n",
      "Epoch [41/300], Step [65/172], Loss: 29.2324\n",
      "Epoch [41/300], Step [66/172], Loss: 9.6012\n",
      "Epoch [41/300], Step [67/172], Loss: 28.1917\n",
      "Epoch [41/300], Step [68/172], Loss: 18.2196\n",
      "Epoch [41/300], Step [69/172], Loss: 81.0138\n",
      "Epoch [41/300], Step [70/172], Loss: 73.8308\n",
      "Epoch [41/300], Step [71/172], Loss: 68.6479\n",
      "Epoch [41/300], Step [72/172], Loss: 67.7768\n",
      "Epoch [41/300], Step [73/172], Loss: 74.4325\n",
      "Epoch [41/300], Step [74/172], Loss: 51.2943\n",
      "Epoch [41/300], Step [75/172], Loss: 32.7617\n",
      "Epoch [41/300], Step [76/172], Loss: 51.9818\n",
      "Epoch [41/300], Step [77/172], Loss: 62.2347\n",
      "Epoch [41/300], Step [78/172], Loss: 62.9402\n",
      "Epoch [41/300], Step [79/172], Loss: 54.5494\n",
      "Epoch [41/300], Step [80/172], Loss: 58.2132\n",
      "Epoch [41/300], Step [81/172], Loss: 54.7272\n",
      "Epoch [41/300], Step [82/172], Loss: 48.5552\n",
      "Epoch [41/300], Step [83/172], Loss: 52.7151\n",
      "Epoch [41/300], Step [84/172], Loss: 46.1383\n",
      "Epoch [41/300], Step [85/172], Loss: 54.1747\n",
      "Epoch [41/300], Step [86/172], Loss: 40.7963\n",
      "Epoch [41/300], Step [87/172], Loss: 33.5049\n",
      "Epoch [41/300], Step [88/172], Loss: 36.5692\n",
      "Epoch [41/300], Step [89/172], Loss: 35.9382\n",
      "Epoch [41/300], Step [90/172], Loss: 34.7136\n",
      "Epoch [41/300], Step [91/172], Loss: 33.6181\n",
      "Epoch [41/300], Step [92/172], Loss: 28.1830\n",
      "Epoch [41/300], Step [93/172], Loss: 30.3421\n",
      "Epoch [41/300], Step [94/172], Loss: 33.4080\n",
      "Epoch [41/300], Step [95/172], Loss: 30.3498\n",
      "Epoch [41/300], Step [96/172], Loss: 26.7227\n",
      "Epoch [41/300], Step [97/172], Loss: 31.1770\n",
      "Epoch [41/300], Step [98/172], Loss: 26.9548\n",
      "Epoch [41/300], Step [99/172], Loss: 24.5271\n",
      "Epoch [41/300], Step [100/172], Loss: 23.7593\n",
      "Epoch [41/300], Step [101/172], Loss: 25.4456\n",
      "Epoch [41/300], Step [102/172], Loss: 23.3662\n",
      "Epoch [41/300], Step [103/172], Loss: 23.1879\n",
      "Epoch [41/300], Step [104/172], Loss: 22.8609\n",
      "Epoch [41/300], Step [105/172], Loss: 23.2232\n",
      "Epoch [41/300], Step [106/172], Loss: 23.2356\n",
      "Epoch [41/300], Step [107/172], Loss: 19.9247\n",
      "Epoch [41/300], Step [108/172], Loss: 23.6891\n",
      "Epoch [41/300], Step [109/172], Loss: 24.9706\n",
      "Epoch [41/300], Step [110/172], Loss: 21.5348\n",
      "Epoch [41/300], Step [111/172], Loss: 20.6916\n",
      "Epoch [41/300], Step [112/172], Loss: 26.8385\n",
      "Epoch [41/300], Step [113/172], Loss: 20.9418\n",
      "Epoch [41/300], Step [114/172], Loss: 21.5405\n",
      "Epoch [41/300], Step [115/172], Loss: 29.2688\n",
      "Epoch [41/300], Step [116/172], Loss: 21.3016\n",
      "Epoch [41/300], Step [117/172], Loss: 18.2759\n",
      "Epoch [41/300], Step [118/172], Loss: 21.6840\n",
      "Epoch [41/300], Step [119/172], Loss: 18.5134\n",
      "Epoch [41/300], Step [120/172], Loss: 17.0751\n",
      "Epoch [41/300], Step [121/172], Loss: 17.7705\n",
      "Epoch [41/300], Step [122/172], Loss: 15.5272\n",
      "Epoch [41/300], Step [123/172], Loss: 16.3940\n",
      "Epoch [41/300], Step [124/172], Loss: 14.2706\n",
      "Epoch [41/300], Step [125/172], Loss: 18.2421\n",
      "Epoch [41/300], Step [126/172], Loss: 17.2495\n",
      "Epoch [41/300], Step [127/172], Loss: 19.4592\n",
      "Epoch [41/300], Step [128/172], Loss: 20.1932\n",
      "Epoch [41/300], Step [129/172], Loss: 14.6144\n",
      "Epoch [41/300], Step [130/172], Loss: 16.7792\n",
      "Epoch [41/300], Step [131/172], Loss: 14.7977\n",
      "Epoch [41/300], Step [132/172], Loss: 14.8763\n",
      "Epoch [41/300], Step [133/172], Loss: 14.7436\n",
      "Epoch [41/300], Step [134/172], Loss: 15.2915\n",
      "Epoch [41/300], Step [135/172], Loss: 12.9887\n",
      "Epoch [41/300], Step [136/172], Loss: 13.7807\n",
      "Epoch [41/300], Step [137/172], Loss: 15.2578\n",
      "Epoch [41/300], Step [138/172], Loss: 13.2611\n",
      "Epoch [41/300], Step [139/172], Loss: 14.6904\n",
      "Epoch [41/300], Step [140/172], Loss: 14.5706\n",
      "Epoch [41/300], Step [141/172], Loss: 17.7323\n",
      "Epoch [41/300], Step [142/172], Loss: 16.2309\n",
      "Epoch [41/300], Step [143/172], Loss: 13.3834\n",
      "Epoch [41/300], Step [144/172], Loss: 12.5428\n",
      "Epoch [41/300], Step [145/172], Loss: 12.7340\n",
      "Epoch [41/300], Step [146/172], Loss: 13.2284\n",
      "Epoch [41/300], Step [147/172], Loss: 10.6436\n",
      "Epoch [41/300], Step [148/172], Loss: 11.1440\n",
      "Epoch [41/300], Step [149/172], Loss: 13.1294\n",
      "Epoch [41/300], Step [150/172], Loss: 12.9195\n",
      "Epoch [41/300], Step [151/172], Loss: 11.1819\n",
      "Epoch [41/300], Step [152/172], Loss: 11.4079\n",
      "Epoch [41/300], Step [153/172], Loss: 11.3770\n",
      "Epoch [41/300], Step [154/172], Loss: 12.6851\n",
      "Epoch [41/300], Step [155/172], Loss: 10.9834\n",
      "Epoch [41/300], Step [156/172], Loss: 12.2447\n",
      "Epoch [41/300], Step [157/172], Loss: 13.3925\n",
      "Epoch [41/300], Step [158/172], Loss: 11.7040\n",
      "Epoch [41/300], Step [159/172], Loss: 11.6735\n",
      "Epoch [41/300], Step [160/172], Loss: 11.5040\n",
      "Epoch [41/300], Step [161/172], Loss: 10.3588\n",
      "Epoch [41/300], Step [162/172], Loss: 10.4979\n",
      "Epoch [41/300], Step [163/172], Loss: 10.3200\n",
      "Epoch [41/300], Step [164/172], Loss: 11.8888\n",
      "Epoch [41/300], Step [165/172], Loss: 9.4338\n",
      "Epoch [41/300], Step [166/172], Loss: 10.1608\n",
      "Epoch [41/300], Step [167/172], Loss: 10.3760\n",
      "Epoch [41/300], Step [168/172], Loss: 9.9277\n",
      "Epoch [41/300], Step [169/172], Loss: 9.1051\n",
      "Epoch [41/300], Step [170/172], Loss: 9.2738\n",
      "Epoch [41/300], Step [171/172], Loss: 7.9198\n",
      "Epoch [41/300], Step [172/172], Loss: 6.8927\n",
      "Epoch [42/300], Step [1/172], Loss: 102.4433\n",
      "Epoch [42/300], Step [2/172], Loss: 100.7757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/300], Step [3/172], Loss: 122.5815\n",
      "Epoch [42/300], Step [4/172], Loss: 75.0390\n",
      "Epoch [42/300], Step [5/172], Loss: 96.5118\n",
      "Epoch [42/300], Step [6/172], Loss: 41.1741\n",
      "Epoch [42/300], Step [7/172], Loss: 51.9555\n",
      "Epoch [42/300], Step [8/172], Loss: 20.2782\n",
      "Epoch [42/300], Step [9/172], Loss: 57.4420\n",
      "Epoch [42/300], Step [10/172], Loss: 63.7790\n",
      "Epoch [42/300], Step [11/172], Loss: 98.8238\n",
      "Epoch [42/300], Step [12/172], Loss: 88.9101\n",
      "Epoch [42/300], Step [13/172], Loss: 51.9063\n",
      "Epoch [42/300], Step [14/172], Loss: 101.0724\n",
      "Epoch [42/300], Step [15/172], Loss: 91.1087\n",
      "Epoch [42/300], Step [16/172], Loss: 69.2626\n",
      "Epoch [42/300], Step [17/172], Loss: 62.1474\n",
      "Epoch [42/300], Step [18/172], Loss: 78.8511\n",
      "Epoch [42/300], Step [19/172], Loss: 80.3337\n",
      "Epoch [42/300], Step [20/172], Loss: 116.0930\n",
      "Epoch [42/300], Step [21/172], Loss: 97.6392\n",
      "Epoch [42/300], Step [22/172], Loss: 94.4425\n",
      "Epoch [42/300], Step [23/172], Loss: 32.1295\n",
      "Epoch [42/300], Step [24/172], Loss: 83.6316\n",
      "Epoch [42/300], Step [25/172], Loss: 57.1472\n",
      "Epoch [42/300], Step [26/172], Loss: 64.8234\n",
      "Epoch [42/300], Step [27/172], Loss: 88.6561\n",
      "Epoch [42/300], Step [28/172], Loss: 70.4705\n",
      "Epoch [42/300], Step [29/172], Loss: 82.0330\n",
      "Epoch [42/300], Step [30/172], Loss: 78.7754\n",
      "Epoch [42/300], Step [31/172], Loss: 48.7359\n",
      "Epoch [42/300], Step [32/172], Loss: 43.2476\n",
      "Epoch [42/300], Step [33/172], Loss: 79.0337\n",
      "Epoch [42/300], Step [34/172], Loss: 14.6884\n",
      "Epoch [42/300], Step [35/172], Loss: 61.5648\n",
      "Epoch [42/300], Step [36/172], Loss: 31.5036\n",
      "Epoch [42/300], Step [37/172], Loss: 22.3528\n",
      "Epoch [42/300], Step [38/172], Loss: 34.1304\n",
      "Epoch [42/300], Step [39/172], Loss: 55.8507\n",
      "Epoch [42/300], Step [40/172], Loss: 32.8208\n",
      "Epoch [42/300], Step [41/172], Loss: 43.7038\n",
      "Epoch [42/300], Step [42/172], Loss: 43.2016\n",
      "Epoch [42/300], Step [43/172], Loss: 34.2149\n",
      "Epoch [42/300], Step [44/172], Loss: 30.4627\n",
      "Epoch [42/300], Step [45/172], Loss: 25.0469\n",
      "Epoch [42/300], Step [46/172], Loss: 47.6529\n",
      "Epoch [42/300], Step [47/172], Loss: 68.9094\n",
      "Epoch [42/300], Step [48/172], Loss: 72.9538\n",
      "Epoch [42/300], Step [49/172], Loss: 23.7807\n",
      "Epoch [42/300], Step [50/172], Loss: 54.6349\n",
      "Epoch [42/300], Step [51/172], Loss: 9.0510\n",
      "Epoch [42/300], Step [52/172], Loss: 24.9180\n",
      "Epoch [42/300], Step [53/172], Loss: 30.1317\n",
      "Epoch [42/300], Step [54/172], Loss: 21.0734\n",
      "Epoch [42/300], Step [55/172], Loss: 17.4699\n",
      "Epoch [42/300], Step [56/172], Loss: 12.1224\n",
      "Epoch [42/300], Step [57/172], Loss: 48.4017\n",
      "Epoch [42/300], Step [58/172], Loss: 20.9353\n",
      "Epoch [42/300], Step [59/172], Loss: 38.1734\n",
      "Epoch [42/300], Step [60/172], Loss: 64.1370\n",
      "Epoch [42/300], Step [61/172], Loss: 12.5460\n",
      "Epoch [42/300], Step [62/172], Loss: 20.0241\n",
      "Epoch [42/300], Step [63/172], Loss: 9.4636\n",
      "Epoch [42/300], Step [64/172], Loss: 6.8892\n",
      "Epoch [42/300], Step [65/172], Loss: 29.2431\n",
      "Epoch [42/300], Step [66/172], Loss: 9.3094\n",
      "Epoch [42/300], Step [67/172], Loss: 28.2168\n",
      "Epoch [42/300], Step [68/172], Loss: 18.1550\n",
      "Epoch [42/300], Step [69/172], Loss: 81.2712\n",
      "Epoch [42/300], Step [70/172], Loss: 73.2797\n",
      "Epoch [42/300], Step [71/172], Loss: 68.0862\n",
      "Epoch [42/300], Step [72/172], Loss: 67.5058\n",
      "Epoch [42/300], Step [73/172], Loss: 74.0356\n",
      "Epoch [42/300], Step [74/172], Loss: 50.6471\n",
      "Epoch [42/300], Step [75/172], Loss: 32.4891\n",
      "Epoch [42/300], Step [76/172], Loss: 51.5715\n",
      "Epoch [42/300], Step [77/172], Loss: 62.1338\n",
      "Epoch [42/300], Step [78/172], Loss: 62.5702\n",
      "Epoch [42/300], Step [79/172], Loss: 54.5906\n",
      "Epoch [42/300], Step [80/172], Loss: 57.9386\n",
      "Epoch [42/300], Step [81/172], Loss: 54.6876\n",
      "Epoch [42/300], Step [82/172], Loss: 48.0224\n",
      "Epoch [42/300], Step [83/172], Loss: 52.7421\n",
      "Epoch [42/300], Step [84/172], Loss: 45.9296\n",
      "Epoch [42/300], Step [85/172], Loss: 53.7375\n",
      "Epoch [42/300], Step [86/172], Loss: 40.6028\n",
      "Epoch [42/300], Step [87/172], Loss: 33.4134\n",
      "Epoch [42/300], Step [88/172], Loss: 36.4342\n",
      "Epoch [42/300], Step [89/172], Loss: 35.7844\n",
      "Epoch [42/300], Step [90/172], Loss: 34.6663\n",
      "Epoch [42/300], Step [91/172], Loss: 33.5228\n",
      "Epoch [42/300], Step [92/172], Loss: 28.0895\n",
      "Epoch [42/300], Step [93/172], Loss: 30.0965\n",
      "Epoch [42/300], Step [94/172], Loss: 33.2058\n",
      "Epoch [42/300], Step [95/172], Loss: 30.1151\n",
      "Epoch [42/300], Step [96/172], Loss: 26.4991\n",
      "Epoch [42/300], Step [97/172], Loss: 31.1119\n",
      "Epoch [42/300], Step [98/172], Loss: 26.7871\n",
      "Epoch [42/300], Step [99/172], Loss: 24.4043\n",
      "Epoch [42/300], Step [100/172], Loss: 23.5682\n",
      "Epoch [42/300], Step [101/172], Loss: 25.2452\n",
      "Epoch [42/300], Step [102/172], Loss: 23.2856\n",
      "Epoch [42/300], Step [103/172], Loss: 23.0238\n",
      "Epoch [42/300], Step [104/172], Loss: 22.6554\n",
      "Epoch [42/300], Step [105/172], Loss: 23.1154\n",
      "Epoch [42/300], Step [106/172], Loss: 23.0467\n",
      "Epoch [42/300], Step [107/172], Loss: 19.8381\n",
      "Epoch [42/300], Step [108/172], Loss: 23.5475\n",
      "Epoch [42/300], Step [109/172], Loss: 24.9338\n",
      "Epoch [42/300], Step [110/172], Loss: 21.4241\n",
      "Epoch [42/300], Step [111/172], Loss: 20.5252\n",
      "Epoch [42/300], Step [112/172], Loss: 26.6460\n",
      "Epoch [42/300], Step [113/172], Loss: 20.8125\n",
      "Epoch [42/300], Step [114/172], Loss: 21.3883\n",
      "Epoch [42/300], Step [115/172], Loss: 29.2006\n",
      "Epoch [42/300], Step [116/172], Loss: 21.1073\n",
      "Epoch [42/300], Step [117/172], Loss: 18.1632\n",
      "Epoch [42/300], Step [118/172], Loss: 21.6315\n",
      "Epoch [42/300], Step [119/172], Loss: 18.4372\n",
      "Epoch [42/300], Step [120/172], Loss: 17.0108\n",
      "Epoch [42/300], Step [121/172], Loss: 17.7126\n",
      "Epoch [42/300], Step [122/172], Loss: 15.5007\n",
      "Epoch [42/300], Step [123/172], Loss: 16.2966\n",
      "Epoch [42/300], Step [124/172], Loss: 14.1544\n",
      "Epoch [42/300], Step [125/172], Loss: 18.2358\n",
      "Epoch [42/300], Step [126/172], Loss: 17.1088\n",
      "Epoch [42/300], Step [127/172], Loss: 19.3279\n",
      "Epoch [42/300], Step [128/172], Loss: 20.0349\n",
      "Epoch [42/300], Step [129/172], Loss: 14.4974\n",
      "Epoch [42/300], Step [130/172], Loss: 16.6955\n",
      "Epoch [42/300], Step [131/172], Loss: 14.7020\n",
      "Epoch [42/300], Step [132/172], Loss: 14.7627\n",
      "Epoch [42/300], Step [133/172], Loss: 14.7210\n",
      "Epoch [42/300], Step [134/172], Loss: 15.2520\n",
      "Epoch [42/300], Step [135/172], Loss: 12.9323\n",
      "Epoch [42/300], Step [136/172], Loss: 13.6512\n",
      "Epoch [42/300], Step [137/172], Loss: 15.1689\n",
      "Epoch [42/300], Step [138/172], Loss: 13.1400\n",
      "Epoch [42/300], Step [139/172], Loss: 14.5611\n",
      "Epoch [42/300], Step [140/172], Loss: 14.4498\n",
      "Epoch [42/300], Step [141/172], Loss: 17.6926\n",
      "Epoch [42/300], Step [142/172], Loss: 16.1702\n",
      "Epoch [42/300], Step [143/172], Loss: 13.2789\n",
      "Epoch [42/300], Step [144/172], Loss: 12.4902\n",
      "Epoch [42/300], Step [145/172], Loss: 12.7021\n",
      "Epoch [42/300], Step [146/172], Loss: 13.1658\n",
      "Epoch [42/300], Step [147/172], Loss: 10.6049\n",
      "Epoch [42/300], Step [148/172], Loss: 11.0835\n",
      "Epoch [42/300], Step [149/172], Loss: 13.0598\n",
      "Epoch [42/300], Step [150/172], Loss: 12.8405\n",
      "Epoch [42/300], Step [151/172], Loss: 11.1633\n",
      "Epoch [42/300], Step [152/172], Loss: 11.3250\n",
      "Epoch [42/300], Step [153/172], Loss: 11.3192\n",
      "Epoch [42/300], Step [154/172], Loss: 12.6101\n",
      "Epoch [42/300], Step [155/172], Loss: 10.9398\n",
      "Epoch [42/300], Step [156/172], Loss: 12.2709\n",
      "Epoch [42/300], Step [157/172], Loss: 13.3613\n",
      "Epoch [42/300], Step [158/172], Loss: 11.6640\n",
      "Epoch [42/300], Step [159/172], Loss: 11.6756\n",
      "Epoch [42/300], Step [160/172], Loss: 11.5235\n",
      "Epoch [42/300], Step [161/172], Loss: 10.3179\n",
      "Epoch [42/300], Step [162/172], Loss: 10.4650\n",
      "Epoch [42/300], Step [163/172], Loss: 10.3237\n",
      "Epoch [42/300], Step [164/172], Loss: 11.9909\n",
      "Epoch [42/300], Step [165/172], Loss: 9.4107\n",
      "Epoch [42/300], Step [166/172], Loss: 10.1679\n",
      "Epoch [42/300], Step [167/172], Loss: 10.3821\n",
      "Epoch [42/300], Step [168/172], Loss: 9.9308\n",
      "Epoch [42/300], Step [169/172], Loss: 9.1437\n",
      "Epoch [42/300], Step [170/172], Loss: 9.3369\n",
      "Epoch [42/300], Step [171/172], Loss: 7.9281\n",
      "Epoch [42/300], Step [172/172], Loss: 6.9297\n",
      "Epoch [43/300], Step [1/172], Loss: 102.4635\n",
      "Epoch [43/300], Step [2/172], Loss: 100.4619\n",
      "Epoch [43/300], Step [3/172], Loss: 121.3898\n",
      "Epoch [43/300], Step [4/172], Loss: 74.3090\n",
      "Epoch [43/300], Step [5/172], Loss: 95.7186\n",
      "Epoch [43/300], Step [6/172], Loss: 40.4028\n",
      "Epoch [43/300], Step [7/172], Loss: 50.7299\n",
      "Epoch [43/300], Step [8/172], Loss: 18.2285\n",
      "Epoch [43/300], Step [9/172], Loss: 56.1634\n",
      "Epoch [43/300], Step [10/172], Loss: 63.3173\n",
      "Epoch [43/300], Step [11/172], Loss: 98.1741\n",
      "Epoch [43/300], Step [12/172], Loss: 87.9116\n",
      "Epoch [43/300], Step [13/172], Loss: 51.5046\n",
      "Epoch [43/300], Step [14/172], Loss: 100.9389\n",
      "Epoch [43/300], Step [15/172], Loss: 90.8507\n",
      "Epoch [43/300], Step [16/172], Loss: 68.4977\n",
      "Epoch [43/300], Step [17/172], Loss: 61.7538\n",
      "Epoch [43/300], Step [18/172], Loss: 77.9365\n",
      "Epoch [43/300], Step [19/172], Loss: 80.4059\n",
      "Epoch [43/300], Step [20/172], Loss: 115.3624\n",
      "Epoch [43/300], Step [21/172], Loss: 97.6355\n",
      "Epoch [43/300], Step [22/172], Loss: 94.3155\n",
      "Epoch [43/300], Step [23/172], Loss: 30.6573\n",
      "Epoch [43/300], Step [24/172], Loss: 83.0654\n",
      "Epoch [43/300], Step [25/172], Loss: 57.0608\n",
      "Epoch [43/300], Step [26/172], Loss: 64.6724\n",
      "Epoch [43/300], Step [27/172], Loss: 88.4214\n",
      "Epoch [43/300], Step [28/172], Loss: 69.4398\n",
      "Epoch [43/300], Step [29/172], Loss: 80.5419\n",
      "Epoch [43/300], Step [30/172], Loss: 78.5561\n",
      "Epoch [43/300], Step [31/172], Loss: 48.4817\n",
      "Epoch [43/300], Step [32/172], Loss: 43.1517\n",
      "Epoch [43/300], Step [33/172], Loss: 78.6801\n",
      "Epoch [43/300], Step [34/172], Loss: 14.0816\n",
      "Epoch [43/300], Step [35/172], Loss: 60.6510\n",
      "Epoch [43/300], Step [36/172], Loss: 31.2328\n",
      "Epoch [43/300], Step [37/172], Loss: 22.3562\n",
      "Epoch [43/300], Step [38/172], Loss: 33.8288\n",
      "Epoch [43/300], Step [39/172], Loss: 55.8830\n",
      "Epoch [43/300], Step [40/172], Loss: 32.4923\n",
      "Epoch [43/300], Step [41/172], Loss: 43.6446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/300], Step [42/172], Loss: 43.2060\n",
      "Epoch [43/300], Step [43/172], Loss: 34.1803\n",
      "Epoch [43/300], Step [44/172], Loss: 29.9504\n",
      "Epoch [43/300], Step [45/172], Loss: 24.9153\n",
      "Epoch [43/300], Step [46/172], Loss: 47.1152\n",
      "Epoch [43/300], Step [47/172], Loss: 68.5628\n",
      "Epoch [43/300], Step [48/172], Loss: 72.2172\n",
      "Epoch [43/300], Step [49/172], Loss: 23.7037\n",
      "Epoch [43/300], Step [50/172], Loss: 54.3761\n",
      "Epoch [43/300], Step [51/172], Loss: 9.0785\n",
      "Epoch [43/300], Step [52/172], Loss: 24.8068\n",
      "Epoch [43/300], Step [53/172], Loss: 30.1522\n",
      "Epoch [43/300], Step [54/172], Loss: 20.7921\n",
      "Epoch [43/300], Step [55/172], Loss: 17.1425\n",
      "Epoch [43/300], Step [56/172], Loss: 12.1235\n",
      "Epoch [43/300], Step [57/172], Loss: 48.0282\n",
      "Epoch [43/300], Step [58/172], Loss: 20.9410\n",
      "Epoch [43/300], Step [59/172], Loss: 38.0946\n",
      "Epoch [43/300], Step [60/172], Loss: 63.7123\n",
      "Epoch [43/300], Step [61/172], Loss: 12.5220\n",
      "Epoch [43/300], Step [62/172], Loss: 20.2633\n",
      "Epoch [43/300], Step [63/172], Loss: 9.5398\n",
      "Epoch [43/300], Step [64/172], Loss: 6.9839\n",
      "Epoch [43/300], Step [65/172], Loss: 29.2569\n",
      "Epoch [43/300], Step [66/172], Loss: 9.1261\n",
      "Epoch [43/300], Step [67/172], Loss: 28.1681\n",
      "Epoch [43/300], Step [68/172], Loss: 17.4309\n",
      "Epoch [43/300], Step [69/172], Loss: 80.9765\n",
      "Epoch [43/300], Step [70/172], Loss: 72.7979\n",
      "Epoch [43/300], Step [71/172], Loss: 67.4830\n",
      "Epoch [43/300], Step [72/172], Loss: 66.9182\n",
      "Epoch [43/300], Step [73/172], Loss: 73.4478\n",
      "Epoch [43/300], Step [74/172], Loss: 49.8213\n",
      "Epoch [43/300], Step [75/172], Loss: 32.2941\n",
      "Epoch [43/300], Step [76/172], Loss: 51.2048\n",
      "Epoch [43/300], Step [77/172], Loss: 62.1247\n",
      "Epoch [43/300], Step [78/172], Loss: 62.0952\n",
      "Epoch [43/300], Step [79/172], Loss: 54.3878\n",
      "Epoch [43/300], Step [80/172], Loss: 57.6806\n",
      "Epoch [43/300], Step [81/172], Loss: 54.5588\n",
      "Epoch [43/300], Step [82/172], Loss: 47.7121\n",
      "Epoch [43/300], Step [83/172], Loss: 52.8912\n",
      "Epoch [43/300], Step [84/172], Loss: 45.7671\n",
      "Epoch [43/300], Step [85/172], Loss: 53.5794\n",
      "Epoch [43/300], Step [86/172], Loss: 40.5518\n",
      "Epoch [43/300], Step [87/172], Loss: 33.3525\n",
      "Epoch [43/300], Step [88/172], Loss: 36.4192\n",
      "Epoch [43/300], Step [89/172], Loss: 35.6708\n",
      "Epoch [43/300], Step [90/172], Loss: 34.6681\n",
      "Epoch [43/300], Step [91/172], Loss: 33.5126\n",
      "Epoch [43/300], Step [92/172], Loss: 28.0132\n",
      "Epoch [43/300], Step [93/172], Loss: 29.9073\n",
      "Epoch [43/300], Step [94/172], Loss: 33.1162\n",
      "Epoch [43/300], Step [95/172], Loss: 29.9902\n",
      "Epoch [43/300], Step [96/172], Loss: 26.3255\n",
      "Epoch [43/300], Step [97/172], Loss: 31.0636\n",
      "Epoch [43/300], Step [98/172], Loss: 26.6602\n",
      "Epoch [43/300], Step [99/172], Loss: 24.2615\n",
      "Epoch [43/300], Step [100/172], Loss: 23.3480\n",
      "Epoch [43/300], Step [101/172], Loss: 25.1102\n",
      "Epoch [43/300], Step [102/172], Loss: 23.0461\n",
      "Epoch [43/300], Step [103/172], Loss: 22.8237\n",
      "Epoch [43/300], Step [104/172], Loss: 22.4581\n",
      "Epoch [43/300], Step [105/172], Loss: 22.9008\n",
      "Epoch [43/300], Step [106/172], Loss: 22.8979\n",
      "Epoch [43/300], Step [107/172], Loss: 19.7132\n",
      "Epoch [43/300], Step [108/172], Loss: 23.3943\n",
      "Epoch [43/300], Step [109/172], Loss: 24.8744\n",
      "Epoch [43/300], Step [110/172], Loss: 21.2872\n",
      "Epoch [43/300], Step [111/172], Loss: 20.3591\n",
      "Epoch [43/300], Step [112/172], Loss: 26.4778\n",
      "Epoch [43/300], Step [113/172], Loss: 20.6994\n",
      "Epoch [43/300], Step [114/172], Loss: 21.2368\n",
      "Epoch [43/300], Step [115/172], Loss: 29.1619\n",
      "Epoch [43/300], Step [116/172], Loss: 20.9580\n",
      "Epoch [43/300], Step [117/172], Loss: 18.0172\n",
      "Epoch [43/300], Step [118/172], Loss: 21.5035\n",
      "Epoch [43/300], Step [119/172], Loss: 18.3291\n",
      "Epoch [43/300], Step [120/172], Loss: 16.8323\n",
      "Epoch [43/300], Step [121/172], Loss: 17.5379\n",
      "Epoch [43/300], Step [122/172], Loss: 15.3816\n",
      "Epoch [43/300], Step [123/172], Loss: 16.1407\n",
      "Epoch [43/300], Step [124/172], Loss: 13.9247\n",
      "Epoch [43/300], Step [125/172], Loss: 18.0548\n",
      "Epoch [43/300], Step [126/172], Loss: 16.9362\n",
      "Epoch [43/300], Step [127/172], Loss: 19.1790\n",
      "Epoch [43/300], Step [128/172], Loss: 19.8860\n",
      "Epoch [43/300], Step [129/172], Loss: 14.3345\n",
      "Epoch [43/300], Step [130/172], Loss: 16.5183\n",
      "Epoch [43/300], Step [131/172], Loss: 14.5357\n",
      "Epoch [43/300], Step [132/172], Loss: 14.5822\n",
      "Epoch [43/300], Step [133/172], Loss: 14.5556\n",
      "Epoch [43/300], Step [134/172], Loss: 15.1195\n",
      "Epoch [43/300], Step [135/172], Loss: 12.7723\n",
      "Epoch [43/300], Step [136/172], Loss: 13.4661\n",
      "Epoch [43/300], Step [137/172], Loss: 15.0280\n",
      "Epoch [43/300], Step [138/172], Loss: 12.9927\n",
      "Epoch [43/300], Step [139/172], Loss: 14.4225\n",
      "Epoch [43/300], Step [140/172], Loss: 14.3228\n",
      "Epoch [43/300], Step [141/172], Loss: 17.6362\n",
      "Epoch [43/300], Step [142/172], Loss: 16.1055\n",
      "Epoch [43/300], Step [143/172], Loss: 13.1663\n",
      "Epoch [43/300], Step [144/172], Loss: 12.3728\n",
      "Epoch [43/300], Step [145/172], Loss: 12.5771\n",
      "Epoch [43/300], Step [146/172], Loss: 13.0717\n",
      "Epoch [43/300], Step [147/172], Loss: 10.3560\n",
      "Epoch [43/300], Step [148/172], Loss: 10.8818\n",
      "Epoch [43/300], Step [149/172], Loss: 12.9549\n",
      "Epoch [43/300], Step [150/172], Loss: 12.6940\n",
      "Epoch [43/300], Step [151/172], Loss: 11.0254\n",
      "Epoch [43/300], Step [152/172], Loss: 11.2102\n",
      "Epoch [43/300], Step [153/172], Loss: 11.1743\n",
      "Epoch [43/300], Step [154/172], Loss: 12.4649\n",
      "Epoch [43/300], Step [155/172], Loss: 10.7978\n",
      "Epoch [43/300], Step [156/172], Loss: 12.2104\n",
      "Epoch [43/300], Step [157/172], Loss: 13.2958\n",
      "Epoch [43/300], Step [158/172], Loss: 11.5533\n",
      "Epoch [43/300], Step [159/172], Loss: 11.5686\n",
      "Epoch [43/300], Step [160/172], Loss: 11.4202\n",
      "Epoch [43/300], Step [161/172], Loss: 10.1618\n",
      "Epoch [43/300], Step [162/172], Loss: 10.3812\n",
      "Epoch [43/300], Step [163/172], Loss: 10.1640\n",
      "Epoch [43/300], Step [164/172], Loss: 11.9183\n",
      "Epoch [43/300], Step [165/172], Loss: 9.2828\n",
      "Epoch [43/300], Step [166/172], Loss: 9.9868\n",
      "Epoch [43/300], Step [167/172], Loss: 10.2556\n",
      "Epoch [43/300], Step [168/172], Loss: 9.7598\n",
      "Epoch [43/300], Step [169/172], Loss: 9.0246\n",
      "Epoch [43/300], Step [170/172], Loss: 9.1409\n",
      "Epoch [43/300], Step [171/172], Loss: 7.7989\n",
      "Epoch [43/300], Step [172/172], Loss: 6.8610\n",
      "Epoch [44/300], Step [1/172], Loss: 101.9508\n",
      "Epoch [44/300], Step [2/172], Loss: 99.9780\n",
      "Epoch [44/300], Step [3/172], Loss: 120.4923\n",
      "Epoch [44/300], Step [4/172], Loss: 73.5022\n",
      "Epoch [44/300], Step [5/172], Loss: 94.8171\n",
      "Epoch [44/300], Step [6/172], Loss: 39.3942\n",
      "Epoch [44/300], Step [7/172], Loss: 49.5817\n",
      "Epoch [44/300], Step [8/172], Loss: 17.2522\n",
      "Epoch [44/300], Step [9/172], Loss: 55.6274\n",
      "Epoch [44/300], Step [10/172], Loss: 62.7163\n",
      "Epoch [44/300], Step [11/172], Loss: 98.8223\n",
      "Epoch [44/300], Step [12/172], Loss: 88.2783\n",
      "Epoch [44/300], Step [13/172], Loss: 50.9455\n",
      "Epoch [44/300], Step [14/172], Loss: 100.9057\n",
      "Epoch [44/300], Step [15/172], Loss: 90.7767\n",
      "Epoch [44/300], Step [16/172], Loss: 67.6700\n",
      "Epoch [44/300], Step [17/172], Loss: 61.8725\n",
      "Epoch [44/300], Step [18/172], Loss: 77.2604\n",
      "Epoch [44/300], Step [19/172], Loss: 80.6131\n",
      "Epoch [44/300], Step [20/172], Loss: 114.7341\n",
      "Epoch [44/300], Step [21/172], Loss: 97.6725\n",
      "Epoch [44/300], Step [22/172], Loss: 94.3510\n",
      "Epoch [44/300], Step [23/172], Loss: 29.5203\n",
      "Epoch [44/300], Step [24/172], Loss: 82.9277\n",
      "Epoch [44/300], Step [25/172], Loss: 57.0028\n",
      "Epoch [44/300], Step [26/172], Loss: 64.7654\n",
      "Epoch [44/300], Step [27/172], Loss: 88.6257\n",
      "Epoch [44/300], Step [28/172], Loss: 68.4594\n",
      "Epoch [44/300], Step [29/172], Loss: 78.9738\n",
      "Epoch [44/300], Step [30/172], Loss: 78.5885\n",
      "Epoch [44/300], Step [31/172], Loss: 48.1954\n",
      "Epoch [44/300], Step [32/172], Loss: 43.2224\n",
      "Epoch [44/300], Step [33/172], Loss: 78.8794\n",
      "Epoch [44/300], Step [34/172], Loss: 13.6953\n",
      "Epoch [44/300], Step [35/172], Loss: 59.8632\n",
      "Epoch [44/300], Step [36/172], Loss: 30.8798\n",
      "Epoch [44/300], Step [37/172], Loss: 22.1758\n",
      "Epoch [44/300], Step [38/172], Loss: 33.2510\n",
      "Epoch [44/300], Step [39/172], Loss: 55.7586\n",
      "Epoch [44/300], Step [40/172], Loss: 32.0994\n",
      "Epoch [44/300], Step [41/172], Loss: 43.5461\n",
      "Epoch [44/300], Step [42/172], Loss: 43.2699\n",
      "Epoch [44/300], Step [43/172], Loss: 33.9168\n",
      "Epoch [44/300], Step [44/172], Loss: 29.7062\n",
      "Epoch [44/300], Step [45/172], Loss: 24.7121\n",
      "Epoch [44/300], Step [46/172], Loss: 46.4891\n",
      "Epoch [44/300], Step [47/172], Loss: 68.0811\n",
      "Epoch [44/300], Step [48/172], Loss: 71.7285\n",
      "Epoch [44/300], Step [49/172], Loss: 23.5708\n",
      "Epoch [44/300], Step [50/172], Loss: 53.8397\n",
      "Epoch [44/300], Step [51/172], Loss: 8.9503\n",
      "Epoch [44/300], Step [52/172], Loss: 24.6024\n",
      "Epoch [44/300], Step [53/172], Loss: 29.8822\n",
      "Epoch [44/300], Step [54/172], Loss: 20.4251\n",
      "Epoch [44/300], Step [55/172], Loss: 16.8022\n",
      "Epoch [44/300], Step [56/172], Loss: 11.8988\n",
      "Epoch [44/300], Step [57/172], Loss: 47.3468\n",
      "Epoch [44/300], Step [58/172], Loss: 20.8831\n",
      "Epoch [44/300], Step [59/172], Loss: 37.9703\n",
      "Epoch [44/300], Step [60/172], Loss: 63.2283\n",
      "Epoch [44/300], Step [61/172], Loss: 12.3583\n",
      "Epoch [44/300], Step [62/172], Loss: 20.1152\n",
      "Epoch [44/300], Step [63/172], Loss: 9.4087\n",
      "Epoch [44/300], Step [64/172], Loss: 6.9452\n",
      "Epoch [44/300], Step [65/172], Loss: 29.0576\n",
      "Epoch [44/300], Step [66/172], Loss: 8.8961\n",
      "Epoch [44/300], Step [67/172], Loss: 28.0399\n",
      "Epoch [44/300], Step [68/172], Loss: 16.9461\n",
      "Epoch [44/300], Step [69/172], Loss: 80.8269\n",
      "Epoch [44/300], Step [70/172], Loss: 72.9017\n",
      "Epoch [44/300], Step [71/172], Loss: 67.2411\n",
      "Epoch [44/300], Step [72/172], Loss: 66.9893\n",
      "Epoch [44/300], Step [73/172], Loss: 73.4940\n",
      "Epoch [44/300], Step [74/172], Loss: 49.3885\n",
      "Epoch [44/300], Step [75/172], Loss: 32.0995\n",
      "Epoch [44/300], Step [76/172], Loss: 50.9788\n",
      "Epoch [44/300], Step [77/172], Loss: 62.2326\n",
      "Epoch [44/300], Step [78/172], Loss: 61.9250\n",
      "Epoch [44/300], Step [79/172], Loss: 54.3823\n",
      "Epoch [44/300], Step [80/172], Loss: 57.5956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/300], Step [81/172], Loss: 54.4794\n",
      "Epoch [44/300], Step [82/172], Loss: 47.5543\n",
      "Epoch [44/300], Step [83/172], Loss: 52.8441\n",
      "Epoch [44/300], Step [84/172], Loss: 45.4918\n",
      "Epoch [44/300], Step [85/172], Loss: 53.0783\n",
      "Epoch [44/300], Step [86/172], Loss: 40.4112\n",
      "Epoch [44/300], Step [87/172], Loss: 33.1388\n",
      "Epoch [44/300], Step [88/172], Loss: 36.1626\n",
      "Epoch [44/300], Step [89/172], Loss: 35.3924\n",
      "Epoch [44/300], Step [90/172], Loss: 34.5150\n",
      "Epoch [44/300], Step [91/172], Loss: 33.3153\n",
      "Epoch [44/300], Step [92/172], Loss: 27.8028\n",
      "Epoch [44/300], Step [93/172], Loss: 29.5919\n",
      "Epoch [44/300], Step [94/172], Loss: 32.8985\n",
      "Epoch [44/300], Step [95/172], Loss: 29.7350\n",
      "Epoch [44/300], Step [96/172], Loss: 26.0405\n",
      "Epoch [44/300], Step [97/172], Loss: 30.8319\n",
      "Epoch [44/300], Step [98/172], Loss: 26.4138\n",
      "Epoch [44/300], Step [99/172], Loss: 23.9946\n",
      "Epoch [44/300], Step [100/172], Loss: 23.0689\n",
      "Epoch [44/300], Step [101/172], Loss: 24.8391\n",
      "Epoch [44/300], Step [102/172], Loss: 22.8488\n",
      "Epoch [44/300], Step [103/172], Loss: 22.5382\n",
      "Epoch [44/300], Step [104/172], Loss: 22.1930\n",
      "Epoch [44/300], Step [105/172], Loss: 22.7175\n",
      "Epoch [44/300], Step [106/172], Loss: 22.6738\n",
      "Epoch [44/300], Step [107/172], Loss: 19.5413\n",
      "Epoch [44/300], Step [108/172], Loss: 23.1905\n",
      "Epoch [44/300], Step [109/172], Loss: 24.7567\n",
      "Epoch [44/300], Step [110/172], Loss: 21.0940\n",
      "Epoch [44/300], Step [111/172], Loss: 20.1434\n",
      "Epoch [44/300], Step [112/172], Loss: 26.2338\n",
      "Epoch [44/300], Step [113/172], Loss: 20.5350\n",
      "Epoch [44/300], Step [114/172], Loss: 21.0272\n",
      "Epoch [44/300], Step [115/172], Loss: 29.0137\n",
      "Epoch [44/300], Step [116/172], Loss: 20.7511\n",
      "Epoch [44/300], Step [117/172], Loss: 17.8433\n",
      "Epoch [44/300], Step [118/172], Loss: 21.3184\n",
      "Epoch [44/300], Step [119/172], Loss: 18.1912\n",
      "Epoch [44/300], Step [120/172], Loss: 16.6594\n",
      "Epoch [44/300], Step [121/172], Loss: 17.3725\n",
      "Epoch [44/300], Step [122/172], Loss: 15.2677\n",
      "Epoch [44/300], Step [123/172], Loss: 15.9541\n",
      "Epoch [44/300], Step [124/172], Loss: 13.7174\n",
      "Epoch [44/300], Step [125/172], Loss: 17.8720\n",
      "Epoch [44/300], Step [126/172], Loss: 16.7294\n",
      "Epoch [44/300], Step [127/172], Loss: 19.0114\n",
      "Epoch [44/300], Step [128/172], Loss: 19.6985\n",
      "Epoch [44/300], Step [129/172], Loss: 14.1579\n",
      "Epoch [44/300], Step [130/172], Loss: 16.3291\n",
      "Epoch [44/300], Step [131/172], Loss: 14.3748\n",
      "Epoch [44/300], Step [132/172], Loss: 14.4015\n",
      "Epoch [44/300], Step [133/172], Loss: 14.4291\n",
      "Epoch [44/300], Step [134/172], Loss: 14.9992\n",
      "Epoch [44/300], Step [135/172], Loss: 12.6321\n",
      "Epoch [44/300], Step [136/172], Loss: 13.2954\n",
      "Epoch [44/300], Step [137/172], Loss: 14.8610\n",
      "Epoch [44/300], Step [138/172], Loss: 12.8362\n",
      "Epoch [44/300], Step [139/172], Loss: 14.2570\n",
      "Epoch [44/300], Step [140/172], Loss: 14.1858\n",
      "Epoch [44/300], Step [141/172], Loss: 17.5543\n",
      "Epoch [44/300], Step [142/172], Loss: 16.0106\n",
      "Epoch [44/300], Step [143/172], Loss: 13.0219\n",
      "Epoch [44/300], Step [144/172], Loss: 12.2561\n",
      "Epoch [44/300], Step [145/172], Loss: 12.4756\n",
      "Epoch [44/300], Step [146/172], Loss: 12.9645\n",
      "Epoch [44/300], Step [147/172], Loss: 10.1666\n",
      "Epoch [44/300], Step [148/172], Loss: 10.7235\n",
      "Epoch [44/300], Step [149/172], Loss: 12.8141\n",
      "Epoch [44/300], Step [150/172], Loss: 12.5508\n",
      "Epoch [44/300], Step [151/172], Loss: 10.9086\n",
      "Epoch [44/300], Step [152/172], Loss: 11.0753\n",
      "Epoch [44/300], Step [153/172], Loss: 11.0506\n",
      "Epoch [44/300], Step [154/172], Loss: 12.2911\n",
      "Epoch [44/300], Step [155/172], Loss: 10.6738\n",
      "Epoch [44/300], Step [156/172], Loss: 12.1664\n",
      "Epoch [44/300], Step [157/172], Loss: 13.1906\n",
      "Epoch [44/300], Step [158/172], Loss: 11.4338\n",
      "Epoch [44/300], Step [159/172], Loss: 11.4649\n",
      "Epoch [44/300], Step [160/172], Loss: 11.3267\n",
      "Epoch [44/300], Step [161/172], Loss: 10.0393\n",
      "Epoch [44/300], Step [162/172], Loss: 10.2786\n",
      "Epoch [44/300], Step [163/172], Loss: 10.0321\n",
      "Epoch [44/300], Step [164/172], Loss: 11.7758\n",
      "Epoch [44/300], Step [165/172], Loss: 9.1848\n",
      "Epoch [44/300], Step [166/172], Loss: 9.8529\n",
      "Epoch [44/300], Step [167/172], Loss: 10.1450\n",
      "Epoch [44/300], Step [168/172], Loss: 9.6327\n",
      "Epoch [44/300], Step [169/172], Loss: 8.9377\n",
      "Epoch [44/300], Step [170/172], Loss: 9.0278\n",
      "Epoch [44/300], Step [171/172], Loss: 7.7248\n",
      "Epoch [44/300], Step [172/172], Loss: 6.7952\n",
      "Epoch [45/300], Step [1/172], Loss: 101.7524\n",
      "Epoch [45/300], Step [2/172], Loss: 100.3319\n",
      "Epoch [45/300], Step [3/172], Loss: 119.7277\n",
      "Epoch [45/300], Step [4/172], Loss: 72.9714\n",
      "Epoch [45/300], Step [5/172], Loss: 94.5598\n",
      "Epoch [45/300], Step [6/172], Loss: 38.8649\n",
      "Epoch [45/300], Step [7/172], Loss: 49.4076\n",
      "Epoch [45/300], Step [8/172], Loss: 18.2730\n",
      "Epoch [45/300], Step [9/172], Loss: 56.2108\n",
      "Epoch [45/300], Step [10/172], Loss: 62.2869\n",
      "Epoch [45/300], Step [11/172], Loss: 99.0643\n",
      "Epoch [45/300], Step [12/172], Loss: 88.3195\n",
      "Epoch [45/300], Step [13/172], Loss: 50.4426\n",
      "Epoch [45/300], Step [14/172], Loss: 100.6516\n",
      "Epoch [45/300], Step [15/172], Loss: 90.3654\n",
      "Epoch [45/300], Step [16/172], Loss: 66.5603\n",
      "Epoch [45/300], Step [17/172], Loss: 61.7700\n",
      "Epoch [45/300], Step [18/172], Loss: 76.4014\n",
      "Epoch [45/300], Step [19/172], Loss: 80.3982\n",
      "Epoch [45/300], Step [20/172], Loss: 113.9225\n",
      "Epoch [45/300], Step [21/172], Loss: 97.2649\n",
      "Epoch [45/300], Step [22/172], Loss: 93.9749\n",
      "Epoch [45/300], Step [23/172], Loss: 28.1544\n",
      "Epoch [45/300], Step [24/172], Loss: 82.4028\n",
      "Epoch [45/300], Step [25/172], Loss: 56.6952\n",
      "Epoch [45/300], Step [26/172], Loss: 64.4696\n",
      "Epoch [45/300], Step [27/172], Loss: 88.2647\n",
      "Epoch [45/300], Step [28/172], Loss: 67.4172\n",
      "Epoch [45/300], Step [29/172], Loss: 77.3579\n",
      "Epoch [45/300], Step [30/172], Loss: 78.3580\n",
      "Epoch [45/300], Step [31/172], Loss: 47.8079\n",
      "Epoch [45/300], Step [32/172], Loss: 43.0706\n",
      "Epoch [45/300], Step [33/172], Loss: 78.6111\n",
      "Epoch [45/300], Step [34/172], Loss: 13.0539\n",
      "Epoch [45/300], Step [35/172], Loss: 58.9360\n",
      "Epoch [45/300], Step [36/172], Loss: 30.5854\n",
      "Epoch [45/300], Step [37/172], Loss: 22.0641\n",
      "Epoch [45/300], Step [38/172], Loss: 32.8363\n",
      "Epoch [45/300], Step [39/172], Loss: 55.7095\n",
      "Epoch [45/300], Step [40/172], Loss: 31.7468\n",
      "Epoch [45/300], Step [41/172], Loss: 43.4042\n",
      "Epoch [45/300], Step [42/172], Loss: 43.2792\n",
      "Epoch [45/300], Step [43/172], Loss: 33.7609\n",
      "Epoch [45/300], Step [44/172], Loss: 29.3528\n",
      "Epoch [45/300], Step [45/172], Loss: 24.6299\n",
      "Epoch [45/300], Step [46/172], Loss: 45.9168\n",
      "Epoch [45/300], Step [47/172], Loss: 67.7062\n",
      "Epoch [45/300], Step [48/172], Loss: 71.1434\n",
      "Epoch [45/300], Step [49/172], Loss: 23.4638\n",
      "Epoch [45/300], Step [50/172], Loss: 53.5884\n",
      "Epoch [45/300], Step [51/172], Loss: 8.9655\n",
      "Epoch [45/300], Step [52/172], Loss: 24.4934\n",
      "Epoch [45/300], Step [53/172], Loss: 29.8380\n",
      "Epoch [45/300], Step [54/172], Loss: 20.2489\n",
      "Epoch [45/300], Step [55/172], Loss: 16.6175\n",
      "Epoch [45/300], Step [56/172], Loss: 11.8634\n",
      "Epoch [45/300], Step [57/172], Loss: 46.7002\n",
      "Epoch [45/300], Step [58/172], Loss: 20.8559\n",
      "Epoch [45/300], Step [59/172], Loss: 37.8221\n",
      "Epoch [45/300], Step [60/172], Loss: 62.7268\n",
      "Epoch [45/300], Step [61/172], Loss: 12.3203\n",
      "Epoch [45/300], Step [62/172], Loss: 20.3678\n",
      "Epoch [45/300], Step [63/172], Loss: 9.4807\n",
      "Epoch [45/300], Step [64/172], Loss: 7.0347\n",
      "Epoch [45/300], Step [65/172], Loss: 29.0658\n",
      "Epoch [45/300], Step [66/172], Loss: 8.7932\n",
      "Epoch [45/300], Step [67/172], Loss: 27.9883\n",
      "Epoch [45/300], Step [68/172], Loss: 16.6263\n",
      "Epoch [45/300], Step [69/172], Loss: 80.4300\n",
      "Epoch [45/300], Step [70/172], Loss: 72.3643\n",
      "Epoch [45/300], Step [71/172], Loss: 66.6004\n",
      "Epoch [45/300], Step [72/172], Loss: 66.4762\n",
      "Epoch [45/300], Step [73/172], Loss: 73.0132\n",
      "Epoch [45/300], Step [74/172], Loss: 48.8142\n",
      "Epoch [45/300], Step [75/172], Loss: 32.1493\n",
      "Epoch [45/300], Step [76/172], Loss: 50.7140\n",
      "Epoch [45/300], Step [77/172], Loss: 62.2780\n",
      "Epoch [45/300], Step [78/172], Loss: 61.6530\n",
      "Epoch [45/300], Step [79/172], Loss: 54.3141\n",
      "Epoch [45/300], Step [80/172], Loss: 57.4381\n",
      "Epoch [45/300], Step [81/172], Loss: 54.4208\n",
      "Epoch [45/300], Step [82/172], Loss: 47.1943\n",
      "Epoch [45/300], Step [83/172], Loss: 53.0794\n",
      "Epoch [45/300], Step [84/172], Loss: 45.4880\n",
      "Epoch [45/300], Step [85/172], Loss: 53.0082\n",
      "Epoch [45/300], Step [86/172], Loss: 40.3738\n",
      "Epoch [45/300], Step [87/172], Loss: 33.1336\n",
      "Epoch [45/300], Step [88/172], Loss: 36.1423\n",
      "Epoch [45/300], Step [89/172], Loss: 35.2932\n",
      "Epoch [45/300], Step [90/172], Loss: 34.5269\n",
      "Epoch [45/300], Step [91/172], Loss: 33.3375\n",
      "Epoch [45/300], Step [92/172], Loss: 27.7422\n",
      "Epoch [45/300], Step [93/172], Loss: 29.3950\n",
      "Epoch [45/300], Step [94/172], Loss: 32.8476\n",
      "Epoch [45/300], Step [95/172], Loss: 29.6132\n",
      "Epoch [45/300], Step [96/172], Loss: 25.8765\n",
      "Epoch [45/300], Step [97/172], Loss: 30.7902\n",
      "Epoch [45/300], Step [98/172], Loss: 26.3158\n",
      "Epoch [45/300], Step [99/172], Loss: 23.8657\n",
      "Epoch [45/300], Step [100/172], Loss: 22.8816\n",
      "Epoch [45/300], Step [101/172], Loss: 24.7060\n",
      "Epoch [45/300], Step [102/172], Loss: 22.6513\n",
      "Epoch [45/300], Step [103/172], Loss: 22.3527\n",
      "Epoch [45/300], Step [104/172], Loss: 22.0170\n",
      "Epoch [45/300], Step [105/172], Loss: 22.5432\n",
      "Epoch [45/300], Step [106/172], Loss: 22.5513\n",
      "Epoch [45/300], Step [107/172], Loss: 19.4304\n",
      "Epoch [45/300], Step [108/172], Loss: 23.0642\n",
      "Epoch [45/300], Step [109/172], Loss: 24.7133\n",
      "Epoch [45/300], Step [110/172], Loss: 21.0033\n",
      "Epoch [45/300], Step [111/172], Loss: 20.0049\n",
      "Epoch [45/300], Step [112/172], Loss: 26.0744\n",
      "Epoch [45/300], Step [113/172], Loss: 20.4501\n",
      "Epoch [45/300], Step [114/172], Loss: 20.8920\n",
      "Epoch [45/300], Step [115/172], Loss: 28.9858\n",
      "Epoch [45/300], Step [116/172], Loss: 20.6310\n",
      "Epoch [45/300], Step [117/172], Loss: 17.7318\n",
      "Epoch [45/300], Step [118/172], Loss: 21.2272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/300], Step [119/172], Loss: 18.1238\n",
      "Epoch [45/300], Step [120/172], Loss: 16.5307\n",
      "Epoch [45/300], Step [121/172], Loss: 17.2220\n",
      "Epoch [45/300], Step [122/172], Loss: 15.1949\n",
      "Epoch [45/300], Step [123/172], Loss: 15.8419\n",
      "Epoch [45/300], Step [124/172], Loss: 13.5458\n",
      "Epoch [45/300], Step [125/172], Loss: 17.7707\n",
      "Epoch [45/300], Step [126/172], Loss: 16.5964\n",
      "Epoch [45/300], Step [127/172], Loss: 18.8819\n",
      "Epoch [45/300], Step [128/172], Loss: 19.5708\n",
      "Epoch [45/300], Step [129/172], Loss: 14.0350\n",
      "Epoch [45/300], Step [130/172], Loss: 16.2112\n",
      "Epoch [45/300], Step [131/172], Loss: 14.2488\n",
      "Epoch [45/300], Step [132/172], Loss: 14.2688\n",
      "Epoch [45/300], Step [133/172], Loss: 14.3195\n",
      "Epoch [45/300], Step [134/172], Loss: 14.9256\n",
      "Epoch [45/300], Step [135/172], Loss: 12.5266\n",
      "Epoch [45/300], Step [136/172], Loss: 13.1477\n",
      "Epoch [45/300], Step [137/172], Loss: 14.7634\n",
      "Epoch [45/300], Step [138/172], Loss: 12.7194\n",
      "Epoch [45/300], Step [139/172], Loss: 14.1438\n",
      "Epoch [45/300], Step [140/172], Loss: 14.0772\n",
      "Epoch [45/300], Step [141/172], Loss: 17.5195\n",
      "Epoch [45/300], Step [142/172], Loss: 15.9730\n",
      "Epoch [45/300], Step [143/172], Loss: 12.9378\n",
      "Epoch [45/300], Step [144/172], Loss: 12.1798\n",
      "Epoch [45/300], Step [145/172], Loss: 12.3895\n",
      "Epoch [45/300], Step [146/172], Loss: 12.8914\n",
      "Epoch [45/300], Step [147/172], Loss: 10.0059\n",
      "Epoch [45/300], Step [148/172], Loss: 10.5823\n",
      "Epoch [45/300], Step [149/172], Loss: 12.7355\n",
      "Epoch [45/300], Step [150/172], Loss: 12.4338\n",
      "Epoch [45/300], Step [151/172], Loss: 10.8173\n",
      "Epoch [45/300], Step [152/172], Loss: 10.9837\n",
      "Epoch [45/300], Step [153/172], Loss: 10.9391\n",
      "Epoch [45/300], Step [154/172], Loss: 12.2089\n",
      "Epoch [45/300], Step [155/172], Loss: 10.5768\n",
      "Epoch [45/300], Step [156/172], Loss: 12.1390\n",
      "Epoch [45/300], Step [157/172], Loss: 13.1621\n",
      "Epoch [45/300], Step [158/172], Loss: 11.3621\n",
      "Epoch [45/300], Step [159/172], Loss: 11.4252\n",
      "Epoch [45/300], Step [160/172], Loss: 11.2777\n",
      "Epoch [45/300], Step [161/172], Loss: 9.9291\n",
      "Epoch [45/300], Step [162/172], Loss: 10.2176\n",
      "Epoch [45/300], Step [163/172], Loss: 9.9525\n",
      "Epoch [45/300], Step [164/172], Loss: 11.8061\n",
      "Epoch [45/300], Step [165/172], Loss: 9.0992\n",
      "Epoch [45/300], Step [166/172], Loss: 9.7584\n",
      "Epoch [45/300], Step [167/172], Loss: 10.0879\n",
      "Epoch [45/300], Step [168/172], Loss: 9.5217\n",
      "Epoch [45/300], Step [169/172], Loss: 8.8890\n",
      "Epoch [45/300], Step [170/172], Loss: 8.9138\n",
      "Epoch [45/300], Step [171/172], Loss: 7.6339\n",
      "Epoch [45/300], Step [172/172], Loss: 6.7870\n",
      "Epoch [46/300], Step [1/172], Loss: 101.4153\n",
      "Epoch [46/300], Step [2/172], Loss: 99.4405\n",
      "Epoch [46/300], Step [3/172], Loss: 118.5558\n",
      "Epoch [46/300], Step [4/172], Loss: 72.0787\n",
      "Epoch [46/300], Step [5/172], Loss: 93.3597\n",
      "Epoch [46/300], Step [6/172], Loss: 37.8928\n",
      "Epoch [46/300], Step [7/172], Loss: 47.5505\n",
      "Epoch [46/300], Step [8/172], Loss: 16.0578\n",
      "Epoch [46/300], Step [9/172], Loss: 54.9002\n",
      "Epoch [46/300], Step [10/172], Loss: 61.7689\n",
      "Epoch [46/300], Step [11/172], Loss: 99.2138\n",
      "Epoch [46/300], Step [12/172], Loss: 88.1662\n",
      "Epoch [46/300], Step [13/172], Loss: 49.8468\n",
      "Epoch [46/300], Step [14/172], Loss: 100.3841\n",
      "Epoch [46/300], Step [15/172], Loss: 90.1140\n",
      "Epoch [46/300], Step [16/172], Loss: 65.6228\n",
      "Epoch [46/300], Step [17/172], Loss: 61.6737\n",
      "Epoch [46/300], Step [18/172], Loss: 75.6447\n",
      "Epoch [46/300], Step [19/172], Loss: 80.4483\n",
      "Epoch [46/300], Step [20/172], Loss: 112.7256\n",
      "Epoch [46/300], Step [21/172], Loss: 97.2115\n",
      "Epoch [46/300], Step [22/172], Loss: 93.8265\n",
      "Epoch [46/300], Step [23/172], Loss: 26.9105\n",
      "Epoch [46/300], Step [24/172], Loss: 82.0403\n",
      "Epoch [46/300], Step [25/172], Loss: 56.3829\n",
      "Epoch [46/300], Step [26/172], Loss: 64.3518\n",
      "Epoch [46/300], Step [27/172], Loss: 88.0784\n",
      "Epoch [46/300], Step [28/172], Loss: 66.4014\n",
      "Epoch [46/300], Step [29/172], Loss: 75.7759\n",
      "Epoch [46/300], Step [30/172], Loss: 78.1999\n",
      "Epoch [46/300], Step [31/172], Loss: 47.4022\n",
      "Epoch [46/300], Step [32/172], Loss: 42.9886\n",
      "Epoch [46/300], Step [33/172], Loss: 78.3269\n",
      "Epoch [46/300], Step [34/172], Loss: 12.6422\n",
      "Epoch [46/300], Step [35/172], Loss: 58.0941\n",
      "Epoch [46/300], Step [36/172], Loss: 30.2826\n",
      "Epoch [46/300], Step [37/172], Loss: 21.8495\n",
      "Epoch [46/300], Step [38/172], Loss: 32.3060\n",
      "Epoch [46/300], Step [39/172], Loss: 55.5662\n",
      "Epoch [46/300], Step [40/172], Loss: 31.3397\n",
      "Epoch [46/300], Step [41/172], Loss: 43.2895\n",
      "Epoch [46/300], Step [42/172], Loss: 43.2239\n",
      "Epoch [46/300], Step [43/172], Loss: 33.5260\n",
      "Epoch [46/300], Step [44/172], Loss: 29.0064\n",
      "Epoch [46/300], Step [45/172], Loss: 24.4933\n",
      "Epoch [46/300], Step [46/172], Loss: 45.2980\n",
      "Epoch [46/300], Step [47/172], Loss: 67.2552\n",
      "Epoch [46/300], Step [48/172], Loss: 70.4236\n",
      "Epoch [46/300], Step [49/172], Loss: 23.4805\n",
      "Epoch [46/300], Step [50/172], Loss: 53.2244\n",
      "Epoch [46/300], Step [51/172], Loss: 8.9402\n",
      "Epoch [46/300], Step [52/172], Loss: 24.3913\n",
      "Epoch [46/300], Step [53/172], Loss: 29.8592\n",
      "Epoch [46/300], Step [54/172], Loss: 20.0123\n",
      "Epoch [46/300], Step [55/172], Loss: 16.4440\n",
      "Epoch [46/300], Step [56/172], Loss: 11.8373\n",
      "Epoch [46/300], Step [57/172], Loss: 45.9576\n",
      "Epoch [46/300], Step [58/172], Loss: 20.7919\n",
      "Epoch [46/300], Step [59/172], Loss: 37.7373\n",
      "Epoch [46/300], Step [60/172], Loss: 62.1383\n",
      "Epoch [46/300], Step [61/172], Loss: 12.2212\n",
      "Epoch [46/300], Step [62/172], Loss: 20.4714\n",
      "Epoch [46/300], Step [63/172], Loss: 9.5036\n",
      "Epoch [46/300], Step [64/172], Loss: 7.0931\n",
      "Epoch [46/300], Step [65/172], Loss: 28.9359\n",
      "Epoch [46/300], Step [66/172], Loss: 8.6365\n",
      "Epoch [46/300], Step [67/172], Loss: 27.8959\n",
      "Epoch [46/300], Step [68/172], Loss: 16.1816\n",
      "Epoch [46/300], Step [69/172], Loss: 80.2844\n",
      "Epoch [46/300], Step [70/172], Loss: 72.2518\n",
      "Epoch [46/300], Step [71/172], Loss: 66.1640\n",
      "Epoch [46/300], Step [72/172], Loss: 66.2799\n",
      "Epoch [46/300], Step [73/172], Loss: 72.8047\n",
      "Epoch [46/300], Step [74/172], Loss: 48.2981\n",
      "Epoch [46/300], Step [75/172], Loss: 31.9562\n",
      "Epoch [46/300], Step [76/172], Loss: 50.4530\n",
      "Epoch [46/300], Step [77/172], Loss: 62.3226\n",
      "Epoch [46/300], Step [78/172], Loss: 61.3914\n",
      "Epoch [46/300], Step [79/172], Loss: 54.3367\n",
      "Epoch [46/300], Step [80/172], Loss: 57.3208\n",
      "Epoch [46/300], Step [81/172], Loss: 54.3485\n",
      "Epoch [46/300], Step [82/172], Loss: 46.9321\n",
      "Epoch [46/300], Step [83/172], Loss: 53.1636\n",
      "Epoch [46/300], Step [84/172], Loss: 45.3204\n",
      "Epoch [46/300], Step [85/172], Loss: 52.7399\n",
      "Epoch [46/300], Step [86/172], Loss: 40.2713\n",
      "Epoch [46/300], Step [87/172], Loss: 33.0460\n",
      "Epoch [46/300], Step [88/172], Loss: 36.0752\n",
      "Epoch [46/300], Step [89/172], Loss: 35.1928\n",
      "Epoch [46/300], Step [90/172], Loss: 34.4886\n",
      "Epoch [46/300], Step [91/172], Loss: 33.3102\n",
      "Epoch [46/300], Step [92/172], Loss: 27.6960\n",
      "Epoch [46/300], Step [93/172], Loss: 29.2079\n",
      "Epoch [46/300], Step [94/172], Loss: 32.7869\n",
      "Epoch [46/300], Step [95/172], Loss: 29.5017\n",
      "Epoch [46/300], Step [96/172], Loss: 25.7478\n",
      "Epoch [46/300], Step [97/172], Loss: 30.7379\n",
      "Epoch [46/300], Step [98/172], Loss: 26.1995\n",
      "Epoch [46/300], Step [99/172], Loss: 23.7514\n",
      "Epoch [46/300], Step [100/172], Loss: 22.7429\n",
      "Epoch [46/300], Step [101/172], Loss: 24.5728\n",
      "Epoch [46/300], Step [102/172], Loss: 22.5423\n",
      "Epoch [46/300], Step [103/172], Loss: 22.2043\n",
      "Epoch [46/300], Step [104/172], Loss: 21.8605\n",
      "Epoch [46/300], Step [105/172], Loss: 22.4208\n",
      "Epoch [46/300], Step [106/172], Loss: 22.4113\n",
      "Epoch [46/300], Step [107/172], Loss: 19.3341\n",
      "Epoch [46/300], Step [108/172], Loss: 22.9526\n",
      "Epoch [46/300], Step [109/172], Loss: 24.6928\n",
      "Epoch [46/300], Step [110/172], Loss: 20.8846\n",
      "Epoch [46/300], Step [111/172], Loss: 19.8290\n",
      "Epoch [46/300], Step [112/172], Loss: 25.9227\n",
      "Epoch [46/300], Step [113/172], Loss: 20.3308\n",
      "Epoch [46/300], Step [114/172], Loss: 20.7544\n",
      "Epoch [46/300], Step [115/172], Loss: 28.9301\n",
      "Epoch [46/300], Step [116/172], Loss: 20.4958\n",
      "Epoch [46/300], Step [117/172], Loss: 17.6012\n",
      "Epoch [46/300], Step [118/172], Loss: 21.1182\n",
      "Epoch [46/300], Step [119/172], Loss: 18.0536\n",
      "Epoch [46/300], Step [120/172], Loss: 16.4017\n",
      "Epoch [46/300], Step [121/172], Loss: 17.1192\n",
      "Epoch [46/300], Step [122/172], Loss: 15.1234\n",
      "Epoch [46/300], Step [123/172], Loss: 15.6788\n",
      "Epoch [46/300], Step [124/172], Loss: 13.3839\n",
      "Epoch [46/300], Step [125/172], Loss: 17.6631\n",
      "Epoch [46/300], Step [126/172], Loss: 16.4438\n",
      "Epoch [46/300], Step [127/172], Loss: 18.7628\n",
      "Epoch [46/300], Step [128/172], Loss: 19.4419\n",
      "Epoch [46/300], Step [129/172], Loss: 13.8974\n",
      "Epoch [46/300], Step [130/172], Loss: 16.0749\n",
      "Epoch [46/300], Step [131/172], Loss: 14.1289\n",
      "Epoch [46/300], Step [132/172], Loss: 14.1315\n",
      "Epoch [46/300], Step [133/172], Loss: 14.2393\n",
      "Epoch [46/300], Step [134/172], Loss: 14.8417\n",
      "Epoch [46/300], Step [135/172], Loss: 12.4233\n",
      "Epoch [46/300], Step [136/172], Loss: 13.0139\n",
      "Epoch [46/300], Step [137/172], Loss: 14.6371\n",
      "Epoch [46/300], Step [138/172], Loss: 12.5810\n",
      "Epoch [46/300], Step [139/172], Loss: 14.0112\n",
      "Epoch [46/300], Step [140/172], Loss: 13.9551\n",
      "Epoch [46/300], Step [141/172], Loss: 17.4528\n",
      "Epoch [46/300], Step [142/172], Loss: 15.9035\n",
      "Epoch [46/300], Step [143/172], Loss: 12.8163\n",
      "Epoch [46/300], Step [144/172], Loss: 12.0875\n",
      "Epoch [46/300], Step [145/172], Loss: 12.3098\n",
      "Epoch [46/300], Step [146/172], Loss: 12.7978\n",
      "Epoch [46/300], Step [147/172], Loss: 9.8691\n",
      "Epoch [46/300], Step [148/172], Loss: 10.4588\n",
      "Epoch [46/300], Step [149/172], Loss: 12.6238\n",
      "Epoch [46/300], Step [150/172], Loss: 12.3096\n",
      "Epoch [46/300], Step [151/172], Loss: 10.7287\n",
      "Epoch [46/300], Step [152/172], Loss: 10.8690\n",
      "Epoch [46/300], Step [153/172], Loss: 10.8330\n",
      "Epoch [46/300], Step [154/172], Loss: 12.0689\n",
      "Epoch [46/300], Step [155/172], Loss: 10.4661\n",
      "Epoch [46/300], Step [156/172], Loss: 12.1100\n",
      "Epoch [46/300], Step [157/172], Loss: 13.0847\n",
      "Epoch [46/300], Step [158/172], Loss: 11.2685\n",
      "Epoch [46/300], Step [159/172], Loss: 11.3610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/300], Step [160/172], Loss: 11.2229\n",
      "Epoch [46/300], Step [161/172], Loss: 9.8261\n",
      "Epoch [46/300], Step [162/172], Loss: 10.1355\n",
      "Epoch [46/300], Step [163/172], Loss: 9.8664\n",
      "Epoch [46/300], Step [164/172], Loss: 11.7781\n",
      "Epoch [46/300], Step [165/172], Loss: 9.0227\n",
      "Epoch [46/300], Step [166/172], Loss: 9.6684\n",
      "Epoch [46/300], Step [167/172], Loss: 10.0084\n",
      "Epoch [46/300], Step [168/172], Loss: 9.4338\n",
      "Epoch [46/300], Step [169/172], Loss: 8.8321\n",
      "Epoch [46/300], Step [170/172], Loss: 8.8498\n",
      "Epoch [46/300], Step [171/172], Loss: 7.5874\n",
      "Epoch [46/300], Step [172/172], Loss: 6.7465\n",
      "Epoch [47/300], Step [1/172], Loss: 101.1847\n",
      "Epoch [47/300], Step [2/172], Loss: 99.2823\n",
      "Epoch [47/300], Step [3/172], Loss: 117.7085\n",
      "Epoch [47/300], Step [4/172], Loss: 71.3267\n",
      "Epoch [47/300], Step [5/172], Loss: 92.5976\n",
      "Epoch [47/300], Step [6/172], Loss: 37.2070\n",
      "Epoch [47/300], Step [7/172], Loss: 46.5612\n",
      "Epoch [47/300], Step [8/172], Loss: 15.5414\n",
      "Epoch [47/300], Step [9/172], Loss: 54.5901\n",
      "Epoch [47/300], Step [10/172], Loss: 61.4462\n",
      "Epoch [47/300], Step [11/172], Loss: 99.3031\n",
      "Epoch [47/300], Step [12/172], Loss: 88.0663\n",
      "Epoch [47/300], Step [13/172], Loss: 49.5804\n",
      "Epoch [47/300], Step [14/172], Loss: 100.4951\n",
      "Epoch [47/300], Step [15/172], Loss: 90.1363\n",
      "Epoch [47/300], Step [16/172], Loss: 64.8843\n",
      "Epoch [47/300], Step [17/172], Loss: 61.6761\n",
      "Epoch [47/300], Step [18/172], Loss: 75.1299\n",
      "Epoch [47/300], Step [19/172], Loss: 80.7554\n",
      "Epoch [47/300], Step [20/172], Loss: 112.2560\n",
      "Epoch [47/300], Step [21/172], Loss: 97.3957\n",
      "Epoch [47/300], Step [22/172], Loss: 94.0040\n",
      "Epoch [47/300], Step [23/172], Loss: 25.8187\n",
      "Epoch [47/300], Step [24/172], Loss: 81.9440\n",
      "Epoch [47/300], Step [25/172], Loss: 56.4706\n",
      "Epoch [47/300], Step [26/172], Loss: 64.4665\n",
      "Epoch [47/300], Step [27/172], Loss: 88.3240\n",
      "Epoch [47/300], Step [28/172], Loss: 65.7471\n",
      "Epoch [47/300], Step [29/172], Loss: 74.6207\n",
      "Epoch [47/300], Step [30/172], Loss: 78.3302\n",
      "Epoch [47/300], Step [31/172], Loss: 47.3958\n",
      "Epoch [47/300], Step [32/172], Loss: 43.0922\n",
      "Epoch [47/300], Step [33/172], Loss: 78.4381\n",
      "Epoch [47/300], Step [34/172], Loss: 12.3821\n",
      "Epoch [47/300], Step [35/172], Loss: 57.2057\n",
      "Epoch [47/300], Step [36/172], Loss: 30.1388\n",
      "Epoch [47/300], Step [37/172], Loss: 21.9025\n",
      "Epoch [47/300], Step [38/172], Loss: 32.2831\n",
      "Epoch [47/300], Step [39/172], Loss: 55.7446\n",
      "Epoch [47/300], Step [40/172], Loss: 31.2995\n",
      "Epoch [47/300], Step [41/172], Loss: 43.4770\n",
      "Epoch [47/300], Step [42/172], Loss: 43.5098\n",
      "Epoch [47/300], Step [43/172], Loss: 33.6384\n",
      "Epoch [47/300], Step [44/172], Loss: 28.9725\n",
      "Epoch [47/300], Step [45/172], Loss: 24.6125\n",
      "Epoch [47/300], Step [46/172], Loss: 44.9601\n",
      "Epoch [47/300], Step [47/172], Loss: 67.0854\n",
      "Epoch [47/300], Step [48/172], Loss: 70.1748\n",
      "Epoch [47/300], Step [49/172], Loss: 23.6371\n",
      "Epoch [47/300], Step [50/172], Loss: 53.1059\n",
      "Epoch [47/300], Step [51/172], Loss: 9.0036\n",
      "Epoch [47/300], Step [52/172], Loss: 24.4182\n",
      "Epoch [47/300], Step [53/172], Loss: 29.8750\n",
      "Epoch [47/300], Step [54/172], Loss: 19.9930\n",
      "Epoch [47/300], Step [55/172], Loss: 16.3409\n",
      "Epoch [47/300], Step [56/172], Loss: 11.8675\n",
      "Epoch [47/300], Step [57/172], Loss: 45.4698\n",
      "Epoch [47/300], Step [58/172], Loss: 20.8443\n",
      "Epoch [47/300], Step [59/172], Loss: 37.7200\n",
      "Epoch [47/300], Step [60/172], Loss: 61.6815\n",
      "Epoch [47/300], Step [61/172], Loss: 12.1772\n",
      "Epoch [47/300], Step [62/172], Loss: 20.5683\n",
      "Epoch [47/300], Step [63/172], Loss: 9.5891\n",
      "Epoch [47/300], Step [64/172], Loss: 7.2162\n",
      "Epoch [47/300], Step [65/172], Loss: 28.9238\n",
      "Epoch [47/300], Step [66/172], Loss: 8.5352\n",
      "Epoch [47/300], Step [67/172], Loss: 27.9527\n",
      "Epoch [47/300], Step [68/172], Loss: 15.8603\n",
      "Epoch [47/300], Step [69/172], Loss: 79.9542\n",
      "Epoch [47/300], Step [70/172], Loss: 71.8403\n",
      "Epoch [47/300], Step [71/172], Loss: 65.5840\n",
      "Epoch [47/300], Step [72/172], Loss: 65.9469\n",
      "Epoch [47/300], Step [73/172], Loss: 72.4218\n",
      "Epoch [47/300], Step [74/172], Loss: 47.7800\n",
      "Epoch [47/300], Step [75/172], Loss: 32.0237\n",
      "Epoch [47/300], Step [76/172], Loss: 50.2630\n",
      "Epoch [47/300], Step [77/172], Loss: 62.3891\n",
      "Epoch [47/300], Step [78/172], Loss: 61.2996\n",
      "Epoch [47/300], Step [79/172], Loss: 54.3896\n",
      "Epoch [47/300], Step [80/172], Loss: 57.5229\n",
      "Epoch [47/300], Step [81/172], Loss: 54.3607\n",
      "Epoch [47/300], Step [82/172], Loss: 47.0376\n",
      "Epoch [47/300], Step [83/172], Loss: 53.3406\n",
      "Epoch [47/300], Step [84/172], Loss: 45.3005\n",
      "Epoch [47/300], Step [85/172], Loss: 52.5944\n",
      "Epoch [47/300], Step [86/172], Loss: 40.3272\n",
      "Epoch [47/300], Step [87/172], Loss: 32.9770\n",
      "Epoch [47/300], Step [88/172], Loss: 35.9827\n",
      "Epoch [47/300], Step [89/172], Loss: 35.0393\n",
      "Epoch [47/300], Step [90/172], Loss: 34.4520\n",
      "Epoch [47/300], Step [91/172], Loss: 33.2525\n",
      "Epoch [47/300], Step [92/172], Loss: 27.5810\n",
      "Epoch [47/300], Step [93/172], Loss: 28.9379\n",
      "Epoch [47/300], Step [94/172], Loss: 32.6750\n",
      "Epoch [47/300], Step [95/172], Loss: 29.3203\n",
      "Epoch [47/300], Step [96/172], Loss: 25.5335\n",
      "Epoch [47/300], Step [97/172], Loss: 30.6170\n",
      "Epoch [47/300], Step [98/172], Loss: 26.0409\n",
      "Epoch [47/300], Step [99/172], Loss: 23.5655\n",
      "Epoch [47/300], Step [100/172], Loss: 22.5179\n",
      "Epoch [47/300], Step [101/172], Loss: 24.3611\n",
      "Epoch [47/300], Step [102/172], Loss: 22.4609\n",
      "Epoch [47/300], Step [103/172], Loss: 21.9739\n",
      "Epoch [47/300], Step [104/172], Loss: 21.6397\n",
      "Epoch [47/300], Step [105/172], Loss: 22.3503\n",
      "Epoch [47/300], Step [106/172], Loss: 22.2265\n",
      "Epoch [47/300], Step [107/172], Loss: 19.1946\n",
      "Epoch [47/300], Step [108/172], Loss: 22.7867\n",
      "Epoch [47/300], Step [109/172], Loss: 24.6207\n",
      "Epoch [47/300], Step [110/172], Loss: 20.7607\n",
      "Epoch [47/300], Step [111/172], Loss: 19.6554\n",
      "Epoch [47/300], Step [112/172], Loss: 25.6944\n",
      "Epoch [47/300], Step [113/172], Loss: 20.1861\n",
      "Epoch [47/300], Step [114/172], Loss: 20.5567\n",
      "Epoch [47/300], Step [115/172], Loss: 28.8284\n",
      "Epoch [47/300], Step [116/172], Loss: 20.3080\n",
      "Epoch [47/300], Step [117/172], Loss: 17.4472\n",
      "Epoch [47/300], Step [118/172], Loss: 20.9470\n",
      "Epoch [47/300], Step [119/172], Loss: 17.9346\n",
      "Epoch [47/300], Step [120/172], Loss: 16.2446\n",
      "Epoch [47/300], Step [121/172], Loss: 16.9534\n",
      "Epoch [47/300], Step [122/172], Loss: 15.0238\n",
      "Epoch [47/300], Step [123/172], Loss: 15.5097\n",
      "Epoch [47/300], Step [124/172], Loss: 13.1856\n",
      "Epoch [47/300], Step [125/172], Loss: 17.4954\n",
      "Epoch [47/300], Step [126/172], Loss: 16.2471\n",
      "Epoch [47/300], Step [127/172], Loss: 18.5758\n",
      "Epoch [47/300], Step [128/172], Loss: 19.2385\n",
      "Epoch [47/300], Step [129/172], Loss: 13.7322\n",
      "Epoch [47/300], Step [130/172], Loss: 15.8912\n",
      "Epoch [47/300], Step [131/172], Loss: 13.9582\n",
      "Epoch [47/300], Step [132/172], Loss: 13.9552\n",
      "Epoch [47/300], Step [133/172], Loss: 14.0998\n",
      "Epoch [47/300], Step [134/172], Loss: 14.7186\n",
      "Epoch [47/300], Step [135/172], Loss: 12.2831\n",
      "Epoch [47/300], Step [136/172], Loss: 12.7981\n",
      "Epoch [47/300], Step [137/172], Loss: 14.4706\n",
      "Epoch [47/300], Step [138/172], Loss: 12.4219\n",
      "Epoch [47/300], Step [139/172], Loss: 13.8434\n",
      "Epoch [47/300], Step [140/172], Loss: 13.7958\n",
      "Epoch [47/300], Step [141/172], Loss: 17.3538\n",
      "Epoch [47/300], Step [142/172], Loss: 15.7956\n",
      "Epoch [47/300], Step [143/172], Loss: 12.6702\n",
      "Epoch [47/300], Step [144/172], Loss: 11.9694\n",
      "Epoch [47/300], Step [145/172], Loss: 12.1839\n",
      "Epoch [47/300], Step [146/172], Loss: 12.6695\n",
      "Epoch [47/300], Step [147/172], Loss: 9.6746\n",
      "Epoch [47/300], Step [148/172], Loss: 10.2887\n",
      "Epoch [47/300], Step [149/172], Loss: 12.4699\n",
      "Epoch [47/300], Step [150/172], Loss: 12.1456\n",
      "Epoch [47/300], Step [151/172], Loss: 10.5978\n",
      "Epoch [47/300], Step [152/172], Loss: 10.7322\n",
      "Epoch [47/300], Step [153/172], Loss: 10.6958\n",
      "Epoch [47/300], Step [154/172], Loss: 11.9129\n",
      "Epoch [47/300], Step [155/172], Loss: 10.3377\n",
      "Epoch [47/300], Step [156/172], Loss: 12.0497\n",
      "Epoch [47/300], Step [157/172], Loss: 12.9692\n",
      "Epoch [47/300], Step [158/172], Loss: 11.1307\n",
      "Epoch [47/300], Step [159/172], Loss: 11.2614\n",
      "Epoch [47/300], Step [160/172], Loss: 11.1271\n",
      "Epoch [47/300], Step [161/172], Loss: 9.6883\n",
      "Epoch [47/300], Step [162/172], Loss: 10.0275\n",
      "Epoch [47/300], Step [163/172], Loss: 9.7334\n",
      "Epoch [47/300], Step [164/172], Loss: 11.7173\n",
      "Epoch [47/300], Step [165/172], Loss: 8.8979\n",
      "Epoch [47/300], Step [166/172], Loss: 9.5446\n",
      "Epoch [47/300], Step [167/172], Loss: 9.9090\n",
      "Epoch [47/300], Step [168/172], Loss: 9.2977\n",
      "Epoch [47/300], Step [169/172], Loss: 8.7549\n",
      "Epoch [47/300], Step [170/172], Loss: 8.7064\n",
      "Epoch [47/300], Step [171/172], Loss: 7.4822\n",
      "Epoch [47/300], Step [172/172], Loss: 6.7043\n",
      "Epoch [48/300], Step [1/172], Loss: 101.0356\n",
      "Epoch [48/300], Step [2/172], Loss: 99.1491\n",
      "Epoch [48/300], Step [3/172], Loss: 116.9368\n",
      "Epoch [48/300], Step [4/172], Loss: 70.7427\n",
      "Epoch [48/300], Step [5/172], Loss: 92.0539\n",
      "Epoch [48/300], Step [6/172], Loss: 36.5366\n",
      "Epoch [48/300], Step [7/172], Loss: 45.6519\n",
      "Epoch [48/300], Step [8/172], Loss: 14.5986\n",
      "Epoch [48/300], Step [9/172], Loss: 54.1270\n",
      "Epoch [48/300], Step [10/172], Loss: 61.1355\n",
      "Epoch [48/300], Step [11/172], Loss: 99.4574\n",
      "Epoch [48/300], Step [12/172], Loss: 87.9140\n",
      "Epoch [48/300], Step [13/172], Loss: 49.0289\n",
      "Epoch [48/300], Step [14/172], Loss: 100.2951\n",
      "Epoch [48/300], Step [15/172], Loss: 89.9253\n",
      "Epoch [48/300], Step [16/172], Loss: 63.7007\n",
      "Epoch [48/300], Step [17/172], Loss: 61.5185\n",
      "Epoch [48/300], Step [18/172], Loss: 74.3690\n",
      "Epoch [48/300], Step [19/172], Loss: 80.6772\n",
      "Epoch [48/300], Step [20/172], Loss: 111.3570\n",
      "Epoch [48/300], Step [21/172], Loss: 97.1298\n",
      "Epoch [48/300], Step [22/172], Loss: 93.6692\n",
      "Epoch [48/300], Step [23/172], Loss: 24.7952\n",
      "Epoch [48/300], Step [24/172], Loss: 81.5520\n",
      "Epoch [48/300], Step [25/172], Loss: 56.0514\n",
      "Epoch [48/300], Step [26/172], Loss: 64.2375\n",
      "Epoch [48/300], Step [27/172], Loss: 87.9760\n",
      "Epoch [48/300], Step [28/172], Loss: 64.6994\n",
      "Epoch [48/300], Step [29/172], Loss: 73.1231\n",
      "Epoch [48/300], Step [30/172], Loss: 78.0580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/300], Step [31/172], Loss: 46.9984\n",
      "Epoch [48/300], Step [32/172], Loss: 42.8518\n",
      "Epoch [48/300], Step [33/172], Loss: 78.1304\n",
      "Epoch [48/300], Step [34/172], Loss: 11.8614\n",
      "Epoch [48/300], Step [35/172], Loss: 56.3129\n",
      "Epoch [48/300], Step [36/172], Loss: 29.8323\n",
      "Epoch [48/300], Step [37/172], Loss: 21.6827\n",
      "Epoch [48/300], Step [38/172], Loss: 31.7309\n",
      "Epoch [48/300], Step [39/172], Loss: 55.5837\n",
      "Epoch [48/300], Step [40/172], Loss: 30.8334\n",
      "Epoch [48/300], Step [41/172], Loss: 43.2495\n",
      "Epoch [48/300], Step [42/172], Loss: 43.4379\n",
      "Epoch [48/300], Step [43/172], Loss: 33.2326\n",
      "Epoch [48/300], Step [44/172], Loss: 28.6859\n",
      "Epoch [48/300], Step [45/172], Loss: 24.4340\n",
      "Epoch [48/300], Step [46/172], Loss: 44.1404\n",
      "Epoch [48/300], Step [47/172], Loss: 66.6064\n",
      "Epoch [48/300], Step [48/172], Loss: 69.5793\n",
      "Epoch [48/300], Step [49/172], Loss: 23.4398\n",
      "Epoch [48/300], Step [50/172], Loss: 52.9076\n",
      "Epoch [48/300], Step [51/172], Loss: 8.9100\n",
      "Epoch [48/300], Step [52/172], Loss: 24.1960\n",
      "Epoch [48/300], Step [53/172], Loss: 29.7045\n",
      "Epoch [48/300], Step [54/172], Loss: 19.5116\n",
      "Epoch [48/300], Step [55/172], Loss: 16.0638\n",
      "Epoch [48/300], Step [56/172], Loss: 11.6807\n",
      "Epoch [48/300], Step [57/172], Loss: 44.7206\n",
      "Epoch [48/300], Step [58/172], Loss: 20.7425\n",
      "Epoch [48/300], Step [59/172], Loss: 37.3897\n",
      "Epoch [48/300], Step [60/172], Loss: 60.9503\n",
      "Epoch [48/300], Step [61/172], Loss: 11.9422\n",
      "Epoch [48/300], Step [62/172], Loss: 20.4753\n",
      "Epoch [48/300], Step [63/172], Loss: 9.2960\n",
      "Epoch [48/300], Step [64/172], Loss: 7.0129\n",
      "Epoch [48/300], Step [65/172], Loss: 28.4063\n",
      "Epoch [48/300], Step [66/172], Loss: 8.2983\n",
      "Epoch [48/300], Step [67/172], Loss: 27.2972\n",
      "Epoch [48/300], Step [68/172], Loss: 15.0401\n",
      "Epoch [48/300], Step [69/172], Loss: 79.7085\n",
      "Epoch [48/300], Step [70/172], Loss: 73.8876\n",
      "Epoch [48/300], Step [71/172], Loss: 66.7393\n",
      "Epoch [48/300], Step [72/172], Loss: 67.0818\n",
      "Epoch [48/300], Step [73/172], Loss: 73.9175\n",
      "Epoch [48/300], Step [74/172], Loss: 48.4546\n",
      "Epoch [48/300], Step [75/172], Loss: 32.2161\n",
      "Epoch [48/300], Step [76/172], Loss: 50.6887\n",
      "Epoch [48/300], Step [77/172], Loss: 62.8864\n",
      "Epoch [48/300], Step [78/172], Loss: 61.2365\n",
      "Epoch [48/300], Step [79/172], Loss: 54.3628\n",
      "Epoch [48/300], Step [80/172], Loss: 57.3377\n",
      "Epoch [48/300], Step [81/172], Loss: 53.9465\n",
      "Epoch [48/300], Step [82/172], Loss: 46.4408\n",
      "Epoch [48/300], Step [83/172], Loss: 53.1362\n",
      "Epoch [48/300], Step [84/172], Loss: 44.8320\n",
      "Epoch [48/300], Step [85/172], Loss: 52.0430\n",
      "Epoch [48/300], Step [86/172], Loss: 39.8712\n",
      "Epoch [48/300], Step [87/172], Loss: 32.7222\n",
      "Epoch [48/300], Step [88/172], Loss: 35.7029\n",
      "Epoch [48/300], Step [89/172], Loss: 34.6408\n",
      "Epoch [48/300], Step [90/172], Loss: 34.2278\n",
      "Epoch [48/300], Step [91/172], Loss: 33.0909\n",
      "Epoch [48/300], Step [92/172], Loss: 27.3062\n",
      "Epoch [48/300], Step [93/172], Loss: 28.6425\n",
      "Epoch [48/300], Step [94/172], Loss: 32.4986\n",
      "Epoch [48/300], Step [95/172], Loss: 29.1033\n",
      "Epoch [48/300], Step [96/172], Loss: 25.3033\n",
      "Epoch [48/300], Step [97/172], Loss: 30.4281\n",
      "Epoch [48/300], Step [98/172], Loss: 25.8625\n",
      "Epoch [48/300], Step [99/172], Loss: 23.3923\n",
      "Epoch [48/300], Step [100/172], Loss: 22.3262\n",
      "Epoch [48/300], Step [101/172], Loss: 24.2081\n",
      "Epoch [48/300], Step [102/172], Loss: 22.1649\n",
      "Epoch [48/300], Step [103/172], Loss: 21.7652\n",
      "Epoch [48/300], Step [104/172], Loss: 21.4541\n",
      "Epoch [48/300], Step [105/172], Loss: 22.0858\n",
      "Epoch [48/300], Step [106/172], Loss: 22.0736\n",
      "Epoch [48/300], Step [107/172], Loss: 19.0800\n",
      "Epoch [48/300], Step [108/172], Loss: 22.6573\n",
      "Epoch [48/300], Step [109/172], Loss: 24.5527\n",
      "Epoch [48/300], Step [110/172], Loss: 20.6314\n",
      "Epoch [48/300], Step [111/172], Loss: 19.4788\n",
      "Epoch [48/300], Step [112/172], Loss: 25.4506\n",
      "Epoch [48/300], Step [113/172], Loss: 20.0687\n",
      "Epoch [48/300], Step [114/172], Loss: 20.3829\n",
      "Epoch [48/300], Step [115/172], Loss: 28.7299\n",
      "Epoch [48/300], Step [116/172], Loss: 20.1537\n",
      "Epoch [48/300], Step [117/172], Loss: 17.2909\n",
      "Epoch [48/300], Step [118/172], Loss: 20.8254\n",
      "Epoch [48/300], Step [119/172], Loss: 17.8323\n",
      "Epoch [48/300], Step [120/172], Loss: 16.1008\n",
      "Epoch [48/300], Step [121/172], Loss: 16.7920\n",
      "Epoch [48/300], Step [122/172], Loss: 14.9472\n",
      "Epoch [48/300], Step [123/172], Loss: 15.3343\n",
      "Epoch [48/300], Step [124/172], Loss: 13.0101\n",
      "Epoch [48/300], Step [125/172], Loss: 17.3490\n",
      "Epoch [48/300], Step [126/172], Loss: 16.0808\n",
      "Epoch [48/300], Step [127/172], Loss: 18.4377\n",
      "Epoch [48/300], Step [128/172], Loss: 19.1035\n",
      "Epoch [48/300], Step [129/172], Loss: 13.5878\n",
      "Epoch [48/300], Step [130/172], Loss: 15.7526\n",
      "Epoch [48/300], Step [131/172], Loss: 13.8396\n",
      "Epoch [48/300], Step [132/172], Loss: 13.8096\n",
      "Epoch [48/300], Step [133/172], Loss: 14.0035\n",
      "Epoch [48/300], Step [134/172], Loss: 14.6374\n",
      "Epoch [48/300], Step [135/172], Loss: 12.1776\n",
      "Epoch [48/300], Step [136/172], Loss: 12.6538\n",
      "Epoch [48/300], Step [137/172], Loss: 14.3397\n",
      "Epoch [48/300], Step [138/172], Loss: 12.2891\n",
      "Epoch [48/300], Step [139/172], Loss: 13.7229\n",
      "Epoch [48/300], Step [140/172], Loss: 13.6859\n",
      "Epoch [48/300], Step [141/172], Loss: 17.2960\n",
      "Epoch [48/300], Step [142/172], Loss: 15.7653\n",
      "Epoch [48/300], Step [143/172], Loss: 12.5613\n",
      "Epoch [48/300], Step [144/172], Loss: 11.8941\n",
      "Epoch [48/300], Step [145/172], Loss: 12.0942\n",
      "Epoch [48/300], Step [146/172], Loss: 12.6083\n",
      "Epoch [48/300], Step [147/172], Loss: 9.5156\n",
      "Epoch [48/300], Step [148/172], Loss: 10.1624\n",
      "Epoch [48/300], Step [149/172], Loss: 12.3755\n",
      "Epoch [48/300], Step [150/172], Loss: 12.0306\n",
      "Epoch [48/300], Step [151/172], Loss: 10.5039\n",
      "Epoch [48/300], Step [152/172], Loss: 10.6371\n",
      "Epoch [48/300], Step [153/172], Loss: 10.5858\n",
      "Epoch [48/300], Step [154/172], Loss: 11.7767\n",
      "Epoch [48/300], Step [155/172], Loss: 10.2378\n",
      "Epoch [48/300], Step [156/172], Loss: 12.0239\n",
      "Epoch [48/300], Step [157/172], Loss: 12.8922\n",
      "Epoch [48/300], Step [158/172], Loss: 11.0485\n",
      "Epoch [48/300], Step [159/172], Loss: 11.1997\n",
      "Epoch [48/300], Step [160/172], Loss: 11.0732\n",
      "Epoch [48/300], Step [161/172], Loss: 9.6002\n",
      "Epoch [48/300], Step [162/172], Loss: 9.9718\n",
      "Epoch [48/300], Step [163/172], Loss: 9.6467\n",
      "Epoch [48/300], Step [164/172], Loss: 11.6842\n",
      "Epoch [48/300], Step [165/172], Loss: 8.8377\n",
      "Epoch [48/300], Step [166/172], Loss: 9.4527\n",
      "Epoch [48/300], Step [167/172], Loss: 9.8437\n",
      "Epoch [48/300], Step [168/172], Loss: 9.2023\n",
      "Epoch [48/300], Step [169/172], Loss: 8.7138\n",
      "Epoch [48/300], Step [170/172], Loss: 8.6239\n",
      "Epoch [48/300], Step [171/172], Loss: 7.4455\n",
      "Epoch [48/300], Step [172/172], Loss: 6.6769\n",
      "Epoch [49/300], Step [1/172], Loss: 100.6790\n",
      "Epoch [49/300], Step [2/172], Loss: 98.7712\n",
      "Epoch [49/300], Step [3/172], Loss: 115.6238\n",
      "Epoch [49/300], Step [4/172], Loss: 69.9124\n",
      "Epoch [49/300], Step [5/172], Loss: 91.5115\n",
      "Epoch [49/300], Step [6/172], Loss: 35.8694\n",
      "Epoch [49/300], Step [7/172], Loss: 44.7292\n",
      "Epoch [49/300], Step [8/172], Loss: 14.1862\n",
      "Epoch [49/300], Step [9/172], Loss: 53.7112\n",
      "Epoch [49/300], Step [10/172], Loss: 60.9189\n",
      "Epoch [49/300], Step [11/172], Loss: 99.8671\n",
      "Epoch [49/300], Step [12/172], Loss: 88.0524\n",
      "Epoch [49/300], Step [13/172], Loss: 48.7536\n",
      "Epoch [49/300], Step [14/172], Loss: 100.4578\n",
      "Epoch [49/300], Step [15/172], Loss: 90.0488\n",
      "Epoch [49/300], Step [16/172], Loss: 63.0989\n",
      "Epoch [49/300], Step [17/172], Loss: 61.8133\n",
      "Epoch [49/300], Step [18/172], Loss: 74.0153\n",
      "Epoch [49/300], Step [19/172], Loss: 81.0759\n",
      "Epoch [49/300], Step [20/172], Loss: 110.8459\n",
      "Epoch [49/300], Step [21/172], Loss: 97.5975\n",
      "Epoch [49/300], Step [22/172], Loss: 94.0387\n",
      "Epoch [49/300], Step [23/172], Loss: 24.0825\n",
      "Epoch [49/300], Step [24/172], Loss: 81.7741\n",
      "Epoch [49/300], Step [25/172], Loss: 56.1439\n",
      "Epoch [49/300], Step [26/172], Loss: 64.4716\n",
      "Epoch [49/300], Step [27/172], Loss: 88.4511\n",
      "Epoch [49/300], Step [28/172], Loss: 64.2545\n",
      "Epoch [49/300], Step [29/172], Loss: 71.7779\n",
      "Epoch [49/300], Step [30/172], Loss: 78.4300\n",
      "Epoch [49/300], Step [31/172], Loss: 46.9512\n",
      "Epoch [49/300], Step [32/172], Loss: 42.9498\n",
      "Epoch [49/300], Step [33/172], Loss: 78.2776\n",
      "Epoch [49/300], Step [34/172], Loss: 11.6722\n",
      "Epoch [49/300], Step [35/172], Loss: 55.4275\n",
      "Epoch [49/300], Step [36/172], Loss: 29.5776\n",
      "Epoch [49/300], Step [37/172], Loss: 21.6330\n",
      "Epoch [49/300], Step [38/172], Loss: 31.6405\n",
      "Epoch [49/300], Step [39/172], Loss: 55.7279\n",
      "Epoch [49/300], Step [40/172], Loss: 30.6820\n",
      "Epoch [49/300], Step [41/172], Loss: 43.3431\n",
      "Epoch [49/300], Step [42/172], Loss: 43.6506\n",
      "Epoch [49/300], Step [43/172], Loss: 33.1663\n",
      "Epoch [49/300], Step [44/172], Loss: 28.5703\n",
      "Epoch [49/300], Step [45/172], Loss: 24.4278\n",
      "Epoch [49/300], Step [46/172], Loss: 43.8310\n",
      "Epoch [49/300], Step [47/172], Loss: 66.2854\n",
      "Epoch [49/300], Step [48/172], Loss: 69.3937\n",
      "Epoch [49/300], Step [49/172], Loss: 23.6227\n",
      "Epoch [49/300], Step [50/172], Loss: 52.7599\n",
      "Epoch [49/300], Step [51/172], Loss: 8.9221\n",
      "Epoch [49/300], Step [52/172], Loss: 24.1179\n",
      "Epoch [49/300], Step [53/172], Loss: 29.6329\n",
      "Epoch [49/300], Step [54/172], Loss: 19.4824\n",
      "Epoch [49/300], Step [55/172], Loss: 15.8943\n",
      "Epoch [49/300], Step [56/172], Loss: 11.7165\n",
      "Epoch [49/300], Step [57/172], Loss: 44.3932\n",
      "Epoch [49/300], Step [58/172], Loss: 20.8984\n",
      "Epoch [49/300], Step [59/172], Loss: 37.5514\n",
      "Epoch [49/300], Step [60/172], Loss: 61.2724\n",
      "Epoch [49/300], Step [61/172], Loss: 12.0326\n",
      "Epoch [49/300], Step [62/172], Loss: 20.8174\n",
      "Epoch [49/300], Step [63/172], Loss: 9.5212\n",
      "Epoch [49/300], Step [64/172], Loss: 7.2863\n",
      "Epoch [49/300], Step [65/172], Loss: 28.7385\n",
      "Epoch [49/300], Step [66/172], Loss: 8.2762\n",
      "Epoch [49/300], Step [67/172], Loss: 27.8839\n",
      "Epoch [49/300], Step [68/172], Loss: 15.1352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/300], Step [69/172], Loss: 79.5592\n",
      "Epoch [49/300], Step [70/172], Loss: 71.4869\n",
      "Epoch [49/300], Step [71/172], Loss: 64.8357\n",
      "Epoch [49/300], Step [72/172], Loss: 65.4919\n",
      "Epoch [49/300], Step [73/172], Loss: 72.0558\n",
      "Epoch [49/300], Step [74/172], Loss: 46.8485\n",
      "Epoch [49/300], Step [75/172], Loss: 32.0246\n",
      "Epoch [49/300], Step [76/172], Loss: 49.8030\n",
      "Epoch [49/300], Step [77/172], Loss: 62.4202\n",
      "Epoch [49/300], Step [78/172], Loss: 60.6046\n",
      "Epoch [49/300], Step [79/172], Loss: 54.1463\n",
      "Epoch [49/300], Step [80/172], Loss: 57.2132\n",
      "Epoch [49/300], Step [81/172], Loss: 53.7885\n",
      "Epoch [49/300], Step [82/172], Loss: 46.4574\n",
      "Epoch [49/300], Step [83/172], Loss: 53.2828\n",
      "Epoch [49/300], Step [84/172], Loss: 44.7782\n",
      "Epoch [49/300], Step [85/172], Loss: 51.7297\n",
      "Epoch [49/300], Step [86/172], Loss: 39.9430\n",
      "Epoch [49/300], Step [87/172], Loss: 32.6313\n",
      "Epoch [49/300], Step [88/172], Loss: 35.5646\n",
      "Epoch [49/300], Step [89/172], Loss: 34.5120\n",
      "Epoch [49/300], Step [90/172], Loss: 34.2184\n",
      "Epoch [49/300], Step [91/172], Loss: 32.9747\n",
      "Epoch [49/300], Step [92/172], Loss: 27.2334\n",
      "Epoch [49/300], Step [93/172], Loss: 28.3446\n",
      "Epoch [49/300], Step [94/172], Loss: 32.3206\n",
      "Epoch [49/300], Step [95/172], Loss: 28.8505\n",
      "Epoch [49/300], Step [96/172], Loss: 25.0405\n",
      "Epoch [49/300], Step [97/172], Loss: 30.2993\n",
      "Epoch [49/300], Step [98/172], Loss: 25.6766\n",
      "Epoch [49/300], Step [99/172], Loss: 23.1846\n",
      "Epoch [49/300], Step [100/172], Loss: 22.1004\n",
      "Epoch [49/300], Step [101/172], Loss: 23.9308\n",
      "Epoch [49/300], Step [102/172], Loss: 22.1042\n",
      "Epoch [49/300], Step [103/172], Loss: 21.5325\n",
      "Epoch [49/300], Step [104/172], Loss: 21.2074\n",
      "Epoch [49/300], Step [105/172], Loss: 22.0387\n",
      "Epoch [49/300], Step [106/172], Loss: 21.8676\n",
      "Epoch [49/300], Step [107/172], Loss: 18.9324\n",
      "Epoch [49/300], Step [108/172], Loss: 22.4616\n",
      "Epoch [49/300], Step [109/172], Loss: 24.4546\n",
      "Epoch [49/300], Step [110/172], Loss: 20.5068\n",
      "Epoch [49/300], Step [111/172], Loss: 19.2892\n",
      "Epoch [49/300], Step [112/172], Loss: 25.2270\n",
      "Epoch [49/300], Step [113/172], Loss: 19.9329\n",
      "Epoch [49/300], Step [114/172], Loss: 20.1848\n",
      "Epoch [49/300], Step [115/172], Loss: 28.6104\n",
      "Epoch [49/300], Step [116/172], Loss: 19.9658\n",
      "Epoch [49/300], Step [117/172], Loss: 17.1358\n",
      "Epoch [49/300], Step [118/172], Loss: 20.6630\n",
      "Epoch [49/300], Step [119/172], Loss: 17.7285\n",
      "Epoch [49/300], Step [120/172], Loss: 15.9522\n",
      "Epoch [49/300], Step [121/172], Loss: 16.6409\n",
      "Epoch [49/300], Step [122/172], Loss: 14.8481\n",
      "Epoch [49/300], Step [123/172], Loss: 15.1686\n",
      "Epoch [49/300], Step [124/172], Loss: 12.8366\n",
      "Epoch [49/300], Step [125/172], Loss: 17.2165\n",
      "Epoch [49/300], Step [126/172], Loss: 15.9005\n",
      "Epoch [49/300], Step [127/172], Loss: 18.2482\n",
      "Epoch [49/300], Step [128/172], Loss: 18.8870\n",
      "Epoch [49/300], Step [129/172], Loss: 13.4393\n",
      "Epoch [49/300], Step [130/172], Loss: 15.5973\n",
      "Epoch [49/300], Step [131/172], Loss: 13.6845\n",
      "Epoch [49/300], Step [132/172], Loss: 13.6491\n",
      "Epoch [49/300], Step [133/172], Loss: 13.8863\n",
      "Epoch [49/300], Step [134/172], Loss: 14.5280\n",
      "Epoch [49/300], Step [135/172], Loss: 12.0709\n",
      "Epoch [49/300], Step [136/172], Loss: 12.4692\n",
      "Epoch [49/300], Step [137/172], Loss: 14.2089\n",
      "Epoch [49/300], Step [138/172], Loss: 12.1644\n",
      "Epoch [49/300], Step [139/172], Loss: 13.5743\n",
      "Epoch [49/300], Step [140/172], Loss: 13.5416\n",
      "Epoch [49/300], Step [141/172], Loss: 17.2043\n",
      "Epoch [49/300], Step [142/172], Loss: 15.6767\n",
      "Epoch [49/300], Step [143/172], Loss: 12.4281\n",
      "Epoch [49/300], Step [144/172], Loss: 11.7971\n",
      "Epoch [49/300], Step [145/172], Loss: 11.9985\n",
      "Epoch [49/300], Step [146/172], Loss: 12.5036\n",
      "Epoch [49/300], Step [147/172], Loss: 9.3734\n",
      "Epoch [49/300], Step [148/172], Loss: 10.0279\n",
      "Epoch [49/300], Step [149/172], Loss: 12.2377\n",
      "Epoch [49/300], Step [150/172], Loss: 11.8863\n",
      "Epoch [49/300], Step [151/172], Loss: 10.4028\n",
      "Epoch [49/300], Step [152/172], Loss: 10.5222\n",
      "Epoch [49/300], Step [153/172], Loss: 10.4685\n",
      "Epoch [49/300], Step [154/172], Loss: 11.6475\n",
      "Epoch [49/300], Step [155/172], Loss: 10.1345\n",
      "Epoch [49/300], Step [156/172], Loss: 11.9887\n",
      "Epoch [49/300], Step [157/172], Loss: 12.7962\n",
      "Epoch [49/300], Step [158/172], Loss: 10.9375\n",
      "Epoch [49/300], Step [159/172], Loss: 11.1332\n",
      "Epoch [49/300], Step [160/172], Loss: 11.0061\n",
      "Epoch [49/300], Step [161/172], Loss: 9.4901\n",
      "Epoch [49/300], Step [162/172], Loss: 9.8782\n",
      "Epoch [49/300], Step [163/172], Loss: 9.5436\n",
      "Epoch [49/300], Step [164/172], Loss: 11.6310\n",
      "Epoch [49/300], Step [165/172], Loss: 8.7397\n",
      "Epoch [49/300], Step [166/172], Loss: 9.3472\n",
      "Epoch [49/300], Step [167/172], Loss: 9.7617\n",
      "Epoch [49/300], Step [168/172], Loss: 9.0953\n",
      "Epoch [49/300], Step [169/172], Loss: 8.6548\n",
      "Epoch [49/300], Step [170/172], Loss: 8.5329\n",
      "Epoch [49/300], Step [171/172], Loss: 7.3688\n",
      "Epoch [49/300], Step [172/172], Loss: 6.6371\n",
      "Epoch [50/300], Step [1/172], Loss: 100.5876\n",
      "Epoch [50/300], Step [2/172], Loss: 98.7173\n",
      "Epoch [50/300], Step [3/172], Loss: 114.9209\n",
      "Epoch [50/300], Step [4/172], Loss: 69.5065\n",
      "Epoch [50/300], Step [5/172], Loss: 91.0556\n",
      "Epoch [50/300], Step [6/172], Loss: 35.2672\n",
      "Epoch [50/300], Step [7/172], Loss: 43.9729\n",
      "Epoch [50/300], Step [8/172], Loss: 13.7378\n",
      "Epoch [50/300], Step [9/172], Loss: 53.5125\n",
      "Epoch [50/300], Step [10/172], Loss: 60.6337\n",
      "Epoch [50/300], Step [11/172], Loss: 99.7274\n",
      "Epoch [50/300], Step [12/172], Loss: 87.8046\n",
      "Epoch [50/300], Step [13/172], Loss: 48.2612\n",
      "Epoch [50/300], Step [14/172], Loss: 100.1869\n",
      "Epoch [50/300], Step [15/172], Loss: 89.7193\n",
      "Epoch [50/300], Step [16/172], Loss: 61.8857\n",
      "Epoch [50/300], Step [17/172], Loss: 61.4495\n",
      "Epoch [50/300], Step [18/172], Loss: 73.1559\n",
      "Epoch [50/300], Step [19/172], Loss: 80.8779\n",
      "Epoch [50/300], Step [20/172], Loss: 109.5845\n",
      "Epoch [50/300], Step [21/172], Loss: 97.1583\n",
      "Epoch [50/300], Step [22/172], Loss: 93.4026\n",
      "Epoch [50/300], Step [23/172], Loss: 23.1149\n",
      "Epoch [50/300], Step [24/172], Loss: 81.1530\n",
      "Epoch [50/300], Step [25/172], Loss: 55.7097\n",
      "Epoch [50/300], Step [26/172], Loss: 64.1010\n",
      "Epoch [50/300], Step [27/172], Loss: 87.9252\n",
      "Epoch [50/300], Step [28/172], Loss: 63.0725\n",
      "Epoch [50/300], Step [29/172], Loss: 70.3245\n",
      "Epoch [50/300], Step [30/172], Loss: 77.9919\n",
      "Epoch [50/300], Step [31/172], Loss: 46.6208\n",
      "Epoch [50/300], Step [32/172], Loss: 42.6590\n",
      "Epoch [50/300], Step [33/172], Loss: 77.7077\n",
      "Epoch [50/300], Step [34/172], Loss: 11.1601\n",
      "Epoch [50/300], Step [35/172], Loss: 54.1204\n",
      "Epoch [50/300], Step [36/172], Loss: 29.3644\n",
      "Epoch [50/300], Step [37/172], Loss: 21.5129\n",
      "Epoch [50/300], Step [38/172], Loss: 31.3410\n",
      "Epoch [50/300], Step [39/172], Loss: 55.5029\n",
      "Epoch [50/300], Step [40/172], Loss: 30.3425\n",
      "Epoch [50/300], Step [41/172], Loss: 43.0751\n",
      "Epoch [50/300], Step [42/172], Loss: 43.5716\n",
      "Epoch [50/300], Step [43/172], Loss: 32.8098\n",
      "Epoch [50/300], Step [44/172], Loss: 28.2988\n",
      "Epoch [50/300], Step [45/172], Loss: 24.3545\n",
      "Epoch [50/300], Step [46/172], Loss: 43.0964\n",
      "Epoch [50/300], Step [47/172], Loss: 65.8754\n",
      "Epoch [50/300], Step [48/172], Loss: 68.7968\n",
      "Epoch [50/300], Step [49/172], Loss: 23.4932\n",
      "Epoch [50/300], Step [50/172], Loss: 52.6584\n",
      "Epoch [50/300], Step [51/172], Loss: 8.8803\n",
      "Epoch [50/300], Step [52/172], Loss: 24.0386\n",
      "Epoch [50/300], Step [53/172], Loss: 29.6993\n",
      "Epoch [50/300], Step [54/172], Loss: 19.0642\n",
      "Epoch [50/300], Step [55/172], Loss: 15.7437\n",
      "Epoch [50/300], Step [56/172], Loss: 11.6437\n",
      "Epoch [50/300], Step [57/172], Loss: 43.8111\n",
      "Epoch [50/300], Step [58/172], Loss: 20.7338\n",
      "Epoch [50/300], Step [59/172], Loss: 37.3658\n",
      "Epoch [50/300], Step [60/172], Loss: 60.7771\n",
      "Epoch [50/300], Step [61/172], Loss: 11.9576\n",
      "Epoch [50/300], Step [62/172], Loss: 21.0164\n",
      "Epoch [50/300], Step [63/172], Loss: 9.5764\n",
      "Epoch [50/300], Step [64/172], Loss: 7.3111\n",
      "Epoch [50/300], Step [65/172], Loss: 28.6019\n",
      "Epoch [50/300], Step [66/172], Loss: 8.3139\n",
      "Epoch [50/300], Step [67/172], Loss: 27.7830\n",
      "Epoch [50/300], Step [68/172], Loss: 14.7878\n",
      "Epoch [50/300], Step [69/172], Loss: 79.3303\n",
      "Epoch [50/300], Step [70/172], Loss: 71.3082\n",
      "Epoch [50/300], Step [71/172], Loss: 64.3770\n",
      "Epoch [50/300], Step [72/172], Loss: 65.0552\n",
      "Epoch [50/300], Step [73/172], Loss: 71.5450\n",
      "Epoch [50/300], Step [74/172], Loss: 46.3976\n",
      "Epoch [50/300], Step [75/172], Loss: 31.8053\n",
      "Epoch [50/300], Step [76/172], Loss: 49.5318\n",
      "Epoch [50/300], Step [77/172], Loss: 62.4134\n",
      "Epoch [50/300], Step [78/172], Loss: 60.4274\n",
      "Epoch [50/300], Step [79/172], Loss: 54.0554\n",
      "Epoch [50/300], Step [80/172], Loss: 57.2081\n",
      "Epoch [50/300], Step [81/172], Loss: 53.4927\n",
      "Epoch [50/300], Step [82/172], Loss: 46.2395\n",
      "Epoch [50/300], Step [83/172], Loss: 53.4495\n",
      "Epoch [50/300], Step [84/172], Loss: 44.8019\n",
      "Epoch [50/300], Step [85/172], Loss: 51.8086\n",
      "Epoch [50/300], Step [86/172], Loss: 39.9612\n",
      "Epoch [50/300], Step [87/172], Loss: 32.6691\n",
      "Epoch [50/300], Step [88/172], Loss: 35.6439\n",
      "Epoch [50/300], Step [89/172], Loss: 34.5158\n",
      "Epoch [50/300], Step [90/172], Loss: 34.2166\n",
      "Epoch [50/300], Step [91/172], Loss: 33.0674\n",
      "Epoch [50/300], Step [92/172], Loss: 27.2688\n",
      "Epoch [50/300], Step [93/172], Loss: 28.2591\n",
      "Epoch [50/300], Step [94/172], Loss: 32.3874\n",
      "Epoch [50/300], Step [95/172], Loss: 28.8377\n",
      "Epoch [50/300], Step [96/172], Loss: 24.9818\n",
      "Epoch [50/300], Step [97/172], Loss: 30.3453\n",
      "Epoch [50/300], Step [98/172], Loss: 25.6955\n",
      "Epoch [50/300], Step [99/172], Loss: 23.1632\n",
      "Epoch [50/300], Step [100/172], Loss: 22.0463\n",
      "Epoch [50/300], Step [101/172], Loss: 23.9101\n",
      "Epoch [50/300], Step [102/172], Loss: 21.9671\n",
      "Epoch [50/300], Step [103/172], Loss: 21.4484\n",
      "Epoch [50/300], Step [104/172], Loss: 21.1396\n",
      "Epoch [50/300], Step [105/172], Loss: 21.9417\n",
      "Epoch [50/300], Step [106/172], Loss: 21.8364\n",
      "Epoch [50/300], Step [107/172], Loss: 18.9193\n",
      "Epoch [50/300], Step [108/172], Loss: 22.4292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/300], Step [109/172], Loss: 24.5020\n",
      "Epoch [50/300], Step [110/172], Loss: 20.5005\n",
      "Epoch [50/300], Step [111/172], Loss: 19.2119\n",
      "Epoch [50/300], Step [112/172], Loss: 25.1450\n",
      "Epoch [50/300], Step [113/172], Loss: 19.9107\n",
      "Epoch [50/300], Step [114/172], Loss: 20.1312\n",
      "Epoch [50/300], Step [115/172], Loss: 28.6538\n",
      "Epoch [50/300], Step [116/172], Loss: 19.9321\n",
      "Epoch [50/300], Step [117/172], Loss: 17.0942\n",
      "Epoch [50/300], Step [118/172], Loss: 20.6176\n",
      "Epoch [50/300], Step [119/172], Loss: 17.7277\n",
      "Epoch [50/300], Step [120/172], Loss: 15.8898\n",
      "Epoch [50/300], Step [121/172], Loss: 16.5880\n",
      "Epoch [50/300], Step [122/172], Loss: 14.8294\n",
      "Epoch [50/300], Step [123/172], Loss: 15.0674\n",
      "Epoch [50/300], Step [124/172], Loss: 12.7215\n",
      "Epoch [50/300], Step [125/172], Loss: 17.1352\n",
      "Epoch [50/300], Step [126/172], Loss: 15.8133\n",
      "Epoch [50/300], Step [127/172], Loss: 18.1909\n",
      "Epoch [50/300], Step [128/172], Loss: 18.8279\n",
      "Epoch [50/300], Step [129/172], Loss: 13.3838\n",
      "Epoch [50/300], Step [130/172], Loss: 15.5242\n",
      "Epoch [50/300], Step [131/172], Loss: 13.6146\n",
      "Epoch [50/300], Step [132/172], Loss: 13.5626\n",
      "Epoch [50/300], Step [133/172], Loss: 13.8318\n",
      "Epoch [50/300], Step [134/172], Loss: 14.4891\n",
      "Epoch [50/300], Step [135/172], Loss: 12.0179\n",
      "Epoch [50/300], Step [136/172], Loss: 12.3834\n",
      "Epoch [50/300], Step [137/172], Loss: 14.1370\n",
      "Epoch [50/300], Step [138/172], Loss: 12.0915\n",
      "Epoch [50/300], Step [139/172], Loss: 13.5131\n",
      "Epoch [50/300], Step [140/172], Loss: 13.4999\n",
      "Epoch [50/300], Step [141/172], Loss: 17.2003\n",
      "Epoch [50/300], Step [142/172], Loss: 15.6872\n",
      "Epoch [50/300], Step [143/172], Loss: 12.3710\n",
      "Epoch [50/300], Step [144/172], Loss: 11.7622\n",
      "Epoch [50/300], Step [145/172], Loss: 11.9660\n",
      "Epoch [50/300], Step [146/172], Loss: 12.4816\n",
      "Epoch [50/300], Step [147/172], Loss: 9.2380\n",
      "Epoch [50/300], Step [148/172], Loss: 9.9412\n",
      "Epoch [50/300], Step [149/172], Loss: 12.1723\n",
      "Epoch [50/300], Step [150/172], Loss: 11.8026\n",
      "Epoch [50/300], Step [151/172], Loss: 10.3392\n",
      "Epoch [50/300], Step [152/172], Loss: 10.4553\n",
      "Epoch [50/300], Step [153/172], Loss: 10.3942\n",
      "Epoch [50/300], Step [154/172], Loss: 11.5491\n",
      "Epoch [50/300], Step [155/172], Loss: 10.0623\n",
      "Epoch [50/300], Step [156/172], Loss: 12.0150\n",
      "Epoch [50/300], Step [157/172], Loss: 12.7528\n",
      "Epoch [50/300], Step [158/172], Loss: 10.8683\n",
      "Epoch [50/300], Step [159/172], Loss: 11.1187\n",
      "Epoch [50/300], Step [160/172], Loss: 10.9697\n",
      "Epoch [50/300], Step [161/172], Loss: 9.4147\n",
      "Epoch [50/300], Step [162/172], Loss: 9.8524\n",
      "Epoch [50/300], Step [163/172], Loss: 9.4651\n",
      "Epoch [50/300], Step [164/172], Loss: 11.6533\n",
      "Epoch [50/300], Step [165/172], Loss: 8.6992\n",
      "Epoch [50/300], Step [166/172], Loss: 9.2742\n",
      "Epoch [50/300], Step [167/172], Loss: 9.7146\n",
      "Epoch [50/300], Step [168/172], Loss: 9.0276\n",
      "Epoch [50/300], Step [169/172], Loss: 8.6207\n",
      "Epoch [50/300], Step [170/172], Loss: 8.4483\n",
      "Epoch [50/300], Step [171/172], Loss: 7.3336\n",
      "Epoch [50/300], Step [172/172], Loss: 6.6324\n",
      "Epoch [51/300], Step [1/172], Loss: 100.0497\n",
      "Epoch [51/300], Step [2/172], Loss: 98.2130\n",
      "Epoch [51/300], Step [3/172], Loss: 113.6678\n",
      "Epoch [51/300], Step [4/172], Loss: 68.5744\n",
      "Epoch [51/300], Step [5/172], Loss: 90.2523\n",
      "Epoch [51/300], Step [6/172], Loss: 34.4931\n",
      "Epoch [51/300], Step [7/172], Loss: 43.0976\n",
      "Epoch [51/300], Step [8/172], Loss: 12.8573\n",
      "Epoch [51/300], Step [9/172], Loss: 52.8107\n",
      "Epoch [51/300], Step [10/172], Loss: 60.2462\n",
      "Epoch [51/300], Step [11/172], Loss: 99.8800\n",
      "Epoch [51/300], Step [12/172], Loss: 87.6473\n",
      "Epoch [51/300], Step [13/172], Loss: 47.7699\n",
      "Epoch [51/300], Step [14/172], Loss: 99.9381\n",
      "Epoch [51/300], Step [15/172], Loss: 89.4272\n",
      "Epoch [51/300], Step [16/172], Loss: 60.7476\n",
      "Epoch [51/300], Step [17/172], Loss: 61.4208\n",
      "Epoch [51/300], Step [18/172], Loss: 72.4139\n",
      "Epoch [51/300], Step [19/172], Loss: 80.7708\n",
      "Epoch [51/300], Step [20/172], Loss: 108.4538\n",
      "Epoch [51/300], Step [21/172], Loss: 97.0183\n",
      "Epoch [51/300], Step [22/172], Loss: 93.0698\n",
      "Epoch [51/300], Step [23/172], Loss: 22.2651\n",
      "Epoch [51/300], Step [24/172], Loss: 80.8493\n",
      "Epoch [51/300], Step [25/172], Loss: 55.3987\n",
      "Epoch [51/300], Step [26/172], Loss: 63.8815\n",
      "Epoch [51/300], Step [27/172], Loss: 87.7008\n",
      "Epoch [51/300], Step [28/172], Loss: 62.1701\n",
      "Epoch [51/300], Step [29/172], Loss: 68.4332\n",
      "Epoch [51/300], Step [30/172], Loss: 77.9532\n",
      "Epoch [51/300], Step [31/172], Loss: 46.2602\n",
      "Epoch [51/300], Step [32/172], Loss: 42.6381\n",
      "Epoch [51/300], Step [33/172], Loss: 77.6809\n",
      "Epoch [51/300], Step [34/172], Loss: 10.7892\n",
      "Epoch [51/300], Step [35/172], Loss: 52.8099\n",
      "Epoch [51/300], Step [36/172], Loss: 29.0012\n",
      "Epoch [51/300], Step [37/172], Loss: 21.2980\n",
      "Epoch [51/300], Step [38/172], Loss: 30.8582\n",
      "Epoch [51/300], Step [39/172], Loss: 55.2602\n",
      "Epoch [51/300], Step [40/172], Loss: 29.9622\n",
      "Epoch [51/300], Step [41/172], Loss: 42.8780\n",
      "Epoch [51/300], Step [42/172], Loss: 43.5502\n",
      "Epoch [51/300], Step [43/172], Loss: 32.4600\n",
      "Epoch [51/300], Step [44/172], Loss: 28.0131\n",
      "Epoch [51/300], Step [45/172], Loss: 24.1607\n",
      "Epoch [51/300], Step [46/172], Loss: 42.1731\n",
      "Epoch [51/300], Step [47/172], Loss: 65.1469\n",
      "Epoch [51/300], Step [48/172], Loss: 67.9510\n",
      "Epoch [51/300], Step [49/172], Loss: 23.3748\n",
      "Epoch [51/300], Step [50/172], Loss: 52.2977\n",
      "Epoch [51/300], Step [51/172], Loss: 8.7790\n",
      "Epoch [51/300], Step [52/172], Loss: 23.7744\n",
      "Epoch [51/300], Step [53/172], Loss: 29.4233\n",
      "Epoch [51/300], Step [54/172], Loss: 18.8028\n",
      "Epoch [51/300], Step [55/172], Loss: 15.5248\n",
      "Epoch [51/300], Step [56/172], Loss: 11.5904\n",
      "Epoch [51/300], Step [57/172], Loss: 43.1212\n",
      "Epoch [51/300], Step [58/172], Loss: 20.7205\n",
      "Epoch [51/300], Step [59/172], Loss: 37.2034\n",
      "Epoch [51/300], Step [60/172], Loss: 60.4183\n",
      "Epoch [51/300], Step [61/172], Loss: 11.8516\n",
      "Epoch [51/300], Step [62/172], Loss: 21.1086\n",
      "Epoch [51/300], Step [63/172], Loss: 9.4924\n",
      "Epoch [51/300], Step [64/172], Loss: 7.3002\n",
      "Epoch [51/300], Step [65/172], Loss: 28.3931\n",
      "Epoch [51/300], Step [66/172], Loss: 8.1479\n",
      "Epoch [51/300], Step [67/172], Loss: 27.7224\n",
      "Epoch [51/300], Step [68/172], Loss: 14.3586\n",
      "Epoch [51/300], Step [69/172], Loss: 78.9822\n",
      "Epoch [51/300], Step [70/172], Loss: 71.0725\n",
      "Epoch [51/300], Step [71/172], Loss: 64.0640\n",
      "Epoch [51/300], Step [72/172], Loss: 64.8919\n",
      "Epoch [51/300], Step [73/172], Loss: 71.4863\n",
      "Epoch [51/300], Step [74/172], Loss: 45.9630\n",
      "Epoch [51/300], Step [75/172], Loss: 31.8021\n",
      "Epoch [51/300], Step [76/172], Loss: 49.2275\n",
      "Epoch [51/300], Step [77/172], Loss: 62.3940\n",
      "Epoch [51/300], Step [78/172], Loss: 60.0341\n",
      "Epoch [51/300], Step [79/172], Loss: 53.8743\n",
      "Epoch [51/300], Step [80/172], Loss: 57.0148\n",
      "Epoch [51/300], Step [81/172], Loss: 53.0104\n",
      "Epoch [51/300], Step [82/172], Loss: 45.6841\n",
      "Epoch [51/300], Step [83/172], Loss: 53.1971\n",
      "Epoch [51/300], Step [84/172], Loss: 44.5493\n",
      "Epoch [51/300], Step [85/172], Loss: 51.4715\n",
      "Epoch [51/300], Step [86/172], Loss: 39.7028\n",
      "Epoch [51/300], Step [87/172], Loss: 32.5218\n",
      "Epoch [51/300], Step [88/172], Loss: 35.4538\n",
      "Epoch [51/300], Step [89/172], Loss: 34.2518\n",
      "Epoch [51/300], Step [90/172], Loss: 33.7976\n",
      "Epoch [51/300], Step [91/172], Loss: 32.9627\n",
      "Epoch [51/300], Step [92/172], Loss: 27.0855\n",
      "Epoch [51/300], Step [93/172], Loss: 27.9683\n",
      "Epoch [51/300], Step [94/172], Loss: 32.2613\n",
      "Epoch [51/300], Step [95/172], Loss: 28.6679\n",
      "Epoch [51/300], Step [96/172], Loss: 24.7725\n",
      "Epoch [51/300], Step [97/172], Loss: 30.2545\n",
      "Epoch [51/300], Step [98/172], Loss: 25.5660\n",
      "Epoch [51/300], Step [99/172], Loss: 23.0070\n",
      "Epoch [51/300], Step [100/172], Loss: 21.8819\n",
      "Epoch [51/300], Step [101/172], Loss: 23.7589\n",
      "Epoch [51/300], Step [102/172], Loss: 21.6863\n",
      "Epoch [51/300], Step [103/172], Loss: 21.2596\n",
      "Epoch [51/300], Step [104/172], Loss: 20.9688\n",
      "Epoch [51/300], Step [105/172], Loss: 21.7161\n",
      "Epoch [51/300], Step [106/172], Loss: 21.6866\n",
      "Epoch [51/300], Step [107/172], Loss: 18.8046\n",
      "Epoch [51/300], Step [108/172], Loss: 22.2909\n",
      "Epoch [51/300], Step [109/172], Loss: 24.3652\n",
      "Epoch [51/300], Step [110/172], Loss: 20.3899\n",
      "Epoch [51/300], Step [111/172], Loss: 19.0522\n",
      "Epoch [51/300], Step [112/172], Loss: 24.9356\n",
      "Epoch [51/300], Step [113/172], Loss: 19.8047\n",
      "Epoch [51/300], Step [114/172], Loss: 19.9923\n",
      "Epoch [51/300], Step [115/172], Loss: 28.4724\n",
      "Epoch [51/300], Step [116/172], Loss: 19.7827\n",
      "Epoch [51/300], Step [117/172], Loss: 16.9497\n",
      "Epoch [51/300], Step [118/172], Loss: 20.5064\n",
      "Epoch [51/300], Step [119/172], Loss: 17.6514\n",
      "Epoch [51/300], Step [120/172], Loss: 15.7676\n",
      "Epoch [51/300], Step [121/172], Loss: 16.4703\n",
      "Epoch [51/300], Step [122/172], Loss: 14.7171\n",
      "Epoch [51/300], Step [123/172], Loss: 14.8929\n",
      "Epoch [51/300], Step [124/172], Loss: 12.5643\n",
      "Epoch [51/300], Step [125/172], Loss: 17.0386\n",
      "Epoch [51/300], Step [126/172], Loss: 15.6669\n",
      "Epoch [51/300], Step [127/172], Loss: 18.0335\n",
      "Epoch [51/300], Step [128/172], Loss: 18.6474\n",
      "Epoch [51/300], Step [129/172], Loss: 13.2557\n",
      "Epoch [51/300], Step [130/172], Loss: 15.3974\n",
      "Epoch [51/300], Step [131/172], Loss: 13.4846\n",
      "Epoch [51/300], Step [132/172], Loss: 13.4264\n",
      "Epoch [51/300], Step [133/172], Loss: 13.7386\n",
      "Epoch [51/300], Step [134/172], Loss: 14.3971\n",
      "Epoch [51/300], Step [135/172], Loss: 11.9289\n",
      "Epoch [51/300], Step [136/172], Loss: 12.2314\n",
      "Epoch [51/300], Step [137/172], Loss: 14.0094\n",
      "Epoch [51/300], Step [138/172], Loss: 11.9624\n",
      "Epoch [51/300], Step [139/172], Loss: 13.3867\n",
      "Epoch [51/300], Step [140/172], Loss: 13.3863\n",
      "Epoch [51/300], Step [141/172], Loss: 17.1413\n",
      "Epoch [51/300], Step [142/172], Loss: 15.6345\n",
      "Epoch [51/300], Step [143/172], Loss: 12.2596\n",
      "Epoch [51/300], Step [144/172], Loss: 11.6908\n",
      "Epoch [51/300], Step [145/172], Loss: 11.8934\n",
      "Epoch [51/300], Step [146/172], Loss: 12.4120\n",
      "Epoch [51/300], Step [147/172], Loss: 9.1158\n",
      "Epoch [51/300], Step [148/172], Loss: 9.8340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/300], Step [149/172], Loss: 12.0573\n",
      "Epoch [51/300], Step [150/172], Loss: 11.6817\n",
      "Epoch [51/300], Step [151/172], Loss: 10.2606\n",
      "Epoch [51/300], Step [152/172], Loss: 10.3522\n",
      "Epoch [51/300], Step [153/172], Loss: 10.2976\n",
      "Epoch [51/300], Step [154/172], Loss: 11.4082\n",
      "Epoch [51/300], Step [155/172], Loss: 9.9700\n",
      "Epoch [51/300], Step [156/172], Loss: 12.0095\n",
      "Epoch [51/300], Step [157/172], Loss: 12.6710\n",
      "Epoch [51/300], Step [158/172], Loss: 10.7706\n",
      "Epoch [51/300], Step [159/172], Loss: 11.0769\n",
      "Epoch [51/300], Step [160/172], Loss: 10.9222\n",
      "Epoch [51/300], Step [161/172], Loss: 9.3203\n",
      "Epoch [51/300], Step [162/172], Loss: 9.7879\n",
      "Epoch [51/300], Step [163/172], Loss: 9.3821\n",
      "Epoch [51/300], Step [164/172], Loss: 11.5505\n",
      "Epoch [51/300], Step [165/172], Loss: 8.6298\n",
      "Epoch [51/300], Step [166/172], Loss: 9.2009\n",
      "Epoch [51/300], Step [167/172], Loss: 9.6602\n",
      "Epoch [51/300], Step [168/172], Loss: 8.9563\n",
      "Epoch [51/300], Step [169/172], Loss: 8.5814\n",
      "Epoch [51/300], Step [170/172], Loss: 8.3884\n",
      "Epoch [51/300], Step [171/172], Loss: 7.2968\n",
      "Epoch [51/300], Step [172/172], Loss: 6.6091\n",
      "Epoch [52/300], Step [1/172], Loss: 99.8029\n",
      "Epoch [52/300], Step [2/172], Loss: 98.1026\n",
      "Epoch [52/300], Step [3/172], Loss: 112.7064\n",
      "Epoch [52/300], Step [4/172], Loss: 67.7987\n",
      "Epoch [52/300], Step [5/172], Loss: 89.5778\n",
      "Epoch [52/300], Step [6/172], Loss: 33.8063\n",
      "Epoch [52/300], Step [7/172], Loss: 42.3014\n",
      "Epoch [52/300], Step [8/172], Loss: 13.4968\n",
      "Epoch [52/300], Step [9/172], Loss: 53.0397\n",
      "Epoch [52/300], Step [10/172], Loss: 59.9845\n",
      "Epoch [52/300], Step [11/172], Loss: 99.8876\n",
      "Epoch [52/300], Step [12/172], Loss: 87.5316\n",
      "Epoch [52/300], Step [13/172], Loss: 47.4783\n",
      "Epoch [52/300], Step [14/172], Loss: 100.0173\n",
      "Epoch [52/300], Step [15/172], Loss: 89.4538\n",
      "Epoch [52/300], Step [16/172], Loss: 60.0560\n",
      "Epoch [52/300], Step [17/172], Loss: 61.5340\n",
      "Epoch [52/300], Step [18/172], Loss: 72.0401\n",
      "Epoch [52/300], Step [19/172], Loss: 81.0622\n",
      "Epoch [52/300], Step [20/172], Loss: 107.9415\n",
      "Epoch [52/300], Step [21/172], Loss: 97.3094\n",
      "Epoch [52/300], Step [22/172], Loss: 93.3479\n",
      "Epoch [52/300], Step [23/172], Loss: 21.3647\n",
      "Epoch [52/300], Step [24/172], Loss: 80.8239\n",
      "Epoch [52/300], Step [25/172], Loss: 55.4253\n",
      "Epoch [52/300], Step [26/172], Loss: 64.0127\n",
      "Epoch [52/300], Step [27/172], Loss: 88.1116\n",
      "Epoch [52/300], Step [28/172], Loss: 61.6501\n",
      "Epoch [52/300], Step [29/172], Loss: 67.3342\n",
      "Epoch [52/300], Step [30/172], Loss: 78.1427\n",
      "Epoch [52/300], Step [31/172], Loss: 46.2924\n",
      "Epoch [52/300], Step [32/172], Loss: 42.6695\n",
      "Epoch [52/300], Step [33/172], Loss: 77.7072\n",
      "Epoch [52/300], Step [34/172], Loss: 10.6665\n",
      "Epoch [52/300], Step [35/172], Loss: 52.0749\n",
      "Epoch [52/300], Step [36/172], Loss: 28.7641\n",
      "Epoch [52/300], Step [37/172], Loss: 21.2007\n",
      "Epoch [52/300], Step [38/172], Loss: 30.8290\n",
      "Epoch [52/300], Step [39/172], Loss: 55.4142\n",
      "Epoch [52/300], Step [40/172], Loss: 29.8188\n",
      "Epoch [52/300], Step [41/172], Loss: 43.0156\n",
      "Epoch [52/300], Step [42/172], Loss: 43.7147\n",
      "Epoch [52/300], Step [43/172], Loss: 32.3849\n",
      "Epoch [52/300], Step [44/172], Loss: 27.9748\n",
      "Epoch [52/300], Step [45/172], Loss: 24.1325\n",
      "Epoch [52/300], Step [46/172], Loss: 41.7728\n",
      "Epoch [52/300], Step [47/172], Loss: 64.7223\n",
      "Epoch [52/300], Step [48/172], Loss: 67.8394\n",
      "Epoch [52/300], Step [49/172], Loss: 23.5176\n",
      "Epoch [52/300], Step [50/172], Loss: 52.3703\n",
      "Epoch [52/300], Step [51/172], Loss: 8.7596\n",
      "Epoch [52/300], Step [52/172], Loss: 23.6777\n",
      "Epoch [52/300], Step [53/172], Loss: 29.4141\n",
      "Epoch [52/300], Step [54/172], Loss: 18.5714\n",
      "Epoch [52/300], Step [55/172], Loss: 15.3859\n",
      "Epoch [52/300], Step [56/172], Loss: 11.5592\n",
      "Epoch [52/300], Step [57/172], Loss: 42.6874\n",
      "Epoch [52/300], Step [58/172], Loss: 20.7759\n",
      "Epoch [52/300], Step [59/172], Loss: 37.2147\n",
      "Epoch [52/300], Step [60/172], Loss: 60.1794\n",
      "Epoch [52/300], Step [61/172], Loss: 11.8200\n",
      "Epoch [52/300], Step [62/172], Loss: 21.1519\n",
      "Epoch [52/300], Step [63/172], Loss: 9.4473\n",
      "Epoch [52/300], Step [64/172], Loss: 7.3380\n",
      "Epoch [52/300], Step [65/172], Loss: 28.2607\n",
      "Epoch [52/300], Step [66/172], Loss: 8.0490\n",
      "Epoch [52/300], Step [67/172], Loss: 27.7197\n",
      "Epoch [52/300], Step [68/172], Loss: 14.0255\n",
      "Epoch [52/300], Step [69/172], Loss: 78.6032\n",
      "Epoch [52/300], Step [70/172], Loss: 70.9215\n",
      "Epoch [52/300], Step [71/172], Loss: 63.7358\n",
      "Epoch [52/300], Step [72/172], Loss: 64.7854\n",
      "Epoch [52/300], Step [73/172], Loss: 71.3156\n",
      "Epoch [52/300], Step [74/172], Loss: 45.6048\n",
      "Epoch [52/300], Step [75/172], Loss: 31.8618\n",
      "Epoch [52/300], Step [76/172], Loss: 48.9802\n",
      "Epoch [52/300], Step [77/172], Loss: 62.4761\n",
      "Epoch [52/300], Step [78/172], Loss: 59.7912\n",
      "Epoch [52/300], Step [79/172], Loss: 53.8052\n",
      "Epoch [52/300], Step [80/172], Loss: 57.1243\n",
      "Epoch [52/300], Step [81/172], Loss: 52.5768\n",
      "Epoch [52/300], Step [82/172], Loss: 45.7614\n",
      "Epoch [52/300], Step [83/172], Loss: 53.2500\n",
      "Epoch [52/300], Step [84/172], Loss: 44.3966\n",
      "Epoch [52/300], Step [85/172], Loss: 51.1469\n",
      "Epoch [52/300], Step [86/172], Loss: 39.6587\n",
      "Epoch [52/300], Step [87/172], Loss: 32.4364\n",
      "Epoch [52/300], Step [88/172], Loss: 35.3199\n",
      "Epoch [52/300], Step [89/172], Loss: 34.0725\n",
      "Epoch [52/300], Step [90/172], Loss: 33.4634\n",
      "Epoch [52/300], Step [91/172], Loss: 32.8740\n",
      "Epoch [52/300], Step [92/172], Loss: 26.9813\n",
      "Epoch [52/300], Step [93/172], Loss: 27.7880\n",
      "Epoch [52/300], Step [94/172], Loss: 32.1555\n",
      "Epoch [52/300], Step [95/172], Loss: 28.4796\n",
      "Epoch [52/300], Step [96/172], Loss: 24.5900\n",
      "Epoch [52/300], Step [97/172], Loss: 30.1209\n",
      "Epoch [52/300], Step [98/172], Loss: 25.4211\n",
      "Epoch [52/300], Step [99/172], Loss: 22.8658\n",
      "Epoch [52/300], Step [100/172], Loss: 21.7185\n",
      "Epoch [52/300], Step [101/172], Loss: 23.5867\n",
      "Epoch [52/300], Step [102/172], Loss: 21.6789\n",
      "Epoch [52/300], Step [103/172], Loss: 21.0674\n",
      "Epoch [52/300], Step [104/172], Loss: 20.8047\n",
      "Epoch [52/300], Step [105/172], Loss: 21.7284\n",
      "Epoch [52/300], Step [106/172], Loss: 21.5500\n",
      "Epoch [52/300], Step [107/172], Loss: 18.7278\n",
      "Epoch [52/300], Step [108/172], Loss: 22.1654\n",
      "Epoch [52/300], Step [109/172], Loss: 24.1598\n",
      "Epoch [52/300], Step [110/172], Loss: 20.3050\n",
      "Epoch [52/300], Step [111/172], Loss: 18.9250\n",
      "Epoch [52/300], Step [112/172], Loss: 24.7816\n",
      "Epoch [52/300], Step [113/172], Loss: 19.7243\n",
      "Epoch [52/300], Step [114/172], Loss: 19.8727\n",
      "Epoch [52/300], Step [115/172], Loss: 28.1815\n",
      "Epoch [52/300], Step [116/172], Loss: 19.6843\n",
      "Epoch [52/300], Step [117/172], Loss: 16.8539\n",
      "Epoch [52/300], Step [118/172], Loss: 20.3865\n",
      "Epoch [52/300], Step [119/172], Loss: 17.6034\n",
      "Epoch [52/300], Step [120/172], Loss: 15.6640\n",
      "Epoch [52/300], Step [121/172], Loss: 16.3469\n",
      "Epoch [52/300], Step [122/172], Loss: 14.6632\n",
      "Epoch [52/300], Step [123/172], Loss: 14.7735\n",
      "Epoch [52/300], Step [124/172], Loss: 12.4481\n",
      "Epoch [52/300], Step [125/172], Loss: 16.9156\n",
      "Epoch [52/300], Step [126/172], Loss: 15.5377\n",
      "Epoch [52/300], Step [127/172], Loss: 17.9425\n",
      "Epoch [52/300], Step [128/172], Loss: 18.5521\n",
      "Epoch [52/300], Step [129/172], Loss: 13.1571\n",
      "Epoch [52/300], Step [130/172], Loss: 15.2946\n",
      "Epoch [52/300], Step [131/172], Loss: 13.3903\n",
      "Epoch [52/300], Step [132/172], Loss: 13.3208\n",
      "Epoch [52/300], Step [133/172], Loss: 13.6671\n",
      "Epoch [52/300], Step [134/172], Loss: 14.3446\n",
      "Epoch [52/300], Step [135/172], Loss: 11.8581\n",
      "Epoch [52/300], Step [136/172], Loss: 12.1272\n",
      "Epoch [52/300], Step [137/172], Loss: 13.9234\n",
      "Epoch [52/300], Step [138/172], Loss: 11.8838\n",
      "Epoch [52/300], Step [139/172], Loss: 13.2994\n",
      "Epoch [52/300], Step [140/172], Loss: 13.3038\n",
      "Epoch [52/300], Step [141/172], Loss: 16.9419\n",
      "Epoch [52/300], Step [142/172], Loss: 15.6210\n",
      "Epoch [52/300], Step [143/172], Loss: 12.1733\n",
      "Epoch [52/300], Step [144/172], Loss: 11.6433\n",
      "Epoch [52/300], Step [145/172], Loss: 11.8344\n",
      "Epoch [52/300], Step [146/172], Loss: 12.3633\n",
      "Epoch [52/300], Step [147/172], Loss: 8.9944\n",
      "Epoch [52/300], Step [148/172], Loss: 9.7342\n",
      "Epoch [52/300], Step [149/172], Loss: 11.9586\n",
      "Epoch [52/300], Step [150/172], Loss: 11.5846\n",
      "Epoch [52/300], Step [151/172], Loss: 10.1843\n",
      "Epoch [52/300], Step [152/172], Loss: 10.2737\n",
      "Epoch [52/300], Step [153/172], Loss: 10.2115\n",
      "Epoch [52/300], Step [154/172], Loss: 11.3115\n",
      "Epoch [52/300], Step [155/172], Loss: 9.9006\n",
      "Epoch [52/300], Step [156/172], Loss: 12.0097\n",
      "Epoch [52/300], Step [157/172], Loss: 12.6149\n",
      "Epoch [52/300], Step [158/172], Loss: 10.7013\n",
      "Epoch [52/300], Step [159/172], Loss: 11.0469\n",
      "Epoch [52/300], Step [160/172], Loss: 10.8903\n",
      "Epoch [52/300], Step [161/172], Loss: 9.2571\n",
      "Epoch [52/300], Step [162/172], Loss: 9.7244\n",
      "Epoch [52/300], Step [163/172], Loss: 9.2963\n",
      "Epoch [52/300], Step [164/172], Loss: 11.5900\n",
      "Epoch [52/300], Step [165/172], Loss: 8.5701\n",
      "Epoch [52/300], Step [166/172], Loss: 9.1276\n",
      "Epoch [52/300], Step [167/172], Loss: 9.6156\n",
      "Epoch [52/300], Step [168/172], Loss: 8.8705\n",
      "Epoch [52/300], Step [169/172], Loss: 8.5465\n",
      "Epoch [52/300], Step [170/172], Loss: 8.3150\n",
      "Epoch [52/300], Step [171/172], Loss: 7.2626\n",
      "Epoch [52/300], Step [172/172], Loss: 6.5872\n",
      "Epoch [53/300], Step [1/172], Loss: 99.4533\n",
      "Epoch [53/300], Step [2/172], Loss: 97.8171\n",
      "Epoch [53/300], Step [3/172], Loss: 111.4984\n",
      "Epoch [53/300], Step [4/172], Loss: 67.0104\n",
      "Epoch [53/300], Step [5/172], Loss: 88.9335\n",
      "Epoch [53/300], Step [6/172], Loss: 33.2588\n",
      "Epoch [53/300], Step [7/172], Loss: 41.4164\n",
      "Epoch [53/300], Step [8/172], Loss: 12.1211\n",
      "Epoch [53/300], Step [9/172], Loss: 52.2442\n",
      "Epoch [53/300], Step [10/172], Loss: 59.6610\n",
      "Epoch [53/300], Step [11/172], Loss: 99.9071\n",
      "Epoch [53/300], Step [12/172], Loss: 87.3733\n",
      "Epoch [53/300], Step [13/172], Loss: 46.9854\n",
      "Epoch [53/300], Step [14/172], Loss: 99.6291\n",
      "Epoch [53/300], Step [15/172], Loss: 89.0954\n",
      "Epoch [53/300], Step [16/172], Loss: 58.8251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/300], Step [17/172], Loss: 61.2865\n",
      "Epoch [53/300], Step [18/172], Loss: 71.3424\n",
      "Epoch [53/300], Step [19/172], Loss: 80.8779\n",
      "Epoch [53/300], Step [20/172], Loss: 106.3950\n",
      "Epoch [53/300], Step [21/172], Loss: 97.0458\n",
      "Epoch [53/300], Step [22/172], Loss: 92.6935\n",
      "Epoch [53/300], Step [23/172], Loss: 20.5491\n",
      "Epoch [53/300], Step [24/172], Loss: 80.4006\n",
      "Epoch [53/300], Step [25/172], Loss: 55.0710\n",
      "Epoch [53/300], Step [26/172], Loss: 63.7057\n",
      "Epoch [53/300], Step [27/172], Loss: 87.5872\n",
      "Epoch [53/300], Step [28/172], Loss: 60.7497\n",
      "Epoch [53/300], Step [29/172], Loss: 65.6215\n",
      "Epoch [53/300], Step [30/172], Loss: 77.9600\n",
      "Epoch [53/300], Step [31/172], Loss: 45.9983\n",
      "Epoch [53/300], Step [32/172], Loss: 42.4946\n",
      "Epoch [53/300], Step [33/172], Loss: 77.3610\n",
      "Epoch [53/300], Step [34/172], Loss: 10.2131\n",
      "Epoch [53/300], Step [35/172], Loss: 50.3934\n",
      "Epoch [53/300], Step [36/172], Loss: 28.5763\n",
      "Epoch [53/300], Step [37/172], Loss: 21.1241\n",
      "Epoch [53/300], Step [38/172], Loss: 30.5076\n",
      "Epoch [53/300], Step [39/172], Loss: 55.2601\n",
      "Epoch [53/300], Step [40/172], Loss: 29.4960\n",
      "Epoch [53/300], Step [41/172], Loss: 42.7975\n",
      "Epoch [53/300], Step [42/172], Loss: 43.6562\n",
      "Epoch [53/300], Step [43/172], Loss: 32.0565\n",
      "Epoch [53/300], Step [44/172], Loss: 27.6666\n",
      "Epoch [53/300], Step [45/172], Loss: 24.0179\n",
      "Epoch [53/300], Step [46/172], Loss: 40.8839\n",
      "Epoch [53/300], Step [47/172], Loss: 63.9358\n",
      "Epoch [53/300], Step [48/172], Loss: 67.1423\n",
      "Epoch [53/300], Step [49/172], Loss: 23.3228\n",
      "Epoch [53/300], Step [50/172], Loss: 52.2299\n",
      "Epoch [53/300], Step [51/172], Loss: 8.7251\n",
      "Epoch [53/300], Step [52/172], Loss: 23.5474\n",
      "Epoch [53/300], Step [53/172], Loss: 29.3632\n",
      "Epoch [53/300], Step [54/172], Loss: 18.3231\n",
      "Epoch [53/300], Step [55/172], Loss: 15.3933\n",
      "Epoch [53/300], Step [56/172], Loss: 11.5725\n",
      "Epoch [53/300], Step [57/172], Loss: 42.2122\n",
      "Epoch [53/300], Step [58/172], Loss: 20.8170\n",
      "Epoch [53/300], Step [59/172], Loss: 37.0650\n",
      "Epoch [53/300], Step [60/172], Loss: 59.8668\n",
      "Epoch [53/300], Step [61/172], Loss: 11.7479\n",
      "Epoch [53/300], Step [62/172], Loss: 21.5719\n",
      "Epoch [53/300], Step [63/172], Loss: 9.5013\n",
      "Epoch [53/300], Step [64/172], Loss: 7.3884\n",
      "Epoch [53/300], Step [65/172], Loss: 28.2861\n",
      "Epoch [53/300], Step [66/172], Loss: 7.9942\n",
      "Epoch [53/300], Step [67/172], Loss: 27.7501\n",
      "Epoch [53/300], Step [68/172], Loss: 13.6541\n",
      "Epoch [53/300], Step [69/172], Loss: 78.0467\n",
      "Epoch [53/300], Step [70/172], Loss: 70.4686\n",
      "Epoch [53/300], Step [71/172], Loss: 63.3368\n",
      "Epoch [53/300], Step [72/172], Loss: 64.4408\n",
      "Epoch [53/300], Step [73/172], Loss: 70.9219\n",
      "Epoch [53/300], Step [74/172], Loss: 45.2243\n",
      "Epoch [53/300], Step [75/172], Loss: 31.9348\n",
      "Epoch [53/300], Step [76/172], Loss: 48.5875\n",
      "Epoch [53/300], Step [77/172], Loss: 62.4521\n",
      "Epoch [53/300], Step [78/172], Loss: 59.3511\n",
      "Epoch [53/300], Step [79/172], Loss: 53.7193\n",
      "Epoch [53/300], Step [80/172], Loss: 57.0005\n",
      "Epoch [53/300], Step [81/172], Loss: 52.0545\n",
      "Epoch [53/300], Step [82/172], Loss: 45.3058\n",
      "Epoch [53/300], Step [83/172], Loss: 53.0694\n",
      "Epoch [53/300], Step [84/172], Loss: 44.3039\n",
      "Epoch [53/300], Step [85/172], Loss: 50.9794\n",
      "Epoch [53/300], Step [86/172], Loss: 39.5614\n",
      "Epoch [53/300], Step [87/172], Loss: 32.3957\n",
      "Epoch [53/300], Step [88/172], Loss: 35.2527\n",
      "Epoch [53/300], Step [89/172], Loss: 33.9419\n",
      "Epoch [53/300], Step [90/172], Loss: 33.0679\n",
      "Epoch [53/300], Step [91/172], Loss: 32.8367\n",
      "Epoch [53/300], Step [92/172], Loss: 26.8939\n",
      "Epoch [53/300], Step [93/172], Loss: 27.6279\n",
      "Epoch [53/300], Step [94/172], Loss: 32.1609\n",
      "Epoch [53/300], Step [95/172], Loss: 28.3689\n",
      "Epoch [53/300], Step [96/172], Loss: 24.3931\n",
      "Epoch [53/300], Step [97/172], Loss: 30.0683\n",
      "Epoch [53/300], Step [98/172], Loss: 25.3278\n",
      "Epoch [53/300], Step [99/172], Loss: 22.7479\n",
      "Epoch [53/300], Step [100/172], Loss: 21.5713\n",
      "Epoch [53/300], Step [101/172], Loss: 23.4320\n",
      "Epoch [53/300], Step [102/172], Loss: 21.4195\n",
      "Epoch [53/300], Step [103/172], Loss: 20.8823\n",
      "Epoch [53/300], Step [104/172], Loss: 20.6209\n",
      "Epoch [53/300], Step [105/172], Loss: 21.5271\n",
      "Epoch [53/300], Step [106/172], Loss: 21.4141\n",
      "Epoch [53/300], Step [107/172], Loss: 18.6359\n",
      "Epoch [53/300], Step [108/172], Loss: 22.0423\n",
      "Epoch [53/300], Step [109/172], Loss: 23.8347\n",
      "Epoch [53/300], Step [110/172], Loss: 20.2198\n",
      "Epoch [53/300], Step [111/172], Loss: 18.7969\n",
      "Epoch [53/300], Step [112/172], Loss: 24.5850\n",
      "Epoch [53/300], Step [113/172], Loss: 19.6465\n",
      "Epoch [53/300], Step [114/172], Loss: 19.7046\n",
      "Epoch [53/300], Step [115/172], Loss: 27.7930\n",
      "Epoch [53/300], Step [116/172], Loss: 19.5249\n",
      "Epoch [53/300], Step [117/172], Loss: 16.7471\n",
      "Epoch [53/300], Step [118/172], Loss: 20.3038\n",
      "Epoch [53/300], Step [119/172], Loss: 17.5277\n",
      "Epoch [53/300], Step [120/172], Loss: 15.5295\n",
      "Epoch [53/300], Step [121/172], Loss: 16.1992\n",
      "Epoch [53/300], Step [122/172], Loss: 14.5763\n",
      "Epoch [53/300], Step [123/172], Loss: 14.6243\n",
      "Epoch [53/300], Step [124/172], Loss: 12.2832\n",
      "Epoch [53/300], Step [125/172], Loss: 16.7951\n",
      "Epoch [53/300], Step [126/172], Loss: 15.3706\n",
      "Epoch [53/300], Step [127/172], Loss: 17.7615\n",
      "Epoch [53/300], Step [128/172], Loss: 18.3764\n",
      "Epoch [53/300], Step [129/172], Loss: 13.0301\n",
      "Epoch [53/300], Step [130/172], Loss: 15.1664\n",
      "Epoch [53/300], Step [131/172], Loss: 13.2546\n",
      "Epoch [53/300], Step [132/172], Loss: 13.1779\n",
      "Epoch [53/300], Step [133/172], Loss: 13.5454\n",
      "Epoch [53/300], Step [134/172], Loss: 14.2461\n",
      "Epoch [53/300], Step [135/172], Loss: 11.7530\n",
      "Epoch [53/300], Step [136/172], Loss: 11.9586\n",
      "Epoch [53/300], Step [137/172], Loss: 13.7820\n",
      "Epoch [53/300], Step [138/172], Loss: 11.7506\n",
      "Epoch [53/300], Step [139/172], Loss: 13.1729\n",
      "Epoch [53/300], Step [140/172], Loss: 13.1821\n",
      "Epoch [53/300], Step [141/172], Loss: 16.6029\n",
      "Epoch [53/300], Step [142/172], Loss: 15.5462\n",
      "Epoch [53/300], Step [143/172], Loss: 12.0599\n",
      "Epoch [53/300], Step [144/172], Loss: 11.5606\n",
      "Epoch [53/300], Step [145/172], Loss: 11.7448\n",
      "Epoch [53/300], Step [146/172], Loss: 12.2670\n",
      "Epoch [53/300], Step [147/172], Loss: 8.8519\n",
      "Epoch [53/300], Step [148/172], Loss: 9.6138\n",
      "Epoch [53/300], Step [149/172], Loss: 11.8352\n",
      "Epoch [53/300], Step [150/172], Loss: 11.4477\n",
      "Epoch [53/300], Step [151/172], Loss: 10.0787\n",
      "Epoch [53/300], Step [152/172], Loss: 10.1656\n",
      "Epoch [53/300], Step [153/172], Loss: 10.0935\n",
      "Epoch [53/300], Step [154/172], Loss: 11.1769\n",
      "Epoch [53/300], Step [155/172], Loss: 9.7948\n",
      "Epoch [53/300], Step [156/172], Loss: 11.9895\n",
      "Epoch [53/300], Step [157/172], Loss: 12.5154\n",
      "Epoch [53/300], Step [158/172], Loss: 10.5850\n",
      "Epoch [53/300], Step [159/172], Loss: 10.9904\n",
      "Epoch [53/300], Step [160/172], Loss: 10.8214\n",
      "Epoch [53/300], Step [161/172], Loss: 9.1430\n",
      "Epoch [53/300], Step [162/172], Loss: 9.6529\n",
      "Epoch [53/300], Step [163/172], Loss: 9.1854\n",
      "Epoch [53/300], Step [164/172], Loss: 11.4643\n",
      "Epoch [53/300], Step [165/172], Loss: 8.4893\n",
      "Epoch [53/300], Step [166/172], Loss: 9.0244\n",
      "Epoch [53/300], Step [167/172], Loss: 9.5375\n",
      "Epoch [53/300], Step [168/172], Loss: 8.7782\n",
      "Epoch [53/300], Step [169/172], Loss: 8.4847\n",
      "Epoch [53/300], Step [170/172], Loss: 8.2172\n",
      "Epoch [53/300], Step [171/172], Loss: 7.1987\n",
      "Epoch [53/300], Step [172/172], Loss: 6.5475\n",
      "Epoch [54/300], Step [1/172], Loss: 99.1517\n",
      "Epoch [54/300], Step [2/172], Loss: 97.5622\n",
      "Epoch [54/300], Step [3/172], Loss: 110.4992\n",
      "Epoch [54/300], Step [4/172], Loss: 66.4163\n",
      "Epoch [54/300], Step [5/172], Loss: 88.4206\n",
      "Epoch [54/300], Step [6/172], Loss: 32.7470\n",
      "Epoch [54/300], Step [7/172], Loss: 40.9370\n",
      "Epoch [54/300], Step [8/172], Loss: 12.6141\n",
      "Epoch [54/300], Step [9/172], Loss: 52.5461\n",
      "Epoch [54/300], Step [10/172], Loss: 59.6061\n",
      "Epoch [54/300], Step [11/172], Loss: 100.1784\n",
      "Epoch [54/300], Step [12/172], Loss: 87.5003\n",
      "Epoch [54/300], Step [13/172], Loss: 46.9788\n",
      "Epoch [54/300], Step [14/172], Loss: 100.0597\n",
      "Epoch [54/300], Step [15/172], Loss: 89.4232\n",
      "Epoch [54/300], Step [16/172], Loss: 58.5205\n",
      "Epoch [54/300], Step [17/172], Loss: 61.5794\n",
      "Epoch [54/300], Step [18/172], Loss: 71.1255\n",
      "Epoch [54/300], Step [19/172], Loss: 81.3905\n",
      "Epoch [54/300], Step [20/172], Loss: 106.5515\n",
      "Epoch [54/300], Step [21/172], Loss: 97.4677\n",
      "Epoch [54/300], Step [22/172], Loss: 93.1826\n",
      "Epoch [54/300], Step [23/172], Loss: 20.1459\n",
      "Epoch [54/300], Step [24/172], Loss: 80.4304\n",
      "Epoch [54/300], Step [25/172], Loss: 55.2891\n",
      "Epoch [54/300], Step [26/172], Loss: 64.0454\n",
      "Epoch [54/300], Step [27/172], Loss: 88.3105\n",
      "Epoch [54/300], Step [28/172], Loss: 59.9135\n",
      "Epoch [54/300], Step [29/172], Loss: 64.3348\n",
      "Epoch [54/300], Step [30/172], Loss: 77.9700\n",
      "Epoch [54/300], Step [31/172], Loss: 45.8355\n",
      "Epoch [54/300], Step [32/172], Loss: 42.2377\n",
      "Epoch [54/300], Step [33/172], Loss: 76.5356\n",
      "Epoch [54/300], Step [34/172], Loss: 9.9303\n",
      "Epoch [54/300], Step [35/172], Loss: 48.6416\n",
      "Epoch [54/300], Step [36/172], Loss: 27.8325\n",
      "Epoch [54/300], Step [37/172], Loss: 20.5636\n",
      "Epoch [54/300], Step [38/172], Loss: 29.4713\n",
      "Epoch [54/300], Step [39/172], Loss: 53.9940\n",
      "Epoch [54/300], Step [40/172], Loss: 28.3059\n",
      "Epoch [54/300], Step [41/172], Loss: 41.3219\n",
      "Epoch [54/300], Step [42/172], Loss: 41.8890\n",
      "Epoch [54/300], Step [43/172], Loss: 30.3746\n",
      "Epoch [54/300], Step [44/172], Loss: 25.9576\n",
      "Epoch [54/300], Step [45/172], Loss: 22.6295\n",
      "Epoch [54/300], Step [46/172], Loss: 38.0991\n",
      "Epoch [54/300], Step [47/172], Loss: 60.4168\n",
      "Epoch [54/300], Step [48/172], Loss: 62.4438\n",
      "Epoch [54/300], Step [49/172], Loss: 21.6189\n",
      "Epoch [54/300], Step [50/172], Loss: 49.9178\n",
      "Epoch [54/300], Step [51/172], Loss: 7.9193\n",
      "Epoch [54/300], Step [52/172], Loss: 21.3135\n",
      "Epoch [54/300], Step [53/172], Loss: 26.9352\n",
      "Epoch [54/300], Step [54/172], Loss: 15.9059\n",
      "Epoch [54/300], Step [55/172], Loss: 13.4257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/300], Step [56/172], Loss: 10.3939\n",
      "Epoch [54/300], Step [57/172], Loss: 38.1570\n",
      "Epoch [54/300], Step [58/172], Loss: 19.1337\n",
      "Epoch [54/300], Step [59/172], Loss: 34.3298\n",
      "Epoch [54/300], Step [60/172], Loss: 57.0585\n",
      "Epoch [54/300], Step [61/172], Loss: 11.1303\n",
      "Epoch [54/300], Step [62/172], Loss: 19.3094\n",
      "Epoch [54/300], Step [63/172], Loss: 8.3858\n",
      "Epoch [54/300], Step [64/172], Loss: 6.5974\n",
      "Epoch [54/300], Step [65/172], Loss: 25.4807\n",
      "Epoch [54/300], Step [66/172], Loss: 6.8752\n",
      "Epoch [54/300], Step [67/172], Loss: 25.0098\n",
      "Epoch [54/300], Step [68/172], Loss: 10.9427\n",
      "Epoch [54/300], Step [69/172], Loss: 78.1990\n",
      "Epoch [54/300], Step [70/172], Loss: 80.6208\n",
      "Epoch [54/300], Step [71/172], Loss: 70.1399\n",
      "Epoch [54/300], Step [72/172], Loss: 72.6698\n",
      "Epoch [54/300], Step [73/172], Loss: 78.9380\n",
      "Epoch [54/300], Step [74/172], Loss: 52.8308\n",
      "Epoch [54/300], Step [75/172], Loss: 37.3839\n",
      "Epoch [54/300], Step [76/172], Loss: 54.1297\n",
      "Epoch [54/300], Step [77/172], Loss: 66.9057\n",
      "Epoch [54/300], Step [78/172], Loss: 63.9065\n",
      "Epoch [54/300], Step [79/172], Loss: 56.8924\n",
      "Epoch [54/300], Step [80/172], Loss: 60.6196\n",
      "Epoch [54/300], Step [81/172], Loss: 54.3024\n",
      "Epoch [54/300], Step [82/172], Loss: 48.7695\n",
      "Epoch [54/300], Step [83/172], Loss: 55.3933\n",
      "Epoch [54/300], Step [84/172], Loss: 45.9690\n",
      "Epoch [54/300], Step [85/172], Loss: 52.6577\n",
      "Epoch [54/300], Step [86/172], Loss: 41.6588\n",
      "Epoch [54/300], Step [87/172], Loss: 34.0089\n",
      "Epoch [54/300], Step [88/172], Loss: 37.0929\n",
      "Epoch [54/300], Step [89/172], Loss: 35.3758\n",
      "Epoch [54/300], Step [90/172], Loss: 33.8838\n",
      "Epoch [54/300], Step [91/172], Loss: 34.2471\n",
      "Epoch [54/300], Step [92/172], Loss: 28.1920\n",
      "Epoch [54/300], Step [93/172], Loss: 29.0711\n",
      "Epoch [54/300], Step [94/172], Loss: 33.6532\n",
      "Epoch [54/300], Step [95/172], Loss: 29.4829\n",
      "Epoch [54/300], Step [96/172], Loss: 25.1093\n",
      "Epoch [54/300], Step [97/172], Loss: 30.2515\n",
      "Epoch [54/300], Step [98/172], Loss: 25.3853\n",
      "Epoch [54/300], Step [99/172], Loss: 22.8842\n",
      "Epoch [54/300], Step [100/172], Loss: 21.5095\n",
      "Epoch [54/300], Step [101/172], Loss: 23.2906\n",
      "Epoch [54/300], Step [102/172], Loss: 21.3913\n",
      "Epoch [54/300], Step [103/172], Loss: 20.6376\n",
      "Epoch [54/300], Step [104/172], Loss: 20.3860\n",
      "Epoch [54/300], Step [105/172], Loss: 21.4176\n",
      "Epoch [54/300], Step [106/172], Loss: 21.2475\n",
      "Epoch [54/300], Step [107/172], Loss: 18.3697\n",
      "Epoch [54/300], Step [108/172], Loss: 21.6594\n",
      "Epoch [54/300], Step [109/172], Loss: 23.3504\n",
      "Epoch [54/300], Step [110/172], Loss: 19.8034\n",
      "Epoch [54/300], Step [111/172], Loss: 18.3529\n",
      "Epoch [54/300], Step [112/172], Loss: 24.1693\n",
      "Epoch [54/300], Step [113/172], Loss: 19.2146\n",
      "Epoch [54/300], Step [114/172], Loss: 19.1184\n",
      "Epoch [54/300], Step [115/172], Loss: 27.1350\n",
      "Epoch [54/300], Step [116/172], Loss: 18.9163\n",
      "Epoch [54/300], Step [117/172], Loss: 16.1267\n",
      "Epoch [54/300], Step [118/172], Loss: 19.6720\n",
      "Epoch [54/300], Step [119/172], Loss: 16.8573\n",
      "Epoch [54/300], Step [120/172], Loss: 14.8473\n",
      "Epoch [54/300], Step [121/172], Loss: 15.3383\n",
      "Epoch [54/300], Step [122/172], Loss: 13.9691\n",
      "Epoch [54/300], Step [123/172], Loss: 13.8564\n",
      "Epoch [54/300], Step [124/172], Loss: 11.5687\n",
      "Epoch [54/300], Step [125/172], Loss: 15.9086\n",
      "Epoch [54/300], Step [126/172], Loss: 14.5481\n",
      "Epoch [54/300], Step [127/172], Loss: 16.7541\n",
      "Epoch [54/300], Step [128/172], Loss: 17.2172\n",
      "Epoch [54/300], Step [129/172], Loss: 12.2279\n",
      "Epoch [54/300], Step [130/172], Loss: 14.2764\n",
      "Epoch [54/300], Step [131/172], Loss: 12.4205\n",
      "Epoch [54/300], Step [132/172], Loss: 12.3049\n",
      "Epoch [54/300], Step [133/172], Loss: 12.6610\n",
      "Epoch [54/300], Step [134/172], Loss: 13.2954\n",
      "Epoch [54/300], Step [135/172], Loss: 10.9144\n",
      "Epoch [54/300], Step [136/172], Loss: 11.0018\n",
      "Epoch [54/300], Step [137/172], Loss: 12.8112\n",
      "Epoch [54/300], Step [138/172], Loss: 10.9388\n",
      "Epoch [54/300], Step [139/172], Loss: 12.2342\n",
      "Epoch [54/300], Step [140/172], Loss: 12.2600\n",
      "Epoch [54/300], Step [141/172], Loss: 15.5666\n",
      "Epoch [54/300], Step [142/172], Loss: 14.4558\n",
      "Epoch [54/300], Step [143/172], Loss: 11.2185\n",
      "Epoch [54/300], Step [144/172], Loss: 10.7373\n",
      "Epoch [54/300], Step [145/172], Loss: 10.8628\n",
      "Epoch [54/300], Step [146/172], Loss: 11.2845\n",
      "Epoch [54/300], Step [147/172], Loss: 8.0856\n",
      "Epoch [54/300], Step [148/172], Loss: 8.8290\n",
      "Epoch [54/300], Step [149/172], Loss: 10.9123\n",
      "Epoch [54/300], Step [150/172], Loss: 10.5097\n",
      "Epoch [54/300], Step [151/172], Loss: 9.2826\n",
      "Epoch [54/300], Step [152/172], Loss: 9.3683\n",
      "Epoch [54/300], Step [153/172], Loss: 9.2724\n",
      "Epoch [54/300], Step [154/172], Loss: 10.2772\n",
      "Epoch [54/300], Step [155/172], Loss: 9.0284\n",
      "Epoch [54/300], Step [156/172], Loss: 11.2068\n",
      "Epoch [54/300], Step [157/172], Loss: 11.5956\n",
      "Epoch [54/300], Step [158/172], Loss: 9.7755\n",
      "Epoch [54/300], Step [159/172], Loss: 10.1505\n",
      "Epoch [54/300], Step [160/172], Loss: 10.0194\n",
      "Epoch [54/300], Step [161/172], Loss: 8.4021\n",
      "Epoch [54/300], Step [162/172], Loss: 8.9757\n",
      "Epoch [54/300], Step [163/172], Loss: 8.4676\n",
      "Epoch [54/300], Step [164/172], Loss: 10.7507\n",
      "Epoch [54/300], Step [165/172], Loss: 7.8279\n",
      "Epoch [54/300], Step [166/172], Loss: 8.3279\n",
      "Epoch [54/300], Step [167/172], Loss: 8.8125\n",
      "Epoch [54/300], Step [168/172], Loss: 8.0969\n",
      "Epoch [54/300], Step [169/172], Loss: 7.8208\n",
      "Epoch [54/300], Step [170/172], Loss: 7.5379\n",
      "Epoch [54/300], Step [171/172], Loss: 6.6271\n",
      "Epoch [54/300], Step [172/172], Loss: 6.0483\n",
      "Epoch [55/300], Step [1/172], Loss: 101.5042\n",
      "Epoch [55/300], Step [2/172], Loss: 99.9272\n",
      "Epoch [55/300], Step [3/172], Loss: 113.3611\n",
      "Epoch [55/300], Step [4/172], Loss: 67.6796\n",
      "Epoch [55/300], Step [5/172], Loss: 90.7759\n",
      "Epoch [55/300], Step [6/172], Loss: 33.6496\n",
      "Epoch [55/300], Step [7/172], Loss: 42.3961\n",
      "Epoch [55/300], Step [8/172], Loss: 12.4416\n",
      "Epoch [55/300], Step [9/172], Loss: 53.4609\n",
      "Epoch [55/300], Step [10/172], Loss: 60.5845\n",
      "Epoch [55/300], Step [11/172], Loss: 102.3090\n",
      "Epoch [55/300], Step [12/172], Loss: 89.3825\n",
      "Epoch [55/300], Step [13/172], Loss: 47.4077\n",
      "Epoch [55/300], Step [14/172], Loss: 101.7327\n",
      "Epoch [55/300], Step [15/172], Loss: 90.9538\n",
      "Epoch [55/300], Step [16/172], Loss: 58.1152\n",
      "Epoch [55/300], Step [17/172], Loss: 62.3124\n",
      "Epoch [55/300], Step [18/172], Loss: 71.5386\n",
      "Epoch [55/300], Step [19/172], Loss: 82.1443\n",
      "Epoch [55/300], Step [20/172], Loss: 107.6855\n",
      "Epoch [55/300], Step [21/172], Loss: 98.5002\n",
      "Epoch [55/300], Step [22/172], Loss: 93.6631\n",
      "Epoch [55/300], Step [23/172], Loss: 20.1492\n",
      "Epoch [55/300], Step [24/172], Loss: 81.3038\n",
      "Epoch [55/300], Step [25/172], Loss: 55.6802\n",
      "Epoch [55/300], Step [26/172], Loss: 64.5419\n",
      "Epoch [55/300], Step [27/172], Loss: 88.8741\n",
      "Epoch [55/300], Step [28/172], Loss: 59.9464\n",
      "Epoch [55/300], Step [29/172], Loss: 64.2554\n",
      "Epoch [55/300], Step [30/172], Loss: 78.9291\n",
      "Epoch [55/300], Step [31/172], Loss: 46.5392\n",
      "Epoch [55/300], Step [32/172], Loss: 42.8885\n",
      "Epoch [55/300], Step [33/172], Loss: 78.1260\n",
      "Epoch [55/300], Step [34/172], Loss: 9.7854\n",
      "Epoch [55/300], Step [35/172], Loss: 47.9165\n",
      "Epoch [55/300], Step [36/172], Loss: 28.2080\n",
      "Epoch [55/300], Step [37/172], Loss: 20.8508\n",
      "Epoch [55/300], Step [38/172], Loss: 30.0796\n",
      "Epoch [55/300], Step [39/172], Loss: 55.2150\n",
      "Epoch [55/300], Step [40/172], Loss: 29.1367\n",
      "Epoch [55/300], Step [41/172], Loss: 42.8173\n",
      "Epoch [55/300], Step [42/172], Loss: 44.3285\n",
      "Epoch [55/300], Step [43/172], Loss: 31.4888\n",
      "Epoch [55/300], Step [44/172], Loss: 28.0832\n",
      "Epoch [55/300], Step [45/172], Loss: 23.9033\n",
      "Epoch [55/300], Step [46/172], Loss: 39.6317\n",
      "Epoch [55/300], Step [47/172], Loss: 63.1549\n",
      "Epoch [55/300], Step [48/172], Loss: 66.5271\n",
      "Epoch [55/300], Step [49/172], Loss: 22.7803\n",
      "Epoch [55/300], Step [50/172], Loss: 51.9216\n",
      "Epoch [55/300], Step [51/172], Loss: 8.4722\n",
      "Epoch [55/300], Step [52/172], Loss: 23.2673\n",
      "Epoch [55/300], Step [53/172], Loss: 28.6861\n",
      "Epoch [55/300], Step [54/172], Loss: 17.8250\n",
      "Epoch [55/300], Step [55/172], Loss: 15.3026\n",
      "Epoch [55/300], Step [56/172], Loss: 11.1766\n",
      "Epoch [55/300], Step [57/172], Loss: 40.7143\n",
      "Epoch [55/300], Step [58/172], Loss: 20.5696\n",
      "Epoch [55/300], Step [59/172], Loss: 36.6758\n",
      "Epoch [55/300], Step [60/172], Loss: 59.1328\n",
      "Epoch [55/300], Step [61/172], Loss: 11.3551\n",
      "Epoch [55/300], Step [62/172], Loss: 21.3770\n",
      "Epoch [55/300], Step [63/172], Loss: 8.8902\n",
      "Epoch [55/300], Step [64/172], Loss: 6.9963\n",
      "Epoch [55/300], Step [65/172], Loss: 27.5458\n",
      "Epoch [55/300], Step [66/172], Loss: 7.6288\n",
      "Epoch [55/300], Step [67/172], Loss: 27.1952\n",
      "Epoch [55/300], Step [68/172], Loss: 13.3671\n",
      "Epoch [55/300], Step [69/172], Loss: 77.3432\n",
      "Epoch [55/300], Step [70/172], Loss: 71.9450\n",
      "Epoch [55/300], Step [71/172], Loss: 64.7503\n",
      "Epoch [55/300], Step [72/172], Loss: 65.8179\n",
      "Epoch [55/300], Step [73/172], Loss: 71.7332\n",
      "Epoch [55/300], Step [74/172], Loss: 46.0816\n",
      "Epoch [55/300], Step [75/172], Loss: 32.7707\n",
      "Epoch [55/300], Step [76/172], Loss: 48.2734\n",
      "Epoch [55/300], Step [77/172], Loss: 62.2540\n",
      "Epoch [55/300], Step [78/172], Loss: 58.9105\n",
      "Epoch [55/300], Step [79/172], Loss: 53.3562\n",
      "Epoch [55/300], Step [80/172], Loss: 56.9183\n",
      "Epoch [55/300], Step [81/172], Loss: 50.6327\n",
      "Epoch [55/300], Step [82/172], Loss: 44.3795\n",
      "Epoch [55/300], Step [83/172], Loss: 52.0451\n",
      "Epoch [55/300], Step [84/172], Loss: 43.2545\n",
      "Epoch [55/300], Step [85/172], Loss: 49.5392\n",
      "Epoch [55/300], Step [86/172], Loss: 38.4391\n",
      "Epoch [55/300], Step [87/172], Loss: 31.4766\n",
      "Epoch [55/300], Step [88/172], Loss: 34.0429\n",
      "Epoch [55/300], Step [89/172], Loss: 32.6902\n",
      "Epoch [55/300], Step [90/172], Loss: 31.4535\n",
      "Epoch [55/300], Step [91/172], Loss: 31.8701\n",
      "Epoch [55/300], Step [92/172], Loss: 25.8997\n",
      "Epoch [55/300], Step [93/172], Loss: 26.4436\n",
      "Epoch [55/300], Step [94/172], Loss: 31.1077\n",
      "Epoch [55/300], Step [95/172], Loss: 27.3306\n",
      "Epoch [55/300], Step [96/172], Loss: 23.4394\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/300], Step [97/172], Loss: 29.1643\n",
      "Epoch [55/300], Step [98/172], Loss: 24.3801\n",
      "Epoch [55/300], Step [99/172], Loss: 21.8402\n",
      "Epoch [55/300], Step [100/172], Loss: 20.7663\n",
      "Epoch [55/300], Step [101/172], Loss: 22.5388\n",
      "Epoch [55/300], Step [102/172], Loss: 20.6196\n",
      "Epoch [55/300], Step [103/172], Loss: 20.0645\n",
      "Epoch [55/300], Step [104/172], Loss: 19.8381\n",
      "Epoch [55/300], Step [105/172], Loss: 20.8430\n",
      "Epoch [55/300], Step [106/172], Loss: 20.7310\n",
      "Epoch [55/300], Step [107/172], Loss: 18.0224\n",
      "Epoch [55/300], Step [108/172], Loss: 21.3214\n",
      "Epoch [55/300], Step [109/172], Loss: 22.9486\n",
      "Epoch [55/300], Step [110/172], Loss: 19.6963\n",
      "Epoch [55/300], Step [111/172], Loss: 18.2079\n",
      "Epoch [55/300], Step [112/172], Loss: 23.8040\n",
      "Epoch [55/300], Step [113/172], Loss: 19.1856\n",
      "Epoch [55/300], Step [114/172], Loss: 19.0917\n",
      "Epoch [55/300], Step [115/172], Loss: 26.7969\n",
      "Epoch [55/300], Step [116/172], Loss: 18.9900\n",
      "Epoch [55/300], Step [117/172], Loss: 16.2121\n",
      "Epoch [55/300], Step [118/172], Loss: 19.7299\n",
      "Epoch [55/300], Step [119/172], Loss: 17.1614\n",
      "Epoch [55/300], Step [120/172], Loss: 15.0943\n",
      "Epoch [55/300], Step [121/172], Loss: 15.7428\n",
      "Epoch [55/300], Step [122/172], Loss: 14.1503\n",
      "Epoch [55/300], Step [123/172], Loss: 14.1271\n",
      "Epoch [55/300], Step [124/172], Loss: 11.8760\n",
      "Epoch [55/300], Step [125/172], Loss: 16.4171\n",
      "Epoch [55/300], Step [126/172], Loss: 14.9192\n",
      "Epoch [55/300], Step [127/172], Loss: 17.3198\n",
      "Epoch [55/300], Step [128/172], Loss: 17.9461\n",
      "Epoch [55/300], Step [129/172], Loss: 12.6604\n",
      "Epoch [55/300], Step [130/172], Loss: 14.7769\n",
      "Epoch [55/300], Step [131/172], Loss: 12.8909\n",
      "Epoch [55/300], Step [132/172], Loss: 12.8121\n",
      "Epoch [55/300], Step [133/172], Loss: 13.2124\n",
      "Epoch [55/300], Step [134/172], Loss: 13.9549\n",
      "Epoch [55/300], Step [135/172], Loss: 11.4695\n",
      "Epoch [55/300], Step [136/172], Loss: 11.6211\n",
      "Epoch [55/300], Step [137/172], Loss: 13.4548\n",
      "Epoch [55/300], Step [138/172], Loss: 11.4871\n",
      "Epoch [55/300], Step [139/172], Loss: 12.8619\n",
      "Epoch [55/300], Step [140/172], Loss: 12.8711\n",
      "Epoch [55/300], Step [141/172], Loss: 16.0565\n",
      "Epoch [55/300], Step [142/172], Loss: 15.3312\n",
      "Epoch [55/300], Step [143/172], Loss: 11.7712\n",
      "Epoch [55/300], Step [144/172], Loss: 11.3660\n",
      "Epoch [55/300], Step [145/172], Loss: 11.5145\n",
      "Epoch [55/300], Step [146/172], Loss: 12.0560\n",
      "Epoch [55/300], Step [147/172], Loss: 8.5533\n",
      "Epoch [55/300], Step [148/172], Loss: 9.3306\n",
      "Epoch [55/300], Step [149/172], Loss: 11.5372\n",
      "Epoch [55/300], Step [150/172], Loss: 11.1820\n",
      "Epoch [55/300], Step [151/172], Loss: 9.8606\n",
      "Epoch [55/300], Step [152/172], Loss: 9.9210\n",
      "Epoch [55/300], Step [153/172], Loss: 9.8435\n",
      "Epoch [55/300], Step [154/172], Loss: 10.8801\n",
      "Epoch [55/300], Step [155/172], Loss: 9.5870\n",
      "Epoch [55/300], Step [156/172], Loss: 11.8629\n",
      "Epoch [55/300], Step [157/172], Loss: 12.2835\n",
      "Epoch [55/300], Step [158/172], Loss: 10.3293\n",
      "Epoch [55/300], Step [159/172], Loss: 10.8379\n",
      "Epoch [55/300], Step [160/172], Loss: 10.6501\n",
      "Epoch [55/300], Step [161/172], Loss: 8.9520\n",
      "Epoch [55/300], Step [162/172], Loss: 9.4759\n",
      "Epoch [55/300], Step [163/172], Loss: 8.9713\n",
      "Epoch [55/300], Step [164/172], Loss: 11.2745\n",
      "Epoch [55/300], Step [165/172], Loss: 8.3048\n",
      "Epoch [55/300], Step [166/172], Loss: 8.8675\n",
      "Epoch [55/300], Step [167/172], Loss: 9.3835\n",
      "Epoch [55/300], Step [168/172], Loss: 8.5965\n",
      "Epoch [55/300], Step [169/172], Loss: 8.3923\n",
      "Epoch [55/300], Step [170/172], Loss: 8.0512\n",
      "Epoch [55/300], Step [171/172], Loss: 7.0921\n",
      "Epoch [55/300], Step [172/172], Loss: 6.4878\n",
      "Epoch [56/300], Step [1/172], Loss: 99.1307\n",
      "Epoch [56/300], Step [2/172], Loss: 97.7937\n",
      "Epoch [56/300], Step [3/172], Loss: 107.9022\n",
      "Epoch [56/300], Step [4/172], Loss: 65.1816\n",
      "Epoch [56/300], Step [5/172], Loss: 87.8878\n",
      "Epoch [56/300], Step [6/172], Loss: 32.1273\n",
      "Epoch [56/300], Step [7/172], Loss: 40.4996\n",
      "Epoch [56/300], Step [8/172], Loss: 12.7959\n",
      "Epoch [56/300], Step [9/172], Loss: 52.6860\n",
      "Epoch [56/300], Step [10/172], Loss: 59.3921\n",
      "Epoch [56/300], Step [11/172], Loss: 100.3360\n",
      "Epoch [56/300], Step [12/172], Loss: 87.5633\n",
      "Epoch [56/300], Step [13/172], Loss: 46.5473\n",
      "Epoch [56/300], Step [14/172], Loss: 99.9385\n",
      "Epoch [56/300], Step [15/172], Loss: 89.2618\n",
      "Epoch [56/300], Step [16/172], Loss: 56.9674\n",
      "Epoch [56/300], Step [17/172], Loss: 61.7278\n",
      "Epoch [56/300], Step [18/172], Loss: 70.5234\n",
      "Epoch [56/300], Step [19/172], Loss: 81.9234\n",
      "Epoch [56/300], Step [20/172], Loss: 106.0478\n",
      "Epoch [56/300], Step [21/172], Loss: 98.0273\n",
      "Epoch [56/300], Step [22/172], Loss: 93.2002\n",
      "Epoch [56/300], Step [23/172], Loss: 18.4680\n",
      "Epoch [56/300], Step [24/172], Loss: 80.4714\n",
      "Epoch [56/300], Step [25/172], Loss: 55.2801\n",
      "Epoch [56/300], Step [26/172], Loss: 64.1283\n",
      "Epoch [56/300], Step [27/172], Loss: 88.7392\n",
      "Epoch [56/300], Step [28/172], Loss: 59.2456\n",
      "Epoch [56/300], Step [29/172], Loss: 62.4179\n",
      "Epoch [56/300], Step [30/172], Loss: 78.8578\n",
      "Epoch [56/300], Step [31/172], Loss: 46.3505\n",
      "Epoch [56/300], Step [32/172], Loss: 42.8316\n",
      "Epoch [56/300], Step [33/172], Loss: 77.9604\n",
      "Epoch [56/300], Step [34/172], Loss: 9.7781\n",
      "Epoch [56/300], Step [35/172], Loss: 45.7551\n",
      "Epoch [56/300], Step [36/172], Loss: 27.8892\n",
      "Epoch [56/300], Step [37/172], Loss: 20.9180\n",
      "Epoch [56/300], Step [38/172], Loss: 30.0711\n",
      "Epoch [56/300], Step [39/172], Loss: 55.2700\n",
      "Epoch [56/300], Step [40/172], Loss: 29.0794\n",
      "Epoch [56/300], Step [41/172], Loss: 43.1495\n",
      "Epoch [56/300], Step [42/172], Loss: 44.4466\n",
      "Epoch [56/300], Step [43/172], Loss: 31.6613\n",
      "Epoch [56/300], Step [44/172], Loss: 27.7494\n",
      "Epoch [56/300], Step [45/172], Loss: 23.9151\n",
      "Epoch [56/300], Step [46/172], Loss: 38.9553\n",
      "Epoch [56/300], Step [47/172], Loss: 62.5589\n",
      "Epoch [56/300], Step [48/172], Loss: 65.8869\n",
      "Epoch [56/300], Step [49/172], Loss: 23.1284\n",
      "Epoch [56/300], Step [50/172], Loss: 51.8456\n",
      "Epoch [56/300], Step [51/172], Loss: 8.5462\n",
      "Epoch [56/300], Step [52/172], Loss: 23.2848\n",
      "Epoch [56/300], Step [53/172], Loss: 29.0075\n",
      "Epoch [56/300], Step [54/172], Loss: 17.6675\n",
      "Epoch [56/300], Step [55/172], Loss: 15.1631\n",
      "Epoch [56/300], Step [56/172], Loss: 11.3071\n",
      "Epoch [56/300], Step [57/172], Loss: 40.2446\n",
      "Epoch [56/300], Step [58/172], Loss: 20.7609\n",
      "Epoch [56/300], Step [59/172], Loss: 36.6708\n",
      "Epoch [56/300], Step [60/172], Loss: 58.7304\n",
      "Epoch [56/300], Step [61/172], Loss: 11.4711\n",
      "Epoch [56/300], Step [62/172], Loss: 21.5884\n",
      "Epoch [56/300], Step [63/172], Loss: 9.1654\n",
      "Epoch [56/300], Step [64/172], Loss: 7.2455\n",
      "Epoch [56/300], Step [65/172], Loss: 27.3481\n",
      "Epoch [56/300], Step [66/172], Loss: 7.7463\n",
      "Epoch [56/300], Step [67/172], Loss: 27.4987\n",
      "Epoch [56/300], Step [68/172], Loss: 13.1461\n",
      "Epoch [56/300], Step [69/172], Loss: 76.7382\n",
      "Epoch [56/300], Step [70/172], Loss: 70.1701\n",
      "Epoch [56/300], Step [71/172], Loss: 62.5208\n",
      "Epoch [56/300], Step [72/172], Loss: 64.1432\n",
      "Epoch [56/300], Step [73/172], Loss: 70.8162\n",
      "Epoch [56/300], Step [74/172], Loss: 44.2168\n",
      "Epoch [56/300], Step [75/172], Loss: 31.6109\n",
      "Epoch [56/300], Step [76/172], Loss: 47.5628\n",
      "Epoch [56/300], Step [77/172], Loss: 61.8343\n",
      "Epoch [56/300], Step [78/172], Loss: 58.1465\n",
      "Epoch [56/300], Step [79/172], Loss: 53.1186\n",
      "Epoch [56/300], Step [80/172], Loss: 56.5484\n",
      "Epoch [56/300], Step [81/172], Loss: 50.2980\n",
      "Epoch [56/300], Step [82/172], Loss: 44.4067\n",
      "Epoch [56/300], Step [83/172], Loss: 52.3588\n",
      "Epoch [56/300], Step [84/172], Loss: 43.3609\n",
      "Epoch [56/300], Step [85/172], Loss: 49.3754\n",
      "Epoch [56/300], Step [86/172], Loss: 38.6739\n",
      "Epoch [56/300], Step [87/172], Loss: 31.5299\n",
      "Epoch [56/300], Step [88/172], Loss: 34.2147\n",
      "Epoch [56/300], Step [89/172], Loss: 32.7675\n",
      "Epoch [56/300], Step [90/172], Loss: 31.5845\n",
      "Epoch [56/300], Step [91/172], Loss: 32.0206\n",
      "Epoch [56/300], Step [92/172], Loss: 26.0932\n",
      "Epoch [56/300], Step [93/172], Loss: 26.5342\n",
      "Epoch [56/300], Step [94/172], Loss: 31.3416\n",
      "Epoch [56/300], Step [95/172], Loss: 27.4481\n",
      "Epoch [56/300], Step [96/172], Loss: 23.4539\n",
      "Epoch [56/300], Step [97/172], Loss: 29.2993\n",
      "Epoch [56/300], Step [98/172], Loss: 24.4668\n",
      "Epoch [56/300], Step [99/172], Loss: 21.9306\n",
      "Epoch [56/300], Step [100/172], Loss: 20.7744\n",
      "Epoch [56/300], Step [101/172], Loss: 22.5725\n",
      "Epoch [56/300], Step [102/172], Loss: 20.8001\n",
      "Epoch [56/300], Step [103/172], Loss: 20.0279\n",
      "Epoch [56/300], Step [104/172], Loss: 19.8578\n",
      "Epoch [56/300], Step [105/172], Loss: 21.0160\n",
      "Epoch [56/300], Step [106/172], Loss: 20.7379\n",
      "Epoch [56/300], Step [107/172], Loss: 18.1206\n",
      "Epoch [56/300], Step [108/172], Loss: 21.4055\n",
      "Epoch [56/300], Step [109/172], Loss: 22.9545\n",
      "Epoch [56/300], Step [110/172], Loss: 19.7637\n",
      "Epoch [56/300], Step [111/172], Loss: 18.2382\n",
      "Epoch [56/300], Step [112/172], Loss: 23.8366\n",
      "Epoch [56/300], Step [113/172], Loss: 19.1552\n",
      "Epoch [56/300], Step [114/172], Loss: 19.1058\n",
      "Epoch [56/300], Step [115/172], Loss: 26.7408\n",
      "Epoch [56/300], Step [116/172], Loss: 18.9834\n",
      "Epoch [56/300], Step [117/172], Loss: 16.2263\n",
      "Epoch [56/300], Step [118/172], Loss: 19.7964\n",
      "Epoch [56/300], Step [119/172], Loss: 17.2109\n",
      "Epoch [56/300], Step [120/172], Loss: 15.0950\n",
      "Epoch [56/300], Step [121/172], Loss: 15.6837\n",
      "Epoch [56/300], Step [122/172], Loss: 14.3296\n",
      "Epoch [56/300], Step [123/172], Loss: 14.1420\n",
      "Epoch [56/300], Step [124/172], Loss: 11.8395\n",
      "Epoch [56/300], Step [125/172], Loss: 16.3541\n",
      "Epoch [56/300], Step [126/172], Loss: 14.8637\n",
      "Epoch [56/300], Step [127/172], Loss: 17.3405\n",
      "Epoch [56/300], Step [128/172], Loss: 17.9235\n",
      "Epoch [56/300], Step [129/172], Loss: 12.6395\n",
      "Epoch [56/300], Step [130/172], Loss: 14.7747\n",
      "Epoch [56/300], Step [131/172], Loss: 12.9014\n",
      "Epoch [56/300], Step [132/172], Loss: 12.7671\n",
      "Epoch [56/300], Step [133/172], Loss: 13.2794\n",
      "Epoch [56/300], Step [134/172], Loss: 14.0118\n",
      "Epoch [56/300], Step [135/172], Loss: 11.4677\n",
      "Epoch [56/300], Step [136/172], Loss: 11.5724\n",
      "Epoch [56/300], Step [137/172], Loss: 13.4453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/300], Step [138/172], Loss: 11.4894\n",
      "Epoch [56/300], Step [139/172], Loss: 12.8313\n",
      "Epoch [56/300], Step [140/172], Loss: 12.8646\n",
      "Epoch [56/300], Step [141/172], Loss: 15.9681\n",
      "Epoch [56/300], Step [142/172], Loss: 15.4444\n",
      "Epoch [56/300], Step [143/172], Loss: 11.7556\n",
      "Epoch [56/300], Step [144/172], Loss: 11.3834\n",
      "Epoch [56/300], Step [145/172], Loss: 11.5383\n",
      "Epoch [56/300], Step [146/172], Loss: 12.0722\n",
      "Epoch [56/300], Step [147/172], Loss: 8.4849\n",
      "Epoch [56/300], Step [148/172], Loss: 9.2951\n",
      "Epoch [56/300], Step [149/172], Loss: 11.4884\n",
      "Epoch [56/300], Step [150/172], Loss: 11.1247\n",
      "Epoch [56/300], Step [151/172], Loss: 9.8327\n",
      "Epoch [56/300], Step [152/172], Loss: 9.8885\n",
      "Epoch [56/300], Step [153/172], Loss: 9.7911\n",
      "Epoch [56/300], Step [154/172], Loss: 10.8297\n",
      "Epoch [56/300], Step [155/172], Loss: 9.5547\n",
      "Epoch [56/300], Step [156/172], Loss: 11.9608\n",
      "Epoch [56/300], Step [157/172], Loss: 12.3179\n",
      "Epoch [56/300], Step [158/172], Loss: 10.3506\n",
      "Epoch [56/300], Step [159/172], Loss: 10.8496\n",
      "Epoch [56/300], Step [160/172], Loss: 10.7016\n",
      "Epoch [56/300], Step [161/172], Loss: 8.9234\n",
      "Epoch [56/300], Step [162/172], Loss: 9.4411\n",
      "Epoch [56/300], Step [163/172], Loss: 8.9233\n",
      "Epoch [56/300], Step [164/172], Loss: 11.3610\n",
      "Epoch [56/300], Step [165/172], Loss: 8.2912\n",
      "Epoch [56/300], Step [166/172], Loss: 8.8060\n",
      "Epoch [56/300], Step [167/172], Loss: 9.3613\n",
      "Epoch [56/300], Step [168/172], Loss: 8.5465\n",
      "Epoch [56/300], Step [169/172], Loss: 8.3680\n",
      "Epoch [56/300], Step [170/172], Loss: 8.0129\n",
      "Epoch [56/300], Step [171/172], Loss: 7.0999\n",
      "Epoch [56/300], Step [172/172], Loss: 6.4745\n",
      "Epoch [57/300], Step [1/172], Loss: 98.6440\n",
      "Epoch [57/300], Step [2/172], Loss: 97.2915\n",
      "Epoch [57/300], Step [3/172], Loss: 107.0209\n",
      "Epoch [57/300], Step [4/172], Loss: 64.5330\n",
      "Epoch [57/300], Step [5/172], Loss: 87.2229\n",
      "Epoch [57/300], Step [6/172], Loss: 31.4009\n",
      "Epoch [57/300], Step [7/172], Loss: 38.8515\n",
      "Epoch [57/300], Step [8/172], Loss: 11.2727\n",
      "Epoch [57/300], Step [9/172], Loss: 51.6716\n",
      "Epoch [57/300], Step [10/172], Loss: 58.8865\n",
      "Epoch [57/300], Step [11/172], Loss: 100.1326\n",
      "Epoch [57/300], Step [12/172], Loss: 87.3666\n",
      "Epoch [57/300], Step [13/172], Loss: 45.8394\n",
      "Epoch [57/300], Step [14/172], Loss: 99.4604\n",
      "Epoch [57/300], Step [15/172], Loss: 88.6751\n",
      "Epoch [57/300], Step [16/172], Loss: 55.4569\n",
      "Epoch [57/300], Step [17/172], Loss: 61.1554\n",
      "Epoch [57/300], Step [18/172], Loss: 69.6709\n",
      "Epoch [57/300], Step [19/172], Loss: 81.3773\n",
      "Epoch [57/300], Step [20/172], Loss: 103.7068\n",
      "Epoch [57/300], Step [21/172], Loss: 97.3680\n",
      "Epoch [57/300], Step [22/172], Loss: 92.0174\n",
      "Epoch [57/300], Step [23/172], Loss: 17.6303\n",
      "Epoch [57/300], Step [24/172], Loss: 79.6053\n",
      "Epoch [57/300], Step [25/172], Loss: 54.3530\n",
      "Epoch [57/300], Step [26/172], Loss: 63.3760\n",
      "Epoch [57/300], Step [27/172], Loss: 87.7071\n",
      "Epoch [57/300], Step [28/172], Loss: 57.8490\n",
      "Epoch [57/300], Step [29/172], Loss: 60.4662\n",
      "Epoch [57/300], Step [30/172], Loss: 78.1561\n",
      "Epoch [57/300], Step [31/172], Loss: 45.7625\n",
      "Epoch [57/300], Step [32/172], Loss: 42.2968\n",
      "Epoch [57/300], Step [33/172], Loss: 76.9164\n",
      "Epoch [57/300], Step [34/172], Loss: 9.2289\n",
      "Epoch [57/300], Step [35/172], Loss: 44.5354\n",
      "Epoch [57/300], Step [36/172], Loss: 27.7172\n",
      "Epoch [57/300], Step [37/172], Loss: 20.7716\n",
      "Epoch [57/300], Step [38/172], Loss: 29.7955\n",
      "Epoch [57/300], Step [39/172], Loss: 55.0570\n",
      "Epoch [57/300], Step [40/172], Loss: 28.6794\n",
      "Epoch [57/300], Step [41/172], Loss: 42.6808\n",
      "Epoch [57/300], Step [42/172], Loss: 44.0704\n",
      "Epoch [57/300], Step [43/172], Loss: 31.1062\n",
      "Epoch [57/300], Step [44/172], Loss: 27.3242\n",
      "Epoch [57/300], Step [45/172], Loss: 23.7781\n",
      "Epoch [57/300], Step [46/172], Loss: 38.2392\n",
      "Epoch [57/300], Step [47/172], Loss: 61.8422\n",
      "Epoch [57/300], Step [48/172], Loss: 65.3053\n",
      "Epoch [57/300], Step [49/172], Loss: 23.0375\n",
      "Epoch [57/300], Step [50/172], Loss: 52.1191\n",
      "Epoch [57/300], Step [51/172], Loss: 8.5629\n",
      "Epoch [57/300], Step [52/172], Loss: 23.0747\n",
      "Epoch [57/300], Step [53/172], Loss: 29.1601\n",
      "Epoch [57/300], Step [54/172], Loss: 17.4845\n",
      "Epoch [57/300], Step [55/172], Loss: 15.2775\n",
      "Epoch [57/300], Step [56/172], Loss: 11.4169\n",
      "Epoch [57/300], Step [57/172], Loss: 39.7782\n",
      "Epoch [57/300], Step [58/172], Loss: 20.9287\n",
      "Epoch [57/300], Step [59/172], Loss: 36.7635\n",
      "Epoch [57/300], Step [60/172], Loss: 58.7766\n",
      "Epoch [57/300], Step [61/172], Loss: 11.5289\n",
      "Epoch [57/300], Step [62/172], Loss: 22.2034\n",
      "Epoch [57/300], Step [63/172], Loss: 9.4314\n",
      "Epoch [57/300], Step [64/172], Loss: 7.4683\n",
      "Epoch [57/300], Step [65/172], Loss: 27.6021\n",
      "Epoch [57/300], Step [66/172], Loss: 7.7309\n",
      "Epoch [57/300], Step [67/172], Loss: 27.8219\n",
      "Epoch [57/300], Step [68/172], Loss: 12.8949\n",
      "Epoch [57/300], Step [69/172], Loss: 76.3947\n",
      "Epoch [57/300], Step [70/172], Loss: 69.0911\n",
      "Epoch [57/300], Step [71/172], Loss: 61.8270\n",
      "Epoch [57/300], Step [72/172], Loss: 63.5916\n",
      "Epoch [57/300], Step [73/172], Loss: 69.9656\n",
      "Epoch [57/300], Step [74/172], Loss: 43.8709\n",
      "Epoch [57/300], Step [75/172], Loss: 31.8798\n",
      "Epoch [57/300], Step [76/172], Loss: 47.2171\n",
      "Epoch [57/300], Step [77/172], Loss: 61.9464\n",
      "Epoch [57/300], Step [78/172], Loss: 58.0932\n",
      "Epoch [57/300], Step [79/172], Loss: 53.4824\n",
      "Epoch [57/300], Step [80/172], Loss: 57.0525\n",
      "Epoch [57/300], Step [81/172], Loss: 50.4922\n",
      "Epoch [57/300], Step [82/172], Loss: 44.3422\n",
      "Epoch [57/300], Step [83/172], Loss: 52.9504\n",
      "Epoch [57/300], Step [84/172], Loss: 43.9078\n",
      "Epoch [57/300], Step [85/172], Loss: 49.9778\n",
      "Epoch [57/300], Step [86/172], Loss: 39.2022\n",
      "Epoch [57/300], Step [87/172], Loss: 31.9918\n",
      "Epoch [57/300], Step [88/172], Loss: 34.7525\n",
      "Epoch [57/300], Step [89/172], Loss: 33.2254\n",
      "Epoch [57/300], Step [90/172], Loss: 31.6980\n",
      "Epoch [57/300], Step [91/172], Loss: 32.4601\n",
      "Epoch [57/300], Step [92/172], Loss: 26.4606\n",
      "Epoch [57/300], Step [93/172], Loss: 26.7668\n",
      "Epoch [57/300], Step [94/172], Loss: 31.7334\n",
      "Epoch [57/300], Step [95/172], Loss: 27.7134\n",
      "Epoch [57/300], Step [96/172], Loss: 23.5807\n",
      "Epoch [57/300], Step [97/172], Loss: 29.6413\n",
      "Epoch [57/300], Step [98/172], Loss: 24.7419\n",
      "Epoch [57/300], Step [99/172], Loss: 22.0932\n",
      "Epoch [57/300], Step [100/172], Loss: 20.9063\n",
      "Epoch [57/300], Step [101/172], Loss: 22.7277\n",
      "Epoch [57/300], Step [102/172], Loss: 20.7761\n",
      "Epoch [57/300], Step [103/172], Loss: 20.1088\n",
      "Epoch [57/300], Step [104/172], Loss: 19.9337\n",
      "Epoch [57/300], Step [105/172], Loss: 21.0643\n",
      "Epoch [57/300], Step [106/172], Loss: 20.8392\n",
      "Epoch [57/300], Step [107/172], Loss: 18.1262\n",
      "Epoch [57/300], Step [108/172], Loss: 21.4728\n",
      "Epoch [57/300], Step [109/172], Loss: 22.9576\n",
      "Epoch [57/300], Step [110/172], Loss: 19.8887\n",
      "Epoch [57/300], Step [111/172], Loss: 18.3454\n",
      "Epoch [57/300], Step [112/172], Loss: 23.9137\n",
      "Epoch [57/300], Step [113/172], Loss: 19.1146\n",
      "Epoch [57/300], Step [114/172], Loss: 19.1575\n",
      "Epoch [57/300], Step [115/172], Loss: 26.6894\n",
      "Epoch [57/300], Step [116/172], Loss: 19.0645\n",
      "Epoch [57/300], Step [117/172], Loss: 16.2635\n",
      "Epoch [57/300], Step [118/172], Loss: 19.8158\n",
      "Epoch [57/300], Step [119/172], Loss: 17.2946\n",
      "Epoch [57/300], Step [120/172], Loss: 15.0746\n",
      "Epoch [57/300], Step [121/172], Loss: 15.7017\n",
      "Epoch [57/300], Step [122/172], Loss: 14.3856\n",
      "Epoch [57/300], Step [123/172], Loss: 14.1017\n",
      "Epoch [57/300], Step [124/172], Loss: 11.7892\n",
      "Epoch [57/300], Step [125/172], Loss: 16.3945\n",
      "Epoch [57/300], Step [126/172], Loss: 14.8437\n",
      "Epoch [57/300], Step [127/172], Loss: 17.3295\n",
      "Epoch [57/300], Step [128/172], Loss: 17.9055\n",
      "Epoch [57/300], Step [129/172], Loss: 12.6206\n",
      "Epoch [57/300], Step [130/172], Loss: 14.7714\n",
      "Epoch [57/300], Step [131/172], Loss: 12.8500\n",
      "Epoch [57/300], Step [132/172], Loss: 12.7290\n",
      "Epoch [57/300], Step [133/172], Loss: 13.2637\n",
      "Epoch [57/300], Step [134/172], Loss: 14.0079\n",
      "Epoch [57/300], Step [135/172], Loss: 11.4534\n",
      "Epoch [57/300], Step [136/172], Loss: 11.5100\n",
      "Epoch [57/300], Step [137/172], Loss: 13.4143\n",
      "Epoch [57/300], Step [138/172], Loss: 11.4638\n",
      "Epoch [57/300], Step [139/172], Loss: 12.8038\n",
      "Epoch [57/300], Step [140/172], Loss: 12.8534\n",
      "Epoch [57/300], Step [141/172], Loss: 15.8250\n",
      "Epoch [57/300], Step [142/172], Loss: 15.4732\n",
      "Epoch [57/300], Step [143/172], Loss: 11.7274\n",
      "Epoch [57/300], Step [144/172], Loss: 11.3812\n",
      "Epoch [57/300], Step [145/172], Loss: 11.5524\n",
      "Epoch [57/300], Step [146/172], Loss: 12.0721\n",
      "Epoch [57/300], Step [147/172], Loss: 8.4043\n",
      "Epoch [57/300], Step [148/172], Loss: 9.2394\n",
      "Epoch [57/300], Step [149/172], Loss: 11.4386\n",
      "Epoch [57/300], Step [150/172], Loss: 11.0514\n",
      "Epoch [57/300], Step [151/172], Loss: 9.7922\n",
      "Epoch [57/300], Step [152/172], Loss: 9.8400\n",
      "Epoch [57/300], Step [153/172], Loss: 9.7514\n",
      "Epoch [57/300], Step [154/172], Loss: 10.7562\n",
      "Epoch [57/300], Step [155/172], Loss: 9.5188\n",
      "Epoch [57/300], Step [156/172], Loss: 12.0216\n",
      "Epoch [57/300], Step [157/172], Loss: 12.2961\n",
      "Epoch [57/300], Step [158/172], Loss: 10.2893\n",
      "Epoch [57/300], Step [159/172], Loss: 10.8695\n",
      "Epoch [57/300], Step [160/172], Loss: 10.7037\n",
      "Epoch [57/300], Step [161/172], Loss: 8.8844\n",
      "Epoch [57/300], Step [162/172], Loss: 9.4182\n",
      "Epoch [57/300], Step [163/172], Loss: 8.8538\n",
      "Epoch [57/300], Step [164/172], Loss: 11.2732\n",
      "Epoch [57/300], Step [165/172], Loss: 8.2568\n",
      "Epoch [57/300], Step [166/172], Loss: 8.7466\n",
      "Epoch [57/300], Step [167/172], Loss: 9.3476\n",
      "Epoch [57/300], Step [168/172], Loss: 8.4944\n",
      "Epoch [57/300], Step [169/172], Loss: 8.3523\n",
      "Epoch [57/300], Step [170/172], Loss: 7.9419\n",
      "Epoch [57/300], Step [171/172], Loss: 7.0610\n",
      "Epoch [57/300], Step [172/172], Loss: 6.4805\n",
      "Epoch [58/300], Step [1/172], Loss: 98.2179\n",
      "Epoch [58/300], Step [2/172], Loss: 97.0475\n",
      "Epoch [58/300], Step [3/172], Loss: 105.9921\n",
      "Epoch [58/300], Step [4/172], Loss: 63.8387\n",
      "Epoch [58/300], Step [5/172], Loss: 86.4335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/300], Step [6/172], Loss: 30.7484\n",
      "Epoch [58/300], Step [7/172], Loss: 38.2669\n",
      "Epoch [58/300], Step [8/172], Loss: 11.8452\n",
      "Epoch [58/300], Step [9/172], Loss: 51.8784\n",
      "Epoch [58/300], Step [10/172], Loss: 58.5178\n",
      "Epoch [58/300], Step [11/172], Loss: 100.0665\n",
      "Epoch [58/300], Step [12/172], Loss: 87.0304\n",
      "Epoch [58/300], Step [13/172], Loss: 45.6749\n",
      "Epoch [58/300], Step [14/172], Loss: 99.4603\n",
      "Epoch [58/300], Step [15/172], Loss: 88.4848\n",
      "Epoch [58/300], Step [16/172], Loss: 54.7407\n",
      "Epoch [58/300], Step [17/172], Loss: 61.1017\n",
      "Epoch [58/300], Step [18/172], Loss: 69.0875\n",
      "Epoch [58/300], Step [19/172], Loss: 81.3885\n",
      "Epoch [58/300], Step [20/172], Loss: 102.9493\n",
      "Epoch [58/300], Step [21/172], Loss: 97.2976\n",
      "Epoch [58/300], Step [22/172], Loss: 92.0459\n",
      "Epoch [58/300], Step [23/172], Loss: 16.5851\n",
      "Epoch [58/300], Step [24/172], Loss: 79.1184\n",
      "Epoch [58/300], Step [25/172], Loss: 54.1386\n",
      "Epoch [58/300], Step [26/172], Loss: 63.2797\n",
      "Epoch [58/300], Step [27/172], Loss: 87.8095\n",
      "Epoch [58/300], Step [28/172], Loss: 56.7826\n",
      "Epoch [58/300], Step [29/172], Loss: 58.7828\n",
      "Epoch [58/300], Step [30/172], Loss: 78.2094\n",
      "Epoch [58/300], Step [31/172], Loss: 45.5873\n",
      "Epoch [58/300], Step [32/172], Loss: 41.9701\n",
      "Epoch [58/300], Step [33/172], Loss: 76.4376\n",
      "Epoch [58/300], Step [34/172], Loss: 8.9994\n",
      "Epoch [58/300], Step [35/172], Loss: 43.5915\n",
      "Epoch [58/300], Step [36/172], Loss: 27.3569\n",
      "Epoch [58/300], Step [37/172], Loss: 20.4149\n",
      "Epoch [58/300], Step [38/172], Loss: 29.4855\n",
      "Epoch [58/300], Step [39/172], Loss: 55.0055\n",
      "Epoch [58/300], Step [40/172], Loss: 28.2034\n",
      "Epoch [58/300], Step [41/172], Loss: 42.5194\n",
      "Epoch [58/300], Step [42/172], Loss: 43.8460\n",
      "Epoch [58/300], Step [43/172], Loss: 30.7450\n",
      "Epoch [58/300], Step [44/172], Loss: 27.1026\n",
      "Epoch [58/300], Step [45/172], Loss: 23.5078\n",
      "Epoch [58/300], Step [46/172], Loss: 37.6428\n",
      "Epoch [58/300], Step [47/172], Loss: 61.2062\n",
      "Epoch [58/300], Step [48/172], Loss: 65.1794\n",
      "Epoch [58/300], Step [49/172], Loss: 22.9339\n",
      "Epoch [58/300], Step [50/172], Loss: 52.3186\n",
      "Epoch [58/300], Step [51/172], Loss: 8.4438\n",
      "Epoch [58/300], Step [52/172], Loss: 22.7941\n",
      "Epoch [58/300], Step [53/172], Loss: 29.0024\n",
      "Epoch [58/300], Step [54/172], Loss: 17.1968\n",
      "Epoch [58/300], Step [55/172], Loss: 14.9995\n",
      "Epoch [58/300], Step [56/172], Loss: 11.3037\n",
      "Epoch [58/300], Step [57/172], Loss: 39.2935\n",
      "Epoch [58/300], Step [58/172], Loss: 20.9592\n",
      "Epoch [58/300], Step [59/172], Loss: 36.7295\n",
      "Epoch [58/300], Step [60/172], Loss: 58.7526\n",
      "Epoch [58/300], Step [61/172], Loss: 11.5558\n",
      "Epoch [58/300], Step [62/172], Loss: 22.2396\n",
      "Epoch [58/300], Step [63/172], Loss: 9.3430\n",
      "Epoch [58/300], Step [64/172], Loss: 7.4856\n",
      "Epoch [58/300], Step [65/172], Loss: 27.4016\n",
      "Epoch [58/300], Step [66/172], Loss: 7.6353\n",
      "Epoch [58/300], Step [67/172], Loss: 27.9112\n",
      "Epoch [58/300], Step [68/172], Loss: 12.5823\n",
      "Epoch [58/300], Step [69/172], Loss: 76.2135\n",
      "Epoch [58/300], Step [70/172], Loss: 68.8322\n",
      "Epoch [58/300], Step [71/172], Loss: 61.5668\n",
      "Epoch [58/300], Step [72/172], Loss: 63.6686\n",
      "Epoch [58/300], Step [73/172], Loss: 69.9561\n",
      "Epoch [58/300], Step [74/172], Loss: 43.5853\n",
      "Epoch [58/300], Step [75/172], Loss: 32.0614\n",
      "Epoch [58/300], Step [76/172], Loss: 46.9704\n",
      "Epoch [58/300], Step [77/172], Loss: 61.9064\n",
      "Epoch [58/300], Step [78/172], Loss: 57.7133\n",
      "Epoch [58/300], Step [79/172], Loss: 53.2588\n",
      "Epoch [58/300], Step [80/172], Loss: 56.9555\n",
      "Epoch [58/300], Step [81/172], Loss: 49.9674\n",
      "Epoch [58/300], Step [82/172], Loss: 44.3161\n",
      "Epoch [58/300], Step [83/172], Loss: 52.9134\n",
      "Epoch [58/300], Step [84/172], Loss: 43.5577\n",
      "Epoch [58/300], Step [85/172], Loss: 49.4756\n",
      "Epoch [58/300], Step [86/172], Loss: 39.0857\n",
      "Epoch [58/300], Step [87/172], Loss: 31.9061\n",
      "Epoch [58/300], Step [88/172], Loss: 34.5579\n",
      "Epoch [58/300], Step [89/172], Loss: 32.9922\n",
      "Epoch [58/300], Step [90/172], Loss: 31.4641\n",
      "Epoch [58/300], Step [91/172], Loss: 32.3463\n",
      "Epoch [58/300], Step [92/172], Loss: 26.3024\n",
      "Epoch [58/300], Step [93/172], Loss: 26.5082\n",
      "Epoch [58/300], Step [94/172], Loss: 31.5944\n",
      "Epoch [58/300], Step [95/172], Loss: 27.5245\n",
      "Epoch [58/300], Step [96/172], Loss: 23.3772\n",
      "Epoch [58/300], Step [97/172], Loss: 29.5093\n",
      "Epoch [58/300], Step [98/172], Loss: 24.5738\n",
      "Epoch [58/300], Step [99/172], Loss: 21.9264\n",
      "Epoch [58/300], Step [100/172], Loss: 20.7324\n",
      "Epoch [58/300], Step [101/172], Loss: 22.5223\n",
      "Epoch [58/300], Step [102/172], Loss: 20.7058\n",
      "Epoch [58/300], Step [103/172], Loss: 19.9105\n",
      "Epoch [58/300], Step [104/172], Loss: 19.7625\n",
      "Epoch [58/300], Step [105/172], Loss: 21.0188\n",
      "Epoch [58/300], Step [106/172], Loss: 20.7019\n",
      "Epoch [58/300], Step [107/172], Loss: 18.0577\n",
      "Epoch [58/300], Step [108/172], Loss: 21.3329\n",
      "Epoch [58/300], Step [109/172], Loss: 22.8292\n",
      "Epoch [58/300], Step [110/172], Loss: 19.7830\n",
      "Epoch [58/300], Step [111/172], Loss: 18.2056\n",
      "Epoch [58/300], Step [112/172], Loss: 23.7323\n",
      "Epoch [58/300], Step [113/172], Loss: 19.0182\n",
      "Epoch [58/300], Step [114/172], Loss: 19.0041\n",
      "Epoch [58/300], Step [115/172], Loss: 26.5176\n",
      "Epoch [58/300], Step [116/172], Loss: 18.9526\n",
      "Epoch [58/300], Step [117/172], Loss: 16.1110\n",
      "Epoch [58/300], Step [118/172], Loss: 19.6468\n",
      "Epoch [58/300], Step [119/172], Loss: 17.2207\n",
      "Epoch [58/300], Step [120/172], Loss: 14.9718\n",
      "Epoch [58/300], Step [121/172], Loss: 15.5331\n",
      "Epoch [58/300], Step [122/172], Loss: 14.2809\n",
      "Epoch [58/300], Step [123/172], Loss: 13.9595\n",
      "Epoch [58/300], Step [124/172], Loss: 11.6481\n",
      "Epoch [58/300], Step [125/172], Loss: 16.2261\n",
      "Epoch [58/300], Step [126/172], Loss: 14.6849\n",
      "Epoch [58/300], Step [127/172], Loss: 17.2088\n",
      "Epoch [58/300], Step [128/172], Loss: 17.7865\n",
      "Epoch [58/300], Step [129/172], Loss: 12.5187\n",
      "Epoch [58/300], Step [130/172], Loss: 14.6573\n",
      "Epoch [58/300], Step [131/172], Loss: 12.7553\n",
      "Epoch [58/300], Step [132/172], Loss: 12.6055\n",
      "Epoch [58/300], Step [133/172], Loss: 13.1712\n",
      "Epoch [58/300], Step [134/172], Loss: 13.9333\n",
      "Epoch [58/300], Step [135/172], Loss: 11.3650\n",
      "Epoch [58/300], Step [136/172], Loss: 11.3917\n",
      "Epoch [58/300], Step [137/172], Loss: 13.3109\n",
      "Epoch [58/300], Step [138/172], Loss: 11.4048\n",
      "Epoch [58/300], Step [139/172], Loss: 12.7130\n",
      "Epoch [58/300], Step [140/172], Loss: 12.7752\n",
      "Epoch [58/300], Step [141/172], Loss: 15.7244\n",
      "Epoch [58/300], Step [142/172], Loss: 15.4689\n",
      "Epoch [58/300], Step [143/172], Loss: 11.6432\n",
      "Epoch [58/300], Step [144/172], Loss: 11.3456\n",
      "Epoch [58/300], Step [145/172], Loss: 11.4959\n",
      "Epoch [58/300], Step [146/172], Loss: 12.0290\n",
      "Epoch [58/300], Step [147/172], Loss: 8.2769\n",
      "Epoch [58/300], Step [148/172], Loss: 9.1424\n",
      "Epoch [58/300], Step [149/172], Loss: 11.3247\n",
      "Epoch [58/300], Step [150/172], Loss: 10.9546\n",
      "Epoch [58/300], Step [151/172], Loss: 9.7061\n",
      "Epoch [58/300], Step [152/172], Loss: 9.7636\n",
      "Epoch [58/300], Step [153/172], Loss: 9.6594\n",
      "Epoch [58/300], Step [154/172], Loss: 10.6529\n",
      "Epoch [58/300], Step [155/172], Loss: 9.4372\n",
      "Epoch [58/300], Step [156/172], Loss: 12.0293\n",
      "Epoch [58/300], Step [157/172], Loss: 12.2284\n",
      "Epoch [58/300], Step [158/172], Loss: 10.2108\n",
      "Epoch [58/300], Step [159/172], Loss: 10.8109\n",
      "Epoch [58/300], Step [160/172], Loss: 10.6595\n",
      "Epoch [58/300], Step [161/172], Loss: 8.8226\n",
      "Epoch [58/300], Step [162/172], Loss: 9.3599\n",
      "Epoch [58/300], Step [163/172], Loss: 8.7650\n",
      "Epoch [58/300], Step [164/172], Loss: 11.2850\n",
      "Epoch [58/300], Step [165/172], Loss: 8.2065\n",
      "Epoch [58/300], Step [166/172], Loss: 8.6767\n",
      "Epoch [58/300], Step [167/172], Loss: 9.2938\n",
      "Epoch [58/300], Step [168/172], Loss: 8.4191\n",
      "Epoch [58/300], Step [169/172], Loss: 8.3062\n",
      "Epoch [58/300], Step [170/172], Loss: 7.8541\n",
      "Epoch [58/300], Step [171/172], Loss: 7.0244\n",
      "Epoch [58/300], Step [172/172], Loss: 6.4571\n",
      "Epoch [59/300], Step [1/172], Loss: 97.9401\n",
      "Epoch [59/300], Step [2/172], Loss: 96.8157\n",
      "Epoch [59/300], Step [3/172], Loss: 104.8944\n",
      "Epoch [59/300], Step [4/172], Loss: 63.2093\n",
      "Epoch [59/300], Step [5/172], Loss: 86.0956\n",
      "Epoch [59/300], Step [6/172], Loss: 30.1838\n",
      "Epoch [59/300], Step [7/172], Loss: 37.7673\n",
      "Epoch [59/300], Step [8/172], Loss: 11.0226\n",
      "Epoch [59/300], Step [9/172], Loss: 51.2006\n",
      "Epoch [59/300], Step [10/172], Loss: 58.1666\n",
      "Epoch [59/300], Step [11/172], Loss: 100.0976\n",
      "Epoch [59/300], Step [12/172], Loss: 86.9478\n",
      "Epoch [59/300], Step [13/172], Loss: 45.1473\n",
      "Epoch [59/300], Step [14/172], Loss: 99.2262\n",
      "Epoch [59/300], Step [15/172], Loss: 88.2032\n",
      "Epoch [59/300], Step [16/172], Loss: 53.4200\n",
      "Epoch [59/300], Step [17/172], Loss: 60.9861\n",
      "Epoch [59/300], Step [18/172], Loss: 68.4024\n",
      "Epoch [59/300], Step [19/172], Loss: 81.2039\n",
      "Epoch [59/300], Step [20/172], Loss: 101.8399\n",
      "Epoch [59/300], Step [21/172], Loss: 96.9725\n",
      "Epoch [59/300], Step [22/172], Loss: 91.0666\n",
      "Epoch [59/300], Step [23/172], Loss: 16.0077\n",
      "Epoch [59/300], Step [24/172], Loss: 78.8842\n",
      "Epoch [59/300], Step [25/172], Loss: 53.5199\n",
      "Epoch [59/300], Step [26/172], Loss: 62.7696\n",
      "Epoch [59/300], Step [27/172], Loss: 87.0912\n",
      "Epoch [59/300], Step [28/172], Loss: 55.9957\n",
      "Epoch [59/300], Step [29/172], Loss: 57.2486\n",
      "Epoch [59/300], Step [30/172], Loss: 77.7316\n",
      "Epoch [59/300], Step [31/172], Loss: 45.3310\n",
      "Epoch [59/300], Step [32/172], Loss: 41.6578\n",
      "Epoch [59/300], Step [33/172], Loss: 75.8300\n",
      "Epoch [59/300], Step [34/172], Loss: 8.5335\n",
      "Epoch [59/300], Step [35/172], Loss: 42.3538\n",
      "Epoch [59/300], Step [36/172], Loss: 27.0853\n",
      "Epoch [59/300], Step [37/172], Loss: 20.2528\n",
      "Epoch [59/300], Step [38/172], Loss: 28.9294\n",
      "Epoch [59/300], Step [39/172], Loss: 54.3792\n",
      "Epoch [59/300], Step [40/172], Loss: 27.8038\n",
      "Epoch [59/300], Step [41/172], Loss: 41.9830\n",
      "Epoch [59/300], Step [42/172], Loss: 43.7505\n",
      "Epoch [59/300], Step [43/172], Loss: 30.1499\n",
      "Epoch [59/300], Step [44/172], Loss: 26.7154\n",
      "Epoch [59/300], Step [45/172], Loss: 23.2680\n",
      "Epoch [59/300], Step [46/172], Loss: 36.6347\n",
      "Epoch [59/300], Step [47/172], Loss: 60.4816\n",
      "Epoch [59/300], Step [48/172], Loss: 64.1169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/300], Step [49/172], Loss: 22.5693\n",
      "Epoch [59/300], Step [50/172], Loss: 52.0243\n",
      "Epoch [59/300], Step [51/172], Loss: 8.3175\n",
      "Epoch [59/300], Step [52/172], Loss: 22.5090\n",
      "Epoch [59/300], Step [53/172], Loss: 28.7533\n",
      "Epoch [59/300], Step [54/172], Loss: 16.6874\n",
      "Epoch [59/300], Step [55/172], Loss: 14.9244\n",
      "Epoch [59/300], Step [56/172], Loss: 11.1331\n",
      "Epoch [59/300], Step [57/172], Loss: 38.6401\n",
      "Epoch [59/300], Step [58/172], Loss: 20.9045\n",
      "Epoch [59/300], Step [59/172], Loss: 36.3974\n",
      "Epoch [59/300], Step [60/172], Loss: 58.7860\n",
      "Epoch [59/300], Step [61/172], Loss: 11.4893\n",
      "Epoch [59/300], Step [62/172], Loss: 22.5456\n",
      "Epoch [59/300], Step [63/172], Loss: 9.3080\n",
      "Epoch [59/300], Step [64/172], Loss: 7.4474\n",
      "Epoch [59/300], Step [65/172], Loss: 27.3775\n",
      "Epoch [59/300], Step [66/172], Loss: 7.4644\n",
      "Epoch [59/300], Step [67/172], Loss: 27.9861\n",
      "Epoch [59/300], Step [68/172], Loss: 12.1711\n",
      "Epoch [59/300], Step [69/172], Loss: 76.0557\n",
      "Epoch [59/300], Step [70/172], Loss: 68.4124\n",
      "Epoch [59/300], Step [71/172], Loss: 61.4184\n",
      "Epoch [59/300], Step [72/172], Loss: 63.5529\n",
      "Epoch [59/300], Step [73/172], Loss: 69.6207\n",
      "Epoch [59/300], Step [74/172], Loss: 43.3368\n",
      "Epoch [59/300], Step [75/172], Loss: 32.5284\n",
      "Epoch [59/300], Step [76/172], Loss: 46.7657\n",
      "Epoch [59/300], Step [77/172], Loss: 61.9415\n",
      "Epoch [59/300], Step [78/172], Loss: 57.4182\n",
      "Epoch [59/300], Step [79/172], Loss: 53.2148\n",
      "Epoch [59/300], Step [80/172], Loss: 56.9514\n",
      "Epoch [59/300], Step [81/172], Loss: 49.6369\n",
      "Epoch [59/300], Step [82/172], Loss: 43.9440\n",
      "Epoch [59/300], Step [83/172], Loss: 52.8620\n",
      "Epoch [59/300], Step [84/172], Loss: 43.4427\n",
      "Epoch [59/300], Step [85/172], Loss: 49.3927\n",
      "Epoch [59/300], Step [86/172], Loss: 39.0654\n",
      "Epoch [59/300], Step [87/172], Loss: 31.9551\n",
      "Epoch [59/300], Step [88/172], Loss: 34.5707\n",
      "Epoch [59/300], Step [89/172], Loss: 32.9477\n",
      "Epoch [59/300], Step [90/172], Loss: 31.3083\n",
      "Epoch [59/300], Step [91/172], Loss: 32.4183\n",
      "Epoch [59/300], Step [92/172], Loss: 26.2906\n",
      "Epoch [59/300], Step [93/172], Loss: 26.4057\n",
      "Epoch [59/300], Step [94/172], Loss: 31.6557\n",
      "Epoch [59/300], Step [95/172], Loss: 27.4900\n",
      "Epoch [59/300], Step [96/172], Loss: 23.2609\n",
      "Epoch [59/300], Step [97/172], Loss: 29.5166\n",
      "Epoch [59/300], Step [98/172], Loss: 24.5166\n",
      "Epoch [59/300], Step [99/172], Loss: 21.8465\n",
      "Epoch [59/300], Step [100/172], Loss: 20.6489\n",
      "Epoch [59/300], Step [101/172], Loss: 22.4126\n",
      "Epoch [59/300], Step [102/172], Loss: 20.5479\n",
      "Epoch [59/300], Step [103/172], Loss: 19.7748\n",
      "Epoch [59/300], Step [104/172], Loss: 19.6485\n",
      "Epoch [59/300], Step [105/172], Loss: 20.9310\n",
      "Epoch [59/300], Step [106/172], Loss: 20.6213\n",
      "Epoch [59/300], Step [107/172], Loss: 17.9459\n",
      "Epoch [59/300], Step [108/172], Loss: 21.2668\n",
      "Epoch [59/300], Step [109/172], Loss: 22.7130\n",
      "Epoch [59/300], Step [110/172], Loss: 19.7790\n",
      "Epoch [59/300], Step [111/172], Loss: 18.1652\n",
      "Epoch [59/300], Step [112/172], Loss: 23.6346\n",
      "Epoch [59/300], Step [113/172], Loss: 18.9056\n",
      "Epoch [59/300], Step [114/172], Loss: 18.9029\n",
      "Epoch [59/300], Step [115/172], Loss: 26.3568\n",
      "Epoch [59/300], Step [116/172], Loss: 18.8580\n",
      "Epoch [59/300], Step [117/172], Loss: 16.0544\n",
      "Epoch [59/300], Step [118/172], Loss: 19.6065\n",
      "Epoch [59/300], Step [119/172], Loss: 17.2011\n",
      "Epoch [59/300], Step [120/172], Loss: 14.8575\n",
      "Epoch [59/300], Step [121/172], Loss: 15.4518\n",
      "Epoch [59/300], Step [122/172], Loss: 14.3196\n",
      "Epoch [59/300], Step [123/172], Loss: 13.8908\n",
      "Epoch [59/300], Step [124/172], Loss: 11.5455\n",
      "Epoch [59/300], Step [125/172], Loss: 16.2088\n",
      "Epoch [59/300], Step [126/172], Loss: 14.5692\n",
      "Epoch [59/300], Step [127/172], Loss: 17.1084\n",
      "Epoch [59/300], Step [128/172], Loss: 17.6373\n",
      "Epoch [59/300], Step [129/172], Loss: 12.4305\n",
      "Epoch [59/300], Step [130/172], Loss: 14.5670\n",
      "Epoch [59/300], Step [131/172], Loss: 12.6673\n",
      "Epoch [59/300], Step [132/172], Loss: 12.4966\n",
      "Epoch [59/300], Step [133/172], Loss: 13.1477\n",
      "Epoch [59/300], Step [134/172], Loss: 13.9061\n",
      "Epoch [59/300], Step [135/172], Loss: 11.3087\n",
      "Epoch [59/300], Step [136/172], Loss: 11.2721\n",
      "Epoch [59/300], Step [137/172], Loss: 13.2210\n",
      "Epoch [59/300], Step [138/172], Loss: 11.3082\n",
      "Epoch [59/300], Step [139/172], Loss: 12.6101\n",
      "Epoch [59/300], Step [140/172], Loss: 12.6762\n",
      "Epoch [59/300], Step [141/172], Loss: 15.5753\n",
      "Epoch [59/300], Step [142/172], Loss: 15.4792\n",
      "Epoch [59/300], Step [143/172], Loss: 11.5534\n",
      "Epoch [59/300], Step [144/172], Loss: 11.3085\n",
      "Epoch [59/300], Step [145/172], Loss: 11.4628\n",
      "Epoch [59/300], Step [146/172], Loss: 11.9790\n",
      "Epoch [59/300], Step [147/172], Loss: 8.2076\n",
      "Epoch [59/300], Step [148/172], Loss: 9.0782\n",
      "Epoch [59/300], Step [149/172], Loss: 11.2469\n",
      "Epoch [59/300], Step [150/172], Loss: 10.8743\n",
      "Epoch [59/300], Step [151/172], Loss: 9.6599\n",
      "Epoch [59/300], Step [152/172], Loss: 9.6944\n",
      "Epoch [59/300], Step [153/172], Loss: 9.5883\n",
      "Epoch [59/300], Step [154/172], Loss: 10.5772\n",
      "Epoch [59/300], Step [155/172], Loss: 9.3846\n",
      "Epoch [59/300], Step [156/172], Loss: 12.0647\n",
      "Epoch [59/300], Step [157/172], Loss: 12.1931\n",
      "Epoch [59/300], Step [158/172], Loss: 10.1569\n",
      "Epoch [59/300], Step [159/172], Loss: 10.8067\n",
      "Epoch [59/300], Step [160/172], Loss: 10.6678\n",
      "Epoch [59/300], Step [161/172], Loss: 8.7792\n",
      "Epoch [59/300], Step [162/172], Loss: 9.3206\n",
      "Epoch [59/300], Step [163/172], Loss: 8.7131\n",
      "Epoch [59/300], Step [164/172], Loss: 11.2598\n",
      "Epoch [59/300], Step [165/172], Loss: 8.1644\n",
      "Epoch [59/300], Step [166/172], Loss: 8.6430\n",
      "Epoch [59/300], Step [167/172], Loss: 9.2792\n",
      "Epoch [59/300], Step [168/172], Loss: 8.3862\n",
      "Epoch [59/300], Step [169/172], Loss: 8.3063\n",
      "Epoch [59/300], Step [170/172], Loss: 7.8340\n",
      "Epoch [59/300], Step [171/172], Loss: 7.0124\n",
      "Epoch [59/300], Step [172/172], Loss: 6.4597\n",
      "Epoch [60/300], Step [1/172], Loss: 97.7132\n",
      "Epoch [60/300], Step [2/172], Loss: 96.6436\n",
      "Epoch [60/300], Step [3/172], Loss: 103.7859\n",
      "Epoch [60/300], Step [4/172], Loss: 62.6182\n",
      "Epoch [60/300], Step [5/172], Loss: 85.5741\n",
      "Epoch [60/300], Step [6/172], Loss: 29.8277\n",
      "Epoch [60/300], Step [7/172], Loss: 36.9850\n",
      "Epoch [60/300], Step [8/172], Loss: 10.9452\n",
      "Epoch [60/300], Step [9/172], Loss: 51.1677\n",
      "Epoch [60/300], Step [10/172], Loss: 58.0027\n",
      "Epoch [60/300], Step [11/172], Loss: 99.9623\n",
      "Epoch [60/300], Step [12/172], Loss: 86.7864\n",
      "Epoch [60/300], Step [13/172], Loss: 45.1026\n",
      "Epoch [60/300], Step [14/172], Loss: 99.2447\n",
      "Epoch [60/300], Step [15/172], Loss: 88.1482\n",
      "Epoch [60/300], Step [16/172], Loss: 52.8492\n",
      "Epoch [60/300], Step [17/172], Loss: 61.0650\n",
      "Epoch [60/300], Step [18/172], Loss: 68.2709\n",
      "Epoch [60/300], Step [19/172], Loss: 81.5895\n",
      "Epoch [60/300], Step [20/172], Loss: 101.0461\n",
      "Epoch [60/300], Step [21/172], Loss: 97.2189\n",
      "Epoch [60/300], Step [22/172], Loss: 91.2810\n",
      "Epoch [60/300], Step [23/172], Loss: 15.5549\n",
      "Epoch [60/300], Step [24/172], Loss: 78.7080\n",
      "Epoch [60/300], Step [25/172], Loss: 53.7479\n",
      "Epoch [60/300], Step [26/172], Loss: 63.0163\n",
      "Epoch [60/300], Step [27/172], Loss: 87.3780\n",
      "Epoch [60/300], Step [28/172], Loss: 55.3948\n",
      "Epoch [60/300], Step [29/172], Loss: 56.1426\n",
      "Epoch [60/300], Step [30/172], Loss: 78.0311\n",
      "Epoch [60/300], Step [31/172], Loss: 45.4642\n",
      "Epoch [60/300], Step [32/172], Loss: 41.6445\n",
      "Epoch [60/300], Step [33/172], Loss: 75.6947\n",
      "Epoch [60/300], Step [34/172], Loss: 8.3669\n",
      "Epoch [60/300], Step [35/172], Loss: 41.6120\n",
      "Epoch [60/300], Step [36/172], Loss: 26.8024\n",
      "Epoch [60/300], Step [37/172], Loss: 20.0977\n",
      "Epoch [60/300], Step [38/172], Loss: 28.8567\n",
      "Epoch [60/300], Step [39/172], Loss: 54.4655\n",
      "Epoch [60/300], Step [40/172], Loss: 27.5478\n",
      "Epoch [60/300], Step [41/172], Loss: 42.0111\n",
      "Epoch [60/300], Step [42/172], Loss: 43.7631\n",
      "Epoch [60/300], Step [43/172], Loss: 29.9950\n",
      "Epoch [60/300], Step [44/172], Loss: 26.5853\n",
      "Epoch [60/300], Step [45/172], Loss: 23.1319\n",
      "Epoch [60/300], Step [46/172], Loss: 36.1810\n",
      "Epoch [60/300], Step [47/172], Loss: 60.1913\n",
      "Epoch [60/300], Step [48/172], Loss: 63.8095\n",
      "Epoch [60/300], Step [49/172], Loss: 22.5046\n",
      "Epoch [60/300], Step [50/172], Loss: 52.1085\n",
      "Epoch [60/300], Step [51/172], Loss: 8.2571\n",
      "Epoch [60/300], Step [52/172], Loss: 22.3912\n",
      "Epoch [60/300], Step [53/172], Loss: 28.6498\n",
      "Epoch [60/300], Step [54/172], Loss: 16.3854\n",
      "Epoch [60/300], Step [55/172], Loss: 14.7197\n",
      "Epoch [60/300], Step [56/172], Loss: 11.0566\n",
      "Epoch [60/300], Step [57/172], Loss: 37.9835\n",
      "Epoch [60/300], Step [58/172], Loss: 20.8617\n",
      "Epoch [60/300], Step [59/172], Loss: 36.3283\n",
      "Epoch [60/300], Step [60/172], Loss: 58.2244\n",
      "Epoch [60/300], Step [61/172], Loss: 11.4181\n",
      "Epoch [60/300], Step [62/172], Loss: 22.4543\n",
      "Epoch [60/300], Step [63/172], Loss: 9.2343\n",
      "Epoch [60/300], Step [64/172], Loss: 7.4332\n",
      "Epoch [60/300], Step [65/172], Loss: 26.9780\n",
      "Epoch [60/300], Step [66/172], Loss: 7.4244\n",
      "Epoch [60/300], Step [67/172], Loss: 27.9279\n",
      "Epoch [60/300], Step [68/172], Loss: 11.9110\n",
      "Epoch [60/300], Step [69/172], Loss: 75.6911\n",
      "Epoch [60/300], Step [70/172], Loss: 68.1798\n",
      "Epoch [60/300], Step [71/172], Loss: 61.0975\n",
      "Epoch [60/300], Step [72/172], Loss: 63.5096\n",
      "Epoch [60/300], Step [73/172], Loss: 69.5753\n",
      "Epoch [60/300], Step [74/172], Loss: 43.0114\n",
      "Epoch [60/300], Step [75/172], Loss: 32.2990\n",
      "Epoch [60/300], Step [76/172], Loss: 46.3123\n",
      "Epoch [60/300], Step [77/172], Loss: 61.8073\n",
      "Epoch [60/300], Step [78/172], Loss: 57.0395\n",
      "Epoch [60/300], Step [79/172], Loss: 52.9437\n",
      "Epoch [60/300], Step [80/172], Loss: 56.7464\n",
      "Epoch [60/300], Step [81/172], Loss: 49.1214\n",
      "Epoch [60/300], Step [82/172], Loss: 43.6969\n",
      "Epoch [60/300], Step [83/172], Loss: 52.7089\n",
      "Epoch [60/300], Step [84/172], Loss: 43.0256\n",
      "Epoch [60/300], Step [85/172], Loss: 48.8106\n",
      "Epoch [60/300], Step [86/172], Loss: 38.7205\n",
      "Epoch [60/300], Step [87/172], Loss: 31.7017\n",
      "Epoch [60/300], Step [88/172], Loss: 34.2006\n",
      "Epoch [60/300], Step [89/172], Loss: 32.5372\n",
      "Epoch [60/300], Step [90/172], Loss: 30.9280\n",
      "Epoch [60/300], Step [91/172], Loss: 32.1360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/300], Step [92/172], Loss: 25.9984\n",
      "Epoch [60/300], Step [93/172], Loss: 26.0307\n",
      "Epoch [60/300], Step [94/172], Loss: 31.4000\n",
      "Epoch [60/300], Step [95/172], Loss: 27.1692\n",
      "Epoch [60/300], Step [96/172], Loss: 22.9623\n",
      "Epoch [60/300], Step [97/172], Loss: 29.2492\n",
      "Epoch [60/300], Step [98/172], Loss: 24.2221\n",
      "Epoch [60/300], Step [99/172], Loss: 21.5860\n",
      "Epoch [60/300], Step [100/172], Loss: 20.3966\n",
      "Epoch [60/300], Step [101/172], Loss: 22.1170\n",
      "Epoch [60/300], Step [102/172], Loss: 20.3032\n",
      "Epoch [60/300], Step [103/172], Loss: 19.4931\n",
      "Epoch [60/300], Step [104/172], Loss: 19.4087\n",
      "Epoch [60/300], Step [105/172], Loss: 20.7433\n",
      "Epoch [60/300], Step [106/172], Loss: 20.4095\n",
      "Epoch [60/300], Step [107/172], Loss: 17.8134\n",
      "Epoch [60/300], Step [108/172], Loss: 21.0578\n",
      "Epoch [60/300], Step [109/172], Loss: 22.5300\n",
      "Epoch [60/300], Step [110/172], Loss: 19.5791\n",
      "Epoch [60/300], Step [111/172], Loss: 17.9467\n",
      "Epoch [60/300], Step [112/172], Loss: 23.3790\n",
      "Epoch [60/300], Step [113/172], Loss: 18.7674\n",
      "Epoch [60/300], Step [114/172], Loss: 18.7230\n",
      "Epoch [60/300], Step [115/172], Loss: 26.1471\n",
      "Epoch [60/300], Step [116/172], Loss: 18.6830\n",
      "Epoch [60/300], Step [117/172], Loss: 15.8676\n",
      "Epoch [60/300], Step [118/172], Loss: 19.4064\n",
      "Epoch [60/300], Step [119/172], Loss: 17.0875\n",
      "Epoch [60/300], Step [120/172], Loss: 14.7357\n",
      "Epoch [60/300], Step [121/172], Loss: 15.2848\n",
      "Epoch [60/300], Step [122/172], Loss: 14.1610\n",
      "Epoch [60/300], Step [123/172], Loss: 13.6860\n",
      "Epoch [60/300], Step [124/172], Loss: 11.3964\n",
      "Epoch [60/300], Step [125/172], Loss: 16.0334\n",
      "Epoch [60/300], Step [126/172], Loss: 14.3916\n",
      "Epoch [60/300], Step [127/172], Loss: 16.9414\n",
      "Epoch [60/300], Step [128/172], Loss: 17.4650\n",
      "Epoch [60/300], Step [129/172], Loss: 12.3037\n",
      "Epoch [60/300], Step [130/172], Loss: 14.4205\n",
      "Epoch [60/300], Step [131/172], Loss: 12.5306\n",
      "Epoch [60/300], Step [132/172], Loss: 12.3553\n",
      "Epoch [60/300], Step [133/172], Loss: 13.0217\n",
      "Epoch [60/300], Step [134/172], Loss: 13.7853\n",
      "Epoch [60/300], Step [135/172], Loss: 11.2067\n",
      "Epoch [60/300], Step [136/172], Loss: 11.1583\n",
      "Epoch [60/300], Step [137/172], Loss: 13.0940\n",
      "Epoch [60/300], Step [138/172], Loss: 11.2114\n",
      "Epoch [60/300], Step [139/172], Loss: 12.4911\n",
      "Epoch [60/300], Step [140/172], Loss: 12.5801\n",
      "Epoch [60/300], Step [141/172], Loss: 15.4646\n",
      "Epoch [60/300], Step [142/172], Loss: 15.3828\n",
      "Epoch [60/300], Step [143/172], Loss: 11.4573\n",
      "Epoch [60/300], Step [144/172], Loss: 11.2478\n",
      "Epoch [60/300], Step [145/172], Loss: 11.4018\n",
      "Epoch [60/300], Step [146/172], Loss: 11.9132\n",
      "Epoch [60/300], Step [147/172], Loss: 8.0929\n",
      "Epoch [60/300], Step [148/172], Loss: 8.9804\n",
      "Epoch [60/300], Step [149/172], Loss: 11.1102\n",
      "Epoch [60/300], Step [150/172], Loss: 10.7608\n",
      "Epoch [60/300], Step [151/172], Loss: 9.5739\n",
      "Epoch [60/300], Step [152/172], Loss: 9.6000\n",
      "Epoch [60/300], Step [153/172], Loss: 9.4898\n",
      "Epoch [60/300], Step [154/172], Loss: 10.4417\n",
      "Epoch [60/300], Step [155/172], Loss: 9.2961\n",
      "Epoch [60/300], Step [156/172], Loss: 12.0376\n",
      "Epoch [60/300], Step [157/172], Loss: 12.0957\n",
      "Epoch [60/300], Step [158/172], Loss: 10.0468\n",
      "Epoch [60/300], Step [159/172], Loss: 10.7332\n",
      "Epoch [60/300], Step [160/172], Loss: 10.5922\n",
      "Epoch [60/300], Step [161/172], Loss: 8.7136\n",
      "Epoch [60/300], Step [162/172], Loss: 9.2467\n",
      "Epoch [60/300], Step [163/172], Loss: 8.6244\n",
      "Epoch [60/300], Step [164/172], Loss: 11.2151\n",
      "Epoch [60/300], Step [165/172], Loss: 8.1049\n",
      "Epoch [60/300], Step [166/172], Loss: 8.5670\n",
      "Epoch [60/300], Step [167/172], Loss: 9.2139\n",
      "Epoch [60/300], Step [168/172], Loss: 8.3048\n",
      "Epoch [60/300], Step [169/172], Loss: 8.2550\n",
      "Epoch [60/300], Step [170/172], Loss: 7.7602\n",
      "Epoch [60/300], Step [171/172], Loss: 6.9875\n",
      "Epoch [60/300], Step [172/172], Loss: 6.4109\n",
      "Epoch [61/300], Step [1/172], Loss: 97.4881\n",
      "Epoch [61/300], Step [2/172], Loss: 96.7359\n",
      "Epoch [61/300], Step [3/172], Loss: 103.0250\n",
      "Epoch [61/300], Step [4/172], Loss: 62.0621\n",
      "Epoch [61/300], Step [5/172], Loss: 85.2636\n",
      "Epoch [61/300], Step [6/172], Loss: 29.4734\n",
      "Epoch [61/300], Step [7/172], Loss: 36.8488\n",
      "Epoch [61/300], Step [8/172], Loss: 10.6697\n",
      "Epoch [61/300], Step [9/172], Loss: 50.9501\n",
      "Epoch [61/300], Step [10/172], Loss: 57.6875\n",
      "Epoch [61/300], Step [11/172], Loss: 99.8633\n",
      "Epoch [61/300], Step [12/172], Loss: 86.6331\n",
      "Epoch [61/300], Step [13/172], Loss: 44.7690\n",
      "Epoch [61/300], Step [14/172], Loss: 99.1298\n",
      "Epoch [61/300], Step [15/172], Loss: 87.8728\n",
      "Epoch [61/300], Step [16/172], Loss: 51.7383\n",
      "Epoch [61/300], Step [17/172], Loss: 60.8496\n",
      "Epoch [61/300], Step [18/172], Loss: 67.6992\n",
      "Epoch [61/300], Step [19/172], Loss: 81.4249\n",
      "Epoch [61/300], Step [20/172], Loss: 100.4801\n",
      "Epoch [61/300], Step [21/172], Loss: 96.9714\n",
      "Epoch [61/300], Step [22/172], Loss: 90.8633\n",
      "Epoch [61/300], Step [23/172], Loss: 14.8317\n",
      "Epoch [61/300], Step [24/172], Loss: 78.4290\n",
      "Epoch [61/300], Step [25/172], Loss: 53.2319\n",
      "Epoch [61/300], Step [26/172], Loss: 62.6991\n",
      "Epoch [61/300], Step [27/172], Loss: 87.0484\n",
      "Epoch [61/300], Step [28/172], Loss: 54.6617\n",
      "Epoch [61/300], Step [29/172], Loss: 54.7126\n",
      "Epoch [61/300], Step [30/172], Loss: 77.8753\n",
      "Epoch [61/300], Step [31/172], Loss: 45.2963\n",
      "Epoch [61/300], Step [32/172], Loss: 41.4250\n",
      "Epoch [61/300], Step [33/172], Loss: 75.2657\n",
      "Epoch [61/300], Step [34/172], Loss: 8.0713\n",
      "Epoch [61/300], Step [35/172], Loss: 40.8106\n",
      "Epoch [61/300], Step [36/172], Loss: 26.6288\n",
      "Epoch [61/300], Step [37/172], Loss: 20.0175\n",
      "Epoch [61/300], Step [38/172], Loss: 28.7458\n",
      "Epoch [61/300], Step [39/172], Loss: 54.2604\n",
      "Epoch [61/300], Step [40/172], Loss: 27.2686\n",
      "Epoch [61/300], Step [41/172], Loss: 41.7936\n",
      "Epoch [61/300], Step [42/172], Loss: 43.7288\n",
      "Epoch [61/300], Step [43/172], Loss: 29.7409\n",
      "Epoch [61/300], Step [44/172], Loss: 26.4063\n",
      "Epoch [61/300], Step [45/172], Loss: 23.0153\n",
      "Epoch [61/300], Step [46/172], Loss: 35.5970\n",
      "Epoch [61/300], Step [47/172], Loss: 59.8205\n",
      "Epoch [61/300], Step [48/172], Loss: 63.2813\n",
      "Epoch [61/300], Step [49/172], Loss: 22.3528\n",
      "Epoch [61/300], Step [50/172], Loss: 52.3065\n",
      "Epoch [61/300], Step [51/172], Loss: 8.2119\n",
      "Epoch [61/300], Step [52/172], Loss: 22.1812\n",
      "Epoch [61/300], Step [53/172], Loss: 28.5780\n",
      "Epoch [61/300], Step [54/172], Loss: 16.0765\n",
      "Epoch [61/300], Step [55/172], Loss: 14.6942\n",
      "Epoch [61/300], Step [56/172], Loss: 10.9980\n",
      "Epoch [61/300], Step [57/172], Loss: 37.2729\n",
      "Epoch [61/300], Step [58/172], Loss: 20.8429\n",
      "Epoch [61/300], Step [59/172], Loss: 36.2736\n",
      "Epoch [61/300], Step [60/172], Loss: 58.2779\n",
      "Epoch [61/300], Step [61/172], Loss: 11.4228\n",
      "Epoch [61/300], Step [62/172], Loss: 22.7617\n",
      "Epoch [61/300], Step [63/172], Loss: 9.2421\n",
      "Epoch [61/300], Step [64/172], Loss: 7.4282\n",
      "Epoch [61/300], Step [65/172], Loss: 27.0026\n",
      "Epoch [61/300], Step [66/172], Loss: 7.3420\n",
      "Epoch [61/300], Step [67/172], Loss: 28.0283\n",
      "Epoch [61/300], Step [68/172], Loss: 11.4330\n",
      "Epoch [61/300], Step [69/172], Loss: 75.6404\n",
      "Epoch [61/300], Step [70/172], Loss: 67.7582\n",
      "Epoch [61/300], Step [71/172], Loss: 60.9065\n",
      "Epoch [61/300], Step [72/172], Loss: 63.4301\n",
      "Epoch [61/300], Step [73/172], Loss: 69.1907\n",
      "Epoch [61/300], Step [74/172], Loss: 42.7888\n",
      "Epoch [61/300], Step [75/172], Loss: 32.6855\n",
      "Epoch [61/300], Step [76/172], Loss: 46.0197\n",
      "Epoch [61/300], Step [77/172], Loss: 61.8466\n",
      "Epoch [61/300], Step [78/172], Loss: 56.7966\n",
      "Epoch [61/300], Step [79/172], Loss: 52.8162\n",
      "Epoch [61/300], Step [80/172], Loss: 56.7834\n",
      "Epoch [61/300], Step [81/172], Loss: 48.7165\n",
      "Epoch [61/300], Step [82/172], Loss: 43.5204\n",
      "Epoch [61/300], Step [83/172], Loss: 52.5347\n",
      "Epoch [61/300], Step [84/172], Loss: 42.8184\n",
      "Epoch [61/300], Step [85/172], Loss: 48.5939\n",
      "Epoch [61/300], Step [86/172], Loss: 38.5361\n",
      "Epoch [61/300], Step [87/172], Loss: 31.6289\n",
      "Epoch [61/300], Step [88/172], Loss: 34.0857\n",
      "Epoch [61/300], Step [89/172], Loss: 32.2567\n",
      "Epoch [61/300], Step [90/172], Loss: 30.6671\n",
      "Epoch [61/300], Step [91/172], Loss: 32.0445\n",
      "Epoch [61/300], Step [92/172], Loss: 25.7954\n",
      "Epoch [61/300], Step [93/172], Loss: 25.7955\n",
      "Epoch [61/300], Step [94/172], Loss: 31.3002\n",
      "Epoch [61/300], Step [95/172], Loss: 26.9754\n",
      "Epoch [61/300], Step [96/172], Loss: 22.7104\n",
      "Epoch [61/300], Step [97/172], Loss: 29.1320\n",
      "Epoch [61/300], Step [98/172], Loss: 24.0072\n",
      "Epoch [61/300], Step [99/172], Loss: 21.4055\n",
      "Epoch [61/300], Step [100/172], Loss: 20.1618\n",
      "Epoch [61/300], Step [101/172], Loss: 21.9149\n",
      "Epoch [61/300], Step [102/172], Loss: 20.1093\n",
      "Epoch [61/300], Step [103/172], Loss: 19.2191\n",
      "Epoch [61/300], Step [104/172], Loss: 19.1973\n",
      "Epoch [61/300], Step [105/172], Loss: 20.5932\n",
      "Epoch [61/300], Step [106/172], Loss: 20.2186\n",
      "Epoch [61/300], Step [107/172], Loss: 17.6171\n",
      "Epoch [61/300], Step [108/172], Loss: 20.8595\n",
      "Epoch [61/300], Step [109/172], Loss: 22.3257\n",
      "Epoch [61/300], Step [110/172], Loss: 19.4379\n",
      "Epoch [61/300], Step [111/172], Loss: 17.7894\n",
      "Epoch [61/300], Step [112/172], Loss: 23.1298\n",
      "Epoch [61/300], Step [113/172], Loss: 18.5609\n",
      "Epoch [61/300], Step [114/172], Loss: 18.4903\n",
      "Epoch [61/300], Step [115/172], Loss: 25.9311\n",
      "Epoch [61/300], Step [116/172], Loss: 18.5054\n",
      "Epoch [61/300], Step [117/172], Loss: 15.7042\n",
      "Epoch [61/300], Step [118/172], Loss: 19.1853\n",
      "Epoch [61/300], Step [119/172], Loss: 16.9710\n",
      "Epoch [61/300], Step [120/172], Loss: 14.5045\n",
      "Epoch [61/300], Step [121/172], Loss: 15.0607\n",
      "Epoch [61/300], Step [122/172], Loss: 14.1084\n",
      "Epoch [61/300], Step [123/172], Loss: 13.5147\n",
      "Epoch [61/300], Step [124/172], Loss: 11.2132\n",
      "Epoch [61/300], Step [125/172], Loss: 15.8702\n",
      "Epoch [61/300], Step [126/172], Loss: 14.2148\n",
      "Epoch [61/300], Step [127/172], Loss: 16.7432\n",
      "Epoch [61/300], Step [128/172], Loss: 17.2376\n",
      "Epoch [61/300], Step [129/172], Loss: 12.1536\n",
      "Epoch [61/300], Step [130/172], Loss: 14.2499\n",
      "Epoch [61/300], Step [131/172], Loss: 12.3805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/300], Step [132/172], Loss: 12.1929\n",
      "Epoch [61/300], Step [133/172], Loss: 12.8994\n",
      "Epoch [61/300], Step [134/172], Loss: 13.6696\n",
      "Epoch [61/300], Step [135/172], Loss: 11.0729\n",
      "Epoch [61/300], Step [136/172], Loss: 11.0057\n",
      "Epoch [61/300], Step [137/172], Loss: 12.9388\n",
      "Epoch [61/300], Step [138/172], Loss: 11.0700\n",
      "Epoch [61/300], Step [139/172], Loss: 12.3629\n",
      "Epoch [61/300], Step [140/172], Loss: 12.4534\n",
      "Epoch [61/300], Step [141/172], Loss: 15.2719\n",
      "Epoch [61/300], Step [142/172], Loss: 15.3190\n",
      "Epoch [61/300], Step [143/172], Loss: 11.3294\n",
      "Epoch [61/300], Step [144/172], Loss: 11.1474\n",
      "Epoch [61/300], Step [145/172], Loss: 11.3028\n",
      "Epoch [61/300], Step [146/172], Loss: 11.7823\n",
      "Epoch [61/300], Step [147/172], Loss: 7.9189\n",
      "Epoch [61/300], Step [148/172], Loss: 8.8281\n",
      "Epoch [61/300], Step [149/172], Loss: 10.9752\n",
      "Epoch [61/300], Step [150/172], Loss: 10.5908\n",
      "Epoch [61/300], Step [151/172], Loss: 9.4410\n",
      "Epoch [61/300], Step [152/172], Loss: 9.4835\n",
      "Epoch [61/300], Step [153/172], Loss: 9.3497\n",
      "Epoch [61/300], Step [154/172], Loss: 10.2850\n",
      "Epoch [61/300], Step [155/172], Loss: 9.1697\n",
      "Epoch [61/300], Step [156/172], Loss: 12.0244\n",
      "Epoch [61/300], Step [157/172], Loss: 12.0076\n",
      "Epoch [61/300], Step [158/172], Loss: 9.9172\n",
      "Epoch [61/300], Step [159/172], Loss: 10.6531\n",
      "Epoch [61/300], Step [160/172], Loss: 10.5176\n",
      "Epoch [61/300], Step [161/172], Loss: 8.5666\n",
      "Epoch [61/300], Step [162/172], Loss: 9.1434\n",
      "Epoch [61/300], Step [163/172], Loss: 8.4675\n",
      "Epoch [61/300], Step [164/172], Loss: 11.0769\n",
      "Epoch [61/300], Step [165/172], Loss: 7.9917\n",
      "Epoch [61/300], Step [166/172], Loss: 8.4121\n",
      "Epoch [61/300], Step [167/172], Loss: 9.1073\n",
      "Epoch [61/300], Step [168/172], Loss: 8.1687\n",
      "Epoch [61/300], Step [169/172], Loss: 8.1452\n",
      "Epoch [61/300], Step [170/172], Loss: 7.5882\n",
      "Epoch [61/300], Step [171/172], Loss: 6.8394\n",
      "Epoch [61/300], Step [172/172], Loss: 6.3484\n",
      "Epoch [62/300], Step [1/172], Loss: 97.3182\n",
      "Epoch [62/300], Step [2/172], Loss: 96.6148\n",
      "Epoch [62/300], Step [3/172], Loss: 102.4759\n",
      "Epoch [62/300], Step [4/172], Loss: 61.7264\n",
      "Epoch [62/300], Step [5/172], Loss: 84.7767\n",
      "Epoch [62/300], Step [6/172], Loss: 28.9234\n",
      "Epoch [62/300], Step [7/172], Loss: 36.0775\n",
      "Epoch [62/300], Step [8/172], Loss: 10.5675\n",
      "Epoch [62/300], Step [9/172], Loss: 50.9186\n",
      "Epoch [62/300], Step [10/172], Loss: 57.3894\n",
      "Epoch [62/300], Step [11/172], Loss: 100.0264\n",
      "Epoch [62/300], Step [12/172], Loss: 86.7940\n",
      "Epoch [62/300], Step [13/172], Loss: 44.6001\n",
      "Epoch [62/300], Step [14/172], Loss: 99.1152\n",
      "Epoch [62/300], Step [15/172], Loss: 87.7600\n",
      "Epoch [62/300], Step [16/172], Loss: 50.7958\n",
      "Epoch [62/300], Step [17/172], Loss: 60.8232\n",
      "Epoch [62/300], Step [18/172], Loss: 67.3213\n",
      "Epoch [62/300], Step [19/172], Loss: 81.5891\n",
      "Epoch [62/300], Step [20/172], Loss: 99.6893\n",
      "Epoch [62/300], Step [21/172], Loss: 96.9618\n",
      "Epoch [62/300], Step [22/172], Loss: 90.6082\n",
      "Epoch [62/300], Step [23/172], Loss: 14.3072\n",
      "Epoch [62/300], Step [24/172], Loss: 78.0693\n",
      "Epoch [62/300], Step [25/172], Loss: 53.1024\n",
      "Epoch [62/300], Step [26/172], Loss: 62.7283\n",
      "Epoch [62/300], Step [27/172], Loss: 86.9927\n",
      "Epoch [62/300], Step [28/172], Loss: 53.6850\n",
      "Epoch [62/300], Step [29/172], Loss: 53.5034\n",
      "Epoch [62/300], Step [30/172], Loss: 77.9494\n",
      "Epoch [62/300], Step [31/172], Loss: 45.3288\n",
      "Epoch [62/300], Step [32/172], Loss: 41.3129\n",
      "Epoch [62/300], Step [33/172], Loss: 75.1118\n",
      "Epoch [62/300], Step [34/172], Loss: 7.8629\n",
      "Epoch [62/300], Step [35/172], Loss: 39.6550\n",
      "Epoch [62/300], Step [36/172], Loss: 26.3162\n",
      "Epoch [62/300], Step [37/172], Loss: 19.7876\n",
      "Epoch [62/300], Step [38/172], Loss: 28.3854\n",
      "Epoch [62/300], Step [39/172], Loss: 54.0168\n",
      "Epoch [62/300], Step [40/172], Loss: 26.9845\n",
      "Epoch [62/300], Step [41/172], Loss: 41.6937\n",
      "Epoch [62/300], Step [42/172], Loss: 43.7275\n",
      "Epoch [62/300], Step [43/172], Loss: 29.4708\n",
      "Epoch [62/300], Step [44/172], Loss: 26.3671\n",
      "Epoch [62/300], Step [45/172], Loss: 22.9225\n",
      "Epoch [62/300], Step [46/172], Loss: 35.0759\n",
      "Epoch [62/300], Step [47/172], Loss: 59.3275\n",
      "Epoch [62/300], Step [48/172], Loss: 62.5272\n",
      "Epoch [62/300], Step [49/172], Loss: 22.1134\n",
      "Epoch [62/300], Step [50/172], Loss: 52.0284\n",
      "Epoch [62/300], Step [51/172], Loss: 8.1033\n",
      "Epoch [62/300], Step [52/172], Loss: 21.9855\n",
      "Epoch [62/300], Step [53/172], Loss: 28.4079\n",
      "Epoch [62/300], Step [54/172], Loss: 15.9143\n",
      "Epoch [62/300], Step [55/172], Loss: 14.5978\n",
      "Epoch [62/300], Step [56/172], Loss: 10.8332\n",
      "Epoch [62/300], Step [57/172], Loss: 36.4929\n",
      "Epoch [62/300], Step [58/172], Loss: 20.7816\n",
      "Epoch [62/300], Step [59/172], Loss: 36.2154\n",
      "Epoch [62/300], Step [60/172], Loss: 57.4419\n",
      "Epoch [62/300], Step [61/172], Loss: 11.3371\n",
      "Epoch [62/300], Step [62/172], Loss: 22.7302\n",
      "Epoch [62/300], Step [63/172], Loss: 9.0447\n",
      "Epoch [62/300], Step [64/172], Loss: 7.3310\n",
      "Epoch [62/300], Step [65/172], Loss: 26.5124\n",
      "Epoch [62/300], Step [66/172], Loss: 7.2176\n",
      "Epoch [62/300], Step [67/172], Loss: 27.9837\n",
      "Epoch [62/300], Step [68/172], Loss: 11.3508\n",
      "Epoch [62/300], Step [69/172], Loss: 75.5288\n",
      "Epoch [62/300], Step [70/172], Loss: 67.7232\n",
      "Epoch [62/300], Step [71/172], Loss: 60.8467\n",
      "Epoch [62/300], Step [72/172], Loss: 63.6682\n",
      "Epoch [62/300], Step [73/172], Loss: 69.4482\n",
      "Epoch [62/300], Step [74/172], Loss: 42.7209\n",
      "Epoch [62/300], Step [75/172], Loss: 32.7365\n",
      "Epoch [62/300], Step [76/172], Loss: 45.9320\n",
      "Epoch [62/300], Step [77/172], Loss: 61.9819\n",
      "Epoch [62/300], Step [78/172], Loss: 56.6693\n",
      "Epoch [62/300], Step [79/172], Loss: 52.8214\n",
      "Epoch [62/300], Step [80/172], Loss: 56.8628\n",
      "Epoch [62/300], Step [81/172], Loss: 48.4367\n",
      "Epoch [62/300], Step [82/172], Loss: 43.4580\n",
      "Epoch [62/300], Step [83/172], Loss: 52.5448\n",
      "Epoch [62/300], Step [84/172], Loss: 42.5935\n",
      "Epoch [62/300], Step [85/172], Loss: 48.1558\n",
      "Epoch [62/300], Step [86/172], Loss: 38.3430\n",
      "Epoch [62/300], Step [87/172], Loss: 31.5023\n",
      "Epoch [62/300], Step [88/172], Loss: 33.8748\n",
      "Epoch [62/300], Step [89/172], Loss: 31.9557\n",
      "Epoch [62/300], Step [90/172], Loss: 30.4181\n",
      "Epoch [62/300], Step [91/172], Loss: 31.8886\n",
      "Epoch [62/300], Step [92/172], Loss: 25.5776\n",
      "Epoch [62/300], Step [93/172], Loss: 25.5156\n",
      "Epoch [62/300], Step [94/172], Loss: 31.1433\n",
      "Epoch [62/300], Step [95/172], Loss: 26.7542\n",
      "Epoch [62/300], Step [96/172], Loss: 22.4691\n",
      "Epoch [62/300], Step [97/172], Loss: 28.9294\n",
      "Epoch [62/300], Step [98/172], Loss: 23.7743\n",
      "Epoch [62/300], Step [99/172], Loss: 21.2038\n",
      "Epoch [62/300], Step [100/172], Loss: 19.9531\n",
      "Epoch [62/300], Step [101/172], Loss: 21.6651\n",
      "Epoch [62/300], Step [102/172], Loss: 19.9393\n",
      "Epoch [62/300], Step [103/172], Loss: 18.9726\n",
      "Epoch [62/300], Step [104/172], Loss: 19.0093\n",
      "Epoch [62/300], Step [105/172], Loss: 20.4534\n",
      "Epoch [62/300], Step [106/172], Loss: 20.0294\n",
      "Epoch [62/300], Step [107/172], Loss: 17.5054\n",
      "Epoch [62/300], Step [108/172], Loss: 20.6932\n",
      "Epoch [62/300], Step [109/172], Loss: 22.1695\n",
      "Epoch [62/300], Step [110/172], Loss: 19.2867\n",
      "Epoch [62/300], Step [111/172], Loss: 17.6164\n",
      "Epoch [62/300], Step [112/172], Loss: 22.9101\n",
      "Epoch [62/300], Step [113/172], Loss: 18.4345\n",
      "Epoch [62/300], Step [114/172], Loss: 18.3566\n",
      "Epoch [62/300], Step [115/172], Loss: 25.7678\n",
      "Epoch [62/300], Step [116/172], Loss: 18.3310\n",
      "Epoch [62/300], Step [117/172], Loss: 15.5400\n",
      "Epoch [62/300], Step [118/172], Loss: 18.9972\n",
      "Epoch [62/300], Step [119/172], Loss: 16.8899\n",
      "Epoch [62/300], Step [120/172], Loss: 14.3966\n",
      "Epoch [62/300], Step [121/172], Loss: 14.9188\n",
      "Epoch [62/300], Step [122/172], Loss: 14.0672\n",
      "Epoch [62/300], Step [123/172], Loss: 13.3545\n",
      "Epoch [62/300], Step [124/172], Loss: 11.0985\n",
      "Epoch [62/300], Step [125/172], Loss: 15.7388\n",
      "Epoch [62/300], Step [126/172], Loss: 14.0552\n",
      "Epoch [62/300], Step [127/172], Loss: 16.6159\n",
      "Epoch [62/300], Step [128/172], Loss: 17.0839\n",
      "Epoch [62/300], Step [129/172], Loss: 12.0402\n",
      "Epoch [62/300], Step [130/172], Loss: 14.1060\n",
      "Epoch [62/300], Step [131/172], Loss: 12.2745\n",
      "Epoch [62/300], Step [132/172], Loss: 12.0655\n",
      "Epoch [62/300], Step [133/172], Loss: 12.8334\n",
      "Epoch [62/300], Step [134/172], Loss: 13.5835\n",
      "Epoch [62/300], Step [135/172], Loss: 10.9834\n",
      "Epoch [62/300], Step [136/172], Loss: 10.8954\n",
      "Epoch [62/300], Step [137/172], Loss: 12.8476\n",
      "Epoch [62/300], Step [138/172], Loss: 11.0038\n",
      "Epoch [62/300], Step [139/172], Loss: 12.2599\n",
      "Epoch [62/300], Step [140/172], Loss: 12.3546\n",
      "Epoch [62/300], Step [141/172], Loss: 15.1961\n",
      "Epoch [62/300], Step [142/172], Loss: 15.2725\n",
      "Epoch [62/300], Step [143/172], Loss: 11.2277\n",
      "Epoch [62/300], Step [144/172], Loss: 11.1015\n",
      "Epoch [62/300], Step [145/172], Loss: 11.2447\n",
      "Epoch [62/300], Step [146/172], Loss: 11.7131\n",
      "Epoch [62/300], Step [147/172], Loss: 7.8274\n",
      "Epoch [62/300], Step [148/172], Loss: 8.7543\n",
      "Epoch [62/300], Step [149/172], Loss: 10.8702\n",
      "Epoch [62/300], Step [150/172], Loss: 10.5121\n",
      "Epoch [62/300], Step [151/172], Loss: 9.3742\n",
      "Epoch [62/300], Step [152/172], Loss: 9.4088\n",
      "Epoch [62/300], Step [153/172], Loss: 9.2725\n",
      "Epoch [62/300], Step [154/172], Loss: 10.1884\n",
      "Epoch [62/300], Step [155/172], Loss: 9.0911\n",
      "Epoch [62/300], Step [156/172], Loss: 12.0335\n",
      "Epoch [62/300], Step [157/172], Loss: 11.9409\n",
      "Epoch [62/300], Step [158/172], Loss: 9.8538\n",
      "Epoch [62/300], Step [159/172], Loss: 10.5901\n",
      "Epoch [62/300], Step [160/172], Loss: 10.5060\n",
      "Epoch [62/300], Step [161/172], Loss: 8.5253\n",
      "Epoch [62/300], Step [162/172], Loss: 9.0884\n",
      "Epoch [62/300], Step [163/172], Loss: 8.4181\n",
      "Epoch [62/300], Step [164/172], Loss: 11.1003\n",
      "Epoch [62/300], Step [165/172], Loss: 7.9513\n",
      "Epoch [62/300], Step [166/172], Loss: 8.3860\n",
      "Epoch [62/300], Step [167/172], Loss: 9.0781\n",
      "Epoch [62/300], Step [168/172], Loss: 8.1235\n",
      "Epoch [62/300], Step [169/172], Loss: 8.1330\n",
      "Epoch [62/300], Step [170/172], Loss: 7.5500\n",
      "Epoch [62/300], Step [171/172], Loss: 6.8490\n",
      "Epoch [62/300], Step [172/172], Loss: 6.3195\n",
      "Epoch [63/300], Step [1/172], Loss: 97.0203\n",
      "Epoch [63/300], Step [2/172], Loss: 96.4422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/300], Step [3/172], Loss: 101.3481\n",
      "Epoch [63/300], Step [4/172], Loss: 61.0599\n",
      "Epoch [63/300], Step [5/172], Loss: 84.4748\n",
      "Epoch [63/300], Step [6/172], Loss: 28.5333\n",
      "Epoch [63/300], Step [7/172], Loss: 35.4304\n",
      "Epoch [63/300], Step [8/172], Loss: 10.1103\n",
      "Epoch [63/300], Step [9/172], Loss: 50.5304\n",
      "Epoch [63/300], Step [10/172], Loss: 57.0524\n",
      "Epoch [63/300], Step [11/172], Loss: 99.7384\n",
      "Epoch [63/300], Step [12/172], Loss: 86.5569\n",
      "Epoch [63/300], Step [13/172], Loss: 44.1571\n",
      "Epoch [63/300], Step [14/172], Loss: 98.6912\n",
      "Epoch [63/300], Step [15/172], Loss: 87.2277\n",
      "Epoch [63/300], Step [16/172], Loss: 49.4843\n",
      "Epoch [63/300], Step [17/172], Loss: 60.6606\n",
      "Epoch [63/300], Step [18/172], Loss: 66.8393\n",
      "Epoch [63/300], Step [19/172], Loss: 81.4054\n",
      "Epoch [63/300], Step [20/172], Loss: 98.4768\n",
      "Epoch [63/300], Step [21/172], Loss: 96.6752\n",
      "Epoch [63/300], Step [22/172], Loss: 90.0063\n",
      "Epoch [63/300], Step [23/172], Loss: 13.6869\n",
      "Epoch [63/300], Step [24/172], Loss: 77.6623\n",
      "Epoch [63/300], Step [25/172], Loss: 52.5311\n",
      "Epoch [63/300], Step [26/172], Loss: 62.3460\n",
      "Epoch [63/300], Step [27/172], Loss: 86.2758\n",
      "Epoch [63/300], Step [28/172], Loss: 52.9988\n",
      "Epoch [63/300], Step [29/172], Loss: 52.1387\n",
      "Epoch [63/300], Step [30/172], Loss: 77.8344\n",
      "Epoch [63/300], Step [31/172], Loss: 44.9642\n",
      "Epoch [63/300], Step [32/172], Loss: 41.0353\n",
      "Epoch [63/300], Step [33/172], Loss: 74.4955\n",
      "Epoch [63/300], Step [34/172], Loss: 7.5477\n",
      "Epoch [63/300], Step [35/172], Loss: 38.7667\n",
      "Epoch [63/300], Step [36/172], Loss: 26.1767\n",
      "Epoch [63/300], Step [37/172], Loss: 19.5720\n",
      "Epoch [63/300], Step [38/172], Loss: 28.0697\n",
      "Epoch [63/300], Step [39/172], Loss: 53.7908\n",
      "Epoch [63/300], Step [40/172], Loss: 26.5382\n",
      "Epoch [63/300], Step [41/172], Loss: 41.3524\n",
      "Epoch [63/300], Step [42/172], Loss: 43.4445\n",
      "Epoch [63/300], Step [43/172], Loss: 29.0745\n",
      "Epoch [63/300], Step [44/172], Loss: 25.9510\n",
      "Epoch [63/300], Step [45/172], Loss: 22.6855\n",
      "Epoch [63/300], Step [46/172], Loss: 34.5197\n",
      "Epoch [63/300], Step [47/172], Loss: 58.8142\n",
      "Epoch [63/300], Step [48/172], Loss: 62.0332\n",
      "Epoch [63/300], Step [49/172], Loss: 22.0101\n",
      "Epoch [63/300], Step [50/172], Loss: 52.1202\n",
      "Epoch [63/300], Step [51/172], Loss: 8.0501\n",
      "Epoch [63/300], Step [52/172], Loss: 21.7689\n",
      "Epoch [63/300], Step [53/172], Loss: 28.2775\n",
      "Epoch [63/300], Step [54/172], Loss: 15.5607\n",
      "Epoch [63/300], Step [55/172], Loss: 14.5228\n",
      "Epoch [63/300], Step [56/172], Loss: 10.8102\n",
      "Epoch [63/300], Step [57/172], Loss: 35.9987\n",
      "Epoch [63/300], Step [58/172], Loss: 20.7883\n",
      "Epoch [63/300], Step [59/172], Loss: 35.7884\n",
      "Epoch [63/300], Step [60/172], Loss: 57.3010\n",
      "Epoch [63/300], Step [61/172], Loss: 11.3357\n",
      "Epoch [63/300], Step [62/172], Loss: 22.9398\n",
      "Epoch [63/300], Step [63/172], Loss: 9.0737\n",
      "Epoch [63/300], Step [64/172], Loss: 7.3588\n",
      "Epoch [63/300], Step [65/172], Loss: 26.4491\n",
      "Epoch [63/300], Step [66/172], Loss: 7.1000\n",
      "Epoch [63/300], Step [67/172], Loss: 28.0211\n",
      "Epoch [63/300], Step [68/172], Loss: 10.9214\n",
      "Epoch [63/300], Step [69/172], Loss: 75.3691\n",
      "Epoch [63/300], Step [70/172], Loss: 67.3306\n",
      "Epoch [63/300], Step [71/172], Loss: 60.6897\n",
      "Epoch [63/300], Step [72/172], Loss: 63.5262\n",
      "Epoch [63/300], Step [73/172], Loss: 69.0835\n",
      "Epoch [63/300], Step [74/172], Loss: 42.4862\n",
      "Epoch [63/300], Step [75/172], Loss: 33.1378\n",
      "Epoch [63/300], Step [76/172], Loss: 45.6624\n",
      "Epoch [63/300], Step [77/172], Loss: 62.1533\n",
      "Epoch [63/300], Step [78/172], Loss: 56.4956\n",
      "Epoch [63/300], Step [79/172], Loss: 52.8426\n",
      "Epoch [63/300], Step [80/172], Loss: 56.9778\n",
      "Epoch [63/300], Step [81/172], Loss: 48.3206\n",
      "Epoch [63/300], Step [82/172], Loss: 43.1756\n",
      "Epoch [63/300], Step [83/172], Loss: 52.5781\n",
      "Epoch [63/300], Step [84/172], Loss: 42.5539\n",
      "Epoch [63/300], Step [85/172], Loss: 48.1402\n",
      "Epoch [63/300], Step [86/172], Loss: 38.3168\n",
      "Epoch [63/300], Step [87/172], Loss: 31.6010\n",
      "Epoch [63/300], Step [88/172], Loss: 33.9444\n",
      "Epoch [63/300], Step [89/172], Loss: 31.9089\n",
      "Epoch [63/300], Step [90/172], Loss: 30.3436\n",
      "Epoch [63/300], Step [91/172], Loss: 31.9933\n",
      "Epoch [63/300], Step [92/172], Loss: 25.5891\n",
      "Epoch [63/300], Step [93/172], Loss: 25.4618\n",
      "Epoch [63/300], Step [94/172], Loss: 31.2441\n",
      "Epoch [63/300], Step [95/172], Loss: 26.7451\n",
      "Epoch [63/300], Step [96/172], Loss: 22.4026\n",
      "Epoch [63/300], Step [97/172], Loss: 28.9283\n",
      "Epoch [63/300], Step [98/172], Loss: 23.7312\n",
      "Epoch [63/300], Step [99/172], Loss: 21.1719\n",
      "Epoch [63/300], Step [100/172], Loss: 19.9027\n",
      "Epoch [63/300], Step [101/172], Loss: 21.6127\n",
      "Epoch [63/300], Step [102/172], Loss: 19.7199\n",
      "Epoch [63/300], Step [103/172], Loss: 18.8542\n",
      "Epoch [63/300], Step [104/172], Loss: 18.9234\n",
      "Epoch [63/300], Step [105/172], Loss: 20.3427\n",
      "Epoch [63/300], Step [106/172], Loss: 19.9900\n",
      "Epoch [63/300], Step [107/172], Loss: 17.4048\n",
      "Epoch [63/300], Step [108/172], Loss: 20.5815\n",
      "Epoch [63/300], Step [109/172], Loss: 22.0050\n",
      "Epoch [63/300], Step [110/172], Loss: 19.1849\n",
      "Epoch [63/300], Step [111/172], Loss: 17.5731\n",
      "Epoch [63/300], Step [112/172], Loss: 22.8532\n",
      "Epoch [63/300], Step [113/172], Loss: 18.3462\n",
      "Epoch [63/300], Step [114/172], Loss: 18.2379\n",
      "Epoch [63/300], Step [115/172], Loss: 25.7211\n",
      "Epoch [63/300], Step [116/172], Loss: 18.2803\n",
      "Epoch [63/300], Step [117/172], Loss: 15.5072\n",
      "Epoch [63/300], Step [118/172], Loss: 18.9383\n",
      "Epoch [63/300], Step [119/172], Loss: 16.9055\n",
      "Epoch [63/300], Step [120/172], Loss: 14.2661\n",
      "Epoch [63/300], Step [121/172], Loss: 14.8253\n",
      "Epoch [63/300], Step [122/172], Loss: 14.0371\n",
      "Epoch [63/300], Step [123/172], Loss: 13.3035\n",
      "Epoch [63/300], Step [124/172], Loss: 11.0302\n",
      "Epoch [63/300], Step [125/172], Loss: 15.7205\n",
      "Epoch [63/300], Step [126/172], Loss: 13.9807\n",
      "Epoch [63/300], Step [127/172], Loss: 16.5518\n",
      "Epoch [63/300], Step [128/172], Loss: 17.0370\n",
      "Epoch [63/300], Step [129/172], Loss: 11.9865\n",
      "Epoch [63/300], Step [130/172], Loss: 14.0645\n",
      "Epoch [63/300], Step [131/172], Loss: 12.2021\n",
      "Epoch [63/300], Step [132/172], Loss: 11.9988\n",
      "Epoch [63/300], Step [133/172], Loss: 12.7868\n",
      "Epoch [63/300], Step [134/172], Loss: 13.5772\n",
      "Epoch [63/300], Step [135/172], Loss: 10.9469\n",
      "Epoch [63/300], Step [136/172], Loss: 10.8390\n",
      "Epoch [63/300], Step [137/172], Loss: 12.7850\n",
      "Epoch [63/300], Step [138/172], Loss: 10.9441\n",
      "Epoch [63/300], Step [139/172], Loss: 12.2061\n",
      "Epoch [63/300], Step [140/172], Loss: 12.2991\n",
      "Epoch [63/300], Step [141/172], Loss: 15.1161\n",
      "Epoch [63/300], Step [142/172], Loss: 15.2786\n",
      "Epoch [63/300], Step [143/172], Loss: 11.1860\n",
      "Epoch [63/300], Step [144/172], Loss: 11.0426\n",
      "Epoch [63/300], Step [145/172], Loss: 11.2417\n",
      "Epoch [63/300], Step [146/172], Loss: 11.6937\n",
      "Epoch [63/300], Step [147/172], Loss: 7.7742\n",
      "Epoch [63/300], Step [148/172], Loss: 8.7038\n",
      "Epoch [63/300], Step [149/172], Loss: 10.8285\n",
      "Epoch [63/300], Step [150/172], Loss: 10.4606\n",
      "Epoch [63/300], Step [151/172], Loss: 9.3315\n",
      "Epoch [63/300], Step [152/172], Loss: 9.3709\n",
      "Epoch [63/300], Step [153/172], Loss: 9.2200\n",
      "Epoch [63/300], Step [154/172], Loss: 10.1231\n",
      "Epoch [63/300], Step [155/172], Loss: 9.0716\n",
      "Epoch [63/300], Step [156/172], Loss: 12.0593\n",
      "Epoch [63/300], Step [157/172], Loss: 11.9089\n",
      "Epoch [63/300], Step [158/172], Loss: 9.7959\n",
      "Epoch [63/300], Step [159/172], Loss: 10.6092\n",
      "Epoch [63/300], Step [160/172], Loss: 10.4894\n",
      "Epoch [63/300], Step [161/172], Loss: 8.5055\n",
      "Epoch [63/300], Step [162/172], Loss: 9.0737\n",
      "Epoch [63/300], Step [163/172], Loss: 8.3559\n",
      "Epoch [63/300], Step [164/172], Loss: 10.9866\n",
      "Epoch [63/300], Step [165/172], Loss: 7.9122\n",
      "Epoch [63/300], Step [166/172], Loss: 8.3132\n",
      "Epoch [63/300], Step [167/172], Loss: 9.0662\n",
      "Epoch [63/300], Step [168/172], Loss: 8.0545\n",
      "Epoch [63/300], Step [169/172], Loss: 8.1054\n",
      "Epoch [63/300], Step [170/172], Loss: 7.4994\n",
      "Epoch [63/300], Step [171/172], Loss: 6.8267\n",
      "Epoch [63/300], Step [172/172], Loss: 6.3011\n",
      "Epoch [64/300], Step [1/172], Loss: 96.6595\n",
      "Epoch [64/300], Step [2/172], Loss: 96.2561\n",
      "Epoch [64/300], Step [3/172], Loss: 100.1790\n",
      "Epoch [64/300], Step [4/172], Loss: 60.6619\n",
      "Epoch [64/300], Step [5/172], Loss: 83.9762\n",
      "Epoch [64/300], Step [6/172], Loss: 28.2939\n",
      "Epoch [64/300], Step [7/172], Loss: 35.3687\n",
      "Epoch [64/300], Step [8/172], Loss: 10.7148\n",
      "Epoch [64/300], Step [9/172], Loss: 50.9270\n",
      "Epoch [64/300], Step [10/172], Loss: 56.8479\n",
      "Epoch [64/300], Step [11/172], Loss: 100.0925\n",
      "Epoch [64/300], Step [12/172], Loss: 86.9858\n",
      "Epoch [64/300], Step [13/172], Loss: 44.3126\n",
      "Epoch [64/300], Step [14/172], Loss: 99.2463\n",
      "Epoch [64/300], Step [15/172], Loss: 87.5075\n",
      "Epoch [64/300], Step [16/172], Loss: 49.1215\n",
      "Epoch [64/300], Step [17/172], Loss: 60.9984\n",
      "Epoch [64/300], Step [18/172], Loss: 67.1562\n",
      "Epoch [64/300], Step [19/172], Loss: 81.9591\n",
      "Epoch [64/300], Step [20/172], Loss: 98.0185\n",
      "Epoch [64/300], Step [21/172], Loss: 97.0234\n",
      "Epoch [64/300], Step [22/172], Loss: 90.4249\n",
      "Epoch [64/300], Step [23/172], Loss: 13.1611\n",
      "Epoch [64/300], Step [24/172], Loss: 77.5256\n",
      "Epoch [64/300], Step [25/172], Loss: 52.6184\n",
      "Epoch [64/300], Step [26/172], Loss: 62.5799\n",
      "Epoch [64/300], Step [27/172], Loss: 86.8166\n",
      "Epoch [64/300], Step [28/172], Loss: 52.1561\n",
      "Epoch [64/300], Step [29/172], Loss: 50.9555\n",
      "Epoch [64/300], Step [30/172], Loss: 78.1607\n",
      "Epoch [64/300], Step [31/172], Loss: 45.0238\n",
      "Epoch [64/300], Step [32/172], Loss: 40.8978\n",
      "Epoch [64/300], Step [33/172], Loss: 74.4633\n",
      "Epoch [64/300], Step [34/172], Loss: 7.4540\n",
      "Epoch [64/300], Step [35/172], Loss: 37.9489\n",
      "Epoch [64/300], Step [36/172], Loss: 25.8143\n",
      "Epoch [64/300], Step [37/172], Loss: 19.4033\n",
      "Epoch [64/300], Step [38/172], Loss: 28.0176\n",
      "Epoch [64/300], Step [39/172], Loss: 53.7188\n",
      "Epoch [64/300], Step [40/172], Loss: 26.2810\n",
      "Epoch [64/300], Step [41/172], Loss: 41.3362\n",
      "Epoch [64/300], Step [42/172], Loss: 43.4556\n",
      "Epoch [64/300], Step [43/172], Loss: 28.9142\n",
      "Epoch [64/300], Step [44/172], Loss: 25.9436\n",
      "Epoch [64/300], Step [45/172], Loss: 22.5478\n",
      "Epoch [64/300], Step [46/172], Loss: 34.1044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/300], Step [47/172], Loss: 58.4087\n",
      "Epoch [64/300], Step [48/172], Loss: 61.6232\n",
      "Epoch [64/300], Step [49/172], Loss: 21.9250\n",
      "Epoch [64/300], Step [50/172], Loss: 52.1483\n",
      "Epoch [64/300], Step [51/172], Loss: 7.9536\n",
      "Epoch [64/300], Step [52/172], Loss: 21.5236\n",
      "Epoch [64/300], Step [53/172], Loss: 28.0663\n",
      "Epoch [64/300], Step [54/172], Loss: 15.3099\n",
      "Epoch [64/300], Step [55/172], Loss: 14.3403\n",
      "Epoch [64/300], Step [56/172], Loss: 10.6925\n",
      "Epoch [64/300], Step [57/172], Loss: 35.3918\n",
      "Epoch [64/300], Step [58/172], Loss: 20.8233\n",
      "Epoch [64/300], Step [59/172], Loss: 35.7102\n",
      "Epoch [64/300], Step [60/172], Loss: 57.2312\n",
      "Epoch [64/300], Step [61/172], Loss: 11.2976\n",
      "Epoch [64/300], Step [62/172], Loss: 22.9177\n",
      "Epoch [64/300], Step [63/172], Loss: 8.9212\n",
      "Epoch [64/300], Step [64/172], Loss: 7.3188\n",
      "Epoch [64/300], Step [65/172], Loss: 26.1320\n",
      "Epoch [64/300], Step [66/172], Loss: 7.0588\n",
      "Epoch [64/300], Step [67/172], Loss: 28.1202\n",
      "Epoch [64/300], Step [68/172], Loss: 10.7075\n",
      "Epoch [64/300], Step [69/172], Loss: 74.9707\n",
      "Epoch [64/300], Step [70/172], Loss: 67.0870\n",
      "Epoch [64/300], Step [71/172], Loss: 60.3269\n",
      "Epoch [64/300], Step [72/172], Loss: 63.4861\n",
      "Epoch [64/300], Step [73/172], Loss: 68.9490\n",
      "Epoch [64/300], Step [74/172], Loss: 42.2122\n",
      "Epoch [64/300], Step [75/172], Loss: 33.0756\n",
      "Epoch [64/300], Step [76/172], Loss: 45.2800\n",
      "Epoch [64/300], Step [77/172], Loss: 61.9316\n",
      "Epoch [64/300], Step [78/172], Loss: 55.9906\n",
      "Epoch [64/300], Step [79/172], Loss: 52.4546\n",
      "Epoch [64/300], Step [80/172], Loss: 56.8288\n",
      "Epoch [64/300], Step [81/172], Loss: 47.7455\n",
      "Epoch [64/300], Step [82/172], Loss: 43.2085\n",
      "Epoch [64/300], Step [83/172], Loss: 52.2379\n",
      "Epoch [64/300], Step [84/172], Loss: 42.0764\n",
      "Epoch [64/300], Step [85/172], Loss: 47.3683\n",
      "Epoch [64/300], Step [86/172], Loss: 38.0026\n",
      "Epoch [64/300], Step [87/172], Loss: 31.3293\n",
      "Epoch [64/300], Step [88/172], Loss: 33.5522\n",
      "Epoch [64/300], Step [89/172], Loss: 31.4578\n",
      "Epoch [64/300], Step [90/172], Loss: 30.0410\n",
      "Epoch [64/300], Step [91/172], Loss: 31.6895\n",
      "Epoch [64/300], Step [92/172], Loss: 25.3016\n",
      "Epoch [64/300], Step [93/172], Loss: 25.1180\n",
      "Epoch [64/300], Step [94/172], Loss: 30.9591\n",
      "Epoch [64/300], Step [95/172], Loss: 26.4495\n",
      "Epoch [64/300], Step [96/172], Loss: 22.1290\n",
      "Epoch [64/300], Step [97/172], Loss: 28.6125\n",
      "Epoch [64/300], Step [98/172], Loss: 23.4372\n",
      "Epoch [64/300], Step [99/172], Loss: 20.8901\n",
      "Epoch [64/300], Step [100/172], Loss: 19.6285\n",
      "Epoch [64/300], Step [101/172], Loss: 21.3122\n",
      "Epoch [64/300], Step [102/172], Loss: 19.6445\n",
      "Epoch [64/300], Step [103/172], Loss: 18.5921\n",
      "Epoch [64/300], Step [104/172], Loss: 18.7218\n",
      "Epoch [64/300], Step [105/172], Loss: 20.2935\n",
      "Epoch [64/300], Step [106/172], Loss: 19.7808\n",
      "Epoch [64/300], Step [107/172], Loss: 17.2751\n",
      "Epoch [64/300], Step [108/172], Loss: 20.3986\n",
      "Epoch [64/300], Step [109/172], Loss: 21.8459\n",
      "Epoch [64/300], Step [110/172], Loss: 19.0175\n",
      "Epoch [64/300], Step [111/172], Loss: 17.3846\n",
      "Epoch [64/300], Step [112/172], Loss: 22.6407\n",
      "Epoch [64/300], Step [113/172], Loss: 18.2369\n",
      "Epoch [64/300], Step [114/172], Loss: 18.1078\n",
      "Epoch [64/300], Step [115/172], Loss: 25.5160\n",
      "Epoch [64/300], Step [116/172], Loss: 18.1504\n",
      "Epoch [64/300], Step [117/172], Loss: 15.3088\n",
      "Epoch [64/300], Step [118/172], Loss: 18.6560\n",
      "Epoch [64/300], Step [119/172], Loss: 16.8215\n",
      "Epoch [64/300], Step [120/172], Loss: 14.1330\n",
      "Epoch [64/300], Step [121/172], Loss: 14.6265\n",
      "Epoch [64/300], Step [122/172], Loss: 13.9565\n",
      "Epoch [64/300], Step [123/172], Loss: 13.1296\n",
      "Epoch [64/300], Step [124/172], Loss: 10.9147\n",
      "Epoch [64/300], Step [125/172], Loss: 15.5298\n",
      "Epoch [64/300], Step [126/172], Loss: 13.8459\n",
      "Epoch [64/300], Step [127/172], Loss: 16.4803\n",
      "Epoch [64/300], Step [128/172], Loss: 16.9538\n",
      "Epoch [64/300], Step [129/172], Loss: 11.8755\n",
      "Epoch [64/300], Step [130/172], Loss: 13.9089\n",
      "Epoch [64/300], Step [131/172], Loss: 12.1308\n",
      "Epoch [64/300], Step [132/172], Loss: 11.9000\n",
      "Epoch [64/300], Step [133/172], Loss: 12.7210\n",
      "Epoch [64/300], Step [134/172], Loss: 13.5116\n",
      "Epoch [64/300], Step [135/172], Loss: 10.8643\n",
      "Epoch [64/300], Step [136/172], Loss: 10.7587\n",
      "Epoch [64/300], Step [137/172], Loss: 12.7064\n",
      "Epoch [64/300], Step [138/172], Loss: 10.9113\n",
      "Epoch [64/300], Step [139/172], Loss: 12.1454\n",
      "Epoch [64/300], Step [140/172], Loss: 12.2225\n",
      "Epoch [64/300], Step [141/172], Loss: 15.0486\n",
      "Epoch [64/300], Step [142/172], Loss: 15.2701\n",
      "Epoch [64/300], Step [143/172], Loss: 11.0702\n",
      "Epoch [64/300], Step [144/172], Loss: 10.9763\n",
      "Epoch [64/300], Step [145/172], Loss: 11.1627\n",
      "Epoch [64/300], Step [146/172], Loss: 11.6265\n",
      "Epoch [64/300], Step [147/172], Loss: 7.6393\n",
      "Epoch [64/300], Step [148/172], Loss: 8.5966\n",
      "Epoch [64/300], Step [149/172], Loss: 10.7091\n",
      "Epoch [64/300], Step [150/172], Loss: 10.3683\n",
      "Epoch [64/300], Step [151/172], Loss: 9.2328\n",
      "Epoch [64/300], Step [152/172], Loss: 9.2987\n",
      "Epoch [64/300], Step [153/172], Loss: 9.1210\n",
      "Epoch [64/300], Step [154/172], Loss: 9.9871\n",
      "Epoch [64/300], Step [155/172], Loss: 8.9832\n",
      "Epoch [64/300], Step [156/172], Loss: 12.0699\n",
      "Epoch [64/300], Step [157/172], Loss: 11.8153\n",
      "Epoch [64/300], Step [158/172], Loss: 9.7088\n",
      "Epoch [64/300], Step [159/172], Loss: 10.5497\n",
      "Epoch [64/300], Step [160/172], Loss: 10.4672\n",
      "Epoch [64/300], Step [161/172], Loss: 8.4222\n",
      "Epoch [64/300], Step [162/172], Loss: 8.9920\n",
      "Epoch [64/300], Step [163/172], Loss: 8.2314\n",
      "Epoch [64/300], Step [164/172], Loss: 11.0013\n",
      "Epoch [64/300], Step [165/172], Loss: 7.8394\n",
      "Epoch [64/300], Step [166/172], Loss: 8.2356\n",
      "Epoch [64/300], Step [167/172], Loss: 8.9980\n",
      "Epoch [64/300], Step [168/172], Loss: 7.9706\n",
      "Epoch [64/300], Step [169/172], Loss: 8.0597\n",
      "Epoch [64/300], Step [170/172], Loss: 7.3819\n",
      "Epoch [64/300], Step [171/172], Loss: 6.7763\n",
      "Epoch [64/300], Step [172/172], Loss: 6.2448\n",
      "Epoch [65/300], Step [1/172], Loss: 96.5112\n",
      "Epoch [65/300], Step [2/172], Loss: 96.3648\n",
      "Epoch [65/300], Step [3/172], Loss: 99.4530\n",
      "Epoch [65/300], Step [4/172], Loss: 60.1216\n",
      "Epoch [65/300], Step [5/172], Loss: 83.6167\n",
      "Epoch [65/300], Step [6/172], Loss: 27.7987\n",
      "Epoch [65/300], Step [7/172], Loss: 34.3610\n",
      "Epoch [65/300], Step [8/172], Loss: 9.7852\n",
      "Epoch [65/300], Step [9/172], Loss: 50.3921\n",
      "Epoch [65/300], Step [10/172], Loss: 56.5180\n",
      "Epoch [65/300], Step [11/172], Loss: 99.9026\n",
      "Epoch [65/300], Step [12/172], Loss: 86.9094\n",
      "Epoch [65/300], Step [13/172], Loss: 43.7513\n",
      "Epoch [65/300], Step [14/172], Loss: 98.4931\n",
      "Epoch [65/300], Step [15/172], Loss: 86.7052\n",
      "Epoch [65/300], Step [16/172], Loss: 47.3975\n",
      "Epoch [65/300], Step [17/172], Loss: 60.7053\n",
      "Epoch [65/300], Step [18/172], Loss: 66.4316\n",
      "Epoch [65/300], Step [19/172], Loss: 81.5413\n",
      "Epoch [65/300], Step [20/172], Loss: 96.4367\n",
      "Epoch [65/300], Step [21/172], Loss: 96.5253\n",
      "Epoch [65/300], Step [22/172], Loss: 89.4179\n",
      "Epoch [65/300], Step [23/172], Loss: 12.6538\n",
      "Epoch [65/300], Step [24/172], Loss: 77.3218\n",
      "Epoch [65/300], Step [25/172], Loss: 52.0554\n",
      "Epoch [65/300], Step [26/172], Loss: 62.2297\n",
      "Epoch [65/300], Step [27/172], Loss: 85.8740\n",
      "Epoch [65/300], Step [28/172], Loss: 51.9199\n",
      "Epoch [65/300], Step [29/172], Loss: 50.0054\n",
      "Epoch [65/300], Step [30/172], Loss: 77.9682\n",
      "Epoch [65/300], Step [31/172], Loss: 44.9487\n",
      "Epoch [65/300], Step [32/172], Loss: 40.9361\n",
      "Epoch [65/300], Step [33/172], Loss: 74.1050\n",
      "Epoch [65/300], Step [34/172], Loss: 7.2418\n",
      "Epoch [65/300], Step [35/172], Loss: 36.6632\n",
      "Epoch [65/300], Step [36/172], Loss: 25.5778\n",
      "Epoch [65/300], Step [37/172], Loss: 19.3781\n",
      "Epoch [65/300], Step [38/172], Loss: 27.8335\n",
      "Epoch [65/300], Step [39/172], Loss: 53.3585\n",
      "Epoch [65/300], Step [40/172], Loss: 26.0563\n",
      "Epoch [65/300], Step [41/172], Loss: 41.0991\n",
      "Epoch [65/300], Step [42/172], Loss: 43.5386\n",
      "Epoch [65/300], Step [43/172], Loss: 28.6892\n",
      "Epoch [65/300], Step [44/172], Loss: 25.6234\n",
      "Epoch [65/300], Step [45/172], Loss: 22.4870\n",
      "Epoch [65/300], Step [46/172], Loss: 33.6071\n",
      "Epoch [65/300], Step [47/172], Loss: 58.1276\n",
      "Epoch [65/300], Step [48/172], Loss: 60.9283\n",
      "Epoch [65/300], Step [49/172], Loss: 21.7751\n",
      "Epoch [65/300], Step [50/172], Loss: 51.9124\n",
      "Epoch [65/300], Step [51/172], Loss: 7.9180\n",
      "Epoch [65/300], Step [52/172], Loss: 21.4546\n",
      "Epoch [65/300], Step [53/172], Loss: 27.9298\n",
      "Epoch [65/300], Step [54/172], Loss: 14.9824\n",
      "Epoch [65/300], Step [55/172], Loss: 14.1584\n",
      "Epoch [65/300], Step [56/172], Loss: 10.5854\n",
      "Epoch [65/300], Step [57/172], Loss: 34.8027\n",
      "Epoch [65/300], Step [58/172], Loss: 20.6175\n",
      "Epoch [65/300], Step [59/172], Loss: 35.1833\n",
      "Epoch [65/300], Step [60/172], Loss: 56.5032\n",
      "Epoch [65/300], Step [61/172], Loss: 11.2124\n",
      "Epoch [65/300], Step [62/172], Loss: 22.9299\n",
      "Epoch [65/300], Step [63/172], Loss: 8.9107\n",
      "Epoch [65/300], Step [64/172], Loss: 7.2873\n",
      "Epoch [65/300], Step [65/172], Loss: 25.8497\n",
      "Epoch [65/300], Step [66/172], Loss: 6.8915\n",
      "Epoch [65/300], Step [67/172], Loss: 27.9870\n",
      "Epoch [65/300], Step [68/172], Loss: 10.3528\n",
      "Epoch [65/300], Step [69/172], Loss: 75.0559\n",
      "Epoch [65/300], Step [70/172], Loss: 67.1131\n",
      "Epoch [65/300], Step [71/172], Loss: 60.3922\n",
      "Epoch [65/300], Step [72/172], Loss: 63.6466\n",
      "Epoch [65/300], Step [73/172], Loss: 69.1173\n",
      "Epoch [65/300], Step [74/172], Loss: 42.1171\n",
      "Epoch [65/300], Step [75/172], Loss: 33.4196\n",
      "Epoch [65/300], Step [76/172], Loss: 45.3661\n",
      "Epoch [65/300], Step [77/172], Loss: 62.3988\n",
      "Epoch [65/300], Step [78/172], Loss: 56.2085\n",
      "Epoch [65/300], Step [79/172], Loss: 52.7195\n",
      "Epoch [65/300], Step [80/172], Loss: 57.1838\n",
      "Epoch [65/300], Step [81/172], Loss: 47.8107\n",
      "Epoch [65/300], Step [82/172], Loss: 43.0312\n",
      "Epoch [65/300], Step [83/172], Loss: 52.5876\n",
      "Epoch [65/300], Step [84/172], Loss: 42.1872\n",
      "Epoch [65/300], Step [85/172], Loss: 47.4852\n",
      "Epoch [65/300], Step [86/172], Loss: 38.0644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/300], Step [87/172], Loss: 31.3981\n",
      "Epoch [65/300], Step [88/172], Loss: 33.6105\n",
      "Epoch [65/300], Step [89/172], Loss: 31.4669\n",
      "Epoch [65/300], Step [90/172], Loss: 30.0660\n",
      "Epoch [65/300], Step [91/172], Loss: 31.7621\n",
      "Epoch [65/300], Step [92/172], Loss: 25.2929\n",
      "Epoch [65/300], Step [93/172], Loss: 25.0527\n",
      "Epoch [65/300], Step [94/172], Loss: 31.0552\n",
      "Epoch [65/300], Step [95/172], Loss: 26.4016\n",
      "Epoch [65/300], Step [96/172], Loss: 22.0167\n",
      "Epoch [65/300], Step [97/172], Loss: 28.6328\n",
      "Epoch [65/300], Step [98/172], Loss: 23.3802\n",
      "Epoch [65/300], Step [99/172], Loss: 20.8361\n",
      "Epoch [65/300], Step [100/172], Loss: 19.5881\n",
      "Epoch [65/300], Step [101/172], Loss: 21.2112\n",
      "Epoch [65/300], Step [102/172], Loss: 19.5141\n",
      "Epoch [65/300], Step [103/172], Loss: 18.4738\n",
      "Epoch [65/300], Step [104/172], Loss: 18.5868\n",
      "Epoch [65/300], Step [105/172], Loss: 20.2004\n",
      "Epoch [65/300], Step [106/172], Loss: 19.6924\n",
      "Epoch [65/300], Step [107/172], Loss: 17.1983\n",
      "Epoch [65/300], Step [108/172], Loss: 20.3268\n",
      "Epoch [65/300], Step [109/172], Loss: 21.8049\n",
      "Epoch [65/300], Step [110/172], Loss: 18.9725\n",
      "Epoch [65/300], Step [111/172], Loss: 17.3015\n",
      "Epoch [65/300], Step [112/172], Loss: 22.4946\n",
      "Epoch [65/300], Step [113/172], Loss: 18.1660\n",
      "Epoch [65/300], Step [114/172], Loss: 17.9634\n",
      "Epoch [65/300], Step [115/172], Loss: 25.4647\n",
      "Epoch [65/300], Step [116/172], Loss: 17.9795\n",
      "Epoch [65/300], Step [117/172], Loss: 15.2468\n",
      "Epoch [65/300], Step [118/172], Loss: 18.6869\n",
      "Epoch [65/300], Step [119/172], Loss: 16.7805\n",
      "Epoch [65/300], Step [120/172], Loss: 14.0752\n",
      "Epoch [65/300], Step [121/172], Loss: 14.5598\n",
      "Epoch [65/300], Step [122/172], Loss: 13.9948\n",
      "Epoch [65/300], Step [123/172], Loss: 13.0815\n",
      "Epoch [65/300], Step [124/172], Loss: 10.8081\n",
      "Epoch [65/300], Step [125/172], Loss: 15.5077\n",
      "Epoch [65/300], Step [126/172], Loss: 13.6961\n",
      "Epoch [65/300], Step [127/172], Loss: 16.3420\n",
      "Epoch [65/300], Step [128/172], Loss: 16.7592\n",
      "Epoch [65/300], Step [129/172], Loss: 11.7723\n",
      "Epoch [65/300], Step [130/172], Loss: 13.8462\n",
      "Epoch [65/300], Step [131/172], Loss: 12.0407\n",
      "Epoch [65/300], Step [132/172], Loss: 11.7968\n",
      "Epoch [65/300], Step [133/172], Loss: 12.7211\n",
      "Epoch [65/300], Step [134/172], Loss: 13.4806\n",
      "Epoch [65/300], Step [135/172], Loss: 10.8234\n",
      "Epoch [65/300], Step [136/172], Loss: 10.6225\n",
      "Epoch [65/300], Step [137/172], Loss: 12.5901\n",
      "Epoch [65/300], Step [138/172], Loss: 10.7784\n",
      "Epoch [65/300], Step [139/172], Loss: 12.0211\n",
      "Epoch [65/300], Step [140/172], Loss: 12.1027\n",
      "Epoch [65/300], Step [141/172], Loss: 15.0270\n",
      "Epoch [65/300], Step [142/172], Loss: 15.2564\n",
      "Epoch [65/300], Step [143/172], Loss: 10.9861\n",
      "Epoch [65/300], Step [144/172], Loss: 10.9160\n",
      "Epoch [65/300], Step [145/172], Loss: 11.1385\n",
      "Epoch [65/300], Step [146/172], Loss: 11.5805\n",
      "Epoch [65/300], Step [147/172], Loss: 7.6135\n",
      "Epoch [65/300], Step [148/172], Loss: 8.5608\n",
      "Epoch [65/300], Step [149/172], Loss: 10.6205\n",
      "Epoch [65/300], Step [150/172], Loss: 10.3053\n",
      "Epoch [65/300], Step [151/172], Loss: 9.2059\n",
      "Epoch [65/300], Step [152/172], Loss: 9.2260\n",
      "Epoch [65/300], Step [153/172], Loss: 9.0855\n",
      "Epoch [65/300], Step [154/172], Loss: 9.9334\n",
      "Epoch [65/300], Step [155/172], Loss: 8.9556\n",
      "Epoch [65/300], Step [156/172], Loss: 12.1052\n",
      "Epoch [65/300], Step [157/172], Loss: 11.7847\n",
      "Epoch [65/300], Step [158/172], Loss: 9.6796\n",
      "Epoch [65/300], Step [159/172], Loss: 10.5278\n",
      "Epoch [65/300], Step [160/172], Loss: 10.4976\n",
      "Epoch [65/300], Step [161/172], Loss: 8.4046\n",
      "Epoch [65/300], Step [162/172], Loss: 8.9705\n",
      "Epoch [65/300], Step [163/172], Loss: 8.2433\n",
      "Epoch [65/300], Step [164/172], Loss: 11.0272\n",
      "Epoch [65/300], Step [165/172], Loss: 7.8230\n",
      "Epoch [65/300], Step [166/172], Loss: 8.2561\n",
      "Epoch [65/300], Step [167/172], Loss: 9.0454\n",
      "Epoch [65/300], Step [168/172], Loss: 7.9989\n",
      "Epoch [65/300], Step [169/172], Loss: 8.0983\n",
      "Epoch [65/300], Step [170/172], Loss: 7.4284\n",
      "Epoch [65/300], Step [171/172], Loss: 6.8391\n",
      "Epoch [65/300], Step [172/172], Loss: 6.2851\n",
      "Epoch [66/300], Step [1/172], Loss: 96.4061\n",
      "Epoch [66/300], Step [2/172], Loss: 96.1969\n",
      "Epoch [66/300], Step [3/172], Loss: 98.2194\n",
      "Epoch [66/300], Step [4/172], Loss: 59.6067\n",
      "Epoch [66/300], Step [5/172], Loss: 83.1123\n",
      "Epoch [66/300], Step [6/172], Loss: 27.6058\n",
      "Epoch [66/300], Step [7/172], Loss: 34.0031\n",
      "Epoch [66/300], Step [8/172], Loss: 9.5587\n",
      "Epoch [66/300], Step [9/172], Loss: 50.0515\n",
      "Epoch [66/300], Step [10/172], Loss: 56.3082\n",
      "Epoch [66/300], Step [11/172], Loss: 99.2216\n",
      "Epoch [66/300], Step [12/172], Loss: 86.3654\n",
      "Epoch [66/300], Step [13/172], Loss: 43.7564\n",
      "Epoch [66/300], Step [14/172], Loss: 98.7219\n",
      "Epoch [66/300], Step [15/172], Loss: 86.8532\n",
      "Epoch [66/300], Step [16/172], Loss: 46.8928\n",
      "Epoch [66/300], Step [17/172], Loss: 60.5107\n",
      "Epoch [66/300], Step [18/172], Loss: 66.3133\n",
      "Epoch [66/300], Step [19/172], Loss: 81.9665\n",
      "Epoch [66/300], Step [20/172], Loss: 95.9473\n",
      "Epoch [66/300], Step [21/172], Loss: 96.8616\n",
      "Epoch [66/300], Step [22/172], Loss: 89.6011\n",
      "Epoch [66/300], Step [23/172], Loss: 12.3684\n",
      "Epoch [66/300], Step [24/172], Loss: 77.2411\n",
      "Epoch [66/300], Step [25/172], Loss: 52.2167\n",
      "Epoch [66/300], Step [26/172], Loss: 62.4389\n",
      "Epoch [66/300], Step [27/172], Loss: 86.1086\n",
      "Epoch [66/300], Step [28/172], Loss: 51.3979\n",
      "Epoch [66/300], Step [29/172], Loss: 49.1886\n",
      "Epoch [66/300], Step [30/172], Loss: 78.3252\n",
      "Epoch [66/300], Step [31/172], Loss: 45.0631\n",
      "Epoch [66/300], Step [32/172], Loss: 40.8558\n",
      "Epoch [66/300], Step [33/172], Loss: 74.0478\n",
      "Epoch [66/300], Step [34/172], Loss: 7.1007\n",
      "Epoch [66/300], Step [35/172], Loss: 36.1405\n",
      "Epoch [66/300], Step [36/172], Loss: 25.4428\n",
      "Epoch [66/300], Step [37/172], Loss: 19.3371\n",
      "Epoch [66/300], Step [38/172], Loss: 28.0598\n",
      "Epoch [66/300], Step [39/172], Loss: 53.4418\n",
      "Epoch [66/300], Step [40/172], Loss: 25.8409\n",
      "Epoch [66/300], Step [41/172], Loss: 41.0990\n",
      "Epoch [66/300], Step [42/172], Loss: 43.6129\n",
      "Epoch [66/300], Step [43/172], Loss: 28.6148\n",
      "Epoch [66/300], Step [44/172], Loss: 25.5606\n",
      "Epoch [66/300], Step [45/172], Loss: 22.3980\n",
      "Epoch [66/300], Step [46/172], Loss: 33.1837\n",
      "Epoch [66/300], Step [47/172], Loss: 57.8303\n",
      "Epoch [66/300], Step [48/172], Loss: 60.6050\n",
      "Epoch [66/300], Step [49/172], Loss: 21.6847\n",
      "Epoch [66/300], Step [50/172], Loss: 52.2877\n",
      "Epoch [66/300], Step [51/172], Loss: 7.8676\n",
      "Epoch [66/300], Step [52/172], Loss: 21.3436\n",
      "Epoch [66/300], Step [53/172], Loss: 27.8510\n",
      "Epoch [66/300], Step [54/172], Loss: 14.8272\n",
      "Epoch [66/300], Step [55/172], Loss: 14.1767\n",
      "Epoch [66/300], Step [56/172], Loss: 10.5869\n",
      "Epoch [66/300], Step [57/172], Loss: 34.1853\n",
      "Epoch [66/300], Step [58/172], Loss: 20.7297\n",
      "Epoch [66/300], Step [59/172], Loss: 35.1695\n",
      "Epoch [66/300], Step [60/172], Loss: 56.2732\n",
      "Epoch [66/300], Step [61/172], Loss: 11.1530\n",
      "Epoch [66/300], Step [62/172], Loss: 23.0047\n",
      "Epoch [66/300], Step [63/172], Loss: 8.8522\n",
      "Epoch [66/300], Step [64/172], Loss: 7.2862\n",
      "Epoch [66/300], Step [65/172], Loss: 25.6065\n",
      "Epoch [66/300], Step [66/172], Loss: 6.9081\n",
      "Epoch [66/300], Step [67/172], Loss: 28.1048\n",
      "Epoch [66/300], Step [68/172], Loss: 10.3103\n",
      "Epoch [66/300], Step [69/172], Loss: 74.7488\n",
      "Epoch [66/300], Step [70/172], Loss: 66.6488\n",
      "Epoch [66/300], Step [71/172], Loss: 60.0177\n",
      "Epoch [66/300], Step [72/172], Loss: 63.3546\n",
      "Epoch [66/300], Step [73/172], Loss: 68.7169\n",
      "Epoch [66/300], Step [74/172], Loss: 41.7904\n",
      "Epoch [66/300], Step [75/172], Loss: 33.3969\n",
      "Epoch [66/300], Step [76/172], Loss: 44.8370\n",
      "Epoch [66/300], Step [77/172], Loss: 62.2144\n",
      "Epoch [66/300], Step [78/172], Loss: 55.7013\n",
      "Epoch [66/300], Step [79/172], Loss: 52.3262\n",
      "Epoch [66/300], Step [80/172], Loss: 56.9983\n",
      "Epoch [66/300], Step [81/172], Loss: 47.3776\n",
      "Epoch [66/300], Step [82/172], Loss: 42.7328\n",
      "Epoch [66/300], Step [83/172], Loss: 52.2986\n",
      "Epoch [66/300], Step [84/172], Loss: 41.8452\n",
      "Epoch [66/300], Step [85/172], Loss: 47.0382\n",
      "Epoch [66/300], Step [86/172], Loss: 37.7227\n",
      "Epoch [66/300], Step [87/172], Loss: 31.2066\n",
      "Epoch [66/300], Step [88/172], Loss: 33.3707\n",
      "Epoch [66/300], Step [89/172], Loss: 31.0475\n",
      "Epoch [66/300], Step [90/172], Loss: 29.7988\n",
      "Epoch [66/300], Step [91/172], Loss: 31.5873\n",
      "Epoch [66/300], Step [92/172], Loss: 25.0631\n",
      "Epoch [66/300], Step [93/172], Loss: 24.8256\n",
      "Epoch [66/300], Step [94/172], Loss: 30.9368\n",
      "Epoch [66/300], Step [95/172], Loss: 26.2160\n",
      "Epoch [66/300], Step [96/172], Loss: 21.7802\n",
      "Epoch [66/300], Step [97/172], Loss: 28.3997\n",
      "Epoch [66/300], Step [98/172], Loss: 23.1643\n",
      "Epoch [66/300], Step [99/172], Loss: 20.6437\n",
      "Epoch [66/300], Step [100/172], Loss: 19.3574\n",
      "Epoch [66/300], Step [101/172], Loss: 21.0118\n",
      "Epoch [66/300], Step [102/172], Loss: 19.2731\n",
      "Epoch [66/300], Step [103/172], Loss: 18.2292\n",
      "Epoch [66/300], Step [104/172], Loss: 18.4089\n",
      "Epoch [66/300], Step [105/172], Loss: 20.0410\n",
      "Epoch [66/300], Step [106/172], Loss: 19.5435\n",
      "Epoch [66/300], Step [107/172], Loss: 17.0549\n",
      "Epoch [66/300], Step [108/172], Loss: 20.1211\n",
      "Epoch [66/300], Step [109/172], Loss: 21.6032\n",
      "Epoch [66/300], Step [110/172], Loss: 18.7968\n",
      "Epoch [66/300], Step [111/172], Loss: 17.1696\n",
      "Epoch [66/300], Step [112/172], Loss: 22.2945\n",
      "Epoch [66/300], Step [113/172], Loss: 18.0514\n",
      "Epoch [66/300], Step [114/172], Loss: 17.7745\n",
      "Epoch [66/300], Step [115/172], Loss: 25.3222\n",
      "Epoch [66/300], Step [116/172], Loss: 17.8650\n",
      "Epoch [66/300], Step [117/172], Loss: 15.0921\n",
      "Epoch [66/300], Step [118/172], Loss: 18.4748\n",
      "Epoch [66/300], Step [119/172], Loss: 16.7072\n",
      "Epoch [66/300], Step [120/172], Loss: 13.9091\n",
      "Epoch [66/300], Step [121/172], Loss: 14.3502\n",
      "Epoch [66/300], Step [122/172], Loss: 13.8355\n",
      "Epoch [66/300], Step [123/172], Loss: 12.9172\n",
      "Epoch [66/300], Step [124/172], Loss: 10.6764\n",
      "Epoch [66/300], Step [125/172], Loss: 15.3535\n",
      "Epoch [66/300], Step [126/172], Loss: 13.5592\n",
      "Epoch [66/300], Step [127/172], Loss: 16.2061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/300], Step [128/172], Loss: 16.6226\n",
      "Epoch [66/300], Step [129/172], Loss: 11.6441\n",
      "Epoch [66/300], Step [130/172], Loss: 13.7158\n",
      "Epoch [66/300], Step [131/172], Loss: 11.9214\n",
      "Epoch [66/300], Step [132/172], Loss: 11.6766\n",
      "Epoch [66/300], Step [133/172], Loss: 12.6220\n",
      "Epoch [66/300], Step [134/172], Loss: 13.4118\n",
      "Epoch [66/300], Step [135/172], Loss: 10.7295\n",
      "Epoch [66/300], Step [136/172], Loss: 10.5230\n",
      "Epoch [66/300], Step [137/172], Loss: 12.4772\n",
      "Epoch [66/300], Step [138/172], Loss: 10.6852\n",
      "Epoch [66/300], Step [139/172], Loss: 11.9321\n",
      "Epoch [66/300], Step [140/172], Loss: 12.0205\n",
      "Epoch [66/300], Step [141/172], Loss: 14.8911\n",
      "Epoch [66/300], Step [142/172], Loss: 15.2059\n",
      "Epoch [66/300], Step [143/172], Loss: 10.8933\n",
      "Epoch [66/300], Step [144/172], Loss: 10.7789\n",
      "Epoch [66/300], Step [145/172], Loss: 11.0792\n",
      "Epoch [66/300], Step [146/172], Loss: 11.5057\n",
      "Epoch [66/300], Step [147/172], Loss: 7.4777\n",
      "Epoch [66/300], Step [148/172], Loss: 8.4527\n",
      "Epoch [66/300], Step [149/172], Loss: 10.5082\n",
      "Epoch [66/300], Step [150/172], Loss: 10.1862\n",
      "Epoch [66/300], Step [151/172], Loss: 9.1046\n",
      "Epoch [66/300], Step [152/172], Loss: 9.1586\n",
      "Epoch [66/300], Step [153/172], Loss: 8.9825\n",
      "Epoch [66/300], Step [154/172], Loss: 9.8254\n",
      "Epoch [66/300], Step [155/172], Loss: 8.8600\n",
      "Epoch [66/300], Step [156/172], Loss: 12.0838\n",
      "Epoch [66/300], Step [157/172], Loss: 11.6923\n",
      "Epoch [66/300], Step [158/172], Loss: 9.5593\n",
      "Epoch [66/300], Step [159/172], Loss: 10.4779\n",
      "Epoch [66/300], Step [160/172], Loss: 10.4202\n",
      "Epoch [66/300], Step [161/172], Loss: 8.3184\n",
      "Epoch [66/300], Step [162/172], Loss: 8.8920\n",
      "Epoch [66/300], Step [163/172], Loss: 8.1166\n",
      "Epoch [66/300], Step [164/172], Loss: 10.9278\n",
      "Epoch [66/300], Step [165/172], Loss: 7.7452\n",
      "Epoch [66/300], Step [166/172], Loss: 8.1589\n",
      "Epoch [66/300], Step [167/172], Loss: 8.9794\n",
      "Epoch [66/300], Step [168/172], Loss: 7.8963\n",
      "Epoch [66/300], Step [169/172], Loss: 8.0410\n",
      "Epoch [66/300], Step [170/172], Loss: 7.2865\n",
      "Epoch [66/300], Step [171/172], Loss: 6.7627\n",
      "Epoch [66/300], Step [172/172], Loss: 6.2448\n",
      "Epoch [67/300], Step [1/172], Loss: 95.9588\n",
      "Epoch [67/300], Step [2/172], Loss: 96.0236\n",
      "Epoch [67/300], Step [3/172], Loss: 97.3216\n",
      "Epoch [67/300], Step [4/172], Loss: 59.1648\n",
      "Epoch [67/300], Step [5/172], Loss: 82.5759\n",
      "Epoch [67/300], Step [6/172], Loss: 27.2806\n",
      "Epoch [67/300], Step [7/172], Loss: 33.7998\n",
      "Epoch [67/300], Step [8/172], Loss: 9.5472\n",
      "Epoch [67/300], Step [9/172], Loss: 49.9011\n",
      "Epoch [67/300], Step [10/172], Loss: 55.9408\n",
      "Epoch [67/300], Step [11/172], Loss: 99.2358\n",
      "Epoch [67/300], Step [12/172], Loss: 86.3751\n",
      "Epoch [67/300], Step [13/172], Loss: 43.6613\n",
      "Epoch [67/300], Step [14/172], Loss: 98.5769\n",
      "Epoch [67/300], Step [15/172], Loss: 86.6077\n",
      "Epoch [67/300], Step [16/172], Loss: 46.0464\n",
      "Epoch [67/300], Step [17/172], Loss: 60.7076\n",
      "Epoch [67/300], Step [18/172], Loss: 66.0752\n",
      "Epoch [67/300], Step [19/172], Loss: 82.0646\n",
      "Epoch [67/300], Step [20/172], Loss: 95.4048\n",
      "Epoch [67/300], Step [21/172], Loss: 97.0118\n",
      "Epoch [67/300], Step [22/172], Loss: 89.7630\n",
      "Epoch [67/300], Step [23/172], Loss: 11.7357\n",
      "Epoch [67/300], Step [24/172], Loss: 77.1097\n",
      "Epoch [67/300], Step [25/172], Loss: 52.0203\n",
      "Epoch [67/300], Step [26/172], Loss: 62.5238\n",
      "Epoch [67/300], Step [27/172], Loss: 86.4173\n",
      "Epoch [67/300], Step [28/172], Loss: 50.9575\n",
      "Epoch [67/300], Step [29/172], Loss: 48.1927\n",
      "Epoch [67/300], Step [30/172], Loss: 78.7030\n",
      "Epoch [67/300], Step [31/172], Loss: 45.1291\n",
      "Epoch [67/300], Step [32/172], Loss: 40.9146\n",
      "Epoch [67/300], Step [33/172], Loss: 74.1127\n",
      "Epoch [67/300], Step [34/172], Loss: 6.9991\n",
      "Epoch [67/300], Step [35/172], Loss: 35.3372\n",
      "Epoch [67/300], Step [36/172], Loss: 25.1607\n",
      "Epoch [67/300], Step [37/172], Loss: 19.2743\n",
      "Epoch [67/300], Step [38/172], Loss: 28.0593\n",
      "Epoch [67/300], Step [39/172], Loss: 53.4184\n",
      "Epoch [67/300], Step [40/172], Loss: 25.6958\n",
      "Epoch [67/300], Step [41/172], Loss: 41.1045\n",
      "Epoch [67/300], Step [42/172], Loss: 43.6791\n",
      "Epoch [67/300], Step [43/172], Loss: 28.5003\n",
      "Epoch [67/300], Step [44/172], Loss: 25.4805\n",
      "Epoch [67/300], Step [45/172], Loss: 22.3770\n",
      "Epoch [67/300], Step [46/172], Loss: 32.8617\n",
      "Epoch [67/300], Step [47/172], Loss: 57.4949\n",
      "Epoch [67/300], Step [48/172], Loss: 60.0429\n",
      "Epoch [67/300], Step [49/172], Loss: 21.6258\n",
      "Epoch [67/300], Step [50/172], Loss: 52.2484\n",
      "Epoch [67/300], Step [51/172], Loss: 7.8211\n",
      "Epoch [67/300], Step [52/172], Loss: 21.1701\n",
      "Epoch [67/300], Step [53/172], Loss: 27.6953\n",
      "Epoch [67/300], Step [54/172], Loss: 14.6100\n",
      "Epoch [67/300], Step [55/172], Loss: 14.0509\n",
      "Epoch [67/300], Step [56/172], Loss: 10.5471\n",
      "Epoch [67/300], Step [57/172], Loss: 33.4442\n",
      "Epoch [67/300], Step [58/172], Loss: 20.6316\n",
      "Epoch [67/300], Step [59/172], Loss: 34.9403\n",
      "Epoch [67/300], Step [60/172], Loss: 55.8808\n",
      "Epoch [67/300], Step [61/172], Loss: 11.1285\n",
      "Epoch [67/300], Step [62/172], Loss: 23.0155\n",
      "Epoch [67/300], Step [63/172], Loss: 8.8528\n",
      "Epoch [67/300], Step [64/172], Loss: 7.3337\n",
      "Epoch [67/300], Step [65/172], Loss: 25.3267\n",
      "Epoch [67/300], Step [66/172], Loss: 6.8550\n",
      "Epoch [67/300], Step [67/172], Loss: 28.0383\n",
      "Epoch [67/300], Step [68/172], Loss: 9.9956\n",
      "Epoch [67/300], Step [69/172], Loss: 74.3401\n",
      "Epoch [67/300], Step [70/172], Loss: 66.3255\n",
      "Epoch [67/300], Step [71/172], Loss: 59.6303\n",
      "Epoch [67/300], Step [72/172], Loss: 63.2046\n",
      "Epoch [67/300], Step [73/172], Loss: 68.4801\n",
      "Epoch [67/300], Step [74/172], Loss: 41.5238\n",
      "Epoch [67/300], Step [75/172], Loss: 33.3898\n",
      "Epoch [67/300], Step [76/172], Loss: 44.5612\n",
      "Epoch [67/300], Step [77/172], Loss: 62.2543\n",
      "Epoch [67/300], Step [78/172], Loss: 55.4606\n",
      "Epoch [67/300], Step [79/172], Loss: 52.2467\n",
      "Epoch [67/300], Step [80/172], Loss: 57.1238\n",
      "Epoch [67/300], Step [81/172], Loss: 47.1416\n",
      "Epoch [67/300], Step [82/172], Loss: 42.7669\n",
      "Epoch [67/300], Step [83/172], Loss: 52.2643\n",
      "Epoch [67/300], Step [84/172], Loss: 41.6980\n",
      "Epoch [67/300], Step [85/172], Loss: 46.6998\n",
      "Epoch [67/300], Step [86/172], Loss: 37.6366\n",
      "Epoch [67/300], Step [87/172], Loss: 31.1361\n",
      "Epoch [67/300], Step [88/172], Loss: 33.2293\n",
      "Epoch [67/300], Step [89/172], Loss: 30.9168\n",
      "Epoch [67/300], Step [90/172], Loss: 29.6245\n",
      "Epoch [67/300], Step [91/172], Loss: 31.5251\n",
      "Epoch [67/300], Step [92/172], Loss: 24.9638\n",
      "Epoch [67/300], Step [93/172], Loss: 24.6707\n",
      "Epoch [67/300], Step [94/172], Loss: 30.8535\n",
      "Epoch [67/300], Step [95/172], Loss: 26.0946\n",
      "Epoch [67/300], Step [96/172], Loss: 21.6476\n",
      "Epoch [67/300], Step [97/172], Loss: 28.2732\n",
      "Epoch [67/300], Step [98/172], Loss: 23.0381\n",
      "Epoch [67/300], Step [99/172], Loss: 20.4968\n",
      "Epoch [67/300], Step [100/172], Loss: 19.2429\n",
      "Epoch [67/300], Step [101/172], Loss: 20.8652\n",
      "Epoch [67/300], Step [102/172], Loss: 19.2073\n",
      "Epoch [67/300], Step [103/172], Loss: 18.0932\n",
      "Epoch [67/300], Step [104/172], Loss: 18.2901\n",
      "Epoch [67/300], Step [105/172], Loss: 20.0388\n",
      "Epoch [67/300], Step [106/172], Loss: 19.4426\n",
      "Epoch [67/300], Step [107/172], Loss: 16.9678\n",
      "Epoch [67/300], Step [108/172], Loss: 20.0158\n",
      "Epoch [67/300], Step [109/172], Loss: 21.4939\n",
      "Epoch [67/300], Step [110/172], Loss: 18.7038\n",
      "Epoch [67/300], Step [111/172], Loss: 17.0827\n",
      "Epoch [67/300], Step [112/172], Loss: 22.2089\n",
      "Epoch [67/300], Step [113/172], Loss: 17.9885\n",
      "Epoch [67/300], Step [114/172], Loss: 17.6762\n",
      "Epoch [67/300], Step [115/172], Loss: 25.2190\n",
      "Epoch [67/300], Step [116/172], Loss: 17.8059\n",
      "Epoch [67/300], Step [117/172], Loss: 15.0043\n",
      "Epoch [67/300], Step [118/172], Loss: 18.3514\n",
      "Epoch [67/300], Step [119/172], Loss: 16.7048\n",
      "Epoch [67/300], Step [120/172], Loss: 13.8293\n",
      "Epoch [67/300], Step [121/172], Loss: 14.2562\n",
      "Epoch [67/300], Step [122/172], Loss: 13.7532\n",
      "Epoch [67/300], Step [123/172], Loss: 12.8461\n",
      "Epoch [67/300], Step [124/172], Loss: 10.6183\n",
      "Epoch [67/300], Step [125/172], Loss: 15.2769\n",
      "Epoch [67/300], Step [126/172], Loss: 13.4725\n",
      "Epoch [67/300], Step [127/172], Loss: 16.1572\n",
      "Epoch [67/300], Step [128/172], Loss: 16.5731\n",
      "Epoch [67/300], Step [129/172], Loss: 11.5888\n",
      "Epoch [67/300], Step [130/172], Loss: 13.6536\n",
      "Epoch [67/300], Step [131/172], Loss: 11.8732\n",
      "Epoch [67/300], Step [132/172], Loss: 11.6119\n",
      "Epoch [67/300], Step [133/172], Loss: 12.6207\n",
      "Epoch [67/300], Step [134/172], Loss: 13.4077\n",
      "Epoch [67/300], Step [135/172], Loss: 10.6932\n",
      "Epoch [67/300], Step [136/172], Loss: 10.5034\n",
      "Epoch [67/300], Step [137/172], Loss: 12.4421\n",
      "Epoch [67/300], Step [138/172], Loss: 10.6534\n",
      "Epoch [67/300], Step [139/172], Loss: 11.8986\n",
      "Epoch [67/300], Step [140/172], Loss: 11.9774\n",
      "Epoch [67/300], Step [141/172], Loss: 14.8632\n",
      "Epoch [67/300], Step [142/172], Loss: 15.2577\n",
      "Epoch [67/300], Step [143/172], Loss: 10.8536\n",
      "Epoch [67/300], Step [144/172], Loss: 10.7346\n",
      "Epoch [67/300], Step [145/172], Loss: 11.0763\n",
      "Epoch [67/300], Step [146/172], Loss: 11.5039\n",
      "Epoch [67/300], Step [147/172], Loss: 7.4236\n",
      "Epoch [67/300], Step [148/172], Loss: 8.4158\n",
      "Epoch [67/300], Step [149/172], Loss: 10.4493\n",
      "Epoch [67/300], Step [150/172], Loss: 10.1556\n",
      "Epoch [67/300], Step [151/172], Loss: 9.0554\n",
      "Epoch [67/300], Step [152/172], Loss: 9.1321\n",
      "Epoch [67/300], Step [153/172], Loss: 8.9419\n",
      "Epoch [67/300], Step [154/172], Loss: 9.7774\n",
      "Epoch [67/300], Step [155/172], Loss: 8.8145\n",
      "Epoch [67/300], Step [156/172], Loss: 12.1264\n",
      "Epoch [67/300], Step [157/172], Loss: 11.6553\n",
      "Epoch [67/300], Step [158/172], Loss: 9.5163\n",
      "Epoch [67/300], Step [159/172], Loss: 10.4788\n",
      "Epoch [67/300], Step [160/172], Loss: 10.4178\n",
      "Epoch [67/300], Step [161/172], Loss: 8.2973\n",
      "Epoch [67/300], Step [162/172], Loss: 8.8605\n",
      "Epoch [67/300], Step [163/172], Loss: 8.0737\n",
      "Epoch [67/300], Step [164/172], Loss: 10.9609\n",
      "Epoch [67/300], Step [165/172], Loss: 7.7280\n",
      "Epoch [67/300], Step [166/172], Loss: 8.1403\n",
      "Epoch [67/300], Step [167/172], Loss: 8.9899\n",
      "Epoch [67/300], Step [168/172], Loss: 7.8597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/300], Step [169/172], Loss: 8.0369\n",
      "Epoch [67/300], Step [170/172], Loss: 7.2466\n",
      "Epoch [67/300], Step [171/172], Loss: 6.7833\n",
      "Epoch [67/300], Step [172/172], Loss: 6.2214\n",
      "Epoch [68/300], Step [1/172], Loss: 95.6398\n",
      "Epoch [68/300], Step [2/172], Loss: 95.9358\n",
      "Epoch [68/300], Step [3/172], Loss: 96.2684\n",
      "Epoch [68/300], Step [4/172], Loss: 58.7075\n",
      "Epoch [68/300], Step [5/172], Loss: 82.0847\n",
      "Epoch [68/300], Step [6/172], Loss: 26.9510\n",
      "Epoch [68/300], Step [7/172], Loss: 33.2829\n",
      "Epoch [68/300], Step [8/172], Loss: 9.2128\n",
      "Epoch [68/300], Step [9/172], Loss: 49.6422\n",
      "Epoch [68/300], Step [10/172], Loss: 55.5404\n",
      "Epoch [68/300], Step [11/172], Loss: 98.9919\n",
      "Epoch [68/300], Step [12/172], Loss: 86.2282\n",
      "Epoch [68/300], Step [13/172], Loss: 43.3453\n",
      "Epoch [68/300], Step [14/172], Loss: 98.0298\n",
      "Epoch [68/300], Step [15/172], Loss: 85.9794\n",
      "Epoch [68/300], Step [16/172], Loss: 44.5465\n",
      "Epoch [68/300], Step [17/172], Loss: 60.4349\n",
      "Epoch [68/300], Step [18/172], Loss: 65.7533\n",
      "Epoch [68/300], Step [19/172], Loss: 81.9385\n",
      "Epoch [68/300], Step [20/172], Loss: 93.6444\n",
      "Epoch [68/300], Step [21/172], Loss: 96.6107\n",
      "Epoch [68/300], Step [22/172], Loss: 88.9235\n",
      "Epoch [68/300], Step [23/172], Loss: 11.5726\n",
      "Epoch [68/300], Step [24/172], Loss: 76.8135\n",
      "Epoch [68/300], Step [25/172], Loss: 51.5885\n",
      "Epoch [68/300], Step [26/172], Loss: 62.3215\n",
      "Epoch [68/300], Step [27/172], Loss: 85.7185\n",
      "Epoch [68/300], Step [28/172], Loss: 50.3472\n",
      "Epoch [68/300], Step [29/172], Loss: 47.1049\n",
      "Epoch [68/300], Step [30/172], Loss: 78.7090\n",
      "Epoch [68/300], Step [31/172], Loss: 45.0110\n",
      "Epoch [68/300], Step [32/172], Loss: 40.7156\n",
      "Epoch [68/300], Step [33/172], Loss: 73.7174\n",
      "Epoch [68/300], Step [34/172], Loss: 7.0729\n",
      "Epoch [68/300], Step [35/172], Loss: 34.5291\n",
      "Epoch [68/300], Step [36/172], Loss: 25.0440\n",
      "Epoch [68/300], Step [37/172], Loss: 19.1765\n",
      "Epoch [68/300], Step [38/172], Loss: 27.9857\n",
      "Epoch [68/300], Step [39/172], Loss: 53.3205\n",
      "Epoch [68/300], Step [40/172], Loss: 25.4958\n",
      "Epoch [68/300], Step [41/172], Loss: 41.0014\n",
      "Epoch [68/300], Step [42/172], Loss: 43.7722\n",
      "Epoch [68/300], Step [43/172], Loss: 28.4101\n",
      "Epoch [68/300], Step [44/172], Loss: 25.4233\n",
      "Epoch [68/300], Step [45/172], Loss: 22.4022\n",
      "Epoch [68/300], Step [46/172], Loss: 32.5123\n",
      "Epoch [68/300], Step [47/172], Loss: 57.2980\n",
      "Epoch [68/300], Step [48/172], Loss: 59.9055\n",
      "Epoch [68/300], Step [49/172], Loss: 21.6687\n",
      "Epoch [68/300], Step [50/172], Loss: 52.4546\n",
      "Epoch [68/300], Step [51/172], Loss: 7.8235\n",
      "Epoch [68/300], Step [52/172], Loss: 21.1296\n",
      "Epoch [68/300], Step [53/172], Loss: 27.6988\n",
      "Epoch [68/300], Step [54/172], Loss: 14.5264\n",
      "Epoch [68/300], Step [55/172], Loss: 14.0040\n",
      "Epoch [68/300], Step [56/172], Loss: 10.5189\n",
      "Epoch [68/300], Step [57/172], Loss: 33.2277\n",
      "Epoch [68/300], Step [58/172], Loss: 20.7134\n",
      "Epoch [68/300], Step [59/172], Loss: 34.8584\n",
      "Epoch [68/300], Step [60/172], Loss: 55.9484\n",
      "Epoch [68/300], Step [61/172], Loss: 11.0835\n",
      "Epoch [68/300], Step [62/172], Loss: 23.2408\n",
      "Epoch [68/300], Step [63/172], Loss: 8.8966\n",
      "Epoch [68/300], Step [64/172], Loss: 7.3630\n",
      "Epoch [68/300], Step [65/172], Loss: 25.2527\n",
      "Epoch [68/300], Step [66/172], Loss: 6.8010\n",
      "Epoch [68/300], Step [67/172], Loss: 28.2701\n",
      "Epoch [68/300], Step [68/172], Loss: 9.9133\n",
      "Epoch [68/300], Step [69/172], Loss: 74.3630\n",
      "Epoch [68/300], Step [70/172], Loss: 66.0749\n",
      "Epoch [68/300], Step [71/172], Loss: 59.5344\n",
      "Epoch [68/300], Step [72/172], Loss: 63.1636\n",
      "Epoch [68/300], Step [73/172], Loss: 68.3628\n",
      "Epoch [68/300], Step [74/172], Loss: 41.3795\n",
      "Epoch [68/300], Step [75/172], Loss: 33.6269\n",
      "Epoch [68/300], Step [76/172], Loss: 44.4291\n",
      "Epoch [68/300], Step [77/172], Loss: 62.4952\n",
      "Epoch [68/300], Step [78/172], Loss: 55.4898\n",
      "Epoch [68/300], Step [79/172], Loss: 52.3387\n",
      "Epoch [68/300], Step [80/172], Loss: 57.3768\n",
      "Epoch [68/300], Step [81/172], Loss: 47.0910\n",
      "Epoch [68/300], Step [82/172], Loss: 42.6281\n",
      "Epoch [68/300], Step [83/172], Loss: 52.4588\n",
      "Epoch [68/300], Step [84/172], Loss: 41.7239\n",
      "Epoch [68/300], Step [85/172], Loss: 46.7272\n",
      "Epoch [68/300], Step [86/172], Loss: 37.6385\n",
      "Epoch [68/300], Step [87/172], Loss: 31.1642\n",
      "Epoch [68/300], Step [88/172], Loss: 33.2440\n",
      "Epoch [68/300], Step [89/172], Loss: 30.8482\n",
      "Epoch [68/300], Step [90/172], Loss: 29.6250\n",
      "Epoch [68/300], Step [91/172], Loss: 31.5634\n",
      "Epoch [68/300], Step [92/172], Loss: 24.9043\n",
      "Epoch [68/300], Step [93/172], Loss: 24.6298\n",
      "Epoch [68/300], Step [94/172], Loss: 30.9478\n",
      "Epoch [68/300], Step [95/172], Loss: 26.0396\n",
      "Epoch [68/300], Step [96/172], Loss: 21.5493\n",
      "Epoch [68/300], Step [97/172], Loss: 28.2742\n",
      "Epoch [68/300], Step [98/172], Loss: 23.0107\n",
      "Epoch [68/300], Step [99/172], Loss: 20.4760\n",
      "Epoch [68/300], Step [100/172], Loss: 19.1874\n",
      "Epoch [68/300], Step [101/172], Loss: 20.8191\n",
      "Epoch [68/300], Step [102/172], Loss: 19.0870\n",
      "Epoch [68/300], Step [103/172], Loss: 17.9734\n",
      "Epoch [68/300], Step [104/172], Loss: 18.2299\n",
      "Epoch [68/300], Step [105/172], Loss: 19.9558\n",
      "Epoch [68/300], Step [106/172], Loss: 19.3811\n",
      "Epoch [68/300], Step [107/172], Loss: 16.9508\n",
      "Epoch [68/300], Step [108/172], Loss: 19.9591\n",
      "Epoch [68/300], Step [109/172], Loss: 21.4500\n",
      "Epoch [68/300], Step [110/172], Loss: 18.6619\n",
      "Epoch [68/300], Step [111/172], Loss: 17.0164\n",
      "Epoch [68/300], Step [112/172], Loss: 22.0786\n",
      "Epoch [68/300], Step [113/172], Loss: 17.9325\n",
      "Epoch [68/300], Step [114/172], Loss: 17.5558\n",
      "Epoch [68/300], Step [115/172], Loss: 25.1923\n",
      "Epoch [68/300], Step [116/172], Loss: 17.7108\n",
      "Epoch [68/300], Step [117/172], Loss: 14.9263\n",
      "Epoch [68/300], Step [118/172], Loss: 18.3180\n",
      "Epoch [68/300], Step [119/172], Loss: 16.6698\n",
      "Epoch [68/300], Step [120/172], Loss: 13.7735\n",
      "Epoch [68/300], Step [121/172], Loss: 14.1659\n",
      "Epoch [68/300], Step [122/172], Loss: 13.7370\n",
      "Epoch [68/300], Step [123/172], Loss: 12.7505\n",
      "Epoch [68/300], Step [124/172], Loss: 10.5280\n",
      "Epoch [68/300], Step [125/172], Loss: 15.2260\n",
      "Epoch [68/300], Step [126/172], Loss: 13.3724\n",
      "Epoch [68/300], Step [127/172], Loss: 16.0224\n",
      "Epoch [68/300], Step [128/172], Loss: 16.4083\n",
      "Epoch [68/300], Step [129/172], Loss: 11.4989\n",
      "Epoch [68/300], Step [130/172], Loss: 13.5779\n",
      "Epoch [68/300], Step [131/172], Loss: 11.7976\n",
      "Epoch [68/300], Step [132/172], Loss: 11.5264\n",
      "Epoch [68/300], Step [133/172], Loss: 12.5977\n",
      "Epoch [68/300], Step [134/172], Loss: 13.3974\n",
      "Epoch [68/300], Step [135/172], Loss: 10.6432\n",
      "Epoch [68/300], Step [136/172], Loss: 10.4080\n",
      "Epoch [68/300], Step [137/172], Loss: 12.3572\n",
      "Epoch [68/300], Step [138/172], Loss: 10.5485\n",
      "Epoch [68/300], Step [139/172], Loss: 11.8139\n",
      "Epoch [68/300], Step [140/172], Loss: 11.8909\n",
      "Epoch [68/300], Step [141/172], Loss: 14.8148\n",
      "Epoch [68/300], Step [142/172], Loss: 15.2560\n",
      "Epoch [68/300], Step [143/172], Loss: 10.7957\n",
      "Epoch [68/300], Step [144/172], Loss: 10.6760\n",
      "Epoch [68/300], Step [145/172], Loss: 11.0520\n",
      "Epoch [68/300], Step [146/172], Loss: 11.4550\n",
      "Epoch [68/300], Step [147/172], Loss: 7.3549\n",
      "Epoch [68/300], Step [148/172], Loss: 8.3634\n",
      "Epoch [68/300], Step [149/172], Loss: 10.3765\n",
      "Epoch [68/300], Step [150/172], Loss: 10.0724\n",
      "Epoch [68/300], Step [151/172], Loss: 9.0076\n",
      "Epoch [68/300], Step [152/172], Loss: 9.0781\n",
      "Epoch [68/300], Step [153/172], Loss: 8.8853\n",
      "Epoch [68/300], Step [154/172], Loss: 9.7190\n",
      "Epoch [68/300], Step [155/172], Loss: 8.7575\n",
      "Epoch [68/300], Step [156/172], Loss: 12.1624\n",
      "Epoch [68/300], Step [157/172], Loss: 11.6225\n",
      "Epoch [68/300], Step [158/172], Loss: 9.4716\n",
      "Epoch [68/300], Step [159/172], Loss: 10.4331\n",
      "Epoch [68/300], Step [160/172], Loss: 10.4255\n",
      "Epoch [68/300], Step [161/172], Loss: 8.2545\n",
      "Epoch [68/300], Step [162/172], Loss: 8.8228\n",
      "Epoch [68/300], Step [163/172], Loss: 8.0251\n",
      "Epoch [68/300], Step [164/172], Loss: 10.9040\n",
      "Epoch [68/300], Step [165/172], Loss: 7.6941\n",
      "Epoch [68/300], Step [166/172], Loss: 8.0920\n",
      "Epoch [68/300], Step [167/172], Loss: 8.9748\n",
      "Epoch [68/300], Step [168/172], Loss: 7.8268\n",
      "Epoch [68/300], Step [169/172], Loss: 8.0228\n",
      "Epoch [68/300], Step [170/172], Loss: 7.2085\n",
      "Epoch [68/300], Step [171/172], Loss: 6.7632\n",
      "Epoch [68/300], Step [172/172], Loss: 6.2222\n",
      "Epoch [69/300], Step [1/172], Loss: 95.2473\n",
      "Epoch [69/300], Step [2/172], Loss: 95.6718\n",
      "Epoch [69/300], Step [3/172], Loss: 95.5652\n",
      "Epoch [69/300], Step [4/172], Loss: 58.2429\n",
      "Epoch [69/300], Step [5/172], Loss: 81.4049\n",
      "Epoch [69/300], Step [6/172], Loss: 26.5897\n",
      "Epoch [69/300], Step [7/172], Loss: 32.6936\n",
      "Epoch [69/300], Step [8/172], Loss: 9.3069\n",
      "Epoch [69/300], Step [9/172], Loss: 49.5174\n",
      "Epoch [69/300], Step [10/172], Loss: 55.1904\n",
      "Epoch [69/300], Step [11/172], Loss: 98.5617\n",
      "Epoch [69/300], Step [12/172], Loss: 86.0351\n",
      "Epoch [69/300], Step [13/172], Loss: 43.3392\n",
      "Epoch [69/300], Step [14/172], Loss: 97.9633\n",
      "Epoch [69/300], Step [15/172], Loss: 85.7179\n",
      "Epoch [69/300], Step [16/172], Loss: 43.9364\n",
      "Epoch [69/300], Step [17/172], Loss: 60.5295\n",
      "Epoch [69/300], Step [18/172], Loss: 65.5714\n",
      "Epoch [69/300], Step [19/172], Loss: 82.2205\n",
      "Epoch [69/300], Step [20/172], Loss: 93.2113\n",
      "Epoch [69/300], Step [21/172], Loss: 96.8171\n",
      "Epoch [69/300], Step [22/172], Loss: 88.9394\n",
      "Epoch [69/300], Step [23/172], Loss: 10.8881\n",
      "Epoch [69/300], Step [24/172], Loss: 76.7222\n",
      "Epoch [69/300], Step [25/172], Loss: 51.6405\n",
      "Epoch [69/300], Step [26/172], Loss: 62.3920\n",
      "Epoch [69/300], Step [27/172], Loss: 85.8599\n",
      "Epoch [69/300], Step [28/172], Loss: 49.9598\n",
      "Epoch [69/300], Step [29/172], Loss: 46.2097\n",
      "Epoch [69/300], Step [30/172], Loss: 79.0318\n",
      "Epoch [69/300], Step [31/172], Loss: 45.1530\n",
      "Epoch [69/300], Step [32/172], Loss: 40.9049\n",
      "Epoch [69/300], Step [33/172], Loss: 73.8338\n",
      "Epoch [69/300], Step [34/172], Loss: 6.7130\n",
      "Epoch [69/300], Step [35/172], Loss: 33.6045\n",
      "Epoch [69/300], Step [36/172], Loss: 24.6623\n",
      "Epoch [69/300], Step [37/172], Loss: 19.1118\n",
      "Epoch [69/300], Step [38/172], Loss: 27.9784\n",
      "Epoch [69/300], Step [39/172], Loss: 53.1942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/300], Step [40/172], Loss: 25.2858\n",
      "Epoch [69/300], Step [41/172], Loss: 41.0636\n",
      "Epoch [69/300], Step [42/172], Loss: 43.8650\n",
      "Epoch [69/300], Step [43/172], Loss: 28.3810\n",
      "Epoch [69/300], Step [44/172], Loss: 25.2479\n",
      "Epoch [69/300], Step [45/172], Loss: 22.3257\n",
      "Epoch [69/300], Step [46/172], Loss: 32.0716\n",
      "Epoch [69/300], Step [47/172], Loss: 56.9724\n",
      "Epoch [69/300], Step [48/172], Loss: 59.3029\n",
      "Epoch [69/300], Step [49/172], Loss: 21.6092\n",
      "Epoch [69/300], Step [50/172], Loss: 52.3124\n",
      "Epoch [69/300], Step [51/172], Loss: 7.7964\n",
      "Epoch [69/300], Step [52/172], Loss: 21.0095\n",
      "Epoch [69/300], Step [53/172], Loss: 27.5056\n",
      "Epoch [69/300], Step [54/172], Loss: 14.3383\n",
      "Epoch [69/300], Step [55/172], Loss: 13.9235\n",
      "Epoch [69/300], Step [56/172], Loss: 10.5095\n",
      "Epoch [69/300], Step [57/172], Loss: 32.3062\n",
      "Epoch [69/300], Step [58/172], Loss: 20.6718\n",
      "Epoch [69/300], Step [59/172], Loss: 34.7390\n",
      "Epoch [69/300], Step [60/172], Loss: 55.6865\n",
      "Epoch [69/300], Step [61/172], Loss: 11.0179\n",
      "Epoch [69/300], Step [62/172], Loss: 23.2281\n",
      "Epoch [69/300], Step [63/172], Loss: 8.8359\n",
      "Epoch [69/300], Step [64/172], Loss: 7.3781\n",
      "Epoch [69/300], Step [65/172], Loss: 24.9991\n",
      "Epoch [69/300], Step [66/172], Loss: 6.7419\n",
      "Epoch [69/300], Step [67/172], Loss: 28.2714\n",
      "Epoch [69/300], Step [68/172], Loss: 9.6181\n",
      "Epoch [69/300], Step [69/172], Loss: 74.1470\n",
      "Epoch [69/300], Step [70/172], Loss: 65.7590\n",
      "Epoch [69/300], Step [71/172], Loss: 59.1996\n",
      "Epoch [69/300], Step [72/172], Loss: 63.0629\n",
      "Epoch [69/300], Step [73/172], Loss: 68.1858\n",
      "Epoch [69/300], Step [74/172], Loss: 41.0567\n",
      "Epoch [69/300], Step [75/172], Loss: 33.6176\n",
      "Epoch [69/300], Step [76/172], Loss: 43.9817\n",
      "Epoch [69/300], Step [77/172], Loss: 62.3226\n",
      "Epoch [69/300], Step [78/172], Loss: 55.0449\n",
      "Epoch [69/300], Step [79/172], Loss: 51.9568\n",
      "Epoch [69/300], Step [80/172], Loss: 57.1668\n",
      "Epoch [69/300], Step [81/172], Loss: 46.6134\n",
      "Epoch [69/300], Step [82/172], Loss: 42.4373\n",
      "Epoch [69/300], Step [83/172], Loss: 52.0986\n",
      "Epoch [69/300], Step [84/172], Loss: 41.2661\n",
      "Epoch [69/300], Step [85/172], Loss: 46.0965\n",
      "Epoch [69/300], Step [86/172], Loss: 37.2300\n",
      "Epoch [69/300], Step [87/172], Loss: 30.8926\n",
      "Epoch [69/300], Step [88/172], Loss: 32.9348\n",
      "Epoch [69/300], Step [89/172], Loss: 30.4281\n",
      "Epoch [69/300], Step [90/172], Loss: 29.3487\n",
      "Epoch [69/300], Step [91/172], Loss: 31.3455\n",
      "Epoch [69/300], Step [92/172], Loss: 24.6307\n",
      "Epoch [69/300], Step [93/172], Loss: 24.3550\n",
      "Epoch [69/300], Step [94/172], Loss: 30.7598\n",
      "Epoch [69/300], Step [95/172], Loss: 25.7897\n",
      "Epoch [69/300], Step [96/172], Loss: 21.3141\n",
      "Epoch [69/300], Step [97/172], Loss: 28.0576\n",
      "Epoch [69/300], Step [98/172], Loss: 22.7542\n",
      "Epoch [69/300], Step [99/172], Loss: 20.2656\n",
      "Epoch [69/300], Step [100/172], Loss: 18.9647\n",
      "Epoch [69/300], Step [101/172], Loss: 20.5727\n",
      "Epoch [69/300], Step [102/172], Loss: 18.9586\n",
      "Epoch [69/300], Step [103/172], Loss: 17.7481\n",
      "Epoch [69/300], Step [104/172], Loss: 18.0349\n",
      "Epoch [69/300], Step [105/172], Loss: 19.8662\n",
      "Epoch [69/300], Step [106/172], Loss: 19.2090\n",
      "Epoch [69/300], Step [107/172], Loss: 16.8188\n",
      "Epoch [69/300], Step [108/172], Loss: 19.7792\n",
      "Epoch [69/300], Step [109/172], Loss: 21.2969\n",
      "Epoch [69/300], Step [110/172], Loss: 18.4887\n",
      "Epoch [69/300], Step [111/172], Loss: 16.8686\n",
      "Epoch [69/300], Step [112/172], Loss: 21.8974\n",
      "Epoch [69/300], Step [113/172], Loss: 17.8190\n",
      "Epoch [69/300], Step [114/172], Loss: 17.3752\n",
      "Epoch [69/300], Step [115/172], Loss: 25.0641\n",
      "Epoch [69/300], Step [116/172], Loss: 17.5414\n",
      "Epoch [69/300], Step [117/172], Loss: 14.8144\n",
      "Epoch [69/300], Step [118/172], Loss: 18.2206\n",
      "Epoch [69/300], Step [119/172], Loss: 16.5911\n",
      "Epoch [69/300], Step [120/172], Loss: 13.6622\n",
      "Epoch [69/300], Step [121/172], Loss: 14.0048\n",
      "Epoch [69/300], Step [122/172], Loss: 13.6058\n",
      "Epoch [69/300], Step [123/172], Loss: 12.6548\n",
      "Epoch [69/300], Step [124/172], Loss: 10.4067\n",
      "Epoch [69/300], Step [125/172], Loss: 15.0727\n",
      "Epoch [69/300], Step [126/172], Loss: 13.2252\n",
      "Epoch [69/300], Step [127/172], Loss: 15.8543\n",
      "Epoch [69/300], Step [128/172], Loss: 16.2214\n",
      "Epoch [69/300], Step [129/172], Loss: 11.3762\n",
      "Epoch [69/300], Step [130/172], Loss: 13.4575\n",
      "Epoch [69/300], Step [131/172], Loss: 11.6739\n",
      "Epoch [69/300], Step [132/172], Loss: 11.4058\n",
      "Epoch [69/300], Step [133/172], Loss: 12.5042\n",
      "Epoch [69/300], Step [134/172], Loss: 13.3421\n",
      "Epoch [69/300], Step [135/172], Loss: 10.5523\n",
      "Epoch [69/300], Step [136/172], Loss: 10.2932\n",
      "Epoch [69/300], Step [137/172], Loss: 12.2462\n",
      "Epoch [69/300], Step [138/172], Loss: 10.4424\n",
      "Epoch [69/300], Step [139/172], Loss: 11.7091\n",
      "Epoch [69/300], Step [140/172], Loss: 11.7696\n",
      "Epoch [69/300], Step [141/172], Loss: 14.7262\n",
      "Epoch [69/300], Step [142/172], Loss: 15.2153\n",
      "Epoch [69/300], Step [143/172], Loss: 10.7134\n",
      "Epoch [69/300], Step [144/172], Loss: 10.5770\n",
      "Epoch [69/300], Step [145/172], Loss: 10.9962\n",
      "Epoch [69/300], Step [146/172], Loss: 11.3750\n",
      "Epoch [69/300], Step [147/172], Loss: 7.2586\n",
      "Epoch [69/300], Step [148/172], Loss: 8.2755\n",
      "Epoch [69/300], Step [149/172], Loss: 10.2695\n",
      "Epoch [69/300], Step [150/172], Loss: 9.9803\n",
      "Epoch [69/300], Step [151/172], Loss: 8.9353\n",
      "Epoch [69/300], Step [152/172], Loss: 9.0028\n",
      "Epoch [69/300], Step [153/172], Loss: 8.8045\n",
      "Epoch [69/300], Step [154/172], Loss: 9.6189\n",
      "Epoch [69/300], Step [155/172], Loss: 8.6821\n",
      "Epoch [69/300], Step [156/172], Loss: 12.1688\n",
      "Epoch [69/300], Step [157/172], Loss: 11.5603\n",
      "Epoch [69/300], Step [158/172], Loss: 9.3958\n",
      "Epoch [69/300], Step [159/172], Loss: 10.3923\n",
      "Epoch [69/300], Step [160/172], Loss: 10.4055\n",
      "Epoch [69/300], Step [161/172], Loss: 8.1945\n",
      "Epoch [69/300], Step [162/172], Loss: 8.7475\n",
      "Epoch [69/300], Step [163/172], Loss: 7.9557\n",
      "Epoch [69/300], Step [164/172], Loss: 10.8856\n",
      "Epoch [69/300], Step [165/172], Loss: 7.6281\n",
      "Epoch [69/300], Step [166/172], Loss: 8.0320\n",
      "Epoch [69/300], Step [167/172], Loss: 8.9434\n",
      "Epoch [69/300], Step [168/172], Loss: 7.7564\n",
      "Epoch [69/300], Step [169/172], Loss: 7.9963\n",
      "Epoch [69/300], Step [170/172], Loss: 7.1334\n",
      "Epoch [69/300], Step [171/172], Loss: 6.7349\n",
      "Epoch [69/300], Step [172/172], Loss: 6.2002\n",
      "Epoch [70/300], Step [1/172], Loss: 94.9304\n",
      "Epoch [70/300], Step [2/172], Loss: 95.5128\n",
      "Epoch [70/300], Step [3/172], Loss: 94.6290\n",
      "Epoch [70/300], Step [4/172], Loss: 57.8952\n",
      "Epoch [70/300], Step [5/172], Loss: 80.8438\n",
      "Epoch [70/300], Step [6/172], Loss: 26.3427\n",
      "Epoch [70/300], Step [7/172], Loss: 32.2183\n",
      "Epoch [70/300], Step [8/172], Loss: 8.9362\n",
      "Epoch [70/300], Step [9/172], Loss: 49.2512\n",
      "Epoch [70/300], Step [10/172], Loss: 54.8678\n",
      "Epoch [70/300], Step [11/172], Loss: 98.3342\n",
      "Epoch [70/300], Step [12/172], Loss: 85.9907\n",
      "Epoch [70/300], Step [13/172], Loss: 43.2307\n",
      "Epoch [70/300], Step [14/172], Loss: 97.5971\n",
      "Epoch [70/300], Step [15/172], Loss: 85.3519\n",
      "Epoch [70/300], Step [16/172], Loss: 42.7077\n",
      "Epoch [70/300], Step [17/172], Loss: 60.3779\n",
      "Epoch [70/300], Step [18/172], Loss: 65.2970\n",
      "Epoch [70/300], Step [19/172], Loss: 82.1558\n",
      "Epoch [70/300], Step [20/172], Loss: 92.4717\n",
      "Epoch [70/300], Step [21/172], Loss: 96.5816\n",
      "Epoch [70/300], Step [22/172], Loss: 88.4347\n",
      "Epoch [70/300], Step [23/172], Loss: 10.6099\n",
      "Epoch [70/300], Step [24/172], Loss: 76.5704\n",
      "Epoch [70/300], Step [25/172], Loss: 51.4457\n",
      "Epoch [70/300], Step [26/172], Loss: 62.4328\n",
      "Epoch [70/300], Step [27/172], Loss: 85.5191\n",
      "Epoch [70/300], Step [28/172], Loss: 49.5922\n",
      "Epoch [70/300], Step [29/172], Loss: 45.4253\n",
      "Epoch [70/300], Step [30/172], Loss: 79.2141\n",
      "Epoch [70/300], Step [31/172], Loss: 45.2658\n",
      "Epoch [70/300], Step [32/172], Loss: 41.0661\n",
      "Epoch [70/300], Step [33/172], Loss: 74.1552\n",
      "Epoch [70/300], Step [34/172], Loss: 6.8832\n",
      "Epoch [70/300], Step [35/172], Loss: 32.3843\n",
      "Epoch [70/300], Step [36/172], Loss: 24.5652\n",
      "Epoch [70/300], Step [37/172], Loss: 19.2092\n",
      "Epoch [70/300], Step [38/172], Loss: 28.0865\n",
      "Epoch [70/300], Step [39/172], Loss: 53.0643\n",
      "Epoch [70/300], Step [40/172], Loss: 25.2902\n",
      "Epoch [70/300], Step [41/172], Loss: 41.1358\n",
      "Epoch [70/300], Step [42/172], Loss: 44.2316\n",
      "Epoch [70/300], Step [43/172], Loss: 28.4542\n",
      "Epoch [70/300], Step [44/172], Loss: 25.2467\n",
      "Epoch [70/300], Step [45/172], Loss: 22.4552\n",
      "Epoch [70/300], Step [46/172], Loss: 31.6703\n",
      "Epoch [70/300], Step [47/172], Loss: 56.8961\n",
      "Epoch [70/300], Step [48/172], Loss: 59.2379\n",
      "Epoch [70/300], Step [49/172], Loss: 21.6770\n",
      "Epoch [70/300], Step [50/172], Loss: 52.3224\n",
      "Epoch [70/300], Step [51/172], Loss: 7.8135\n",
      "Epoch [70/300], Step [52/172], Loss: 21.0320\n",
      "Epoch [70/300], Step [53/172], Loss: 27.5466\n",
      "Epoch [70/300], Step [54/172], Loss: 14.2061\n",
      "Epoch [70/300], Step [55/172], Loss: 13.9345\n",
      "Epoch [70/300], Step [56/172], Loss: 10.4840\n",
      "Epoch [70/300], Step [57/172], Loss: 31.8773\n",
      "Epoch [70/300], Step [58/172], Loss: 20.6142\n",
      "Epoch [70/300], Step [59/172], Loss: 34.5943\n",
      "Epoch [70/300], Step [60/172], Loss: 54.7767\n",
      "Epoch [70/300], Step [61/172], Loss: 10.8983\n",
      "Epoch [70/300], Step [62/172], Loss: 23.3209\n",
      "Epoch [70/300], Step [63/172], Loss: 8.8398\n",
      "Epoch [70/300], Step [64/172], Loss: 7.3829\n",
      "Epoch [70/300], Step [65/172], Loss: 24.5367\n",
      "Epoch [70/300], Step [66/172], Loss: 6.7214\n",
      "Epoch [70/300], Step [67/172], Loss: 28.0830\n",
      "Epoch [70/300], Step [68/172], Loss: 9.3900\n",
      "Epoch [70/300], Step [69/172], Loss: 73.8792\n",
      "Epoch [70/300], Step [70/172], Loss: 65.5727\n",
      "Epoch [70/300], Step [71/172], Loss: 58.9448\n",
      "Epoch [70/300], Step [72/172], Loss: 62.8540\n",
      "Epoch [70/300], Step [73/172], Loss: 68.0488\n",
      "Epoch [70/300], Step [74/172], Loss: 40.7727\n",
      "Epoch [70/300], Step [75/172], Loss: 33.4260\n",
      "Epoch [70/300], Step [76/172], Loss: 43.7253\n",
      "Epoch [70/300], Step [77/172], Loss: 62.4573\n",
      "Epoch [70/300], Step [78/172], Loss: 54.8664\n",
      "Epoch [70/300], Step [79/172], Loss: 51.8428\n",
      "Epoch [70/300], Step [80/172], Loss: 57.1341\n",
      "Epoch [70/300], Step [81/172], Loss: 46.4379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/300], Step [82/172], Loss: 42.1766\n",
      "Epoch [70/300], Step [83/172], Loss: 52.1638\n",
      "Epoch [70/300], Step [84/172], Loss: 41.2202\n",
      "Epoch [70/300], Step [85/172], Loss: 46.0041\n",
      "Epoch [70/300], Step [86/172], Loss: 37.0842\n",
      "Epoch [70/300], Step [87/172], Loss: 30.7931\n",
      "Epoch [70/300], Step [88/172], Loss: 32.8057\n",
      "Epoch [70/300], Step [89/172], Loss: 30.2202\n",
      "Epoch [70/300], Step [90/172], Loss: 29.1858\n",
      "Epoch [70/300], Step [91/172], Loss: 31.2740\n",
      "Epoch [70/300], Step [92/172], Loss: 24.4625\n",
      "Epoch [70/300], Step [93/172], Loss: 24.1986\n",
      "Epoch [70/300], Step [94/172], Loss: 30.7413\n",
      "Epoch [70/300], Step [95/172], Loss: 25.6421\n",
      "Epoch [70/300], Step [96/172], Loss: 21.1625\n",
      "Epoch [70/300], Step [97/172], Loss: 27.9422\n",
      "Epoch [70/300], Step [98/172], Loss: 22.6137\n",
      "Epoch [70/300], Step [99/172], Loss: 20.1291\n",
      "Epoch [70/300], Step [100/172], Loss: 18.8189\n",
      "Epoch [70/300], Step [101/172], Loss: 20.4264\n",
      "Epoch [70/300], Step [102/172], Loss: 18.7430\n",
      "Epoch [70/300], Step [103/172], Loss: 17.5708\n",
      "Epoch [70/300], Step [104/172], Loss: 17.8975\n",
      "Epoch [70/300], Step [105/172], Loss: 19.7239\n",
      "Epoch [70/300], Step [106/172], Loss: 19.1034\n",
      "Epoch [70/300], Step [107/172], Loss: 16.7332\n",
      "Epoch [70/300], Step [108/172], Loss: 19.6434\n",
      "Epoch [70/300], Step [109/172], Loss: 21.1738\n",
      "Epoch [70/300], Step [110/172], Loss: 18.3506\n",
      "Epoch [70/300], Step [111/172], Loss: 16.7499\n",
      "Epoch [70/300], Step [112/172], Loss: 21.7214\n",
      "Epoch [70/300], Step [113/172], Loss: 17.7144\n",
      "Epoch [70/300], Step [114/172], Loss: 17.2334\n",
      "Epoch [70/300], Step [115/172], Loss: 24.9639\n",
      "Epoch [70/300], Step [116/172], Loss: 17.3872\n",
      "Epoch [70/300], Step [117/172], Loss: 14.6961\n",
      "Epoch [70/300], Step [118/172], Loss: 18.1164\n",
      "Epoch [70/300], Step [119/172], Loss: 16.5270\n",
      "Epoch [70/300], Step [120/172], Loss: 13.5635\n",
      "Epoch [70/300], Step [121/172], Loss: 13.8718\n",
      "Epoch [70/300], Step [122/172], Loss: 13.4307\n",
      "Epoch [70/300], Step [123/172], Loss: 12.5595\n",
      "Epoch [70/300], Step [124/172], Loss: 10.3013\n",
      "Epoch [70/300], Step [125/172], Loss: 14.9817\n",
      "Epoch [70/300], Step [126/172], Loss: 13.1035\n",
      "Epoch [70/300], Step [127/172], Loss: 15.6939\n",
      "Epoch [70/300], Step [128/172], Loss: 16.0378\n",
      "Epoch [70/300], Step [129/172], Loss: 11.2774\n",
      "Epoch [70/300], Step [130/172], Loss: 13.3648\n",
      "Epoch [70/300], Step [131/172], Loss: 11.5470\n",
      "Epoch [70/300], Step [132/172], Loss: 11.3004\n",
      "Epoch [70/300], Step [133/172], Loss: 12.3854\n",
      "Epoch [70/300], Step [134/172], Loss: 13.2487\n",
      "Epoch [70/300], Step [135/172], Loss: 10.4842\n",
      "Epoch [70/300], Step [136/172], Loss: 10.2305\n",
      "Epoch [70/300], Step [137/172], Loss: 12.1757\n",
      "Epoch [70/300], Step [138/172], Loss: 10.3781\n",
      "Epoch [70/300], Step [139/172], Loss: 11.6359\n",
      "Epoch [70/300], Step [140/172], Loss: 11.6988\n",
      "Epoch [70/300], Step [141/172], Loss: 14.6483\n",
      "Epoch [70/300], Step [142/172], Loss: 15.1612\n",
      "Epoch [70/300], Step [143/172], Loss: 10.6557\n",
      "Epoch [70/300], Step [144/172], Loss: 10.5123\n",
      "Epoch [70/300], Step [145/172], Loss: 10.9650\n",
      "Epoch [70/300], Step [146/172], Loss: 11.3099\n",
      "Epoch [70/300], Step [147/172], Loss: 7.1867\n",
      "Epoch [70/300], Step [148/172], Loss: 8.2108\n",
      "Epoch [70/300], Step [149/172], Loss: 10.2040\n",
      "Epoch [70/300], Step [150/172], Loss: 9.9078\n",
      "Epoch [70/300], Step [151/172], Loss: 8.8752\n",
      "Epoch [70/300], Step [152/172], Loss: 8.9479\n",
      "Epoch [70/300], Step [153/172], Loss: 8.7405\n",
      "Epoch [70/300], Step [154/172], Loss: 9.5399\n",
      "Epoch [70/300], Step [155/172], Loss: 8.6070\n",
      "Epoch [70/300], Step [156/172], Loss: 12.1699\n",
      "Epoch [70/300], Step [157/172], Loss: 11.4933\n",
      "Epoch [70/300], Step [158/172], Loss: 9.3337\n",
      "Epoch [70/300], Step [159/172], Loss: 10.3458\n",
      "Epoch [70/300], Step [160/172], Loss: 10.3768\n",
      "Epoch [70/300], Step [161/172], Loss: 8.1427\n",
      "Epoch [70/300], Step [162/172], Loss: 8.7137\n",
      "Epoch [70/300], Step [163/172], Loss: 7.8885\n",
      "Epoch [70/300], Step [164/172], Loss: 10.7658\n",
      "Epoch [70/300], Step [165/172], Loss: 7.5793\n",
      "Epoch [70/300], Step [166/172], Loss: 7.9915\n",
      "Epoch [70/300], Step [167/172], Loss: 8.9234\n",
      "Epoch [70/300], Step [168/172], Loss: 7.7116\n",
      "Epoch [70/300], Step [169/172], Loss: 7.9666\n",
      "Epoch [70/300], Step [170/172], Loss: 7.0749\n",
      "Epoch [70/300], Step [171/172], Loss: 6.7103\n",
      "Epoch [70/300], Step [172/172], Loss: 6.1671\n",
      "Epoch [71/300], Step [1/172], Loss: 94.6201\n",
      "Epoch [71/300], Step [2/172], Loss: 95.1811\n",
      "Epoch [71/300], Step [3/172], Loss: 93.9418\n",
      "Epoch [71/300], Step [4/172], Loss: 57.3416\n",
      "Epoch [71/300], Step [5/172], Loss: 80.1557\n",
      "Epoch [71/300], Step [6/172], Loss: 26.1160\n",
      "Epoch [71/300], Step [7/172], Loss: 32.4357\n",
      "Epoch [71/300], Step [8/172], Loss: 9.4310\n",
      "Epoch [71/300], Step [9/172], Loss: 49.3193\n",
      "Epoch [71/300], Step [10/172], Loss: 54.5139\n",
      "Epoch [71/300], Step [11/172], Loss: 98.0129\n",
      "Epoch [71/300], Step [12/172], Loss: 85.9514\n",
      "Epoch [71/300], Step [13/172], Loss: 43.1144\n",
      "Epoch [71/300], Step [14/172], Loss: 97.6471\n",
      "Epoch [71/300], Step [15/172], Loss: 85.1748\n",
      "Epoch [71/300], Step [16/172], Loss: 41.9137\n",
      "Epoch [71/300], Step [17/172], Loss: 60.3929\n",
      "Epoch [71/300], Step [18/172], Loss: 65.1350\n",
      "Epoch [71/300], Step [19/172], Loss: 82.3468\n",
      "Epoch [71/300], Step [20/172], Loss: 91.8316\n",
      "Epoch [71/300], Step [21/172], Loss: 96.7793\n",
      "Epoch [71/300], Step [22/172], Loss: 88.4449\n",
      "Epoch [71/300], Step [23/172], Loss: 10.0684\n",
      "Epoch [71/300], Step [24/172], Loss: 76.4296\n",
      "Epoch [71/300], Step [25/172], Loss: 51.2221\n",
      "Epoch [71/300], Step [26/172], Loss: 62.3154\n",
      "Epoch [71/300], Step [27/172], Loss: 85.5369\n",
      "Epoch [71/300], Step [28/172], Loss: 48.9072\n",
      "Epoch [71/300], Step [29/172], Loss: 44.2049\n",
      "Epoch [71/300], Step [30/172], Loss: 79.3408\n",
      "Epoch [71/300], Step [31/172], Loss: 45.0926\n",
      "Epoch [71/300], Step [32/172], Loss: 40.8328\n",
      "Epoch [71/300], Step [33/172], Loss: 73.6349\n",
      "Epoch [71/300], Step [34/172], Loss: 6.4991\n",
      "Epoch [71/300], Step [35/172], Loss: 31.5101\n",
      "Epoch [71/300], Step [36/172], Loss: 24.0782\n",
      "Epoch [71/300], Step [37/172], Loss: 18.9623\n",
      "Epoch [71/300], Step [38/172], Loss: 27.8958\n",
      "Epoch [71/300], Step [39/172], Loss: 52.9235\n",
      "Epoch [71/300], Step [40/172], Loss: 24.9179\n",
      "Epoch [71/300], Step [41/172], Loss: 40.9335\n",
      "Epoch [71/300], Step [42/172], Loss: 43.9398\n",
      "Epoch [71/300], Step [43/172], Loss: 28.2202\n",
      "Epoch [71/300], Step [44/172], Loss: 25.0209\n",
      "Epoch [71/300], Step [45/172], Loss: 22.2948\n",
      "Epoch [71/300], Step [46/172], Loss: 31.3884\n",
      "Epoch [71/300], Step [47/172], Loss: 56.5226\n",
      "Epoch [71/300], Step [48/172], Loss: 58.4041\n",
      "Epoch [71/300], Step [49/172], Loss: 21.5111\n",
      "Epoch [71/300], Step [50/172], Loss: 52.2117\n",
      "Epoch [71/300], Step [51/172], Loss: 7.7493\n",
      "Epoch [71/300], Step [52/172], Loss: 20.8468\n",
      "Epoch [71/300], Step [53/172], Loss: 27.2550\n",
      "Epoch [71/300], Step [54/172], Loss: 14.0646\n",
      "Epoch [71/300], Step [55/172], Loss: 13.8506\n",
      "Epoch [71/300], Step [56/172], Loss: 10.4633\n",
      "Epoch [71/300], Step [57/172], Loss: 31.1929\n",
      "Epoch [71/300], Step [58/172], Loss: 20.6415\n",
      "Epoch [71/300], Step [59/172], Loss: 34.3221\n",
      "Epoch [71/300], Step [60/172], Loss: 54.6957\n",
      "Epoch [71/300], Step [61/172], Loss: 10.8743\n",
      "Epoch [71/300], Step [62/172], Loss: 23.3578\n",
      "Epoch [71/300], Step [63/172], Loss: 8.8295\n",
      "Epoch [71/300], Step [64/172], Loss: 7.4473\n",
      "Epoch [71/300], Step [65/172], Loss: 24.3397\n",
      "Epoch [71/300], Step [66/172], Loss: 6.6872\n",
      "Epoch [71/300], Step [67/172], Loss: 28.2075\n",
      "Epoch [71/300], Step [68/172], Loss: 9.3613\n",
      "Epoch [71/300], Step [69/172], Loss: 73.5278\n",
      "Epoch [71/300], Step [70/172], Loss: 65.1555\n",
      "Epoch [71/300], Step [71/172], Loss: 58.6396\n",
      "Epoch [71/300], Step [72/172], Loss: 62.7704\n",
      "Epoch [71/300], Step [73/172], Loss: 67.9666\n",
      "Epoch [71/300], Step [74/172], Loss: 40.6146\n",
      "Epoch [71/300], Step [75/172], Loss: 33.6232\n",
      "Epoch [71/300], Step [76/172], Loss: 43.5176\n",
      "Epoch [71/300], Step [77/172], Loss: 62.5459\n",
      "Epoch [71/300], Step [78/172], Loss: 54.7171\n",
      "Epoch [71/300], Step [79/172], Loss: 51.8173\n",
      "Epoch [71/300], Step [80/172], Loss: 57.3708\n",
      "Epoch [71/300], Step [81/172], Loss: 46.2686\n",
      "Epoch [71/300], Step [82/172], Loss: 42.3211\n",
      "Epoch [71/300], Step [83/172], Loss: 52.1609\n",
      "Epoch [71/300], Step [84/172], Loss: 41.1250\n",
      "Epoch [71/300], Step [85/172], Loss: 45.7052\n",
      "Epoch [71/300], Step [86/172], Loss: 36.9653\n",
      "Epoch [71/300], Step [87/172], Loss: 30.7370\n",
      "Epoch [71/300], Step [88/172], Loss: 32.7345\n",
      "Epoch [71/300], Step [89/172], Loss: 30.0517\n",
      "Epoch [71/300], Step [90/172], Loss: 29.0798\n",
      "Epoch [71/300], Step [91/172], Loss: 31.2276\n",
      "Epoch [71/300], Step [92/172], Loss: 24.3905\n",
      "Epoch [71/300], Step [93/172], Loss: 24.0946\n",
      "Epoch [71/300], Step [94/172], Loss: 30.6904\n",
      "Epoch [71/300], Step [95/172], Loss: 25.5359\n",
      "Epoch [71/300], Step [96/172], Loss: 21.0607\n",
      "Epoch [71/300], Step [97/172], Loss: 27.8591\n",
      "Epoch [71/300], Step [98/172], Loss: 22.5004\n",
      "Epoch [71/300], Step [99/172], Loss: 20.0376\n",
      "Epoch [71/300], Step [100/172], Loss: 18.7086\n",
      "Epoch [71/300], Step [101/172], Loss: 20.3071\n",
      "Epoch [71/300], Step [102/172], Loss: 18.7244\n",
      "Epoch [71/300], Step [103/172], Loss: 17.4346\n",
      "Epoch [71/300], Step [104/172], Loss: 17.8058\n",
      "Epoch [71/300], Step [105/172], Loss: 19.7567\n",
      "Epoch [71/300], Step [106/172], Loss: 19.0402\n",
      "Epoch [71/300], Step [107/172], Loss: 16.7022\n",
      "Epoch [71/300], Step [108/172], Loss: 19.5461\n",
      "Epoch [71/300], Step [109/172], Loss: 21.0978\n",
      "Epoch [71/300], Step [110/172], Loss: 18.2682\n",
      "Epoch [71/300], Step [111/172], Loss: 16.7035\n",
      "Epoch [71/300], Step [112/172], Loss: 21.6517\n",
      "Epoch [71/300], Step [113/172], Loss: 17.6446\n",
      "Epoch [71/300], Step [114/172], Loss: 17.1626\n",
      "Epoch [71/300], Step [115/172], Loss: 24.9170\n",
      "Epoch [71/300], Step [116/172], Loss: 17.3157\n",
      "Epoch [71/300], Step [117/172], Loss: 14.6280\n",
      "Epoch [71/300], Step [118/172], Loss: 18.0632\n",
      "Epoch [71/300], Step [119/172], Loss: 16.5112\n",
      "Epoch [71/300], Step [120/172], Loss: 13.4990\n",
      "Epoch [71/300], Step [121/172], Loss: 13.7735\n",
      "Epoch [71/300], Step [122/172], Loss: 13.4297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/300], Step [123/172], Loss: 12.4808\n",
      "Epoch [71/300], Step [124/172], Loss: 10.2492\n",
      "Epoch [71/300], Step [125/172], Loss: 14.9122\n",
      "Epoch [71/300], Step [126/172], Loss: 13.0210\n",
      "Epoch [71/300], Step [127/172], Loss: 15.6328\n",
      "Epoch [71/300], Step [128/172], Loss: 15.9489\n",
      "Epoch [71/300], Step [129/172], Loss: 11.2073\n",
      "Epoch [71/300], Step [130/172], Loss: 13.2898\n",
      "Epoch [71/300], Step [131/172], Loss: 11.5053\n",
      "Epoch [71/300], Step [132/172], Loss: 11.2330\n",
      "Epoch [71/300], Step [133/172], Loss: 12.4138\n",
      "Epoch [71/300], Step [134/172], Loss: 13.2831\n",
      "Epoch [71/300], Step [135/172], Loss: 10.4485\n",
      "Epoch [71/300], Step [136/172], Loss: 10.1961\n",
      "Epoch [71/300], Step [137/172], Loss: 12.1400\n",
      "Epoch [71/300], Step [138/172], Loss: 10.3476\n",
      "Epoch [71/300], Step [139/172], Loss: 11.5854\n",
      "Epoch [71/300], Step [140/172], Loss: 11.6363\n",
      "Epoch [71/300], Step [141/172], Loss: 14.5864\n",
      "Epoch [71/300], Step [142/172], Loss: 15.2110\n",
      "Epoch [71/300], Step [143/172], Loss: 10.5985\n",
      "Epoch [71/300], Step [144/172], Loss: 10.4955\n",
      "Epoch [71/300], Step [145/172], Loss: 10.9490\n",
      "Epoch [71/300], Step [146/172], Loss: 11.2732\n",
      "Epoch [71/300], Step [147/172], Loss: 7.1258\n",
      "Epoch [71/300], Step [148/172], Loss: 8.1617\n",
      "Epoch [71/300], Step [149/172], Loss: 10.1301\n",
      "Epoch [71/300], Step [150/172], Loss: 9.8465\n",
      "Epoch [71/300], Step [151/172], Loss: 8.8425\n",
      "Epoch [71/300], Step [152/172], Loss: 8.9111\n",
      "Epoch [71/300], Step [153/172], Loss: 8.6879\n",
      "Epoch [71/300], Step [154/172], Loss: 9.5050\n",
      "Epoch [71/300], Step [155/172], Loss: 8.5634\n",
      "Epoch [71/300], Step [156/172], Loss: 12.2385\n",
      "Epoch [71/300], Step [157/172], Loss: 11.4850\n",
      "Epoch [71/300], Step [158/172], Loss: 9.3065\n",
      "Epoch [71/300], Step [159/172], Loss: 10.3214\n",
      "Epoch [71/300], Step [160/172], Loss: 10.4118\n",
      "Epoch [71/300], Step [161/172], Loss: 8.1133\n",
      "Epoch [71/300], Step [162/172], Loss: 8.6578\n",
      "Epoch [71/300], Step [163/172], Loss: 7.8320\n",
      "Epoch [71/300], Step [164/172], Loss: 10.8239\n",
      "Epoch [71/300], Step [165/172], Loss: 7.5392\n",
      "Epoch [71/300], Step [166/172], Loss: 7.9520\n",
      "Epoch [71/300], Step [167/172], Loss: 8.9190\n",
      "Epoch [71/300], Step [168/172], Loss: 7.6606\n",
      "Epoch [71/300], Step [169/172], Loss: 7.9483\n",
      "Epoch [71/300], Step [170/172], Loss: 7.0160\n",
      "Epoch [71/300], Step [171/172], Loss: 6.6942\n",
      "Epoch [71/300], Step [172/172], Loss: 6.1520\n",
      "Epoch [72/300], Step [1/172], Loss: 94.1497\n",
      "Epoch [72/300], Step [2/172], Loss: 94.8603\n",
      "Epoch [72/300], Step [3/172], Loss: 93.1349\n",
      "Epoch [72/300], Step [4/172], Loss: 56.8751\n",
      "Epoch [72/300], Step [5/172], Loss: 79.5025\n",
      "Epoch [72/300], Step [6/172], Loss: 25.6191\n",
      "Epoch [72/300], Step [7/172], Loss: 31.0769\n",
      "Epoch [72/300], Step [8/172], Loss: 8.6108\n",
      "Epoch [72/300], Step [9/172], Loss: 48.8132\n",
      "Epoch [72/300], Step [10/172], Loss: 54.0952\n",
      "Epoch [72/300], Step [11/172], Loss: 97.5390\n",
      "Epoch [72/300], Step [12/172], Loss: 85.7157\n",
      "Epoch [72/300], Step [13/172], Loss: 42.8819\n",
      "Epoch [72/300], Step [14/172], Loss: 97.0786\n",
      "Epoch [72/300], Step [15/172], Loss: 84.5692\n",
      "Epoch [72/300], Step [16/172], Loss: 40.7049\n",
      "Epoch [72/300], Step [17/172], Loss: 60.1059\n",
      "Epoch [72/300], Step [18/172], Loss: 64.7447\n",
      "Epoch [72/300], Step [19/172], Loss: 82.1295\n",
      "Epoch [72/300], Step [20/172], Loss: 90.3731\n",
      "Epoch [72/300], Step [21/172], Loss: 96.5475\n",
      "Epoch [72/300], Step [22/172], Loss: 87.6683\n",
      "Epoch [72/300], Step [23/172], Loss: 9.8024\n",
      "Epoch [72/300], Step [24/172], Loss: 76.0736\n",
      "Epoch [72/300], Step [25/172], Loss: 50.8535\n",
      "Epoch [72/300], Step [26/172], Loss: 62.1473\n",
      "Epoch [72/300], Step [27/172], Loss: 84.9011\n",
      "Epoch [72/300], Step [28/172], Loss: 48.4222\n",
      "Epoch [72/300], Step [29/172], Loss: 43.3503\n",
      "Epoch [72/300], Step [30/172], Loss: 79.4794\n",
      "Epoch [72/300], Step [31/172], Loss: 45.0964\n",
      "Epoch [72/300], Step [32/172], Loss: 40.9007\n",
      "Epoch [72/300], Step [33/172], Loss: 73.6646\n",
      "Epoch [72/300], Step [34/172], Loss: 6.4365\n",
      "Epoch [72/300], Step [35/172], Loss: 30.7187\n",
      "Epoch [72/300], Step [36/172], Loss: 24.1095\n",
      "Epoch [72/300], Step [37/172], Loss: 18.9499\n",
      "Epoch [72/300], Step [38/172], Loss: 27.9426\n",
      "Epoch [72/300], Step [39/172], Loss: 52.8496\n",
      "Epoch [72/300], Step [40/172], Loss: 24.7646\n",
      "Epoch [72/300], Step [41/172], Loss: 40.7666\n",
      "Epoch [72/300], Step [42/172], Loss: 43.9152\n",
      "Epoch [72/300], Step [43/172], Loss: 28.1539\n",
      "Epoch [72/300], Step [44/172], Loss: 24.7798\n",
      "Epoch [72/300], Step [45/172], Loss: 22.2226\n",
      "Epoch [72/300], Step [46/172], Loss: 31.0851\n",
      "Epoch [72/300], Step [47/172], Loss: 56.3945\n",
      "Epoch [72/300], Step [48/172], Loss: 58.4042\n",
      "Epoch [72/300], Step [49/172], Loss: 21.4819\n",
      "Epoch [72/300], Step [50/172], Loss: 52.3579\n",
      "Epoch [72/300], Step [51/172], Loss: 7.7383\n",
      "Epoch [72/300], Step [52/172], Loss: 20.7802\n",
      "Epoch [72/300], Step [53/172], Loss: 27.1725\n",
      "Epoch [72/300], Step [54/172], Loss: 13.8801\n",
      "Epoch [72/300], Step [55/172], Loss: 13.8241\n",
      "Epoch [72/300], Step [56/172], Loss: 10.4709\n",
      "Epoch [72/300], Step [57/172], Loss: 30.7142\n",
      "Epoch [72/300], Step [58/172], Loss: 20.4935\n",
      "Epoch [72/300], Step [59/172], Loss: 34.1554\n",
      "Epoch [72/300], Step [60/172], Loss: 54.5385\n",
      "Epoch [72/300], Step [61/172], Loss: 10.8416\n",
      "Epoch [72/300], Step [62/172], Loss: 23.5946\n",
      "Epoch [72/300], Step [63/172], Loss: 8.8898\n",
      "Epoch [72/300], Step [64/172], Loss: 7.4949\n",
      "Epoch [72/300], Step [65/172], Loss: 24.2647\n",
      "Epoch [72/300], Step [66/172], Loss: 6.6765\n",
      "Epoch [72/300], Step [67/172], Loss: 28.2636\n",
      "Epoch [72/300], Step [68/172], Loss: 9.1819\n",
      "Epoch [72/300], Step [69/172], Loss: 73.2899\n",
      "Epoch [72/300], Step [70/172], Loss: 64.8346\n",
      "Epoch [72/300], Step [71/172], Loss: 58.3605\n",
      "Epoch [72/300], Step [72/172], Loss: 62.5689\n",
      "Epoch [72/300], Step [73/172], Loss: 67.7057\n",
      "Epoch [72/300], Step [74/172], Loss: 40.3982\n",
      "Epoch [72/300], Step [75/172], Loss: 33.6418\n",
      "Epoch [72/300], Step [76/172], Loss: 43.2813\n",
      "Epoch [72/300], Step [77/172], Loss: 62.6404\n",
      "Epoch [72/300], Step [78/172], Loss: 54.6116\n",
      "Epoch [72/300], Step [79/172], Loss: 51.7665\n",
      "Epoch [72/300], Step [80/172], Loss: 57.4207\n",
      "Epoch [72/300], Step [81/172], Loss: 46.1045\n",
      "Epoch [72/300], Step [82/172], Loss: 42.0535\n",
      "Epoch [72/300], Step [83/172], Loss: 52.2392\n",
      "Epoch [72/300], Step [84/172], Loss: 41.0798\n",
      "Epoch [72/300], Step [85/172], Loss: 45.6733\n",
      "Epoch [72/300], Step [86/172], Loss: 36.8669\n",
      "Epoch [72/300], Step [87/172], Loss: 30.7178\n",
      "Epoch [72/300], Step [88/172], Loss: 32.6967\n",
      "Epoch [72/300], Step [89/172], Loss: 29.9324\n",
      "Epoch [72/300], Step [90/172], Loss: 28.9654\n",
      "Epoch [72/300], Step [91/172], Loss: 31.2542\n",
      "Epoch [72/300], Step [92/172], Loss: 24.3208\n",
      "Epoch [72/300], Step [93/172], Loss: 24.0182\n",
      "Epoch [72/300], Step [94/172], Loss: 30.7544\n",
      "Epoch [72/300], Step [95/172], Loss: 25.5006\n",
      "Epoch [72/300], Step [96/172], Loss: 20.9834\n",
      "Epoch [72/300], Step [97/172], Loss: 27.8095\n",
      "Epoch [72/300], Step [98/172], Loss: 22.4312\n",
      "Epoch [72/300], Step [99/172], Loss: 19.9848\n",
      "Epoch [72/300], Step [100/172], Loss: 18.6321\n",
      "Epoch [72/300], Step [101/172], Loss: 20.2325\n",
      "Epoch [72/300], Step [102/172], Loss: 18.5455\n",
      "Epoch [72/300], Step [103/172], Loss: 17.3048\n",
      "Epoch [72/300], Step [104/172], Loss: 17.7344\n",
      "Epoch [72/300], Step [105/172], Loss: 19.6429\n",
      "Epoch [72/300], Step [106/172], Loss: 18.9767\n",
      "Epoch [72/300], Step [107/172], Loss: 16.6487\n",
      "Epoch [72/300], Step [108/172], Loss: 19.4513\n",
      "Epoch [72/300], Step [109/172], Loss: 20.9778\n",
      "Epoch [72/300], Step [110/172], Loss: 18.1748\n",
      "Epoch [72/300], Step [111/172], Loss: 16.6256\n",
      "Epoch [72/300], Step [112/172], Loss: 21.5703\n",
      "Epoch [72/300], Step [113/172], Loss: 17.5556\n",
      "Epoch [72/300], Step [114/172], Loss: 17.0387\n",
      "Epoch [72/300], Step [115/172], Loss: 24.8188\n",
      "Epoch [72/300], Step [116/172], Loss: 17.2457\n",
      "Epoch [72/300], Step [117/172], Loss: 14.5589\n",
      "Epoch [72/300], Step [118/172], Loss: 17.9590\n",
      "Epoch [72/300], Step [119/172], Loss: 16.5037\n",
      "Epoch [72/300], Step [120/172], Loss: 13.4177\n",
      "Epoch [72/300], Step [121/172], Loss: 13.6793\n",
      "Epoch [72/300], Step [122/172], Loss: 13.2175\n",
      "Epoch [72/300], Step [123/172], Loss: 12.4161\n",
      "Epoch [72/300], Step [124/172], Loss: 10.1842\n",
      "Epoch [72/300], Step [125/172], Loss: 14.8553\n",
      "Epoch [72/300], Step [126/172], Loss: 12.9567\n",
      "Epoch [72/300], Step [127/172], Loss: 15.5124\n",
      "Epoch [72/300], Step [128/172], Loss: 15.8624\n",
      "Epoch [72/300], Step [129/172], Loss: 11.1381\n",
      "Epoch [72/300], Step [130/172], Loss: 13.2354\n",
      "Epoch [72/300], Step [131/172], Loss: 11.3999\n",
      "Epoch [72/300], Step [132/172], Loss: 11.1700\n",
      "Epoch [72/300], Step [133/172], Loss: 12.3015\n",
      "Epoch [72/300], Step [134/172], Loss: 13.2166\n",
      "Epoch [72/300], Step [135/172], Loss: 10.4020\n",
      "Epoch [72/300], Step [136/172], Loss: 10.1550\n",
      "Epoch [72/300], Step [137/172], Loss: 12.0846\n",
      "Epoch [72/300], Step [138/172], Loss: 10.2781\n",
      "Epoch [72/300], Step [139/172], Loss: 11.5478\n",
      "Epoch [72/300], Step [140/172], Loss: 11.5886\n",
      "Epoch [72/300], Step [141/172], Loss: 14.4994\n",
      "Epoch [72/300], Step [142/172], Loss: 15.1559\n",
      "Epoch [72/300], Step [143/172], Loss: 10.5664\n",
      "Epoch [72/300], Step [144/172], Loss: 10.4042\n",
      "Epoch [72/300], Step [145/172], Loss: 10.9356\n",
      "Epoch [72/300], Step [146/172], Loss: 11.2374\n",
      "Epoch [72/300], Step [147/172], Loss: 7.0648\n",
      "Epoch [72/300], Step [148/172], Loss: 8.1130\n",
      "Epoch [72/300], Step [149/172], Loss: 10.0899\n",
      "Epoch [72/300], Step [150/172], Loss: 9.7926\n",
      "Epoch [72/300], Step [151/172], Loss: 8.7732\n",
      "Epoch [72/300], Step [152/172], Loss: 8.8837\n",
      "Epoch [72/300], Step [153/172], Loss: 8.6451\n",
      "Epoch [72/300], Step [154/172], Loss: 9.4258\n",
      "Epoch [72/300], Step [155/172], Loss: 8.5075\n",
      "Epoch [72/300], Step [156/172], Loss: 12.2830\n",
      "Epoch [72/300], Step [157/172], Loss: 11.4567\n",
      "Epoch [72/300], Step [158/172], Loss: 9.2690\n",
      "Epoch [72/300], Step [159/172], Loss: 10.3324\n",
      "Epoch [72/300], Step [160/172], Loss: 10.4251\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/300], Step [161/172], Loss: 8.0815\n",
      "Epoch [72/300], Step [162/172], Loss: 8.6407\n",
      "Epoch [72/300], Step [163/172], Loss: 7.7775\n",
      "Epoch [72/300], Step [164/172], Loss: 10.7283\n",
      "Epoch [72/300], Step [165/172], Loss: 7.5049\n",
      "Epoch [72/300], Step [166/172], Loss: 7.9052\n",
      "Epoch [72/300], Step [167/172], Loss: 8.9208\n",
      "Epoch [72/300], Step [168/172], Loss: 7.6327\n",
      "Epoch [72/300], Step [169/172], Loss: 7.9310\n",
      "Epoch [72/300], Step [170/172], Loss: 6.9565\n",
      "Epoch [72/300], Step [171/172], Loss: 6.6700\n",
      "Epoch [72/300], Step [172/172], Loss: 6.1359\n",
      "Epoch [73/300], Step [1/172], Loss: 93.7389\n",
      "Epoch [73/300], Step [2/172], Loss: 94.3728\n",
      "Epoch [73/300], Step [3/172], Loss: 91.8861\n",
      "Epoch [73/300], Step [4/172], Loss: 56.4464\n",
      "Epoch [73/300], Step [5/172], Loss: 78.6522\n",
      "Epoch [73/300], Step [6/172], Loss: 25.3335\n",
      "Epoch [73/300], Step [7/172], Loss: 31.2142\n",
      "Epoch [73/300], Step [8/172], Loss: 9.0616\n",
      "Epoch [73/300], Step [9/172], Loss: 48.7957\n",
      "Epoch [73/300], Step [10/172], Loss: 53.7769\n",
      "Epoch [73/300], Step [11/172], Loss: 97.3926\n",
      "Epoch [73/300], Step [12/172], Loss: 85.8093\n",
      "Epoch [73/300], Step [13/172], Loss: 42.9094\n",
      "Epoch [73/300], Step [14/172], Loss: 97.1874\n",
      "Epoch [73/300], Step [15/172], Loss: 84.5344\n",
      "Epoch [73/300], Step [16/172], Loss: 40.0988\n",
      "Epoch [73/300], Step [17/172], Loss: 60.2468\n",
      "Epoch [73/300], Step [18/172], Loss: 64.7637\n",
      "Epoch [73/300], Step [19/172], Loss: 82.4132\n",
      "Epoch [73/300], Step [20/172], Loss: 90.0537\n",
      "Epoch [73/300], Step [21/172], Loss: 96.8809\n",
      "Epoch [73/300], Step [22/172], Loss: 87.7516\n",
      "Epoch [73/300], Step [23/172], Loss: 9.3387\n",
      "Epoch [73/300], Step [24/172], Loss: 76.0679\n",
      "Epoch [73/300], Step [25/172], Loss: 50.8501\n",
      "Epoch [73/300], Step [26/172], Loss: 62.2628\n",
      "Epoch [73/300], Step [27/172], Loss: 85.0818\n",
      "Epoch [73/300], Step [28/172], Loss: 47.9210\n",
      "Epoch [73/300], Step [29/172], Loss: 42.4464\n",
      "Epoch [73/300], Step [30/172], Loss: 79.7236\n",
      "Epoch [73/300], Step [31/172], Loss: 45.1299\n",
      "Epoch [73/300], Step [32/172], Loss: 40.8888\n",
      "Epoch [73/300], Step [33/172], Loss: 73.5853\n",
      "Epoch [73/300], Step [34/172], Loss: 6.2581\n",
      "Epoch [73/300], Step [35/172], Loss: 29.9554\n",
      "Epoch [73/300], Step [36/172], Loss: 23.8269\n",
      "Epoch [73/300], Step [37/172], Loss: 18.8285\n",
      "Epoch [73/300], Step [38/172], Loss: 27.9009\n",
      "Epoch [73/300], Step [39/172], Loss: 52.6922\n",
      "Epoch [73/300], Step [40/172], Loss: 24.5897\n",
      "Epoch [73/300], Step [41/172], Loss: 40.7780\n",
      "Epoch [73/300], Step [42/172], Loss: 44.0226\n",
      "Epoch [73/300], Step [43/172], Loss: 28.0926\n",
      "Epoch [73/300], Step [44/172], Loss: 24.6767\n",
      "Epoch [73/300], Step [45/172], Loss: 22.2492\n",
      "Epoch [73/300], Step [46/172], Loss: 30.7538\n",
      "Epoch [73/300], Step [47/172], Loss: 55.9900\n",
      "Epoch [73/300], Step [48/172], Loss: 57.7246\n",
      "Epoch [73/300], Step [49/172], Loss: 21.4428\n",
      "Epoch [73/300], Step [50/172], Loss: 52.3806\n",
      "Epoch [73/300], Step [51/172], Loss: 7.6935\n",
      "Epoch [73/300], Step [52/172], Loss: 20.6040\n",
      "Epoch [73/300], Step [53/172], Loss: 26.9384\n",
      "Epoch [73/300], Step [54/172], Loss: 13.7537\n",
      "Epoch [73/300], Step [55/172], Loss: 13.7560\n",
      "Epoch [73/300], Step [56/172], Loss: 10.4229\n",
      "Epoch [73/300], Step [57/172], Loss: 30.0516\n",
      "Epoch [73/300], Step [58/172], Loss: 20.3388\n",
      "Epoch [73/300], Step [59/172], Loss: 33.9910\n",
      "Epoch [73/300], Step [60/172], Loss: 53.9541\n",
      "Epoch [73/300], Step [61/172], Loss: 10.7546\n",
      "Epoch [73/300], Step [62/172], Loss: 23.5088\n",
      "Epoch [73/300], Step [63/172], Loss: 8.7892\n",
      "Epoch [73/300], Step [64/172], Loss: 7.4800\n",
      "Epoch [73/300], Step [65/172], Loss: 23.8866\n",
      "Epoch [73/300], Step [66/172], Loss: 6.5805\n",
      "Epoch [73/300], Step [67/172], Loss: 28.1343\n",
      "Epoch [73/300], Step [68/172], Loss: 8.9280\n",
      "Epoch [73/300], Step [69/172], Loss: 72.9599\n",
      "Epoch [73/300], Step [70/172], Loss: 64.6617\n",
      "Epoch [73/300], Step [71/172], Loss: 58.1718\n",
      "Epoch [73/300], Step [72/172], Loss: 62.4621\n",
      "Epoch [73/300], Step [73/172], Loss: 67.5777\n",
      "Epoch [73/300], Step [74/172], Loss: 40.1885\n",
      "Epoch [73/300], Step [75/172], Loss: 33.7144\n",
      "Epoch [73/300], Step [76/172], Loss: 43.0259\n",
      "Epoch [73/300], Step [77/172], Loss: 62.5655\n",
      "Epoch [73/300], Step [78/172], Loss: 54.3015\n",
      "Epoch [73/300], Step [79/172], Loss: 51.5254\n",
      "Epoch [73/300], Step [80/172], Loss: 57.5133\n",
      "Epoch [73/300], Step [81/172], Loss: 45.8237\n",
      "Epoch [73/300], Step [82/172], Loss: 42.1550\n",
      "Epoch [73/300], Step [83/172], Loss: 52.0578\n",
      "Epoch [73/300], Step [84/172], Loss: 40.9063\n",
      "Epoch [73/300], Step [85/172], Loss: 45.3449\n",
      "Epoch [73/300], Step [86/172], Loss: 36.6744\n",
      "Epoch [73/300], Step [87/172], Loss: 30.5958\n",
      "Epoch [73/300], Step [88/172], Loss: 32.5542\n",
      "Epoch [73/300], Step [89/172], Loss: 29.7487\n",
      "Epoch [73/300], Step [90/172], Loss: 28.7951\n",
      "Epoch [73/300], Step [91/172], Loss: 31.1858\n",
      "Epoch [73/300], Step [92/172], Loss: 24.1726\n",
      "Epoch [73/300], Step [93/172], Loss: 23.9158\n",
      "Epoch [73/300], Step [94/172], Loss: 30.7049\n",
      "Epoch [73/300], Step [95/172], Loss: 25.3798\n",
      "Epoch [73/300], Step [96/172], Loss: 20.8868\n",
      "Epoch [73/300], Step [97/172], Loss: 27.7275\n",
      "Epoch [73/300], Step [98/172], Loss: 22.3573\n",
      "Epoch [73/300], Step [99/172], Loss: 19.8961\n",
      "Epoch [73/300], Step [100/172], Loss: 18.5435\n",
      "Epoch [73/300], Step [101/172], Loss: 20.1413\n",
      "Epoch [73/300], Step [102/172], Loss: 18.5397\n",
      "Epoch [73/300], Step [103/172], Loss: 17.1982\n",
      "Epoch [73/300], Step [104/172], Loss: 17.6843\n",
      "Epoch [73/300], Step [105/172], Loss: 19.7098\n",
      "Epoch [73/300], Step [106/172], Loss: 18.9478\n",
      "Epoch [73/300], Step [107/172], Loss: 16.6044\n",
      "Epoch [73/300], Step [108/172], Loss: 19.3398\n",
      "Epoch [73/300], Step [109/172], Loss: 20.8491\n",
      "Epoch [73/300], Step [110/172], Loss: 18.1086\n",
      "Epoch [73/300], Step [111/172], Loss: 16.5975\n",
      "Epoch [73/300], Step [112/172], Loss: 21.5189\n",
      "Epoch [73/300], Step [113/172], Loss: 17.4839\n",
      "Epoch [73/300], Step [114/172], Loss: 16.9723\n",
      "Epoch [73/300], Step [115/172], Loss: 24.7190\n",
      "Epoch [73/300], Step [116/172], Loss: 17.1957\n",
      "Epoch [73/300], Step [117/172], Loss: 14.5288\n",
      "Epoch [73/300], Step [118/172], Loss: 17.8611\n",
      "Epoch [73/300], Step [119/172], Loss: 16.5293\n",
      "Epoch [73/300], Step [120/172], Loss: 13.3478\n",
      "Epoch [73/300], Step [121/172], Loss: 13.6203\n",
      "Epoch [73/300], Step [122/172], Loss: 13.1203\n",
      "Epoch [73/300], Step [123/172], Loss: 12.4051\n",
      "Epoch [73/300], Step [124/172], Loss: 10.1526\n",
      "Epoch [73/300], Step [125/172], Loss: 14.8138\n",
      "Epoch [73/300], Step [126/172], Loss: 12.9006\n",
      "Epoch [73/300], Step [127/172], Loss: 15.4487\n",
      "Epoch [73/300], Step [128/172], Loss: 15.7844\n",
      "Epoch [73/300], Step [129/172], Loss: 11.1032\n",
      "Epoch [73/300], Step [130/172], Loss: 13.2150\n",
      "Epoch [73/300], Step [131/172], Loss: 11.3578\n",
      "Epoch [73/300], Step [132/172], Loss: 11.1334\n",
      "Epoch [73/300], Step [133/172], Loss: 12.2989\n",
      "Epoch [73/300], Step [134/172], Loss: 13.2387\n",
      "Epoch [73/300], Step [135/172], Loss: 10.3788\n",
      "Epoch [73/300], Step [136/172], Loss: 10.1526\n",
      "Epoch [73/300], Step [137/172], Loss: 12.0823\n",
      "Epoch [73/300], Step [138/172], Loss: 10.2888\n",
      "Epoch [73/300], Step [139/172], Loss: 11.5183\n",
      "Epoch [73/300], Step [140/172], Loss: 11.5531\n",
      "Epoch [73/300], Step [141/172], Loss: 14.4492\n",
      "Epoch [73/300], Step [142/172], Loss: 15.1628\n",
      "Epoch [73/300], Step [143/172], Loss: 10.5786\n",
      "Epoch [73/300], Step [144/172], Loss: 10.3632\n",
      "Epoch [73/300], Step [145/172], Loss: 10.9563\n",
      "Epoch [73/300], Step [146/172], Loss: 11.2411\n",
      "Epoch [73/300], Step [147/172], Loss: 7.0337\n",
      "Epoch [73/300], Step [148/172], Loss: 8.0972\n",
      "Epoch [73/300], Step [149/172], Loss: 10.0537\n",
      "Epoch [73/300], Step [150/172], Loss: 9.7853\n",
      "Epoch [73/300], Step [151/172], Loss: 8.7497\n",
      "Epoch [73/300], Step [152/172], Loss: 8.8807\n",
      "Epoch [73/300], Step [153/172], Loss: 8.6326\n",
      "Epoch [73/300], Step [154/172], Loss: 9.4129\n",
      "Epoch [73/300], Step [155/172], Loss: 8.4983\n",
      "Epoch [73/300], Step [156/172], Loss: 12.4040\n",
      "Epoch [73/300], Step [157/172], Loss: 11.4888\n",
      "Epoch [73/300], Step [158/172], Loss: 9.2851\n",
      "Epoch [73/300], Step [159/172], Loss: 10.3276\n",
      "Epoch [73/300], Step [160/172], Loss: 10.5029\n",
      "Epoch [73/300], Step [161/172], Loss: 8.0998\n",
      "Epoch [73/300], Step [162/172], Loss: 8.6254\n",
      "Epoch [73/300], Step [163/172], Loss: 7.7609\n",
      "Epoch [73/300], Step [164/172], Loss: 10.8050\n",
      "Epoch [73/300], Step [165/172], Loss: 7.4941\n",
      "Epoch [73/300], Step [166/172], Loss: 7.8951\n",
      "Epoch [73/300], Step [167/172], Loss: 8.9575\n",
      "Epoch [73/300], Step [168/172], Loss: 7.6307\n",
      "Epoch [73/300], Step [169/172], Loss: 7.9479\n",
      "Epoch [73/300], Step [170/172], Loss: 6.9424\n",
      "Epoch [73/300], Step [171/172], Loss: 6.6895\n",
      "Epoch [73/300], Step [172/172], Loss: 6.1512\n",
      "Epoch [74/300], Step [1/172], Loss: 93.1000\n",
      "Epoch [74/300], Step [2/172], Loss: 93.7610\n",
      "Epoch [74/300], Step [3/172], Loss: 91.0518\n",
      "Epoch [74/300], Step [4/172], Loss: 55.9160\n",
      "Epoch [74/300], Step [5/172], Loss: 78.0419\n",
      "Epoch [74/300], Step [6/172], Loss: 24.8846\n",
      "Epoch [74/300], Step [7/172], Loss: 30.4442\n",
      "Epoch [74/300], Step [8/172], Loss: 8.3718\n",
      "Epoch [74/300], Step [9/172], Loss: 48.2165\n",
      "Epoch [74/300], Step [10/172], Loss: 53.2035\n",
      "Epoch [74/300], Step [11/172], Loss: 96.5710\n",
      "Epoch [74/300], Step [12/172], Loss: 85.3878\n",
      "Epoch [74/300], Step [13/172], Loss: 42.6032\n",
      "Epoch [74/300], Step [14/172], Loss: 96.1214\n",
      "Epoch [74/300], Step [15/172], Loss: 83.5215\n",
      "Epoch [74/300], Step [16/172], Loss: 38.5030\n",
      "Epoch [74/300], Step [17/172], Loss: 59.7426\n",
      "Epoch [74/300], Step [18/172], Loss: 64.2485\n",
      "Epoch [74/300], Step [19/172], Loss: 81.9932\n",
      "Epoch [74/300], Step [20/172], Loss: 88.7802\n",
      "Epoch [74/300], Step [21/172], Loss: 96.2663\n",
      "Epoch [74/300], Step [22/172], Loss: 86.6902\n",
      "Epoch [74/300], Step [23/172], Loss: 9.3231\n",
      "Epoch [74/300], Step [24/172], Loss: 75.7240\n",
      "Epoch [74/300], Step [25/172], Loss: 50.5459\n",
      "Epoch [74/300], Step [26/172], Loss: 62.1706\n",
      "Epoch [74/300], Step [27/172], Loss: 84.4476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/300], Step [28/172], Loss: 47.5993\n",
      "Epoch [74/300], Step [29/172], Loss: 41.8389\n",
      "Epoch [74/300], Step [30/172], Loss: 79.8730\n",
      "Epoch [74/300], Step [31/172], Loss: 45.3887\n",
      "Epoch [74/300], Step [32/172], Loss: 41.0826\n",
      "Epoch [74/300], Step [33/172], Loss: 73.7906\n",
      "Epoch [74/300], Step [34/172], Loss: 6.3555\n",
      "Epoch [74/300], Step [35/172], Loss: 28.9059\n",
      "Epoch [74/300], Step [36/172], Loss: 23.6590\n",
      "Epoch [74/300], Step [37/172], Loss: 18.9537\n",
      "Epoch [74/300], Step [38/172], Loss: 28.0370\n",
      "Epoch [74/300], Step [39/172], Loss: 52.4771\n",
      "Epoch [74/300], Step [40/172], Loss: 24.6172\n",
      "Epoch [74/300], Step [41/172], Loss: 40.8706\n",
      "Epoch [74/300], Step [42/172], Loss: 44.2827\n",
      "Epoch [74/300], Step [43/172], Loss: 28.2368\n",
      "Epoch [74/300], Step [44/172], Loss: 24.8447\n",
      "Epoch [74/300], Step [45/172], Loss: 22.3961\n",
      "Epoch [74/300], Step [46/172], Loss: 30.5413\n",
      "Epoch [74/300], Step [47/172], Loss: 56.1524\n",
      "Epoch [74/300], Step [48/172], Loss: 57.8599\n",
      "Epoch [74/300], Step [49/172], Loss: 21.5433\n",
      "Epoch [74/300], Step [50/172], Loss: 52.2683\n",
      "Epoch [74/300], Step [51/172], Loss: 7.7544\n",
      "Epoch [74/300], Step [52/172], Loss: 20.7033\n",
      "Epoch [74/300], Step [53/172], Loss: 27.0420\n",
      "Epoch [74/300], Step [54/172], Loss: 13.7312\n",
      "Epoch [74/300], Step [55/172], Loss: 13.8183\n",
      "Epoch [74/300], Step [56/172], Loss: 10.4613\n",
      "Epoch [74/300], Step [57/172], Loss: 29.8682\n",
      "Epoch [74/300], Step [58/172], Loss: 20.3902\n",
      "Epoch [74/300], Step [59/172], Loss: 33.8223\n",
      "Epoch [74/300], Step [60/172], Loss: 53.7250\n",
      "Epoch [74/300], Step [61/172], Loss: 10.6810\n",
      "Epoch [74/300], Step [62/172], Loss: 23.6362\n",
      "Epoch [74/300], Step [63/172], Loss: 8.8415\n",
      "Epoch [74/300], Step [64/172], Loss: 7.4994\n",
      "Epoch [74/300], Step [65/172], Loss: 23.7333\n",
      "Epoch [74/300], Step [66/172], Loss: 6.5581\n",
      "Epoch [74/300], Step [67/172], Loss: 28.1011\n",
      "Epoch [74/300], Step [68/172], Loss: 8.7411\n",
      "Epoch [74/300], Step [69/172], Loss: 72.6878\n",
      "Epoch [74/300], Step [70/172], Loss: 64.3815\n",
      "Epoch [74/300], Step [71/172], Loss: 57.9785\n",
      "Epoch [74/300], Step [72/172], Loss: 62.2929\n",
      "Epoch [74/300], Step [73/172], Loss: 67.4048\n",
      "Epoch [74/300], Step [74/172], Loss: 39.9549\n",
      "Epoch [74/300], Step [75/172], Loss: 33.5591\n",
      "Epoch [74/300], Step [76/172], Loss: 42.7434\n",
      "Epoch [74/300], Step [77/172], Loss: 62.6187\n",
      "Epoch [74/300], Step [78/172], Loss: 54.1051\n",
      "Epoch [74/300], Step [79/172], Loss: 51.4373\n",
      "Epoch [74/300], Step [80/172], Loss: 57.4306\n",
      "Epoch [74/300], Step [81/172], Loss: 45.6543\n",
      "Epoch [74/300], Step [82/172], Loss: 41.8269\n",
      "Epoch [74/300], Step [83/172], Loss: 52.0595\n",
      "Epoch [74/300], Step [84/172], Loss: 40.8433\n",
      "Epoch [74/300], Step [85/172], Loss: 45.2259\n",
      "Epoch [74/300], Step [86/172], Loss: 36.5754\n",
      "Epoch [74/300], Step [87/172], Loss: 30.5093\n",
      "Epoch [74/300], Step [88/172], Loss: 32.5127\n",
      "Epoch [74/300], Step [89/172], Loss: 29.6293\n",
      "Epoch [74/300], Step [90/172], Loss: 28.6412\n",
      "Epoch [74/300], Step [91/172], Loss: 31.1764\n",
      "Epoch [74/300], Step [92/172], Loss: 24.0788\n",
      "Epoch [74/300], Step [93/172], Loss: 23.8687\n",
      "Epoch [74/300], Step [94/172], Loss: 30.7318\n",
      "Epoch [74/300], Step [95/172], Loss: 25.3744\n",
      "Epoch [74/300], Step [96/172], Loss: 20.8270\n",
      "Epoch [74/300], Step [97/172], Loss: 27.6587\n",
      "Epoch [74/300], Step [98/172], Loss: 22.2963\n",
      "Epoch [74/300], Step [99/172], Loss: 19.8471\n",
      "Epoch [74/300], Step [100/172], Loss: 18.4735\n",
      "Epoch [74/300], Step [101/172], Loss: 20.0759\n",
      "Epoch [74/300], Step [102/172], Loss: 18.3039\n",
      "Epoch [74/300], Step [103/172], Loss: 17.0724\n",
      "Epoch [74/300], Step [104/172], Loss: 17.6381\n",
      "Epoch [74/300], Step [105/172], Loss: 19.5941\n",
      "Epoch [74/300], Step [106/172], Loss: 18.8940\n",
      "Epoch [74/300], Step [107/172], Loss: 16.5736\n",
      "Epoch [74/300], Step [108/172], Loss: 19.1858\n",
      "Epoch [74/300], Step [109/172], Loss: 20.6741\n",
      "Epoch [74/300], Step [110/172], Loss: 18.0041\n",
      "Epoch [74/300], Step [111/172], Loss: 16.5510\n",
      "Epoch [74/300], Step [112/172], Loss: 21.4330\n",
      "Epoch [74/300], Step [113/172], Loss: 17.3944\n",
      "Epoch [74/300], Step [114/172], Loss: 16.8672\n",
      "Epoch [74/300], Step [115/172], Loss: 24.5994\n",
      "Epoch [74/300], Step [116/172], Loss: 17.1413\n",
      "Epoch [74/300], Step [117/172], Loss: 14.4309\n",
      "Epoch [74/300], Step [118/172], Loss: 17.7449\n",
      "Epoch [74/300], Step [119/172], Loss: 16.5186\n",
      "Epoch [74/300], Step [120/172], Loss: 13.2831\n",
      "Epoch [74/300], Step [121/172], Loss: 13.5620\n",
      "Epoch [74/300], Step [122/172], Loss: 12.8087\n",
      "Epoch [74/300], Step [123/172], Loss: 12.3507\n",
      "Epoch [74/300], Step [124/172], Loss: 10.0954\n",
      "Epoch [74/300], Step [125/172], Loss: 14.7872\n",
      "Epoch [74/300], Step [126/172], Loss: 12.8275\n",
      "Epoch [74/300], Step [127/172], Loss: 15.2692\n",
      "Epoch [74/300], Step [128/172], Loss: 15.6620\n",
      "Epoch [74/300], Step [129/172], Loss: 11.0267\n",
      "Epoch [74/300], Step [130/172], Loss: 13.1417\n",
      "Epoch [74/300], Step [131/172], Loss: 11.2382\n",
      "Epoch [74/300], Step [132/172], Loss: 11.0707\n",
      "Epoch [74/300], Step [133/172], Loss: 12.1351\n",
      "Epoch [74/300], Step [134/172], Loss: 13.1526\n",
      "Epoch [74/300], Step [135/172], Loss: 10.3336\n",
      "Epoch [74/300], Step [136/172], Loss: 10.1026\n",
      "Epoch [74/300], Step [137/172], Loss: 12.0474\n",
      "Epoch [74/300], Step [138/172], Loss: 10.2246\n",
      "Epoch [74/300], Step [139/172], Loss: 11.4676\n",
      "Epoch [74/300], Step [140/172], Loss: 11.5001\n",
      "Epoch [74/300], Step [141/172], Loss: 14.3444\n",
      "Epoch [74/300], Step [142/172], Loss: 15.0376\n",
      "Epoch [74/300], Step [143/172], Loss: 10.5398\n",
      "Epoch [74/300], Step [144/172], Loss: 10.3007\n",
      "Epoch [74/300], Step [145/172], Loss: 10.9374\n",
      "Epoch [74/300], Step [146/172], Loss: 11.2179\n",
      "Epoch [74/300], Step [147/172], Loss: 6.9956\n",
      "Epoch [74/300], Step [148/172], Loss: 8.0740\n",
      "Epoch [74/300], Step [149/172], Loss: 10.0281\n",
      "Epoch [74/300], Step [150/172], Loss: 9.7375\n",
      "Epoch [74/300], Step [151/172], Loss: 8.6973\n",
      "Epoch [74/300], Step [152/172], Loss: 8.8624\n",
      "Epoch [74/300], Step [153/172], Loss: 8.5957\n",
      "Epoch [74/300], Step [154/172], Loss: 9.3682\n",
      "Epoch [74/300], Step [155/172], Loss: 8.4437\n",
      "Epoch [74/300], Step [156/172], Loss: 12.3595\n",
      "Epoch [74/300], Step [157/172], Loss: 11.4170\n",
      "Epoch [74/300], Step [158/172], Loss: 9.2171\n",
      "Epoch [74/300], Step [159/172], Loss: 10.3191\n",
      "Epoch [74/300], Step [160/172], Loss: 10.4745\n",
      "Epoch [74/300], Step [161/172], Loss: 8.0925\n",
      "Epoch [74/300], Step [162/172], Loss: 8.6188\n",
      "Epoch [74/300], Step [163/172], Loss: 7.7223\n",
      "Epoch [74/300], Step [164/172], Loss: 10.6743\n",
      "Epoch [74/300], Step [165/172], Loss: 7.4864\n",
      "Epoch [74/300], Step [166/172], Loss: 7.8616\n",
      "Epoch [74/300], Step [167/172], Loss: 8.9906\n",
      "Epoch [74/300], Step [168/172], Loss: 7.6361\n",
      "Epoch [74/300], Step [169/172], Loss: 7.9523\n",
      "Epoch [74/300], Step [170/172], Loss: 6.9252\n",
      "Epoch [74/300], Step [171/172], Loss: 6.7037\n",
      "Epoch [74/300], Step [172/172], Loss: 6.1561\n",
      "Epoch [75/300], Step [1/172], Loss: 92.3435\n",
      "Epoch [75/300], Step [2/172], Loss: 93.0477\n",
      "Epoch [75/300], Step [3/172], Loss: 90.1445\n",
      "Epoch [75/300], Step [4/172], Loss: 55.4945\n",
      "Epoch [75/300], Step [5/172], Loss: 77.0320\n",
      "Epoch [75/300], Step [6/172], Loss: 24.6802\n",
      "Epoch [75/300], Step [7/172], Loss: 31.0862\n",
      "Epoch [75/300], Step [8/172], Loss: 9.0778\n",
      "Epoch [75/300], Step [9/172], Loss: 48.2968\n",
      "Epoch [75/300], Step [10/172], Loss: 52.8757\n",
      "Epoch [75/300], Step [11/172], Loss: 96.4349\n",
      "Epoch [75/300], Step [12/172], Loss: 85.4998\n",
      "Epoch [75/300], Step [13/172], Loss: 42.7114\n",
      "Epoch [75/300], Step [14/172], Loss: 96.4647\n",
      "Epoch [75/300], Step [15/172], Loss: 83.6447\n",
      "Epoch [75/300], Step [16/172], Loss: 38.2770\n",
      "Epoch [75/300], Step [17/172], Loss: 60.0704\n",
      "Epoch [75/300], Step [18/172], Loss: 64.4008\n",
      "Epoch [75/300], Step [19/172], Loss: 82.5683\n",
      "Epoch [75/300], Step [20/172], Loss: 88.4988\n",
      "Epoch [75/300], Step [21/172], Loss: 96.7911\n",
      "Epoch [75/300], Step [22/172], Loss: 87.0635\n",
      "Epoch [75/300], Step [23/172], Loss: 8.7695\n",
      "Epoch [75/300], Step [24/172], Loss: 75.7853\n",
      "Epoch [75/300], Step [25/172], Loss: 50.5259\n",
      "Epoch [75/300], Step [26/172], Loss: 62.3077\n",
      "Epoch [75/300], Step [27/172], Loss: 84.8423\n",
      "Epoch [75/300], Step [28/172], Loss: 47.0039\n",
      "Epoch [75/300], Step [29/172], Loss: 40.8333\n",
      "Epoch [75/300], Step [30/172], Loss: 80.0976\n",
      "Epoch [75/300], Step [31/172], Loss: 45.2219\n",
      "Epoch [75/300], Step [32/172], Loss: 40.9187\n",
      "Epoch [75/300], Step [33/172], Loss: 73.5603\n",
      "Epoch [75/300], Step [34/172], Loss: 5.9413\n",
      "Epoch [75/300], Step [35/172], Loss: 28.4632\n",
      "Epoch [75/300], Step [36/172], Loss: 23.5417\n",
      "Epoch [75/300], Step [37/172], Loss: 18.6934\n",
      "Epoch [75/300], Step [38/172], Loss: 27.8019\n",
      "Epoch [75/300], Step [39/172], Loss: 52.4742\n",
      "Epoch [75/300], Step [40/172], Loss: 24.2646\n",
      "Epoch [75/300], Step [41/172], Loss: 40.6925\n",
      "Epoch [75/300], Step [42/172], Loss: 43.9978\n",
      "Epoch [75/300], Step [43/172], Loss: 27.9321\n",
      "Epoch [75/300], Step [44/172], Loss: 24.5170\n",
      "Epoch [75/300], Step [45/172], Loss: 22.1590\n",
      "Epoch [75/300], Step [46/172], Loss: 30.1898\n",
      "Epoch [75/300], Step [47/172], Loss: 55.6690\n",
      "Epoch [75/300], Step [48/172], Loss: 57.5495\n",
      "Epoch [75/300], Step [49/172], Loss: 21.3934\n",
      "Epoch [75/300], Step [50/172], Loss: 52.4333\n",
      "Epoch [75/300], Step [51/172], Loss: 7.6631\n",
      "Epoch [75/300], Step [52/172], Loss: 20.4215\n",
      "Epoch [75/300], Step [53/172], Loss: 26.7278\n",
      "Epoch [75/300], Step [54/172], Loss: 13.4730\n",
      "Epoch [75/300], Step [55/172], Loss: 13.6440\n",
      "Epoch [75/300], Step [56/172], Loss: 10.3953\n",
      "Epoch [75/300], Step [57/172], Loss: 29.2929\n",
      "Epoch [75/300], Step [58/172], Loss: 20.2657\n",
      "Epoch [75/300], Step [59/172], Loss: 33.8065\n",
      "Epoch [75/300], Step [60/172], Loss: 53.4842\n",
      "Epoch [75/300], Step [61/172], Loss: 10.6421\n",
      "Epoch [75/300], Step [62/172], Loss: 23.7465\n",
      "Epoch [75/300], Step [63/172], Loss: 8.7920\n",
      "Epoch [75/300], Step [64/172], Loss: 7.5393\n",
      "Epoch [75/300], Step [65/172], Loss: 23.6146\n",
      "Epoch [75/300], Step [66/172], Loss: 6.4852\n",
      "Epoch [75/300], Step [67/172], Loss: 28.2012\n",
      "Epoch [75/300], Step [68/172], Loss: 8.5323\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/300], Step [69/172], Loss: 72.3150\n",
      "Epoch [75/300], Step [70/172], Loss: 63.9682\n",
      "Epoch [75/300], Step [71/172], Loss: 57.7416\n",
      "Epoch [75/300], Step [72/172], Loss: 62.2668\n",
      "Epoch [75/300], Step [73/172], Loss: 67.2784\n",
      "Epoch [75/300], Step [74/172], Loss: 39.8142\n",
      "Epoch [75/300], Step [75/172], Loss: 33.7557\n",
      "Epoch [75/300], Step [76/172], Loss: 42.5963\n",
      "Epoch [75/300], Step [77/172], Loss: 62.7135\n",
      "Epoch [75/300], Step [78/172], Loss: 53.9812\n",
      "Epoch [75/300], Step [79/172], Loss: 51.4176\n",
      "Epoch [75/300], Step [80/172], Loss: 57.7140\n",
      "Epoch [75/300], Step [81/172], Loss: 45.5665\n",
      "Epoch [75/300], Step [82/172], Loss: 41.9917\n",
      "Epoch [75/300], Step [83/172], Loss: 52.0253\n",
      "Epoch [75/300], Step [84/172], Loss: 40.7911\n",
      "Epoch [75/300], Step [85/172], Loss: 44.9621\n",
      "Epoch [75/300], Step [86/172], Loss: 36.4448\n",
      "Epoch [75/300], Step [87/172], Loss: 30.4429\n",
      "Epoch [75/300], Step [88/172], Loss: 32.4152\n",
      "Epoch [75/300], Step [89/172], Loss: 29.4623\n",
      "Epoch [75/300], Step [90/172], Loss: 28.4869\n",
      "Epoch [75/300], Step [91/172], Loss: 31.1399\n",
      "Epoch [75/300], Step [92/172], Loss: 23.9223\n",
      "Epoch [75/300], Step [93/172], Loss: 23.6896\n",
      "Epoch [75/300], Step [94/172], Loss: 30.6015\n",
      "Epoch [75/300], Step [95/172], Loss: 25.1986\n",
      "Epoch [75/300], Step [96/172], Loss: 20.7009\n",
      "Epoch [75/300], Step [97/172], Loss: 27.5313\n",
      "Epoch [75/300], Step [98/172], Loss: 22.1675\n",
      "Epoch [75/300], Step [99/172], Loss: 19.7114\n",
      "Epoch [75/300], Step [100/172], Loss: 18.3362\n",
      "Epoch [75/300], Step [101/172], Loss: 19.9260\n",
      "Epoch [75/300], Step [102/172], Loss: 18.2453\n",
      "Epoch [75/300], Step [103/172], Loss: 16.8928\n",
      "Epoch [75/300], Step [104/172], Loss: 17.5282\n",
      "Epoch [75/300], Step [105/172], Loss: 19.5541\n",
      "Epoch [75/300], Step [106/172], Loss: 18.7719\n",
      "Epoch [75/300], Step [107/172], Loss: 16.4928\n",
      "Epoch [75/300], Step [108/172], Loss: 19.0189\n",
      "Epoch [75/300], Step [109/172], Loss: 20.5206\n",
      "Epoch [75/300], Step [110/172], Loss: 17.8743\n",
      "Epoch [75/300], Step [111/172], Loss: 16.4438\n",
      "Epoch [75/300], Step [112/172], Loss: 21.2774\n",
      "Epoch [75/300], Step [113/172], Loss: 17.2698\n",
      "Epoch [75/300], Step [114/172], Loss: 16.7205\n",
      "Epoch [75/300], Step [115/172], Loss: 24.4149\n",
      "Epoch [75/300], Step [116/172], Loss: 16.9993\n",
      "Epoch [75/300], Step [117/172], Loss: 14.3207\n",
      "Epoch [75/300], Step [118/172], Loss: 17.5314\n",
      "Epoch [75/300], Step [119/172], Loss: 16.4787\n",
      "Epoch [75/300], Step [120/172], Loss: 13.1431\n",
      "Epoch [75/300], Step [121/172], Loss: 13.4185\n",
      "Epoch [75/300], Step [122/172], Loss: 12.6827\n",
      "Epoch [75/300], Step [123/172], Loss: 12.2323\n",
      "Epoch [75/300], Step [124/172], Loss: 10.0103\n",
      "Epoch [75/300], Step [125/172], Loss: 14.6628\n",
      "Epoch [75/300], Step [126/172], Loss: 12.7112\n",
      "Epoch [75/300], Step [127/172], Loss: 15.1001\n",
      "Epoch [75/300], Step [128/172], Loss: 15.4980\n",
      "Epoch [75/300], Step [129/172], Loss: 10.9417\n",
      "Epoch [75/300], Step [130/172], Loss: 13.0349\n",
      "Epoch [75/300], Step [131/172], Loss: 11.1318\n",
      "Epoch [75/300], Step [132/172], Loss: 10.9619\n",
      "Epoch [75/300], Step [133/172], Loss: 12.0217\n",
      "Epoch [75/300], Step [134/172], Loss: 13.0872\n",
      "Epoch [75/300], Step [135/172], Loss: 10.2603\n",
      "Epoch [75/300], Step [136/172], Loss: 10.0642\n",
      "Epoch [75/300], Step [137/172], Loss: 11.9779\n",
      "Epoch [75/300], Step [138/172], Loss: 10.1751\n",
      "Epoch [75/300], Step [139/172], Loss: 11.4052\n",
      "Epoch [75/300], Step [140/172], Loss: 11.4219\n",
      "Epoch [75/300], Step [141/172], Loss: 14.2451\n",
      "Epoch [75/300], Step [142/172], Loss: 14.9910\n",
      "Epoch [75/300], Step [143/172], Loss: 10.4761\n",
      "Epoch [75/300], Step [144/172], Loss: 10.2353\n",
      "Epoch [75/300], Step [145/172], Loss: 10.9070\n",
      "Epoch [75/300], Step [146/172], Loss: 11.1493\n",
      "Epoch [75/300], Step [147/172], Loss: 6.9058\n",
      "Epoch [75/300], Step [148/172], Loss: 7.9918\n",
      "Epoch [75/300], Step [149/172], Loss: 9.9203\n",
      "Epoch [75/300], Step [150/172], Loss: 9.6596\n",
      "Epoch [75/300], Step [151/172], Loss: 8.6251\n",
      "Epoch [75/300], Step [152/172], Loss: 8.8016\n",
      "Epoch [75/300], Step [153/172], Loss: 8.5258\n",
      "Epoch [75/300], Step [154/172], Loss: 9.2676\n",
      "Epoch [75/300], Step [155/172], Loss: 8.3849\n",
      "Epoch [75/300], Step [156/172], Loss: 12.3650\n",
      "Epoch [75/300], Step [157/172], Loss: 11.3435\n",
      "Epoch [75/300], Step [158/172], Loss: 9.1300\n",
      "Epoch [75/300], Step [159/172], Loss: 10.3009\n",
      "Epoch [75/300], Step [160/172], Loss: 10.4232\n",
      "Epoch [75/300], Step [161/172], Loss: 8.0361\n",
      "Epoch [75/300], Step [162/172], Loss: 8.5544\n",
      "Epoch [75/300], Step [163/172], Loss: 7.6447\n",
      "Epoch [75/300], Step [164/172], Loss: 10.7086\n",
      "Epoch [75/300], Step [165/172], Loss: 7.4299\n",
      "Epoch [75/300], Step [166/172], Loss: 7.8124\n",
      "Epoch [75/300], Step [167/172], Loss: 8.9454\n",
      "Epoch [75/300], Step [168/172], Loss: 7.5815\n",
      "Epoch [75/300], Step [169/172], Loss: 7.9081\n",
      "Epoch [75/300], Step [170/172], Loss: 6.8475\n",
      "Epoch [75/300], Step [171/172], Loss: 6.6658\n",
      "Epoch [75/300], Step [172/172], Loss: 6.1277\n",
      "Epoch [76/300], Step [1/172], Loss: 92.0037\n",
      "Epoch [76/300], Step [2/172], Loss: 92.7087\n",
      "Epoch [76/300], Step [3/172], Loss: 89.2666\n",
      "Epoch [76/300], Step [4/172], Loss: 55.0153\n",
      "Epoch [76/300], Step [5/172], Loss: 76.7616\n",
      "Epoch [76/300], Step [6/172], Loss: 24.4638\n",
      "Epoch [76/300], Step [7/172], Loss: 30.7221\n",
      "Epoch [76/300], Step [8/172], Loss: 8.1092\n",
      "Epoch [76/300], Step [9/172], Loss: 47.7551\n",
      "Epoch [76/300], Step [10/172], Loss: 52.5311\n",
      "Epoch [76/300], Step [11/172], Loss: 96.0691\n",
      "Epoch [76/300], Step [12/172], Loss: 85.3505\n",
      "Epoch [76/300], Step [13/172], Loss: 42.5594\n",
      "Epoch [76/300], Step [14/172], Loss: 95.6362\n",
      "Epoch [76/300], Step [15/172], Loss: 82.8478\n",
      "Epoch [76/300], Step [16/172], Loss: 36.9325\n",
      "Epoch [76/300], Step [17/172], Loss: 59.8633\n",
      "Epoch [76/300], Step [18/172], Loss: 64.0239\n",
      "Epoch [76/300], Step [19/172], Loss: 82.2960\n",
      "Epoch [76/300], Step [20/172], Loss: 87.6603\n",
      "Epoch [76/300], Step [21/172], Loss: 96.3022\n",
      "Epoch [76/300], Step [22/172], Loss: 86.3901\n",
      "Epoch [76/300], Step [23/172], Loss: 8.5219\n",
      "Epoch [76/300], Step [24/172], Loss: 75.5645\n",
      "Epoch [76/300], Step [25/172], Loss: 50.2321\n",
      "Epoch [76/300], Step [26/172], Loss: 62.2617\n",
      "Epoch [76/300], Step [27/172], Loss: 84.4426\n",
      "Epoch [76/300], Step [28/172], Loss: 46.6969\n",
      "Epoch [76/300], Step [29/172], Loss: 40.1293\n",
      "Epoch [76/300], Step [30/172], Loss: 80.3053\n",
      "Epoch [76/300], Step [31/172], Loss: 45.4358\n",
      "Epoch [76/300], Step [32/172], Loss: 41.0403\n",
      "Epoch [76/300], Step [33/172], Loss: 73.7125\n",
      "Epoch [76/300], Step [34/172], Loss: 6.0680\n",
      "Epoch [76/300], Step [35/172], Loss: 27.4564\n",
      "Epoch [76/300], Step [36/172], Loss: 23.3910\n",
      "Epoch [76/300], Step [37/172], Loss: 18.8303\n",
      "Epoch [76/300], Step [38/172], Loss: 27.9291\n",
      "Epoch [76/300], Step [39/172], Loss: 52.2837\n",
      "Epoch [76/300], Step [40/172], Loss: 24.3479\n",
      "Epoch [76/300], Step [41/172], Loss: 40.7802\n",
      "Epoch [76/300], Step [42/172], Loss: 44.1680\n",
      "Epoch [76/300], Step [43/172], Loss: 28.1032\n",
      "Epoch [76/300], Step [44/172], Loss: 24.5129\n",
      "Epoch [76/300], Step [45/172], Loss: 22.3436\n",
      "Epoch [76/300], Step [46/172], Loss: 29.9484\n",
      "Epoch [76/300], Step [47/172], Loss: 55.6428\n",
      "Epoch [76/300], Step [48/172], Loss: 57.4093\n",
      "Epoch [76/300], Step [49/172], Loss: 21.4632\n",
      "Epoch [76/300], Step [50/172], Loss: 52.2444\n",
      "Epoch [76/300], Step [51/172], Loss: 7.7129\n",
      "Epoch [76/300], Step [52/172], Loss: 20.5358\n",
      "Epoch [76/300], Step [53/172], Loss: 26.7616\n",
      "Epoch [76/300], Step [54/172], Loss: 13.3935\n",
      "Epoch [76/300], Step [55/172], Loss: 13.6931\n",
      "Epoch [76/300], Step [56/172], Loss: 10.4306\n",
      "Epoch [76/300], Step [57/172], Loss: 29.0011\n",
      "Epoch [76/300], Step [58/172], Loss: 20.1636\n",
      "Epoch [76/300], Step [59/172], Loss: 33.3476\n",
      "Epoch [76/300], Step [60/172], Loss: 52.8904\n",
      "Epoch [76/300], Step [61/172], Loss: 10.5279\n",
      "Epoch [76/300], Step [62/172], Loss: 23.7896\n",
      "Epoch [76/300], Step [63/172], Loss: 8.8875\n",
      "Epoch [76/300], Step [64/172], Loss: 7.5964\n",
      "Epoch [76/300], Step [65/172], Loss: 23.3444\n",
      "Epoch [76/300], Step [66/172], Loss: 6.4982\n",
      "Epoch [76/300], Step [67/172], Loss: 28.1186\n",
      "Epoch [76/300], Step [68/172], Loss: 8.4962\n",
      "Epoch [76/300], Step [69/172], Loss: 71.6628\n",
      "Epoch [76/300], Step [70/172], Loss: 63.5645\n",
      "Epoch [76/300], Step [71/172], Loss: 57.3188\n",
      "Epoch [76/300], Step [72/172], Loss: 61.8611\n",
      "Epoch [76/300], Step [73/172], Loss: 66.8828\n",
      "Epoch [76/300], Step [74/172], Loss: 39.4654\n",
      "Epoch [76/300], Step [75/172], Loss: 33.4422\n",
      "Epoch [76/300], Step [76/172], Loss: 42.2422\n",
      "Epoch [76/300], Step [77/172], Loss: 62.4224\n",
      "Epoch [76/300], Step [78/172], Loss: 53.6087\n",
      "Epoch [76/300], Step [79/172], Loss: 51.1468\n",
      "Epoch [76/300], Step [80/172], Loss: 57.4913\n",
      "Epoch [76/300], Step [81/172], Loss: 45.2603\n",
      "Epoch [76/300], Step [82/172], Loss: 41.7380\n",
      "Epoch [76/300], Step [83/172], Loss: 51.9211\n",
      "Epoch [76/300], Step [84/172], Loss: 40.6296\n",
      "Epoch [76/300], Step [85/172], Loss: 44.7529\n",
      "Epoch [76/300], Step [86/172], Loss: 36.3271\n",
      "Epoch [76/300], Step [87/172], Loss: 30.2986\n",
      "Epoch [76/300], Step [88/172], Loss: 32.3082\n",
      "Epoch [76/300], Step [89/172], Loss: 29.3453\n",
      "Epoch [76/300], Step [90/172], Loss: 28.3670\n",
      "Epoch [76/300], Step [91/172], Loss: 31.1026\n",
      "Epoch [76/300], Step [92/172], Loss: 23.8433\n",
      "Epoch [76/300], Step [93/172], Loss: 23.6108\n",
      "Epoch [76/300], Step [94/172], Loss: 30.5845\n",
      "Epoch [76/300], Step [95/172], Loss: 25.1618\n",
      "Epoch [76/300], Step [96/172], Loss: 20.6054\n",
      "Epoch [76/300], Step [97/172], Loss: 27.4544\n",
      "Epoch [76/300], Step [98/172], Loss: 22.0910\n",
      "Epoch [76/300], Step [99/172], Loss: 19.6367\n",
      "Epoch [76/300], Step [100/172], Loss: 18.2589\n",
      "Epoch [76/300], Step [101/172], Loss: 19.8394\n",
      "Epoch [76/300], Step [102/172], Loss: 18.1422\n",
      "Epoch [76/300], Step [103/172], Loss: 16.7901\n",
      "Epoch [76/300], Step [104/172], Loss: 17.4719\n",
      "Epoch [76/300], Step [105/172], Loss: 19.5292\n",
      "Epoch [76/300], Step [106/172], Loss: 18.7242\n",
      "Epoch [76/300], Step [107/172], Loss: 16.4796\n",
      "Epoch [76/300], Step [108/172], Loss: 18.9246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/300], Step [109/172], Loss: 20.4538\n",
      "Epoch [76/300], Step [110/172], Loss: 17.8325\n",
      "Epoch [76/300], Step [111/172], Loss: 16.3790\n",
      "Epoch [76/300], Step [112/172], Loss: 21.2114\n",
      "Epoch [76/300], Step [113/172], Loss: 17.2158\n",
      "Epoch [76/300], Step [114/172], Loss: 16.6419\n",
      "Epoch [76/300], Step [115/172], Loss: 24.3714\n",
      "Epoch [76/300], Step [116/172], Loss: 16.9464\n",
      "Epoch [76/300], Step [117/172], Loss: 14.2582\n",
      "Epoch [76/300], Step [118/172], Loss: 17.5077\n",
      "Epoch [76/300], Step [119/172], Loss: 16.4759\n",
      "Epoch [76/300], Step [120/172], Loss: 13.1004\n",
      "Epoch [76/300], Step [121/172], Loss: 13.3699\n",
      "Epoch [76/300], Step [122/172], Loss: 12.6539\n",
      "Epoch [76/300], Step [123/172], Loss: 12.1885\n",
      "Epoch [76/300], Step [124/172], Loss: 9.9699\n",
      "Epoch [76/300], Step [125/172], Loss: 14.6451\n",
      "Epoch [76/300], Step [126/172], Loss: 12.6593\n",
      "Epoch [76/300], Step [127/172], Loss: 15.0173\n",
      "Epoch [76/300], Step [128/172], Loss: 15.4060\n",
      "Epoch [76/300], Step [129/172], Loss: 10.8894\n",
      "Epoch [76/300], Step [130/172], Loss: 12.9974\n",
      "Epoch [76/300], Step [131/172], Loss: 11.0925\n",
      "Epoch [76/300], Step [132/172], Loss: 10.9154\n",
      "Epoch [76/300], Step [133/172], Loss: 12.0253\n",
      "Epoch [76/300], Step [134/172], Loss: 13.1071\n",
      "Epoch [76/300], Step [135/172], Loss: 10.2391\n",
      "Epoch [76/300], Step [136/172], Loss: 10.0106\n",
      "Epoch [76/300], Step [137/172], Loss: 11.9395\n",
      "Epoch [76/300], Step [138/172], Loss: 10.1284\n",
      "Epoch [76/300], Step [139/172], Loss: 11.3595\n",
      "Epoch [76/300], Step [140/172], Loss: 11.3729\n",
      "Epoch [76/300], Step [141/172], Loss: 14.1920\n",
      "Epoch [76/300], Step [142/172], Loss: 15.0137\n",
      "Epoch [76/300], Step [143/172], Loss: 10.4567\n",
      "Epoch [76/300], Step [144/172], Loss: 10.2082\n",
      "Epoch [76/300], Step [145/172], Loss: 10.9111\n",
      "Epoch [76/300], Step [146/172], Loss: 11.1358\n",
      "Epoch [76/300], Step [147/172], Loss: 6.8789\n",
      "Epoch [76/300], Step [148/172], Loss: 7.9761\n",
      "Epoch [76/300], Step [149/172], Loss: 9.8861\n",
      "Epoch [76/300], Step [150/172], Loss: 9.6251\n",
      "Epoch [76/300], Step [151/172], Loss: 8.6083\n",
      "Epoch [76/300], Step [152/172], Loss: 8.7902\n",
      "Epoch [76/300], Step [153/172], Loss: 8.4973\n",
      "Epoch [76/300], Step [154/172], Loss: 9.2442\n",
      "Epoch [76/300], Step [155/172], Loss: 8.3553\n",
      "Epoch [76/300], Step [156/172], Loss: 12.4514\n",
      "Epoch [76/300], Step [157/172], Loss: 11.3506\n",
      "Epoch [76/300], Step [158/172], Loss: 9.1241\n",
      "Epoch [76/300], Step [159/172], Loss: 10.2797\n",
      "Epoch [76/300], Step [160/172], Loss: 10.4789\n",
      "Epoch [76/300], Step [161/172], Loss: 8.0328\n",
      "Epoch [76/300], Step [162/172], Loss: 8.5447\n",
      "Epoch [76/300], Step [163/172], Loss: 7.6383\n",
      "Epoch [76/300], Step [164/172], Loss: 10.7129\n",
      "Epoch [76/300], Step [165/172], Loss: 7.4241\n",
      "Epoch [76/300], Step [166/172], Loss: 7.8103\n",
      "Epoch [76/300], Step [167/172], Loss: 8.9788\n",
      "Epoch [76/300], Step [168/172], Loss: 7.5936\n",
      "Epoch [76/300], Step [169/172], Loss: 7.9293\n",
      "Epoch [76/300], Step [170/172], Loss: 6.8363\n",
      "Epoch [76/300], Step [171/172], Loss: 6.6819\n",
      "Epoch [76/300], Step [172/172], Loss: 6.1251\n",
      "Epoch [77/300], Step [1/172], Loss: 91.4769\n",
      "Epoch [77/300], Step [2/172], Loss: 92.1172\n",
      "Epoch [77/300], Step [3/172], Loss: 88.6160\n",
      "Epoch [77/300], Step [4/172], Loss: 54.5716\n",
      "Epoch [77/300], Step [5/172], Loss: 76.2038\n",
      "Epoch [77/300], Step [6/172], Loss: 24.1104\n",
      "Epoch [77/300], Step [7/172], Loss: 30.1299\n",
      "Epoch [77/300], Step [8/172], Loss: 8.0821\n",
      "Epoch [77/300], Step [9/172], Loss: 47.5522\n",
      "Epoch [77/300], Step [10/172], Loss: 52.2117\n",
      "Epoch [77/300], Step [11/172], Loss: 95.7327\n",
      "Epoch [77/300], Step [12/172], Loss: 85.2953\n",
      "Epoch [77/300], Step [13/172], Loss: 42.5021\n",
      "Epoch [77/300], Step [14/172], Loss: 95.5575\n",
      "Epoch [77/300], Step [15/172], Loss: 82.6083\n",
      "Epoch [77/300], Step [16/172], Loss: 36.2724\n",
      "Epoch [77/300], Step [17/172], Loss: 59.9650\n",
      "Epoch [77/300], Step [18/172], Loss: 63.9535\n",
      "Epoch [77/300], Step [19/172], Loss: 82.5770\n",
      "Epoch [77/300], Step [20/172], Loss: 86.7561\n",
      "Epoch [77/300], Step [21/172], Loss: 96.3935\n",
      "Epoch [77/300], Step [22/172], Loss: 86.0915\n",
      "Epoch [77/300], Step [23/172], Loss: 8.1811\n",
      "Epoch [77/300], Step [24/172], Loss: 75.4170\n",
      "Epoch [77/300], Step [25/172], Loss: 49.9736\n",
      "Epoch [77/300], Step [26/172], Loss: 62.1593\n",
      "Epoch [77/300], Step [27/172], Loss: 84.1158\n",
      "Epoch [77/300], Step [28/172], Loss: 46.2212\n",
      "Epoch [77/300], Step [29/172], Loss: 39.2375\n",
      "Epoch [77/300], Step [30/172], Loss: 80.3799\n",
      "Epoch [77/300], Step [31/172], Loss: 45.2267\n",
      "Epoch [77/300], Step [32/172], Loss: 40.8369\n",
      "Epoch [77/300], Step [33/172], Loss: 73.2182\n",
      "Epoch [77/300], Step [34/172], Loss: 5.8031\n",
      "Epoch [77/300], Step [35/172], Loss: 27.1829\n",
      "Epoch [77/300], Step [36/172], Loss: 23.4633\n",
      "Epoch [77/300], Step [37/172], Loss: 18.6403\n",
      "Epoch [77/300], Step [38/172], Loss: 27.8354\n",
      "Epoch [77/300], Step [39/172], Loss: 52.2452\n",
      "Epoch [77/300], Step [40/172], Loss: 24.0913\n",
      "Epoch [77/300], Step [41/172], Loss: 40.6933\n",
      "Epoch [77/300], Step [42/172], Loss: 43.9723\n",
      "Epoch [77/300], Step [43/172], Loss: 28.0102\n",
      "Epoch [77/300], Step [44/172], Loss: 24.3658\n",
      "Epoch [77/300], Step [45/172], Loss: 22.2812\n",
      "Epoch [77/300], Step [46/172], Loss: 29.7888\n",
      "Epoch [77/300], Step [47/172], Loss: 55.3504\n",
      "Epoch [77/300], Step [48/172], Loss: 57.1889\n",
      "Epoch [77/300], Step [49/172], Loss: 21.4627\n",
      "Epoch [77/300], Step [50/172], Loss: 52.4208\n",
      "Epoch [77/300], Step [51/172], Loss: 7.7219\n",
      "Epoch [77/300], Step [52/172], Loss: 20.3927\n",
      "Epoch [77/300], Step [53/172], Loss: 26.6982\n",
      "Epoch [77/300], Step [54/172], Loss: 13.3545\n",
      "Epoch [77/300], Step [55/172], Loss: 13.6081\n",
      "Epoch [77/300], Step [56/172], Loss: 10.4825\n",
      "Epoch [77/300], Step [57/172], Loss: 28.7060\n",
      "Epoch [77/300], Step [58/172], Loss: 20.1949\n",
      "Epoch [77/300], Step [59/172], Loss: 33.4865\n",
      "Epoch [77/300], Step [60/172], Loss: 53.0011\n",
      "Epoch [77/300], Step [61/172], Loss: 10.5467\n",
      "Epoch [77/300], Step [62/172], Loss: 23.9841\n",
      "Epoch [77/300], Step [63/172], Loss: 8.9429\n",
      "Epoch [77/300], Step [64/172], Loss: 7.7078\n",
      "Epoch [77/300], Step [65/172], Loss: 23.2709\n",
      "Epoch [77/300], Step [66/172], Loss: 6.4670\n",
      "Epoch [77/300], Step [67/172], Loss: 28.3954\n",
      "Epoch [77/300], Step [68/172], Loss: 8.4155\n",
      "Epoch [77/300], Step [69/172], Loss: 71.5464\n",
      "Epoch [77/300], Step [70/172], Loss: 63.0550\n",
      "Epoch [77/300], Step [71/172], Loss: 56.9242\n",
      "Epoch [77/300], Step [72/172], Loss: 61.7324\n",
      "Epoch [77/300], Step [73/172], Loss: 66.7013\n",
      "Epoch [77/300], Step [74/172], Loss: 39.2393\n",
      "Epoch [77/300], Step [75/172], Loss: 33.6830\n",
      "Epoch [77/300], Step [76/172], Loss: 42.0967\n",
      "Epoch [77/300], Step [77/172], Loss: 62.5156\n",
      "Epoch [77/300], Step [78/172], Loss: 53.5760\n",
      "Epoch [77/300], Step [79/172], Loss: 51.1894\n",
      "Epoch [77/300], Step [80/172], Loss: 57.8385\n",
      "Epoch [77/300], Step [81/172], Loss: 45.2576\n",
      "Epoch [77/300], Step [82/172], Loss: 41.8859\n",
      "Epoch [77/300], Step [83/172], Loss: 52.1073\n",
      "Epoch [77/300], Step [84/172], Loss: 40.6904\n",
      "Epoch [77/300], Step [85/172], Loss: 44.7008\n",
      "Epoch [77/300], Step [86/172], Loss: 36.3489\n",
      "Epoch [77/300], Step [87/172], Loss: 30.3326\n",
      "Epoch [77/300], Step [88/172], Loss: 32.3131\n",
      "Epoch [77/300], Step [89/172], Loss: 29.2991\n",
      "Epoch [77/300], Step [90/172], Loss: 28.3501\n",
      "Epoch [77/300], Step [91/172], Loss: 31.1013\n",
      "Epoch [77/300], Step [92/172], Loss: 23.8010\n",
      "Epoch [77/300], Step [93/172], Loss: 23.4775\n",
      "Epoch [77/300], Step [94/172], Loss: 30.5455\n",
      "Epoch [77/300], Step [95/172], Loss: 25.0460\n",
      "Epoch [77/300], Step [96/172], Loss: 20.5125\n",
      "Epoch [77/300], Step [97/172], Loss: 27.4235\n",
      "Epoch [77/300], Step [98/172], Loss: 21.9938\n",
      "Epoch [77/300], Step [99/172], Loss: 19.5403\n",
      "Epoch [77/300], Step [100/172], Loss: 18.1753\n",
      "Epoch [77/300], Step [101/172], Loss: 19.7144\n",
      "Epoch [77/300], Step [102/172], Loss: 18.0989\n",
      "Epoch [77/300], Step [103/172], Loss: 16.6712\n",
      "Epoch [77/300], Step [104/172], Loss: 17.3776\n",
      "Epoch [77/300], Step [105/172], Loss: 19.5065\n",
      "Epoch [77/300], Step [106/172], Loss: 18.6433\n",
      "Epoch [77/300], Step [107/172], Loss: 16.4631\n",
      "Epoch [77/300], Step [108/172], Loss: 18.8370\n",
      "Epoch [77/300], Step [109/172], Loss: 20.4273\n",
      "Epoch [77/300], Step [110/172], Loss: 17.7591\n",
      "Epoch [77/300], Step [111/172], Loss: 16.2716\n",
      "Epoch [77/300], Step [112/172], Loss: 21.0677\n",
      "Epoch [77/300], Step [113/172], Loss: 17.1425\n",
      "Epoch [77/300], Step [114/172], Loss: 16.5475\n",
      "Epoch [77/300], Step [115/172], Loss: 24.3325\n",
      "Epoch [77/300], Step [116/172], Loss: 16.8306\n",
      "Epoch [77/300], Step [117/172], Loss: 14.1634\n",
      "Epoch [77/300], Step [118/172], Loss: 17.4903\n",
      "Epoch [77/300], Step [119/172], Loss: 16.4247\n",
      "Epoch [77/300], Step [120/172], Loss: 13.0405\n",
      "Epoch [77/300], Step [121/172], Loss: 13.2727\n",
      "Epoch [77/300], Step [122/172], Loss: 12.6036\n",
      "Epoch [77/300], Step [123/172], Loss: 12.1079\n",
      "Epoch [77/300], Step [124/172], Loss: 9.8837\n",
      "Epoch [77/300], Step [125/172], Loss: 14.5618\n",
      "Epoch [77/300], Step [126/172], Loss: 12.5439\n",
      "Epoch [77/300], Step [127/172], Loss: 14.8816\n",
      "Epoch [77/300], Step [128/172], Loss: 15.2578\n",
      "Epoch [77/300], Step [129/172], Loss: 10.7960\n",
      "Epoch [77/300], Step [130/172], Loss: 12.9254\n",
      "Epoch [77/300], Step [131/172], Loss: 11.0001\n",
      "Epoch [77/300], Step [132/172], Loss: 10.8201\n",
      "Epoch [77/300], Step [133/172], Loss: 12.0062\n",
      "Epoch [77/300], Step [134/172], Loss: 13.0831\n",
      "Epoch [77/300], Step [135/172], Loss: 10.1899\n",
      "Epoch [77/300], Step [136/172], Loss: 9.9323\n",
      "Epoch [77/300], Step [137/172], Loss: 11.8506\n",
      "Epoch [77/300], Step [138/172], Loss: 10.0590\n",
      "Epoch [77/300], Step [139/172], Loss: 11.2775\n",
      "Epoch [77/300], Step [140/172], Loss: 11.2846\n",
      "Epoch [77/300], Step [141/172], Loss: 14.1237\n",
      "Epoch [77/300], Step [142/172], Loss: 15.0021\n",
      "Epoch [77/300], Step [143/172], Loss: 10.4121\n",
      "Epoch [77/300], Step [144/172], Loss: 10.1894\n",
      "Epoch [77/300], Step [145/172], Loss: 10.8798\n",
      "Epoch [77/300], Step [146/172], Loss: 11.1006\n",
      "Epoch [77/300], Step [147/172], Loss: 6.8218\n",
      "Epoch [77/300], Step [148/172], Loss: 7.9237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/300], Step [149/172], Loss: 9.8042\n",
      "Epoch [77/300], Step [150/172], Loss: 9.5619\n",
      "Epoch [77/300], Step [151/172], Loss: 8.5955\n",
      "Epoch [77/300], Step [152/172], Loss: 8.7442\n",
      "Epoch [77/300], Step [153/172], Loss: 8.4411\n",
      "Epoch [77/300], Step [154/172], Loss: 9.1889\n",
      "Epoch [77/300], Step [155/172], Loss: 8.3051\n",
      "Epoch [77/300], Step [156/172], Loss: 12.5038\n",
      "Epoch [77/300], Step [157/172], Loss: 11.3342\n",
      "Epoch [77/300], Step [158/172], Loss: 9.0813\n",
      "Epoch [77/300], Step [159/172], Loss: 10.2584\n",
      "Epoch [77/300], Step [160/172], Loss: 10.4982\n",
      "Epoch [77/300], Step [161/172], Loss: 8.0137\n",
      "Epoch [77/300], Step [162/172], Loss: 8.4983\n",
      "Epoch [77/300], Step [163/172], Loss: 7.6024\n",
      "Epoch [77/300], Step [164/172], Loss: 10.7349\n",
      "Epoch [77/300], Step [165/172], Loss: 7.3779\n",
      "Epoch [77/300], Step [166/172], Loss: 7.8114\n",
      "Epoch [77/300], Step [167/172], Loss: 8.9895\n",
      "Epoch [77/300], Step [168/172], Loss: 7.5824\n",
      "Epoch [77/300], Step [169/172], Loss: 7.9245\n",
      "Epoch [77/300], Step [170/172], Loss: 6.8037\n",
      "Epoch [77/300], Step [171/172], Loss: 6.6800\n",
      "Epoch [77/300], Step [172/172], Loss: 6.1082\n",
      "Epoch [78/300], Step [1/172], Loss: 91.1434\n",
      "Epoch [78/300], Step [2/172], Loss: 91.7879\n",
      "Epoch [78/300], Step [3/172], Loss: 87.8809\n",
      "Epoch [78/300], Step [4/172], Loss: 54.0996\n",
      "Epoch [78/300], Step [5/172], Loss: 75.7241\n",
      "Epoch [78/300], Step [6/172], Loss: 23.8132\n",
      "Epoch [78/300], Step [7/172], Loss: 29.5485\n",
      "Epoch [78/300], Step [8/172], Loss: 7.9397\n",
      "Epoch [78/300], Step [9/172], Loss: 47.3245\n",
      "Epoch [78/300], Step [10/172], Loss: 51.8387\n",
      "Epoch [78/300], Step [11/172], Loss: 95.1966\n",
      "Epoch [78/300], Step [12/172], Loss: 85.0791\n",
      "Epoch [78/300], Step [13/172], Loss: 42.3197\n",
      "Epoch [78/300], Step [14/172], Loss: 95.0222\n",
      "Epoch [78/300], Step [15/172], Loss: 82.0085\n",
      "Epoch [78/300], Step [16/172], Loss: 35.0784\n",
      "Epoch [78/300], Step [17/172], Loss: 59.6289\n",
      "Epoch [78/300], Step [18/172], Loss: 63.5913\n",
      "Epoch [78/300], Step [19/172], Loss: 82.3179\n",
      "Epoch [78/300], Step [20/172], Loss: 85.8753\n",
      "Epoch [78/300], Step [21/172], Loss: 95.8855\n",
      "Epoch [78/300], Step [22/172], Loss: 85.3916\n",
      "Epoch [78/300], Step [23/172], Loss: 8.1127\n",
      "Epoch [78/300], Step [24/172], Loss: 75.1767\n",
      "Epoch [78/300], Step [25/172], Loss: 49.6613\n",
      "Epoch [78/300], Step [26/172], Loss: 61.9561\n",
      "Epoch [78/300], Step [27/172], Loss: 83.4885\n",
      "Epoch [78/300], Step [28/172], Loss: 45.7999\n",
      "Epoch [78/300], Step [29/172], Loss: 38.4165\n",
      "Epoch [78/300], Step [30/172], Loss: 80.4788\n",
      "Epoch [78/300], Step [31/172], Loss: 45.2401\n",
      "Epoch [78/300], Step [32/172], Loss: 40.8445\n",
      "Epoch [78/300], Step [33/172], Loss: 73.1029\n",
      "Epoch [78/300], Step [34/172], Loss: 5.9882\n",
      "Epoch [78/300], Step [35/172], Loss: 26.3721\n",
      "Epoch [78/300], Step [36/172], Loss: 23.4176\n",
      "Epoch [78/300], Step [37/172], Loss: 18.7012\n",
      "Epoch [78/300], Step [38/172], Loss: 27.9480\n",
      "Epoch [78/300], Step [39/172], Loss: 51.9533\n",
      "Epoch [78/300], Step [40/172], Loss: 23.9771\n",
      "Epoch [78/300], Step [41/172], Loss: 40.5172\n",
      "Epoch [78/300], Step [42/172], Loss: 44.0814\n",
      "Epoch [78/300], Step [43/172], Loss: 27.9906\n",
      "Epoch [78/300], Step [44/172], Loss: 24.2178\n",
      "Epoch [78/300], Step [45/172], Loss: 22.3124\n",
      "Epoch [78/300], Step [46/172], Loss: 29.3971\n",
      "Epoch [78/300], Step [47/172], Loss: 55.0678\n",
      "Epoch [78/300], Step [48/172], Loss: 56.8752\n",
      "Epoch [78/300], Step [49/172], Loss: 21.3554\n",
      "Epoch [78/300], Step [50/172], Loss: 52.4619\n",
      "Epoch [78/300], Step [51/172], Loss: 7.6943\n",
      "Epoch [78/300], Step [52/172], Loss: 20.3795\n",
      "Epoch [78/300], Step [53/172], Loss: 26.5593\n",
      "Epoch [78/300], Step [54/172], Loss: 13.2739\n",
      "Epoch [78/300], Step [55/172], Loss: 13.7159\n",
      "Epoch [78/300], Step [56/172], Loss: 10.4805\n",
      "Epoch [78/300], Step [57/172], Loss: 28.2702\n",
      "Epoch [78/300], Step [58/172], Loss: 19.9946\n",
      "Epoch [78/300], Step [59/172], Loss: 33.2529\n",
      "Epoch [78/300], Step [60/172], Loss: 52.1623\n",
      "Epoch [78/300], Step [61/172], Loss: 10.4449\n",
      "Epoch [78/300], Step [62/172], Loss: 24.2222\n",
      "Epoch [78/300], Step [63/172], Loss: 8.9467\n",
      "Epoch [78/300], Step [64/172], Loss: 7.7065\n",
      "Epoch [78/300], Step [65/172], Loss: 23.0601\n",
      "Epoch [78/300], Step [66/172], Loss: 6.4460\n",
      "Epoch [78/300], Step [67/172], Loss: 28.3958\n",
      "Epoch [78/300], Step [68/172], Loss: 8.3351\n",
      "Epoch [78/300], Step [69/172], Loss: 71.1457\n",
      "Epoch [78/300], Step [70/172], Loss: 62.7016\n",
      "Epoch [78/300], Step [71/172], Loss: 56.7935\n",
      "Epoch [78/300], Step [72/172], Loss: 61.6495\n",
      "Epoch [78/300], Step [73/172], Loss: 66.3504\n",
      "Epoch [78/300], Step [74/172], Loss: 39.1489\n",
      "Epoch [78/300], Step [75/172], Loss: 33.7444\n",
      "Epoch [78/300], Step [76/172], Loss: 41.8682\n",
      "Epoch [78/300], Step [77/172], Loss: 62.5432\n",
      "Epoch [78/300], Step [78/172], Loss: 53.5202\n",
      "Epoch [78/300], Step [79/172], Loss: 51.2263\n",
      "Epoch [78/300], Step [80/172], Loss: 58.0995\n",
      "Epoch [78/300], Step [81/172], Loss: 45.2666\n",
      "Epoch [78/300], Step [82/172], Loss: 41.9424\n",
      "Epoch [78/300], Step [83/172], Loss: 52.1645\n",
      "Epoch [78/300], Step [84/172], Loss: 40.7238\n",
      "Epoch [78/300], Step [85/172], Loss: 44.7616\n",
      "Epoch [78/300], Step [86/172], Loss: 36.3106\n",
      "Epoch [78/300], Step [87/172], Loss: 30.3292\n",
      "Epoch [78/300], Step [88/172], Loss: 32.3115\n",
      "Epoch [78/300], Step [89/172], Loss: 29.2676\n",
      "Epoch [78/300], Step [90/172], Loss: 28.2245\n",
      "Epoch [78/300], Step [91/172], Loss: 31.1840\n",
      "Epoch [78/300], Step [92/172], Loss: 23.7317\n",
      "Epoch [78/300], Step [93/172], Loss: 23.4276\n",
      "Epoch [78/300], Step [94/172], Loss: 30.5520\n",
      "Epoch [78/300], Step [95/172], Loss: 25.0279\n",
      "Epoch [78/300], Step [96/172], Loss: 20.5015\n",
      "Epoch [78/300], Step [97/172], Loss: 27.3906\n",
      "Epoch [78/300], Step [98/172], Loss: 21.9809\n",
      "Epoch [78/300], Step [99/172], Loss: 19.5102\n",
      "Epoch [78/300], Step [100/172], Loss: 18.1358\n",
      "Epoch [78/300], Step [101/172], Loss: 19.6762\n",
      "Epoch [78/300], Step [102/172], Loss: 18.0095\n",
      "Epoch [78/300], Step [103/172], Loss: 16.5746\n",
      "Epoch [78/300], Step [104/172], Loss: 17.3472\n",
      "Epoch [78/300], Step [105/172], Loss: 19.5146\n",
      "Epoch [78/300], Step [106/172], Loss: 18.6126\n",
      "Epoch [78/300], Step [107/172], Loss: 16.4298\n",
      "Epoch [78/300], Step [108/172], Loss: 18.7122\n",
      "Epoch [78/300], Step [109/172], Loss: 20.2783\n",
      "Epoch [78/300], Step [110/172], Loss: 17.6885\n",
      "Epoch [78/300], Step [111/172], Loss: 16.2360\n",
      "Epoch [78/300], Step [112/172], Loss: 20.9914\n",
      "Epoch [78/300], Step [113/172], Loss: 17.0369\n",
      "Epoch [78/300], Step [114/172], Loss: 16.4551\n",
      "Epoch [78/300], Step [115/172], Loss: 24.1971\n",
      "Epoch [78/300], Step [116/172], Loss: 16.7715\n",
      "Epoch [78/300], Step [117/172], Loss: 14.1296\n",
      "Epoch [78/300], Step [118/172], Loss: 17.3614\n",
      "Epoch [78/300], Step [119/172], Loss: 16.4366\n",
      "Epoch [78/300], Step [120/172], Loss: 12.9681\n",
      "Epoch [78/300], Step [121/172], Loss: 13.2291\n",
      "Epoch [78/300], Step [122/172], Loss: 12.4268\n",
      "Epoch [78/300], Step [123/172], Loss: 12.0846\n",
      "Epoch [78/300], Step [124/172], Loss: 9.8439\n",
      "Epoch [78/300], Step [125/172], Loss: 14.5355\n",
      "Epoch [78/300], Step [126/172], Loss: 12.4892\n",
      "Epoch [78/300], Step [127/172], Loss: 14.7820\n",
      "Epoch [78/300], Step [128/172], Loss: 15.1823\n",
      "Epoch [78/300], Step [129/172], Loss: 10.7497\n",
      "Epoch [78/300], Step [130/172], Loss: 12.8892\n",
      "Epoch [78/300], Step [131/172], Loss: 10.9073\n",
      "Epoch [78/300], Step [132/172], Loss: 10.7723\n",
      "Epoch [78/300], Step [133/172], Loss: 11.8802\n",
      "Epoch [78/300], Step [134/172], Loss: 13.0652\n",
      "Epoch [78/300], Step [135/172], Loss: 10.1729\n",
      "Epoch [78/300], Step [136/172], Loss: 9.9142\n",
      "Epoch [78/300], Step [137/172], Loss: 11.8363\n",
      "Epoch [78/300], Step [138/172], Loss: 10.0312\n",
      "Epoch [78/300], Step [139/172], Loss: 11.2610\n",
      "Epoch [78/300], Step [140/172], Loss: 11.2590\n",
      "Epoch [78/300], Step [141/172], Loss: 14.0554\n",
      "Epoch [78/300], Step [142/172], Loss: 14.9768\n",
      "Epoch [78/300], Step [143/172], Loss: 10.3837\n",
      "Epoch [78/300], Step [144/172], Loss: 10.1372\n",
      "Epoch [78/300], Step [145/172], Loss: 10.8867\n",
      "Epoch [78/300], Step [146/172], Loss: 11.0926\n",
      "Epoch [78/300], Step [147/172], Loss: 6.7909\n",
      "Epoch [78/300], Step [148/172], Loss: 7.8980\n",
      "Epoch [78/300], Step [149/172], Loss: 9.7689\n",
      "Epoch [78/300], Step [150/172], Loss: 9.5316\n",
      "Epoch [78/300], Step [151/172], Loss: 8.5405\n",
      "Epoch [78/300], Step [152/172], Loss: 8.7298\n",
      "Epoch [78/300], Step [153/172], Loss: 8.4195\n",
      "Epoch [78/300], Step [154/172], Loss: 9.1419\n",
      "Epoch [78/300], Step [155/172], Loss: 8.2786\n",
      "Epoch [78/300], Step [156/172], Loss: 12.5046\n",
      "Epoch [78/300], Step [157/172], Loss: 11.2829\n",
      "Epoch [78/300], Step [158/172], Loss: 9.0363\n",
      "Epoch [78/300], Step [159/172], Loss: 10.2750\n",
      "Epoch [78/300], Step [160/172], Loss: 10.4826\n",
      "Epoch [78/300], Step [161/172], Loss: 8.0016\n",
      "Epoch [78/300], Step [162/172], Loss: 8.4835\n",
      "Epoch [78/300], Step [163/172], Loss: 7.5702\n",
      "Epoch [78/300], Step [164/172], Loss: 10.7406\n",
      "Epoch [78/300], Step [165/172], Loss: 7.3633\n",
      "Epoch [78/300], Step [166/172], Loss: 7.7888\n",
      "Epoch [78/300], Step [167/172], Loss: 9.0245\n",
      "Epoch [78/300], Step [168/172], Loss: 7.6015\n",
      "Epoch [78/300], Step [169/172], Loss: 7.9426\n",
      "Epoch [78/300], Step [170/172], Loss: 6.7886\n",
      "Epoch [78/300], Step [171/172], Loss: 6.7043\n",
      "Epoch [78/300], Step [172/172], Loss: 6.1135\n",
      "Epoch [79/300], Step [1/172], Loss: 90.7359\n",
      "Epoch [79/300], Step [2/172], Loss: 91.1442\n",
      "Epoch [79/300], Step [3/172], Loss: 87.1288\n",
      "Epoch [79/300], Step [4/172], Loss: 53.7974\n",
      "Epoch [79/300], Step [5/172], Loss: 75.0530\n",
      "Epoch [79/300], Step [6/172], Loss: 23.6764\n",
      "Epoch [79/300], Step [7/172], Loss: 29.6649\n",
      "Epoch [79/300], Step [8/172], Loss: 7.7035\n",
      "Epoch [79/300], Step [9/172], Loss: 47.0721\n",
      "Epoch [79/300], Step [10/172], Loss: 51.5937\n",
      "Epoch [79/300], Step [11/172], Loss: 94.7783\n",
      "Epoch [79/300], Step [12/172], Loss: 85.0106\n",
      "Epoch [79/300], Step [13/172], Loss: 42.3499\n",
      "Epoch [79/300], Step [14/172], Loss: 94.8331\n",
      "Epoch [79/300], Step [15/172], Loss: 81.7248\n",
      "Epoch [79/300], Step [16/172], Loss: 34.2832\n",
      "Epoch [79/300], Step [17/172], Loss: 59.6451\n",
      "Epoch [79/300], Step [18/172], Loss: 63.5087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/300], Step [19/172], Loss: 82.4571\n",
      "Epoch [79/300], Step [20/172], Loss: 85.1968\n",
      "Epoch [79/300], Step [21/172], Loss: 95.9379\n",
      "Epoch [79/300], Step [22/172], Loss: 85.0713\n",
      "Epoch [79/300], Step [23/172], Loss: 7.6897\n",
      "Epoch [79/300], Step [24/172], Loss: 75.0251\n",
      "Epoch [79/300], Step [25/172], Loss: 49.6375\n",
      "Epoch [79/300], Step [26/172], Loss: 62.0633\n",
      "Epoch [79/300], Step [27/172], Loss: 83.3666\n",
      "Epoch [79/300], Step [28/172], Loss: 45.4869\n",
      "Epoch [79/300], Step [29/172], Loss: 37.5854\n",
      "Epoch [79/300], Step [30/172], Loss: 80.7342\n",
      "Epoch [79/300], Step [31/172], Loss: 45.2473\n",
      "Epoch [79/300], Step [32/172], Loss: 40.8814\n",
      "Epoch [79/300], Step [33/172], Loss: 73.1996\n",
      "Epoch [79/300], Step [34/172], Loss: 5.8195\n",
      "Epoch [79/300], Step [35/172], Loss: 25.6686\n",
      "Epoch [79/300], Step [36/172], Loss: 23.1572\n",
      "Epoch [79/300], Step [37/172], Loss: 18.6582\n",
      "Epoch [79/300], Step [38/172], Loss: 27.9354\n",
      "Epoch [79/300], Step [39/172], Loss: 51.9184\n",
      "Epoch [79/300], Step [40/172], Loss: 23.7770\n",
      "Epoch [79/300], Step [41/172], Loss: 40.4170\n",
      "Epoch [79/300], Step [42/172], Loss: 43.9835\n",
      "Epoch [79/300], Step [43/172], Loss: 27.8618\n",
      "Epoch [79/300], Step [44/172], Loss: 24.1096\n",
      "Epoch [79/300], Step [45/172], Loss: 22.2990\n",
      "Epoch [79/300], Step [46/172], Loss: 29.1241\n",
      "Epoch [79/300], Step [47/172], Loss: 54.9166\n",
      "Epoch [79/300], Step [48/172], Loss: 56.7713\n",
      "Epoch [79/300], Step [49/172], Loss: 21.2972\n",
      "Epoch [79/300], Step [50/172], Loss: 52.5231\n",
      "Epoch [79/300], Step [51/172], Loss: 7.6538\n",
      "Epoch [79/300], Step [52/172], Loss: 20.2444\n",
      "Epoch [79/300], Step [53/172], Loss: 26.4579\n",
      "Epoch [79/300], Step [54/172], Loss: 13.0874\n",
      "Epoch [79/300], Step [55/172], Loss: 13.6300\n",
      "Epoch [79/300], Step [56/172], Loss: 10.4398\n",
      "Epoch [79/300], Step [57/172], Loss: 27.8588\n",
      "Epoch [79/300], Step [58/172], Loss: 20.0380\n",
      "Epoch [79/300], Step [59/172], Loss: 33.0723\n",
      "Epoch [79/300], Step [60/172], Loss: 52.2989\n",
      "Epoch [79/300], Step [61/172], Loss: 10.4363\n",
      "Epoch [79/300], Step [62/172], Loss: 24.3053\n",
      "Epoch [79/300], Step [63/172], Loss: 8.9641\n",
      "Epoch [79/300], Step [64/172], Loss: 7.7539\n",
      "Epoch [79/300], Step [65/172], Loss: 22.9473\n",
      "Epoch [79/300], Step [66/172], Loss: 6.4132\n",
      "Epoch [79/300], Step [67/172], Loss: 28.4308\n",
      "Epoch [79/300], Step [68/172], Loss: 8.2416\n",
      "Epoch [79/300], Step [69/172], Loss: 70.8351\n",
      "Epoch [79/300], Step [70/172], Loss: 62.3411\n",
      "Epoch [79/300], Step [71/172], Loss: 56.5229\n",
      "Epoch [79/300], Step [72/172], Loss: 61.4212\n",
      "Epoch [79/300], Step [73/172], Loss: 66.0994\n",
      "Epoch [79/300], Step [74/172], Loss: 38.8659\n",
      "Epoch [79/300], Step [75/172], Loss: 33.6533\n",
      "Epoch [79/300], Step [76/172], Loss: 41.6205\n",
      "Epoch [79/300], Step [77/172], Loss: 62.4221\n",
      "Epoch [79/300], Step [78/172], Loss: 53.1923\n",
      "Epoch [79/300], Step [79/172], Loss: 51.0146\n",
      "Epoch [79/300], Step [80/172], Loss: 57.8688\n",
      "Epoch [79/300], Step [81/172], Loss: 44.9893\n",
      "Epoch [79/300], Step [82/172], Loss: 41.5387\n",
      "Epoch [79/300], Step [83/172], Loss: 52.0375\n",
      "Epoch [79/300], Step [84/172], Loss: 40.4771\n",
      "Epoch [79/300], Step [85/172], Loss: 44.5134\n",
      "Epoch [79/300], Step [86/172], Loss: 36.1023\n",
      "Epoch [79/300], Step [87/172], Loss: 30.2176\n",
      "Epoch [79/300], Step [88/172], Loss: 32.1759\n",
      "Epoch [79/300], Step [89/172], Loss: 29.0848\n",
      "Epoch [79/300], Step [90/172], Loss: 28.0676\n",
      "Epoch [79/300], Step [91/172], Loss: 31.1428\n",
      "Epoch [79/300], Step [92/172], Loss: 23.6164\n",
      "Epoch [79/300], Step [93/172], Loss: 23.3109\n",
      "Epoch [79/300], Step [94/172], Loss: 30.5201\n",
      "Epoch [79/300], Step [95/172], Loss: 24.9491\n",
      "Epoch [79/300], Step [96/172], Loss: 20.4197\n",
      "Epoch [79/300], Step [97/172], Loss: 27.3382\n",
      "Epoch [79/300], Step [98/172], Loss: 21.8951\n",
      "Epoch [79/300], Step [99/172], Loss: 19.4494\n",
      "Epoch [79/300], Step [100/172], Loss: 18.0653\n",
      "Epoch [79/300], Step [101/172], Loss: 19.6018\n",
      "Epoch [79/300], Step [102/172], Loss: 17.8501\n",
      "Epoch [79/300], Step [103/172], Loss: 16.4742\n",
      "Epoch [79/300], Step [104/172], Loss: 17.2973\n",
      "Epoch [79/300], Step [105/172], Loss: 19.4291\n",
      "Epoch [79/300], Step [106/172], Loss: 18.5811\n",
      "Epoch [79/300], Step [107/172], Loss: 16.4167\n",
      "Epoch [79/300], Step [108/172], Loss: 18.6292\n",
      "Epoch [79/300], Step [109/172], Loss: 20.1956\n",
      "Epoch [79/300], Step [110/172], Loss: 17.6355\n",
      "Epoch [79/300], Step [111/172], Loss: 16.1777\n",
      "Epoch [79/300], Step [112/172], Loss: 20.9116\n",
      "Epoch [79/300], Step [113/172], Loss: 16.9778\n",
      "Epoch [79/300], Step [114/172], Loss: 16.3695\n",
      "Epoch [79/300], Step [115/172], Loss: 24.1467\n",
      "Epoch [79/300], Step [116/172], Loss: 16.7150\n",
      "Epoch [79/300], Step [117/172], Loss: 14.0765\n",
      "Epoch [79/300], Step [118/172], Loss: 17.3235\n",
      "Epoch [79/300], Step [119/172], Loss: 16.4331\n",
      "Epoch [79/300], Step [120/172], Loss: 12.9183\n",
      "Epoch [79/300], Step [121/172], Loss: 13.1681\n",
      "Epoch [79/300], Step [122/172], Loss: 12.3015\n",
      "Epoch [79/300], Step [123/172], Loss: 12.0176\n",
      "Epoch [79/300], Step [124/172], Loss: 9.7957\n",
      "Epoch [79/300], Step [125/172], Loss: 14.5157\n",
      "Epoch [79/300], Step [126/172], Loss: 12.4333\n",
      "Epoch [79/300], Step [127/172], Loss: 14.6807\n",
      "Epoch [79/300], Step [128/172], Loss: 15.0979\n",
      "Epoch [79/300], Step [129/172], Loss: 10.6999\n",
      "Epoch [79/300], Step [130/172], Loss: 12.8706\n",
      "Epoch [79/300], Step [131/172], Loss: 10.8291\n",
      "Epoch [79/300], Step [132/172], Loss: 10.7223\n",
      "Epoch [79/300], Step [133/172], Loss: 11.8088\n",
      "Epoch [79/300], Step [134/172], Loss: 13.0505\n",
      "Epoch [79/300], Step [135/172], Loss: 10.1533\n",
      "Epoch [79/300], Step [136/172], Loss: 9.8830\n",
      "Epoch [79/300], Step [137/172], Loss: 11.7825\n",
      "Epoch [79/300], Step [138/172], Loss: 9.9733\n",
      "Epoch [79/300], Step [139/172], Loss: 11.2237\n",
      "Epoch [79/300], Step [140/172], Loss: 11.2091\n",
      "Epoch [79/300], Step [141/172], Loss: 14.0019\n",
      "Epoch [79/300], Step [142/172], Loss: 14.9594\n",
      "Epoch [79/300], Step [143/172], Loss: 10.3725\n",
      "Epoch [79/300], Step [144/172], Loss: 10.1020\n",
      "Epoch [79/300], Step [145/172], Loss: 10.8952\n",
      "Epoch [79/300], Step [146/172], Loss: 11.1013\n",
      "Epoch [79/300], Step [147/172], Loss: 6.7506\n",
      "Epoch [79/300], Step [148/172], Loss: 7.8663\n",
      "Epoch [79/300], Step [149/172], Loss: 9.7244\n",
      "Epoch [79/300], Step [150/172], Loss: 9.4954\n",
      "Epoch [79/300], Step [151/172], Loss: 8.5173\n",
      "Epoch [79/300], Step [152/172], Loss: 8.7131\n",
      "Epoch [79/300], Step [153/172], Loss: 8.3893\n",
      "Epoch [79/300], Step [154/172], Loss: 9.1056\n",
      "Epoch [79/300], Step [155/172], Loss: 8.2506\n",
      "Epoch [79/300], Step [156/172], Loss: 12.5269\n",
      "Epoch [79/300], Step [157/172], Loss: 11.2615\n",
      "Epoch [79/300], Step [158/172], Loss: 8.9827\n",
      "Epoch [79/300], Step [159/172], Loss: 10.2913\n",
      "Epoch [79/300], Step [160/172], Loss: 10.4673\n",
      "Epoch [79/300], Step [161/172], Loss: 7.9932\n",
      "Epoch [79/300], Step [162/172], Loss: 8.4767\n",
      "Epoch [79/300], Step [163/172], Loss: 7.5539\n",
      "Epoch [79/300], Step [164/172], Loss: 10.6729\n",
      "Epoch [79/300], Step [165/172], Loss: 7.3335\n",
      "Epoch [79/300], Step [166/172], Loss: 7.7834\n",
      "Epoch [79/300], Step [167/172], Loss: 9.0508\n",
      "Epoch [79/300], Step [168/172], Loss: 7.6164\n",
      "Epoch [79/300], Step [169/172], Loss: 7.9612\n",
      "Epoch [79/300], Step [170/172], Loss: 6.7684\n",
      "Epoch [79/300], Step [171/172], Loss: 6.7128\n",
      "Epoch [79/300], Step [172/172], Loss: 6.1171\n",
      "Epoch [80/300], Step [1/172], Loss: 90.2761\n",
      "Epoch [80/300], Step [2/172], Loss: 90.7712\n",
      "Epoch [80/300], Step [3/172], Loss: 86.2797\n",
      "Epoch [80/300], Step [4/172], Loss: 53.4101\n",
      "Epoch [80/300], Step [5/172], Loss: 74.4312\n",
      "Epoch [80/300], Step [6/172], Loss: 23.5271\n",
      "Epoch [80/300], Step [7/172], Loss: 29.9839\n",
      "Epoch [80/300], Step [8/172], Loss: 8.0161\n",
      "Epoch [80/300], Step [9/172], Loss: 46.9828\n",
      "Epoch [80/300], Step [10/172], Loss: 51.2488\n",
      "Epoch [80/300], Step [11/172], Loss: 94.3374\n",
      "Epoch [80/300], Step [12/172], Loss: 84.9468\n",
      "Epoch [80/300], Step [13/172], Loss: 42.4071\n",
      "Epoch [80/300], Step [14/172], Loss: 94.8304\n",
      "Epoch [80/300], Step [15/172], Loss: 81.5856\n",
      "Epoch [80/300], Step [16/172], Loss: 33.7875\n",
      "Epoch [80/300], Step [17/172], Loss: 59.6665\n",
      "Epoch [80/300], Step [18/172], Loss: 63.5030\n",
      "Epoch [80/300], Step [19/172], Loss: 82.6843\n",
      "Epoch [80/300], Step [20/172], Loss: 84.8272\n",
      "Epoch [80/300], Step [21/172], Loss: 96.1008\n",
      "Epoch [80/300], Step [22/172], Loss: 85.0784\n",
      "Epoch [80/300], Step [23/172], Loss: 7.4200\n",
      "Epoch [80/300], Step [24/172], Loss: 74.9165\n",
      "Epoch [80/300], Step [25/172], Loss: 49.5167\n",
      "Epoch [80/300], Step [26/172], Loss: 62.0830\n",
      "Epoch [80/300], Step [27/172], Loss: 83.4700\n",
      "Epoch [80/300], Step [28/172], Loss: 45.0557\n",
      "Epoch [80/300], Step [29/172], Loss: 36.7911\n",
      "Epoch [80/300], Step [30/172], Loss: 81.1087\n",
      "Epoch [80/300], Step [31/172], Loss: 45.1902\n",
      "Epoch [80/300], Step [32/172], Loss: 40.8716\n",
      "Epoch [80/300], Step [33/172], Loss: 72.9194\n",
      "Epoch [80/300], Step [34/172], Loss: 5.5092\n",
      "Epoch [80/300], Step [35/172], Loss: 25.2336\n",
      "Epoch [80/300], Step [36/172], Loss: 22.9581\n",
      "Epoch [80/300], Step [37/172], Loss: 18.5392\n",
      "Epoch [80/300], Step [38/172], Loss: 28.0197\n",
      "Epoch [80/300], Step [39/172], Loss: 51.8746\n",
      "Epoch [80/300], Step [40/172], Loss: 23.5481\n",
      "Epoch [80/300], Step [41/172], Loss: 40.3681\n",
      "Epoch [80/300], Step [42/172], Loss: 43.7716\n",
      "Epoch [80/300], Step [43/172], Loss: 27.8026\n",
      "Epoch [80/300], Step [44/172], Loss: 23.9229\n",
      "Epoch [80/300], Step [45/172], Loss: 22.1811\n",
      "Epoch [80/300], Step [46/172], Loss: 28.9440\n",
      "Epoch [80/300], Step [47/172], Loss: 54.6457\n",
      "Epoch [80/300], Step [48/172], Loss: 56.4428\n",
      "Epoch [80/300], Step [49/172], Loss: 21.1482\n",
      "Epoch [80/300], Step [50/172], Loss: 52.5394\n",
      "Epoch [80/300], Step [51/172], Loss: 7.6235\n",
      "Epoch [80/300], Step [52/172], Loss: 20.0105\n",
      "Epoch [80/300], Step [53/172], Loss: 26.2460\n",
      "Epoch [80/300], Step [54/172], Loss: 12.9536\n",
      "Epoch [80/300], Step [55/172], Loss: 13.4251\n",
      "Epoch [80/300], Step [56/172], Loss: 10.4286\n",
      "Epoch [80/300], Step [57/172], Loss: 27.5473\n",
      "Epoch [80/300], Step [58/172], Loss: 19.9560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/300], Step [59/172], Loss: 33.0015\n",
      "Epoch [80/300], Step [60/172], Loss: 52.2237\n",
      "Epoch [80/300], Step [61/172], Loss: 10.3497\n",
      "Epoch [80/300], Step [62/172], Loss: 24.2104\n",
      "Epoch [80/300], Step [63/172], Loss: 8.9199\n",
      "Epoch [80/300], Step [64/172], Loss: 7.8038\n",
      "Epoch [80/300], Step [65/172], Loss: 22.7181\n",
      "Epoch [80/300], Step [66/172], Loss: 6.3838\n",
      "Epoch [80/300], Step [67/172], Loss: 28.4944\n",
      "Epoch [80/300], Step [68/172], Loss: 8.1368\n",
      "Epoch [80/300], Step [69/172], Loss: 70.3919\n",
      "Epoch [80/300], Step [70/172], Loss: 61.9924\n",
      "Epoch [80/300], Step [71/172], Loss: 56.1329\n",
      "Epoch [80/300], Step [72/172], Loss: 61.2933\n",
      "Epoch [80/300], Step [73/172], Loss: 66.0328\n",
      "Epoch [80/300], Step [74/172], Loss: 38.6037\n",
      "Epoch [80/300], Step [75/172], Loss: 33.6480\n",
      "Epoch [80/300], Step [76/172], Loss: 41.3946\n",
      "Epoch [80/300], Step [77/172], Loss: 62.3132\n",
      "Epoch [80/300], Step [78/172], Loss: 52.9118\n",
      "Epoch [80/300], Step [79/172], Loss: 50.8503\n",
      "Epoch [80/300], Step [80/172], Loss: 58.0367\n",
      "Epoch [80/300], Step [81/172], Loss: 44.7526\n",
      "Epoch [80/300], Step [82/172], Loss: 41.7137\n",
      "Epoch [80/300], Step [83/172], Loss: 51.9515\n",
      "Epoch [80/300], Step [84/172], Loss: 40.3200\n",
      "Epoch [80/300], Step [85/172], Loss: 44.1883\n",
      "Epoch [80/300], Step [86/172], Loss: 35.9891\n",
      "Epoch [80/300], Step [87/172], Loss: 30.0794\n",
      "Epoch [80/300], Step [88/172], Loss: 32.0234\n",
      "Epoch [80/300], Step [89/172], Loss: 28.9109\n",
      "Epoch [80/300], Step [90/172], Loss: 27.9475\n",
      "Epoch [80/300], Step [91/172], Loss: 31.0312\n",
      "Epoch [80/300], Step [92/172], Loss: 23.4881\n",
      "Epoch [80/300], Step [93/172], Loss: 23.1066\n",
      "Epoch [80/300], Step [94/172], Loss: 30.3861\n",
      "Epoch [80/300], Step [95/172], Loss: 24.7697\n",
      "Epoch [80/300], Step [96/172], Loss: 20.2759\n",
      "Epoch [80/300], Step [97/172], Loss: 27.2085\n",
      "Epoch [80/300], Step [98/172], Loss: 21.7579\n",
      "Epoch [80/300], Step [99/172], Loss: 19.3040\n",
      "Epoch [80/300], Step [100/172], Loss: 17.9293\n",
      "Epoch [80/300], Step [101/172], Loss: 19.4391\n",
      "Epoch [80/300], Step [102/172], Loss: 17.9022\n",
      "Epoch [80/300], Step [103/172], Loss: 16.3394\n",
      "Epoch [80/300], Step [104/172], Loss: 17.1882\n",
      "Epoch [80/300], Step [105/172], Loss: 19.4862\n",
      "Epoch [80/300], Step [106/172], Loss: 18.4774\n",
      "Epoch [80/300], Step [107/172], Loss: 16.3767\n",
      "Epoch [80/300], Step [108/172], Loss: 18.5404\n",
      "Epoch [80/300], Step [109/172], Loss: 20.1709\n",
      "Epoch [80/300], Step [110/172], Loss: 17.5416\n",
      "Epoch [80/300], Step [111/172], Loss: 16.0622\n",
      "Epoch [80/300], Step [112/172], Loss: 20.7533\n",
      "Epoch [80/300], Step [113/172], Loss: 16.9001\n",
      "Epoch [80/300], Step [114/172], Loss: 16.2859\n",
      "Epoch [80/300], Step [115/172], Loss: 24.0761\n",
      "Epoch [80/300], Step [116/172], Loss: 16.5857\n",
      "Epoch [80/300], Step [117/172], Loss: 13.9837\n",
      "Epoch [80/300], Step [118/172], Loss: 17.2266\n",
      "Epoch [80/300], Step [119/172], Loss: 16.3848\n",
      "Epoch [80/300], Step [120/172], Loss: 12.8792\n",
      "Epoch [80/300], Step [121/172], Loss: 13.0605\n",
      "Epoch [80/300], Step [122/172], Loss: 12.2677\n",
      "Epoch [80/300], Step [123/172], Loss: 11.9713\n",
      "Epoch [80/300], Step [124/172], Loss: 9.7207\n",
      "Epoch [80/300], Step [125/172], Loss: 14.4199\n",
      "Epoch [80/300], Step [126/172], Loss: 12.3479\n",
      "Epoch [80/300], Step [127/172], Loss: 14.5796\n",
      "Epoch [80/300], Step [128/172], Loss: 14.9557\n",
      "Epoch [80/300], Step [129/172], Loss: 10.6204\n",
      "Epoch [80/300], Step [130/172], Loss: 12.7953\n",
      "Epoch [80/300], Step [131/172], Loss: 10.7646\n",
      "Epoch [80/300], Step [132/172], Loss: 10.6385\n",
      "Epoch [80/300], Step [133/172], Loss: 11.7917\n",
      "Epoch [80/300], Step [134/172], Loss: 13.0226\n",
      "Epoch [80/300], Step [135/172], Loss: 10.1147\n",
      "Epoch [80/300], Step [136/172], Loss: 9.8433\n",
      "Epoch [80/300], Step [137/172], Loss: 11.7194\n",
      "Epoch [80/300], Step [138/172], Loss: 9.9357\n",
      "Epoch [80/300], Step [139/172], Loss: 11.1706\n",
      "Epoch [80/300], Step [140/172], Loss: 11.1475\n",
      "Epoch [80/300], Step [141/172], Loss: 13.9568\n",
      "Epoch [80/300], Step [142/172], Loss: 14.9721\n",
      "Epoch [80/300], Step [143/172], Loss: 10.3230\n",
      "Epoch [80/300], Step [144/172], Loss: 10.0864\n",
      "Epoch [80/300], Step [145/172], Loss: 10.8796\n",
      "Epoch [80/300], Step [146/172], Loss: 11.0502\n",
      "Epoch [80/300], Step [147/172], Loss: 6.6916\n",
      "Epoch [80/300], Step [148/172], Loss: 7.8121\n",
      "Epoch [80/300], Step [149/172], Loss: 9.6501\n",
      "Epoch [80/300], Step [150/172], Loss: 9.4363\n",
      "Epoch [80/300], Step [151/172], Loss: 8.5021\n",
      "Epoch [80/300], Step [152/172], Loss: 8.6756\n",
      "Epoch [80/300], Step [153/172], Loss: 8.3335\n",
      "Epoch [80/300], Step [154/172], Loss: 9.0530\n",
      "Epoch [80/300], Step [155/172], Loss: 8.2010\n",
      "Epoch [80/300], Step [156/172], Loss: 12.5568\n",
      "Epoch [80/300], Step [157/172], Loss: 11.2378\n",
      "Epoch [80/300], Step [158/172], Loss: 8.9534\n",
      "Epoch [80/300], Step [159/172], Loss: 10.2710\n",
      "Epoch [80/300], Step [160/172], Loss: 10.4799\n",
      "Epoch [80/300], Step [161/172], Loss: 7.9799\n",
      "Epoch [80/300], Step [162/172], Loss: 8.4181\n",
      "Epoch [80/300], Step [163/172], Loss: 7.5029\n",
      "Epoch [80/300], Step [164/172], Loss: 10.7391\n",
      "Epoch [80/300], Step [165/172], Loss: 7.3017\n",
      "Epoch [80/300], Step [166/172], Loss: 7.7744\n",
      "Epoch [80/300], Step [167/172], Loss: 9.0372\n",
      "Epoch [80/300], Step [168/172], Loss: 7.6025\n",
      "Epoch [80/300], Step [169/172], Loss: 7.9348\n",
      "Epoch [80/300], Step [170/172], Loss: 6.7219\n",
      "Epoch [80/300], Step [171/172], Loss: 6.6944\n",
      "Epoch [80/300], Step [172/172], Loss: 6.1046\n",
      "Epoch [81/300], Step [1/172], Loss: 90.0610\n",
      "Epoch [81/300], Step [2/172], Loss: 90.5668\n",
      "Epoch [81/300], Step [3/172], Loss: 85.6448\n",
      "Epoch [81/300], Step [4/172], Loss: 53.0374\n",
      "Epoch [81/300], Step [5/172], Loss: 74.1133\n",
      "Epoch [81/300], Step [6/172], Loss: 23.2258\n",
      "Epoch [81/300], Step [7/172], Loss: 29.3661\n",
      "Epoch [81/300], Step [8/172], Loss: 7.4911\n",
      "Epoch [81/300], Step [9/172], Loss: 46.7154\n",
      "Epoch [81/300], Step [10/172], Loss: 50.9505\n",
      "Epoch [81/300], Step [11/172], Loss: 93.7717\n",
      "Epoch [81/300], Step [12/172], Loss: 84.8045\n",
      "Epoch [81/300], Step [13/172], Loss: 42.2011\n",
      "Epoch [81/300], Step [14/172], Loss: 94.1061\n",
      "Epoch [81/300], Step [15/172], Loss: 80.9362\n",
      "Epoch [81/300], Step [16/172], Loss: 32.4900\n",
      "Epoch [81/300], Step [17/172], Loss: 59.3130\n",
      "Epoch [81/300], Step [18/172], Loss: 63.1727\n",
      "Epoch [81/300], Step [19/172], Loss: 82.4759\n",
      "Epoch [81/300], Step [20/172], Loss: 83.6634\n",
      "Epoch [81/300], Step [21/172], Loss: 95.5841\n",
      "Epoch [81/300], Step [22/172], Loss: 84.1037\n",
      "Epoch [81/300], Step [23/172], Loss: 7.3119\n",
      "Epoch [81/300], Step [24/172], Loss: 74.5543\n",
      "Epoch [81/300], Step [25/172], Loss: 49.3806\n",
      "Epoch [81/300], Step [26/172], Loss: 61.8605\n",
      "Epoch [81/300], Step [27/172], Loss: 82.7164\n",
      "Epoch [81/300], Step [28/172], Loss: 44.5883\n",
      "Epoch [81/300], Step [29/172], Loss: 36.1462\n",
      "Epoch [81/300], Step [30/172], Loss: 81.2128\n",
      "Epoch [81/300], Step [31/172], Loss: 45.4277\n",
      "Epoch [81/300], Step [32/172], Loss: 40.9463\n",
      "Epoch [81/300], Step [33/172], Loss: 72.9418\n",
      "Epoch [81/300], Step [34/172], Loss: 5.5402\n",
      "Epoch [81/300], Step [35/172], Loss: 24.1398\n",
      "Epoch [81/300], Step [36/172], Loss: 22.8496\n",
      "Epoch [81/300], Step [37/172], Loss: 18.6597\n",
      "Epoch [81/300], Step [38/172], Loss: 28.1988\n",
      "Epoch [81/300], Step [39/172], Loss: 51.6139\n",
      "Epoch [81/300], Step [40/172], Loss: 23.5867\n",
      "Epoch [81/300], Step [41/172], Loss: 40.3785\n",
      "Epoch [81/300], Step [42/172], Loss: 44.1324\n",
      "Epoch [81/300], Step [43/172], Loss: 27.9167\n",
      "Epoch [81/300], Step [44/172], Loss: 24.0151\n",
      "Epoch [81/300], Step [45/172], Loss: 22.3658\n",
      "Epoch [81/300], Step [46/172], Loss: 28.7140\n",
      "Epoch [81/300], Step [47/172], Loss: 54.5827\n",
      "Epoch [81/300], Step [48/172], Loss: 56.3388\n",
      "Epoch [81/300], Step [49/172], Loss: 21.2287\n",
      "Epoch [81/300], Step [50/172], Loss: 52.5723\n",
      "Epoch [81/300], Step [51/172], Loss: 7.6396\n",
      "Epoch [81/300], Step [52/172], Loss: 20.1060\n",
      "Epoch [81/300], Step [53/172], Loss: 26.3468\n",
      "Epoch [81/300], Step [54/172], Loss: 12.9783\n",
      "Epoch [81/300], Step [55/172], Loss: 13.5270\n",
      "Epoch [81/300], Step [56/172], Loss: 10.4374\n",
      "Epoch [81/300], Step [57/172], Loss: 27.2149\n",
      "Epoch [81/300], Step [58/172], Loss: 19.8234\n",
      "Epoch [81/300], Step [59/172], Loss: 32.9172\n",
      "Epoch [81/300], Step [60/172], Loss: 51.6380\n",
      "Epoch [81/300], Step [61/172], Loss: 10.3273\n",
      "Epoch [81/300], Step [62/172], Loss: 24.4171\n",
      "Epoch [81/300], Step [63/172], Loss: 9.0161\n",
      "Epoch [81/300], Step [64/172], Loss: 7.8387\n",
      "Epoch [81/300], Step [65/172], Loss: 22.6715\n",
      "Epoch [81/300], Step [66/172], Loss: 6.4074\n",
      "Epoch [81/300], Step [67/172], Loss: 28.5102\n",
      "Epoch [81/300], Step [68/172], Loss: 8.1397\n",
      "Epoch [81/300], Step [69/172], Loss: 69.9365\n",
      "Epoch [81/300], Step [70/172], Loss: 61.6765\n",
      "Epoch [81/300], Step [71/172], Loss: 56.0089\n",
      "Epoch [81/300], Step [72/172], Loss: 61.0696\n",
      "Epoch [81/300], Step [73/172], Loss: 65.5710\n",
      "Epoch [81/300], Step [74/172], Loss: 38.4213\n",
      "Epoch [81/300], Step [75/172], Loss: 33.6383\n",
      "Epoch [81/300], Step [76/172], Loss: 41.2471\n",
      "Epoch [81/300], Step [77/172], Loss: 62.3192\n",
      "Epoch [81/300], Step [78/172], Loss: 52.7923\n",
      "Epoch [81/300], Step [79/172], Loss: 50.8058\n",
      "Epoch [81/300], Step [80/172], Loss: 58.0692\n",
      "Epoch [81/300], Step [81/172], Loss: 44.7174\n",
      "Epoch [81/300], Step [82/172], Loss: 41.5536\n",
      "Epoch [81/300], Step [83/172], Loss: 52.0267\n",
      "Epoch [81/300], Step [84/172], Loss: 40.3567\n",
      "Epoch [81/300], Step [85/172], Loss: 44.2760\n",
      "Epoch [81/300], Step [86/172], Loss: 35.9985\n",
      "Epoch [81/300], Step [87/172], Loss: 30.0899\n",
      "Epoch [81/300], Step [88/172], Loss: 32.0450\n",
      "Epoch [81/300], Step [89/172], Loss: 28.8775\n",
      "Epoch [81/300], Step [90/172], Loss: 27.8807\n",
      "Epoch [81/300], Step [91/172], Loss: 31.1016\n",
      "Epoch [81/300], Step [92/172], Loss: 23.4244\n",
      "Epoch [81/300], Step [93/172], Loss: 23.1095\n",
      "Epoch [81/300], Step [94/172], Loss: 30.4553\n",
      "Epoch [81/300], Step [95/172], Loss: 24.7709\n",
      "Epoch [81/300], Step [96/172], Loss: 20.2601\n",
      "Epoch [81/300], Step [97/172], Loss: 27.2188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/300], Step [98/172], Loss: 21.7239\n",
      "Epoch [81/300], Step [99/172], Loss: 19.2999\n",
      "Epoch [81/300], Step [100/172], Loss: 17.8857\n",
      "Epoch [81/300], Step [101/172], Loss: 19.4200\n",
      "Epoch [81/300], Step [102/172], Loss: 17.7586\n",
      "Epoch [81/300], Step [103/172], Loss: 16.2411\n",
      "Epoch [81/300], Step [104/172], Loss: 17.1533\n",
      "Epoch [81/300], Step [105/172], Loss: 19.4090\n",
      "Epoch [81/300], Step [106/172], Loss: 18.4581\n",
      "Epoch [81/300], Step [107/172], Loss: 16.3579\n",
      "Epoch [81/300], Step [108/172], Loss: 18.4581\n",
      "Epoch [81/300], Step [109/172], Loss: 20.0412\n",
      "Epoch [81/300], Step [110/172], Loss: 17.5067\n",
      "Epoch [81/300], Step [111/172], Loss: 16.0172\n",
      "Epoch [81/300], Step [112/172], Loss: 20.6794\n",
      "Epoch [81/300], Step [113/172], Loss: 16.7979\n",
      "Epoch [81/300], Step [114/172], Loss: 16.1716\n",
      "Epoch [81/300], Step [115/172], Loss: 23.9757\n",
      "Epoch [81/300], Step [116/172], Loss: 16.5197\n",
      "Epoch [81/300], Step [117/172], Loss: 13.9509\n",
      "Epoch [81/300], Step [118/172], Loss: 17.1968\n",
      "Epoch [81/300], Step [119/172], Loss: 16.3781\n",
      "Epoch [81/300], Step [120/172], Loss: 12.7974\n",
      "Epoch [81/300], Step [121/172], Loss: 12.9824\n",
      "Epoch [81/300], Step [122/172], Loss: 12.2168\n",
      "Epoch [81/300], Step [123/172], Loss: 11.9248\n",
      "Epoch [81/300], Step [124/172], Loss: 9.6758\n",
      "Epoch [81/300], Step [125/172], Loss: 14.4100\n",
      "Epoch [81/300], Step [126/172], Loss: 12.2852\n",
      "Epoch [81/300], Step [127/172], Loss: 14.4754\n",
      "Epoch [81/300], Step [128/172], Loss: 14.8577\n",
      "Epoch [81/300], Step [129/172], Loss: 10.5666\n",
      "Epoch [81/300], Step [130/172], Loss: 12.7645\n",
      "Epoch [81/300], Step [131/172], Loss: 10.6903\n",
      "Epoch [81/300], Step [132/172], Loss: 10.5799\n",
      "Epoch [81/300], Step [133/172], Loss: 11.7164\n",
      "Epoch [81/300], Step [134/172], Loss: 13.0161\n",
      "Epoch [81/300], Step [135/172], Loss: 10.0715\n",
      "Epoch [81/300], Step [136/172], Loss: 9.7979\n",
      "Epoch [81/300], Step [137/172], Loss: 11.6652\n",
      "Epoch [81/300], Step [138/172], Loss: 9.8663\n",
      "Epoch [81/300], Step [139/172], Loss: 11.1225\n",
      "Epoch [81/300], Step [140/172], Loss: 11.0940\n",
      "Epoch [81/300], Step [141/172], Loss: 13.8386\n",
      "Epoch [81/300], Step [142/172], Loss: 14.9436\n",
      "Epoch [81/300], Step [143/172], Loss: 10.3063\n",
      "Epoch [81/300], Step [144/172], Loss: 10.0081\n",
      "Epoch [81/300], Step [145/172], Loss: 10.8530\n",
      "Epoch [81/300], Step [146/172], Loss: 11.0163\n",
      "Epoch [81/300], Step [147/172], Loss: 6.6455\n",
      "Epoch [81/300], Step [148/172], Loss: 7.7765\n",
      "Epoch [81/300], Step [149/172], Loss: 9.6087\n",
      "Epoch [81/300], Step [150/172], Loss: 9.3777\n",
      "Epoch [81/300], Step [151/172], Loss: 8.4353\n",
      "Epoch [81/300], Step [152/172], Loss: 8.6516\n",
      "Epoch [81/300], Step [153/172], Loss: 8.2832\n",
      "Epoch [81/300], Step [154/172], Loss: 9.0257\n",
      "Epoch [81/300], Step [155/172], Loss: 8.1439\n",
      "Epoch [81/300], Step [156/172], Loss: 12.5887\n",
      "Epoch [81/300], Step [157/172], Loss: 11.2163\n",
      "Epoch [81/300], Step [158/172], Loss: 8.9051\n",
      "Epoch [81/300], Step [159/172], Loss: 10.2779\n",
      "Epoch [81/300], Step [160/172], Loss: 10.4748\n",
      "Epoch [81/300], Step [161/172], Loss: 7.9487\n",
      "Epoch [81/300], Step [162/172], Loss: 8.4015\n",
      "Epoch [81/300], Step [163/172], Loss: 7.4755\n",
      "Epoch [81/300], Step [164/172], Loss: 10.6715\n",
      "Epoch [81/300], Step [165/172], Loss: 7.2658\n",
      "Epoch [81/300], Step [166/172], Loss: 7.7352\n",
      "Epoch [81/300], Step [167/172], Loss: 9.0506\n",
      "Epoch [81/300], Step [168/172], Loss: 7.5928\n",
      "Epoch [81/300], Step [169/172], Loss: 7.9247\n",
      "Epoch [81/300], Step [170/172], Loss: 6.6735\n",
      "Epoch [81/300], Step [171/172], Loss: 6.6853\n",
      "Epoch [81/300], Step [172/172], Loss: 6.0825\n",
      "Epoch [82/300], Step [1/172], Loss: 89.4779\n",
      "Epoch [82/300], Step [2/172], Loss: 89.7883\n",
      "Epoch [82/300], Step [3/172], Loss: 84.9798\n",
      "Epoch [82/300], Step [4/172], Loss: 52.7101\n",
      "Epoch [82/300], Step [5/172], Loss: 73.5229\n",
      "Epoch [82/300], Step [6/172], Loss: 23.0667\n",
      "Epoch [82/300], Step [7/172], Loss: 29.3020\n",
      "Epoch [82/300], Step [8/172], Loss: 7.5653\n",
      "Epoch [82/300], Step [9/172], Loss: 46.5139\n",
      "Epoch [82/300], Step [10/172], Loss: 50.6932\n",
      "Epoch [82/300], Step [11/172], Loss: 93.4041\n",
      "Epoch [82/300], Step [12/172], Loss: 84.7616\n",
      "Epoch [82/300], Step [13/172], Loss: 42.2877\n",
      "Epoch [82/300], Step [14/172], Loss: 93.9072\n",
      "Epoch [82/300], Step [15/172], Loss: 80.6806\n",
      "Epoch [82/300], Step [16/172], Loss: 31.8751\n",
      "Epoch [82/300], Step [17/172], Loss: 59.3943\n",
      "Epoch [82/300], Step [18/172], Loss: 63.1479\n",
      "Epoch [82/300], Step [19/172], Loss: 82.6266\n",
      "Epoch [82/300], Step [20/172], Loss: 83.2906\n",
      "Epoch [82/300], Step [21/172], Loss: 95.5198\n",
      "Epoch [82/300], Step [22/172], Loss: 83.9895\n",
      "Epoch [82/300], Step [23/172], Loss: 6.9671\n",
      "Epoch [82/300], Step [24/172], Loss: 74.3434\n",
      "Epoch [82/300], Step [25/172], Loss: 49.0593\n",
      "Epoch [82/300], Step [26/172], Loss: 61.7287\n",
      "Epoch [82/300], Step [27/172], Loss: 82.6212\n",
      "Epoch [82/300], Step [28/172], Loss: 43.9333\n",
      "Epoch [82/300], Step [29/172], Loss: 35.2616\n",
      "Epoch [82/300], Step [30/172], Loss: 81.2807\n",
      "Epoch [82/300], Step [31/172], Loss: 45.2290\n",
      "Epoch [82/300], Step [32/172], Loss: 40.8400\n",
      "Epoch [82/300], Step [33/172], Loss: 72.7855\n",
      "Epoch [82/300], Step [34/172], Loss: 5.4137\n",
      "Epoch [82/300], Step [35/172], Loss: 23.7611\n",
      "Epoch [82/300], Step [36/172], Loss: 22.7785\n",
      "Epoch [82/300], Step [37/172], Loss: 18.4969\n",
      "Epoch [82/300], Step [38/172], Loss: 28.0221\n",
      "Epoch [82/300], Step [39/172], Loss: 51.5293\n",
      "Epoch [82/300], Step [40/172], Loss: 23.2649\n",
      "Epoch [82/300], Step [41/172], Loss: 40.1808\n",
      "Epoch [82/300], Step [42/172], Loss: 43.7498\n",
      "Epoch [82/300], Step [43/172], Loss: 27.7145\n",
      "Epoch [82/300], Step [44/172], Loss: 23.7581\n",
      "Epoch [82/300], Step [45/172], Loss: 22.2248\n",
      "Epoch [82/300], Step [46/172], Loss: 28.3505\n",
      "Epoch [82/300], Step [47/172], Loss: 54.1693\n",
      "Epoch [82/300], Step [48/172], Loss: 56.2337\n",
      "Epoch [82/300], Step [49/172], Loss: 21.1217\n",
      "Epoch [82/300], Step [50/172], Loss: 52.6945\n",
      "Epoch [82/300], Step [51/172], Loss: 7.6119\n",
      "Epoch [82/300], Step [52/172], Loss: 19.8981\n",
      "Epoch [82/300], Step [53/172], Loss: 26.1582\n",
      "Epoch [82/300], Step [54/172], Loss: 12.8567\n",
      "Epoch [82/300], Step [55/172], Loss: 13.5063\n",
      "Epoch [82/300], Step [56/172], Loss: 10.4848\n",
      "Epoch [82/300], Step [57/172], Loss: 26.7615\n",
      "Epoch [82/300], Step [58/172], Loss: 19.7454\n",
      "Epoch [82/300], Step [59/172], Loss: 32.9395\n",
      "Epoch [82/300], Step [60/172], Loss: 51.2803\n",
      "Epoch [82/300], Step [61/172], Loss: 10.2457\n",
      "Epoch [82/300], Step [62/172], Loss: 24.4725\n",
      "Epoch [82/300], Step [63/172], Loss: 9.0229\n",
      "Epoch [82/300], Step [64/172], Loss: 7.9026\n",
      "Epoch [82/300], Step [65/172], Loss: 22.5232\n",
      "Epoch [82/300], Step [66/172], Loss: 6.3974\n",
      "Epoch [82/300], Step [67/172], Loss: 28.5622\n",
      "Epoch [82/300], Step [68/172], Loss: 8.1075\n",
      "Epoch [82/300], Step [69/172], Loss: 69.4641\n",
      "Epoch [82/300], Step [70/172], Loss: 61.1497\n",
      "Epoch [82/300], Step [71/172], Loss: 55.6649\n",
      "Epoch [82/300], Step [72/172], Loss: 60.8893\n",
      "Epoch [82/300], Step [73/172], Loss: 65.2858\n",
      "Epoch [82/300], Step [74/172], Loss: 38.2676\n",
      "Epoch [82/300], Step [75/172], Loss: 33.5734\n",
      "Epoch [82/300], Step [76/172], Loss: 40.9798\n",
      "Epoch [82/300], Step [77/172], Loss: 62.2760\n",
      "Epoch [82/300], Step [78/172], Loss: 52.6046\n",
      "Epoch [82/300], Step [79/172], Loss: 50.7740\n",
      "Epoch [82/300], Step [80/172], Loss: 58.2390\n",
      "Epoch [82/300], Step [81/172], Loss: 44.5936\n",
      "Epoch [82/300], Step [82/172], Loss: 41.4845\n",
      "Epoch [82/300], Step [83/172], Loss: 52.0038\n",
      "Epoch [82/300], Step [84/172], Loss: 40.3270\n",
      "Epoch [82/300], Step [85/172], Loss: 44.2437\n",
      "Epoch [82/300], Step [86/172], Loss: 35.8729\n",
      "Epoch [82/300], Step [87/172], Loss: 30.0296\n",
      "Epoch [82/300], Step [88/172], Loss: 32.0005\n",
      "Epoch [82/300], Step [89/172], Loss: 28.7807\n",
      "Epoch [82/300], Step [90/172], Loss: 27.7240\n",
      "Epoch [82/300], Step [91/172], Loss: 31.0506\n",
      "Epoch [82/300], Step [92/172], Loss: 23.2959\n",
      "Epoch [82/300], Step [93/172], Loss: 22.9388\n",
      "Epoch [82/300], Step [94/172], Loss: 30.3702\n",
      "Epoch [82/300], Step [95/172], Loss: 24.6250\n",
      "Epoch [82/300], Step [96/172], Loss: 20.1571\n",
      "Epoch [82/300], Step [97/172], Loss: 27.1810\n",
      "Epoch [82/300], Step [98/172], Loss: 21.6577\n",
      "Epoch [82/300], Step [99/172], Loss: 19.2274\n",
      "Epoch [82/300], Step [100/172], Loss: 17.7996\n",
      "Epoch [82/300], Step [101/172], Loss: 19.3423\n",
      "Epoch [82/300], Step [102/172], Loss: 17.6544\n",
      "Epoch [82/300], Step [103/172], Loss: 16.1232\n",
      "Epoch [82/300], Step [104/172], Loss: 17.1024\n",
      "Epoch [82/300], Step [105/172], Loss: 19.3083\n",
      "Epoch [82/300], Step [106/172], Loss: 18.3846\n",
      "Epoch [82/300], Step [107/172], Loss: 16.3134\n",
      "Epoch [82/300], Step [108/172], Loss: 18.3361\n",
      "Epoch [82/300], Step [109/172], Loss: 19.9026\n",
      "Epoch [82/300], Step [110/172], Loss: 17.3917\n",
      "Epoch [82/300], Step [111/172], Loss: 15.8888\n",
      "Epoch [82/300], Step [112/172], Loss: 20.5256\n",
      "Epoch [82/300], Step [113/172], Loss: 16.6739\n",
      "Epoch [82/300], Step [114/172], Loss: 16.0612\n",
      "Epoch [82/300], Step [115/172], Loss: 23.8064\n",
      "Epoch [82/300], Step [116/172], Loss: 16.4052\n",
      "Epoch [82/300], Step [117/172], Loss: 13.8216\n",
      "Epoch [82/300], Step [118/172], Loss: 17.0375\n",
      "Epoch [82/300], Step [119/172], Loss: 16.3378\n",
      "Epoch [82/300], Step [120/172], Loss: 12.7071\n",
      "Epoch [82/300], Step [121/172], Loss: 12.8964\n",
      "Epoch [82/300], Step [122/172], Loss: 12.0514\n",
      "Epoch [82/300], Step [123/172], Loss: 11.8056\n",
      "Epoch [82/300], Step [124/172], Loss: 9.6101\n",
      "Epoch [82/300], Step [125/172], Loss: 14.3573\n",
      "Epoch [82/300], Step [126/172], Loss: 12.2105\n",
      "Epoch [82/300], Step [127/172], Loss: 14.3244\n",
      "Epoch [82/300], Step [128/172], Loss: 14.7075\n",
      "Epoch [82/300], Step [129/172], Loss: 10.4887\n",
      "Epoch [82/300], Step [130/172], Loss: 12.6868\n",
      "Epoch [82/300], Step [131/172], Loss: 10.5811\n",
      "Epoch [82/300], Step [132/172], Loss: 10.4987\n",
      "Epoch [82/300], Step [133/172], Loss: 11.5889\n",
      "Epoch [82/300], Step [134/172], Loss: 12.9395\n",
      "Epoch [82/300], Step [135/172], Loss: 10.0322\n",
      "Epoch [82/300], Step [136/172], Loss: 9.7503\n",
      "Epoch [82/300], Step [137/172], Loss: 11.5813\n",
      "Epoch [82/300], Step [138/172], Loss: 9.7993\n",
      "Epoch [82/300], Step [139/172], Loss: 11.0519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/300], Step [140/172], Loss: 11.0180\n",
      "Epoch [82/300], Step [141/172], Loss: 13.7388\n",
      "Epoch [82/300], Step [142/172], Loss: 14.8309\n",
      "Epoch [82/300], Step [143/172], Loss: 10.2557\n",
      "Epoch [82/300], Step [144/172], Loss: 9.9432\n",
      "Epoch [82/300], Step [145/172], Loss: 10.8101\n",
      "Epoch [82/300], Step [146/172], Loss: 10.9450\n",
      "Epoch [82/300], Step [147/172], Loss: 6.5886\n",
      "Epoch [82/300], Step [148/172], Loss: 7.7248\n",
      "Epoch [82/300], Step [149/172], Loss: 9.5305\n",
      "Epoch [82/300], Step [150/172], Loss: 9.3057\n",
      "Epoch [82/300], Step [151/172], Loss: 8.3789\n",
      "Epoch [82/300], Step [152/172], Loss: 8.6082\n",
      "Epoch [82/300], Step [153/172], Loss: 8.2225\n",
      "Epoch [82/300], Step [154/172], Loss: 8.9387\n",
      "Epoch [82/300], Step [155/172], Loss: 8.0893\n",
      "Epoch [82/300], Step [156/172], Loss: 12.5733\n",
      "Epoch [82/300], Step [157/172], Loss: 11.1399\n",
      "Epoch [82/300], Step [158/172], Loss: 8.8238\n",
      "Epoch [82/300], Step [159/172], Loss: 10.2281\n",
      "Epoch [82/300], Step [160/172], Loss: 10.4186\n",
      "Epoch [82/300], Step [161/172], Loss: 7.9189\n",
      "Epoch [82/300], Step [162/172], Loss: 8.3722\n",
      "Epoch [82/300], Step [163/172], Loss: 7.4196\n",
      "Epoch [82/300], Step [164/172], Loss: 10.6304\n",
      "Epoch [82/300], Step [165/172], Loss: 7.2240\n",
      "Epoch [82/300], Step [166/172], Loss: 7.6899\n",
      "Epoch [82/300], Step [167/172], Loss: 9.0344\n",
      "Epoch [82/300], Step [168/172], Loss: 7.5786\n",
      "Epoch [82/300], Step [169/172], Loss: 7.8780\n",
      "Epoch [82/300], Step [170/172], Loss: 6.6342\n",
      "Epoch [82/300], Step [171/172], Loss: 6.6632\n",
      "Epoch [82/300], Step [172/172], Loss: 6.0567\n",
      "Epoch [83/300], Step [1/172], Loss: 89.0670\n",
      "Epoch [83/300], Step [2/172], Loss: 89.4597\n",
      "Epoch [83/300], Step [3/172], Loss: 84.4202\n",
      "Epoch [83/300], Step [4/172], Loss: 52.4197\n",
      "Epoch [83/300], Step [5/172], Loss: 73.2621\n",
      "Epoch [83/300], Step [6/172], Loss: 22.9717\n",
      "Epoch [83/300], Step [7/172], Loss: 29.7041\n",
      "Epoch [83/300], Step [8/172], Loss: 7.4735\n",
      "Epoch [83/300], Step [9/172], Loss: 46.3129\n",
      "Epoch [83/300], Step [10/172], Loss: 50.4983\n",
      "Epoch [83/300], Step [11/172], Loss: 93.0818\n",
      "Epoch [83/300], Step [12/172], Loss: 84.8460\n",
      "Epoch [83/300], Step [13/172], Loss: 42.2989\n",
      "Epoch [83/300], Step [14/172], Loss: 93.7396\n",
      "Epoch [83/300], Step [15/172], Loss: 80.3711\n",
      "Epoch [83/300], Step [16/172], Loss: 31.1799\n",
      "Epoch [83/300], Step [17/172], Loss: 59.4697\n",
      "Epoch [83/300], Step [18/172], Loss: 63.1450\n",
      "Epoch [83/300], Step [19/172], Loss: 82.8271\n",
      "Epoch [83/300], Step [20/172], Loss: 83.0307\n",
      "Epoch [83/300], Step [21/172], Loss: 95.6598\n",
      "Epoch [83/300], Step [22/172], Loss: 83.7279\n",
      "Epoch [83/300], Step [23/172], Loss: 6.7444\n",
      "Epoch [83/300], Step [24/172], Loss: 74.2664\n",
      "Epoch [83/300], Step [25/172], Loss: 49.0192\n",
      "Epoch [83/300], Step [26/172], Loss: 61.6616\n",
      "Epoch [83/300], Step [27/172], Loss: 82.3123\n",
      "Epoch [83/300], Step [28/172], Loss: 43.5091\n",
      "Epoch [83/300], Step [29/172], Loss: 34.6856\n",
      "Epoch [83/300], Step [30/172], Loss: 81.4023\n",
      "Epoch [83/300], Step [31/172], Loss: 45.3017\n",
      "Epoch [83/300], Step [32/172], Loss: 40.9246\n",
      "Epoch [83/300], Step [33/172], Loss: 72.7900\n",
      "Epoch [83/300], Step [34/172], Loss: 5.3355\n",
      "Epoch [83/300], Step [35/172], Loss: 23.2770\n",
      "Epoch [83/300], Step [36/172], Loss: 22.8693\n",
      "Epoch [83/300], Step [37/172], Loss: 18.4912\n",
      "Epoch [83/300], Step [38/172], Loss: 28.1005\n",
      "Epoch [83/300], Step [39/172], Loss: 51.4114\n",
      "Epoch [83/300], Step [40/172], Loss: 23.2280\n",
      "Epoch [83/300], Step [41/172], Loss: 40.1943\n",
      "Epoch [83/300], Step [42/172], Loss: 43.8422\n",
      "Epoch [83/300], Step [43/172], Loss: 27.8184\n",
      "Epoch [83/300], Step [44/172], Loss: 23.6153\n",
      "Epoch [83/300], Step [45/172], Loss: 22.2858\n",
      "Epoch [83/300], Step [46/172], Loss: 28.2246\n",
      "Epoch [83/300], Step [47/172], Loss: 54.0484\n",
      "Epoch [83/300], Step [48/172], Loss: 56.0385\n",
      "Epoch [83/300], Step [49/172], Loss: 21.1650\n",
      "Epoch [83/300], Step [50/172], Loss: 52.7385\n",
      "Epoch [83/300], Step [51/172], Loss: 7.6189\n",
      "Epoch [83/300], Step [52/172], Loss: 19.8443\n",
      "Epoch [83/300], Step [53/172], Loss: 26.0295\n",
      "Epoch [83/300], Step [54/172], Loss: 12.7544\n",
      "Epoch [83/300], Step [55/172], Loss: 13.3507\n",
      "Epoch [83/300], Step [56/172], Loss: 10.4607\n",
      "Epoch [83/300], Step [57/172], Loss: 26.6051\n",
      "Epoch [83/300], Step [58/172], Loss: 19.6244\n",
      "Epoch [83/300], Step [59/172], Loss: 32.7212\n",
      "Epoch [83/300], Step [60/172], Loss: 51.2678\n",
      "Epoch [83/300], Step [61/172], Loss: 10.2115\n",
      "Epoch [83/300], Step [62/172], Loss: 24.4160\n",
      "Epoch [83/300], Step [63/172], Loss: 9.0402\n",
      "Epoch [83/300], Step [64/172], Loss: 7.9493\n",
      "Epoch [83/300], Step [65/172], Loss: 22.3794\n",
      "Epoch [83/300], Step [66/172], Loss: 6.3939\n",
      "Epoch [83/300], Step [67/172], Loss: 28.6505\n",
      "Epoch [83/300], Step [68/172], Loss: 8.0657\n",
      "Epoch [83/300], Step [69/172], Loss: 68.9371\n",
      "Epoch [83/300], Step [70/172], Loss: 60.7070\n",
      "Epoch [83/300], Step [71/172], Loss: 55.3233\n",
      "Epoch [83/300], Step [72/172], Loss: 60.5953\n",
      "Epoch [83/300], Step [73/172], Loss: 64.9447\n",
      "Epoch [83/300], Step [74/172], Loss: 37.9607\n",
      "Epoch [83/300], Step [75/172], Loss: 33.4977\n",
      "Epoch [83/300], Step [76/172], Loss: 40.7393\n",
      "Epoch [83/300], Step [77/172], Loss: 62.1037\n",
      "Epoch [83/300], Step [78/172], Loss: 52.2215\n",
      "Epoch [83/300], Step [79/172], Loss: 50.4992\n",
      "Epoch [83/300], Step [80/172], Loss: 58.1388\n",
      "Epoch [83/300], Step [81/172], Loss: 44.3435\n",
      "Epoch [83/300], Step [82/172], Loss: 41.4516\n",
      "Epoch [83/300], Step [83/172], Loss: 51.7970\n",
      "Epoch [83/300], Step [84/172], Loss: 40.1106\n",
      "Epoch [83/300], Step [85/172], Loss: 43.9326\n",
      "Epoch [83/300], Step [86/172], Loss: 35.7524\n",
      "Epoch [83/300], Step [87/172], Loss: 29.9063\n",
      "Epoch [83/300], Step [88/172], Loss: 31.8416\n",
      "Epoch [83/300], Step [89/172], Loss: 28.5997\n",
      "Epoch [83/300], Step [90/172], Loss: 27.6044\n",
      "Epoch [83/300], Step [91/172], Loss: 30.9693\n",
      "Epoch [83/300], Step [92/172], Loss: 23.1763\n",
      "Epoch [83/300], Step [93/172], Loss: 22.8532\n",
      "Epoch [83/300], Step [94/172], Loss: 30.2844\n",
      "Epoch [83/300], Step [95/172], Loss: 24.5029\n",
      "Epoch [83/300], Step [96/172], Loss: 20.0504\n",
      "Epoch [83/300], Step [97/172], Loss: 27.0910\n",
      "Epoch [83/300], Step [98/172], Loss: 21.5282\n",
      "Epoch [83/300], Step [99/172], Loss: 19.1319\n",
      "Epoch [83/300], Step [100/172], Loss: 17.6837\n",
      "Epoch [83/300], Step [101/172], Loss: 19.2221\n",
      "Epoch [83/300], Step [102/172], Loss: 17.6382\n",
      "Epoch [83/300], Step [103/172], Loss: 15.9943\n",
      "Epoch [83/300], Step [104/172], Loss: 17.0311\n",
      "Epoch [83/300], Step [105/172], Loss: 19.3315\n",
      "Epoch [83/300], Step [106/172], Loss: 18.3210\n",
      "Epoch [83/300], Step [107/172], Loss: 16.2939\n",
      "Epoch [83/300], Step [108/172], Loss: 18.2641\n",
      "Epoch [83/300], Step [109/172], Loss: 19.8107\n",
      "Epoch [83/300], Step [110/172], Loss: 17.3278\n",
      "Epoch [83/300], Step [111/172], Loss: 15.8098\n",
      "Epoch [83/300], Step [112/172], Loss: 20.4125\n",
      "Epoch [83/300], Step [113/172], Loss: 16.5744\n",
      "Epoch [83/300], Step [114/172], Loss: 15.9668\n",
      "Epoch [83/300], Step [115/172], Loss: 23.7109\n",
      "Epoch [83/300], Step [116/172], Loss: 16.3191\n",
      "Epoch [83/300], Step [117/172], Loss: 13.7451\n",
      "Epoch [83/300], Step [118/172], Loss: 16.9784\n",
      "Epoch [83/300], Step [119/172], Loss: 16.2998\n",
      "Epoch [83/300], Step [120/172], Loss: 12.6546\n",
      "Epoch [83/300], Step [121/172], Loss: 12.8072\n",
      "Epoch [83/300], Step [122/172], Loss: 12.0572\n",
      "Epoch [83/300], Step [123/172], Loss: 11.7526\n",
      "Epoch [83/300], Step [124/172], Loss: 9.5610\n",
      "Epoch [83/300], Step [125/172], Loss: 14.2835\n",
      "Epoch [83/300], Step [126/172], Loss: 12.1397\n",
      "Epoch [83/300], Step [127/172], Loss: 14.2190\n",
      "Epoch [83/300], Step [128/172], Loss: 14.5842\n",
      "Epoch [83/300], Step [129/172], Loss: 10.4289\n",
      "Epoch [83/300], Step [130/172], Loss: 12.6317\n",
      "Epoch [83/300], Step [131/172], Loss: 10.5232\n",
      "Epoch [83/300], Step [132/172], Loss: 10.4292\n",
      "Epoch [83/300], Step [133/172], Loss: 11.5796\n",
      "Epoch [83/300], Step [134/172], Loss: 12.9342\n",
      "Epoch [83/300], Step [135/172], Loss: 9.9862\n",
      "Epoch [83/300], Step [136/172], Loss: 9.7069\n",
      "Epoch [83/300], Step [137/172], Loss: 11.5318\n",
      "Epoch [83/300], Step [138/172], Loss: 9.7416\n",
      "Epoch [83/300], Step [139/172], Loss: 10.9974\n",
      "Epoch [83/300], Step [140/172], Loss: 10.9607\n",
      "Epoch [83/300], Step [141/172], Loss: 13.6437\n",
      "Epoch [83/300], Step [142/172], Loss: 14.8308\n",
      "Epoch [83/300], Step [143/172], Loss: 10.2305\n",
      "Epoch [83/300], Step [144/172], Loss: 9.9152\n",
      "Epoch [83/300], Step [145/172], Loss: 10.7922\n",
      "Epoch [83/300], Step [146/172], Loss: 10.9115\n",
      "Epoch [83/300], Step [147/172], Loss: 6.5461\n",
      "Epoch [83/300], Step [148/172], Loss: 7.6906\n",
      "Epoch [83/300], Step [149/172], Loss: 9.4757\n",
      "Epoch [83/300], Step [150/172], Loss: 9.2532\n",
      "Epoch [83/300], Step [151/172], Loss: 8.3436\n",
      "Epoch [83/300], Step [152/172], Loss: 8.5857\n",
      "Epoch [83/300], Step [153/172], Loss: 8.1805\n",
      "Epoch [83/300], Step [154/172], Loss: 8.9107\n",
      "Epoch [83/300], Step [155/172], Loss: 8.0421\n",
      "Epoch [83/300], Step [156/172], Loss: 12.6288\n",
      "Epoch [83/300], Step [157/172], Loss: 11.1164\n",
      "Epoch [83/300], Step [158/172], Loss: 8.7981\n",
      "Epoch [83/300], Step [159/172], Loss: 10.2073\n",
      "Epoch [83/300], Step [160/172], Loss: 10.4368\n",
      "Epoch [83/300], Step [161/172], Loss: 7.8990\n",
      "Epoch [83/300], Step [162/172], Loss: 8.3319\n",
      "Epoch [83/300], Step [163/172], Loss: 7.3887\n",
      "Epoch [83/300], Step [164/172], Loss: 10.6280\n",
      "Epoch [83/300], Step [165/172], Loss: 7.2067\n",
      "Epoch [83/300], Step [166/172], Loss: 7.6700\n",
      "Epoch [83/300], Step [167/172], Loss: 9.0409\n",
      "Epoch [83/300], Step [168/172], Loss: 7.5644\n",
      "Epoch [83/300], Step [169/172], Loss: 7.8611\n",
      "Epoch [83/300], Step [170/172], Loss: 6.5971\n",
      "Epoch [83/300], Step [171/172], Loss: 6.6540\n",
      "Epoch [83/300], Step [172/172], Loss: 6.0306\n",
      "Epoch [84/300], Step [1/172], Loss: 88.4600\n",
      "Epoch [84/300], Step [2/172], Loss: 88.9016\n",
      "Epoch [84/300], Step [3/172], Loss: 83.9699\n",
      "Epoch [84/300], Step [4/172], Loss: 52.0498\n",
      "Epoch [84/300], Step [5/172], Loss: 72.8171\n",
      "Epoch [84/300], Step [6/172], Loss: 22.6871\n",
      "Epoch [84/300], Step [7/172], Loss: 29.1887\n",
      "Epoch [84/300], Step [8/172], Loss: 7.2686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/300], Step [9/172], Loss: 46.0428\n",
      "Epoch [84/300], Step [10/172], Loss: 50.1309\n",
      "Epoch [84/300], Step [11/172], Loss: 92.5792\n",
      "Epoch [84/300], Step [12/172], Loss: 84.6739\n",
      "Epoch [84/300], Step [13/172], Loss: 42.2085\n",
      "Epoch [84/300], Step [14/172], Loss: 93.0971\n",
      "Epoch [84/300], Step [15/172], Loss: 79.7486\n",
      "Epoch [84/300], Step [16/172], Loss: 30.1750\n",
      "Epoch [84/300], Step [17/172], Loss: 59.2644\n",
      "Epoch [84/300], Step [18/172], Loss: 62.8548\n",
      "Epoch [84/300], Step [19/172], Loss: 82.7194\n",
      "Epoch [84/300], Step [20/172], Loss: 82.1345\n",
      "Epoch [84/300], Step [21/172], Loss: 95.2587\n",
      "Epoch [84/300], Step [22/172], Loss: 83.1906\n",
      "Epoch [84/300], Step [23/172], Loss: 6.5346\n",
      "Epoch [84/300], Step [24/172], Loss: 73.9771\n",
      "Epoch [84/300], Step [25/172], Loss: 48.7192\n",
      "Epoch [84/300], Step [26/172], Loss: 61.4499\n",
      "Epoch [84/300], Step [27/172], Loss: 81.7257\n",
      "Epoch [84/300], Step [28/172], Loss: 43.0951\n",
      "Epoch [84/300], Step [29/172], Loss: 33.8209\n",
      "Epoch [84/300], Step [30/172], Loss: 81.5796\n",
      "Epoch [84/300], Step [31/172], Loss: 45.2895\n",
      "Epoch [84/300], Step [32/172], Loss: 40.8717\n",
      "Epoch [84/300], Step [33/172], Loss: 72.5963\n",
      "Epoch [84/300], Step [34/172], Loss: 5.3679\n",
      "Epoch [84/300], Step [35/172], Loss: 22.5194\n",
      "Epoch [84/300], Step [36/172], Loss: 22.7498\n",
      "Epoch [84/300], Step [37/172], Loss: 18.4292\n",
      "Epoch [84/300], Step [38/172], Loss: 27.9614\n",
      "Epoch [84/300], Step [39/172], Loss: 51.1215\n",
      "Epoch [84/300], Step [40/172], Loss: 23.0482\n",
      "Epoch [84/300], Step [41/172], Loss: 40.0179\n",
      "Epoch [84/300], Step [42/172], Loss: 43.7489\n",
      "Epoch [84/300], Step [43/172], Loss: 27.7222\n",
      "Epoch [84/300], Step [44/172], Loss: 23.4375\n",
      "Epoch [84/300], Step [45/172], Loss: 22.2351\n",
      "Epoch [84/300], Step [46/172], Loss: 27.8751\n",
      "Epoch [84/300], Step [47/172], Loss: 53.7609\n",
      "Epoch [84/300], Step [48/172], Loss: 55.8089\n",
      "Epoch [84/300], Step [49/172], Loss: 21.1058\n",
      "Epoch [84/300], Step [50/172], Loss: 52.5900\n",
      "Epoch [84/300], Step [51/172], Loss: 7.6012\n",
      "Epoch [84/300], Step [52/172], Loss: 19.7660\n",
      "Epoch [84/300], Step [53/172], Loss: 25.9734\n",
      "Epoch [84/300], Step [54/172], Loss: 12.6695\n",
      "Epoch [84/300], Step [55/172], Loss: 13.2970\n",
      "Epoch [84/300], Step [56/172], Loss: 10.4499\n",
      "Epoch [84/300], Step [57/172], Loss: 26.1908\n",
      "Epoch [84/300], Step [58/172], Loss: 19.5246\n",
      "Epoch [84/300], Step [59/172], Loss: 32.6152\n",
      "Epoch [84/300], Step [60/172], Loss: 50.9339\n",
      "Epoch [84/300], Step [61/172], Loss: 10.1627\n",
      "Epoch [84/300], Step [62/172], Loss: 24.4507\n",
      "Epoch [84/300], Step [63/172], Loss: 9.0402\n",
      "Epoch [84/300], Step [64/172], Loss: 7.9508\n",
      "Epoch [84/300], Step [65/172], Loss: 22.3009\n",
      "Epoch [84/300], Step [66/172], Loss: 6.3405\n",
      "Epoch [84/300], Step [67/172], Loss: 28.7729\n",
      "Epoch [84/300], Step [68/172], Loss: 7.9216\n",
      "Epoch [84/300], Step [69/172], Loss: 68.7393\n",
      "Epoch [84/300], Step [70/172], Loss: 60.5901\n",
      "Epoch [84/300], Step [71/172], Loss: 55.3817\n",
      "Epoch [84/300], Step [72/172], Loss: 60.7129\n",
      "Epoch [84/300], Step [73/172], Loss: 64.8331\n",
      "Epoch [84/300], Step [74/172], Loss: 37.9756\n",
      "Epoch [84/300], Step [75/172], Loss: 33.7039\n",
      "Epoch [84/300], Step [76/172], Loss: 40.6673\n",
      "Epoch [84/300], Step [77/172], Loss: 62.2799\n",
      "Epoch [84/300], Step [78/172], Loss: 52.1439\n",
      "Epoch [84/300], Step [79/172], Loss: 50.6056\n",
      "Epoch [84/300], Step [80/172], Loss: 58.3535\n",
      "Epoch [84/300], Step [81/172], Loss: 44.3366\n",
      "Epoch [84/300], Step [82/172], Loss: 41.4051\n",
      "Epoch [84/300], Step [83/172], Loss: 51.8908\n",
      "Epoch [84/300], Step [84/172], Loss: 40.0888\n",
      "Epoch [84/300], Step [85/172], Loss: 43.8842\n",
      "Epoch [84/300], Step [86/172], Loss: 35.7383\n",
      "Epoch [84/300], Step [87/172], Loss: 29.9074\n",
      "Epoch [84/300], Step [88/172], Loss: 31.8550\n",
      "Epoch [84/300], Step [89/172], Loss: 28.5393\n",
      "Epoch [84/300], Step [90/172], Loss: 27.5240\n",
      "Epoch [84/300], Step [91/172], Loss: 30.9791\n",
      "Epoch [84/300], Step [92/172], Loss: 23.1336\n",
      "Epoch [84/300], Step [93/172], Loss: 22.8048\n",
      "Epoch [84/300], Step [94/172], Loss: 30.2577\n",
      "Epoch [84/300], Step [95/172], Loss: 24.4474\n",
      "Epoch [84/300], Step [96/172], Loss: 20.0094\n",
      "Epoch [84/300], Step [97/172], Loss: 27.0557\n",
      "Epoch [84/300], Step [98/172], Loss: 21.4274\n",
      "Epoch [84/300], Step [99/172], Loss: 19.0770\n",
      "Epoch [84/300], Step [100/172], Loss: 17.6001\n",
      "Epoch [84/300], Step [101/172], Loss: 19.1237\n",
      "Epoch [84/300], Step [102/172], Loss: 17.5213\n",
      "Epoch [84/300], Step [103/172], Loss: 15.8529\n",
      "Epoch [84/300], Step [104/172], Loss: 16.9791\n",
      "Epoch [84/300], Step [105/172], Loss: 19.2762\n",
      "Epoch [84/300], Step [106/172], Loss: 18.2382\n",
      "Epoch [84/300], Step [107/172], Loss: 16.2569\n",
      "Epoch [84/300], Step [108/172], Loss: 18.1775\n",
      "Epoch [84/300], Step [109/172], Loss: 19.6510\n",
      "Epoch [84/300], Step [110/172], Loss: 17.2652\n",
      "Epoch [84/300], Step [111/172], Loss: 15.7381\n",
      "Epoch [84/300], Step [112/172], Loss: 20.3170\n",
      "Epoch [84/300], Step [113/172], Loss: 16.4571\n",
      "Epoch [84/300], Step [114/172], Loss: 15.8519\n",
      "Epoch [84/300], Step [115/172], Loss: 23.5152\n",
      "Epoch [84/300], Step [116/172], Loss: 16.2583\n",
      "Epoch [84/300], Step [117/172], Loss: 13.6283\n",
      "Epoch [84/300], Step [118/172], Loss: 16.8016\n",
      "Epoch [84/300], Step [119/172], Loss: 16.2829\n",
      "Epoch [84/300], Step [120/172], Loss: 12.5594\n",
      "Epoch [84/300], Step [121/172], Loss: 12.7189\n",
      "Epoch [84/300], Step [122/172], Loss: 11.9872\n",
      "Epoch [84/300], Step [123/172], Loss: 11.6494\n",
      "Epoch [84/300], Step [124/172], Loss: 9.5225\n",
      "Epoch [84/300], Step [125/172], Loss: 14.2447\n",
      "Epoch [84/300], Step [126/172], Loss: 12.0782\n",
      "Epoch [84/300], Step [127/172], Loss: 14.1378\n",
      "Epoch [84/300], Step [128/172], Loss: 14.5259\n",
      "Epoch [84/300], Step [129/172], Loss: 10.3666\n",
      "Epoch [84/300], Step [130/172], Loss: 12.5855\n",
      "Epoch [84/300], Step [131/172], Loss: 10.4427\n",
      "Epoch [84/300], Step [132/172], Loss: 10.3773\n",
      "Epoch [84/300], Step [133/172], Loss: 11.5139\n",
      "Epoch [84/300], Step [134/172], Loss: 12.9097\n",
      "Epoch [84/300], Step [135/172], Loss: 9.9543\n",
      "Epoch [84/300], Step [136/172], Loss: 9.6886\n",
      "Epoch [84/300], Step [137/172], Loss: 11.4739\n",
      "Epoch [84/300], Step [138/172], Loss: 9.6747\n",
      "Epoch [84/300], Step [139/172], Loss: 10.9689\n",
      "Epoch [84/300], Step [140/172], Loss: 10.9130\n",
      "Epoch [84/300], Step [141/172], Loss: 13.5485\n",
      "Epoch [84/300], Step [142/172], Loss: 14.8263\n",
      "Epoch [84/300], Step [143/172], Loss: 10.1938\n",
      "Epoch [84/300], Step [144/172], Loss: 9.8680\n",
      "Epoch [84/300], Step [145/172], Loss: 10.7776\n",
      "Epoch [84/300], Step [146/172], Loss: 10.8932\n",
      "Epoch [84/300], Step [147/172], Loss: 6.5101\n",
      "Epoch [84/300], Step [148/172], Loss: 7.6534\n",
      "Epoch [84/300], Step [149/172], Loss: 9.4172\n",
      "Epoch [84/300], Step [150/172], Loss: 9.2155\n",
      "Epoch [84/300], Step [151/172], Loss: 8.2779\n",
      "Epoch [84/300], Step [152/172], Loss: 8.5624\n",
      "Epoch [84/300], Step [153/172], Loss: 8.1504\n",
      "Epoch [84/300], Step [154/172], Loss: 8.8485\n",
      "Epoch [84/300], Step [155/172], Loss: 8.0125\n",
      "Epoch [84/300], Step [156/172], Loss: 12.6674\n",
      "Epoch [84/300], Step [157/172], Loss: 11.0944\n",
      "Epoch [84/300], Step [158/172], Loss: 8.7536\n",
      "Epoch [84/300], Step [159/172], Loss: 10.2279\n",
      "Epoch [84/300], Step [160/172], Loss: 10.4487\n",
      "Epoch [84/300], Step [161/172], Loss: 7.8878\n",
      "Epoch [84/300], Step [162/172], Loss: 8.3265\n",
      "Epoch [84/300], Step [163/172], Loss: 7.3562\n",
      "Epoch [84/300], Step [164/172], Loss: 10.5878\n",
      "Epoch [84/300], Step [165/172], Loss: 7.1731\n",
      "Epoch [84/300], Step [166/172], Loss: 7.6514\n",
      "Epoch [84/300], Step [167/172], Loss: 9.0707\n",
      "Epoch [84/300], Step [168/172], Loss: 7.5779\n",
      "Epoch [84/300], Step [169/172], Loss: 7.8536\n",
      "Epoch [84/300], Step [170/172], Loss: 6.5776\n",
      "Epoch [84/300], Step [171/172], Loss: 6.6618\n",
      "Epoch [84/300], Step [172/172], Loss: 6.0206\n",
      "Epoch [85/300], Step [1/172], Loss: 87.9522\n",
      "Epoch [85/300], Step [2/172], Loss: 88.5001\n",
      "Epoch [85/300], Step [3/172], Loss: 83.1621\n",
      "Epoch [85/300], Step [4/172], Loss: 51.7496\n",
      "Epoch [85/300], Step [5/172], Loss: 72.2989\n",
      "Epoch [85/300], Step [6/172], Loss: 22.4512\n",
      "Epoch [85/300], Step [7/172], Loss: 29.2209\n",
      "Epoch [85/300], Step [8/172], Loss: 7.1658\n",
      "Epoch [85/300], Step [9/172], Loss: 45.8919\n",
      "Epoch [85/300], Step [10/172], Loss: 49.8203\n",
      "Epoch [85/300], Step [11/172], Loss: 92.0969\n",
      "Epoch [85/300], Step [12/172], Loss: 84.5541\n",
      "Epoch [85/300], Step [13/172], Loss: 42.2217\n",
      "Epoch [85/300], Step [14/172], Loss: 92.7924\n",
      "Epoch [85/300], Step [15/172], Loss: 79.3964\n",
      "Epoch [85/300], Step [16/172], Loss: 29.5593\n",
      "Epoch [85/300], Step [17/172], Loss: 59.2523\n",
      "Epoch [85/300], Step [18/172], Loss: 62.8301\n",
      "Epoch [85/300], Step [19/172], Loss: 82.9093\n",
      "Epoch [85/300], Step [20/172], Loss: 81.3431\n",
      "Epoch [85/300], Step [21/172], Loss: 95.3162\n",
      "Epoch [85/300], Step [22/172], Loss: 83.0328\n",
      "Epoch [85/300], Step [23/172], Loss: 6.2403\n",
      "Epoch [85/300], Step [24/172], Loss: 73.7071\n",
      "Epoch [85/300], Step [25/172], Loss: 48.5364\n",
      "Epoch [85/300], Step [26/172], Loss: 61.2586\n",
      "Epoch [85/300], Step [27/172], Loss: 81.5599\n",
      "Epoch [85/300], Step [28/172], Loss: 42.6109\n",
      "Epoch [85/300], Step [29/172], Loss: 33.0055\n",
      "Epoch [85/300], Step [30/172], Loss: 81.7307\n",
      "Epoch [85/300], Step [31/172], Loss: 45.0837\n",
      "Epoch [85/300], Step [32/172], Loss: 40.7724\n",
      "Epoch [85/300], Step [33/172], Loss: 72.4041\n",
      "Epoch [85/300], Step [34/172], Loss: 5.1933\n",
      "Epoch [85/300], Step [35/172], Loss: 22.1213\n",
      "Epoch [85/300], Step [36/172], Loss: 22.5214\n",
      "Epoch [85/300], Step [37/172], Loss: 18.3249\n",
      "Epoch [85/300], Step [38/172], Loss: 27.9122\n",
      "Epoch [85/300], Step [39/172], Loss: 51.0093\n",
      "Epoch [85/300], Step [40/172], Loss: 22.7348\n",
      "Epoch [85/300], Step [41/172], Loss: 39.8788\n",
      "Epoch [85/300], Step [42/172], Loss: 43.4100\n",
      "Epoch [85/300], Step [43/172], Loss: 27.5629\n",
      "Epoch [85/300], Step [44/172], Loss: 23.1172\n",
      "Epoch [85/300], Step [45/172], Loss: 22.0659\n",
      "Epoch [85/300], Step [46/172], Loss: 27.5251\n",
      "Epoch [85/300], Step [47/172], Loss: 53.4630\n",
      "Epoch [85/300], Step [48/172], Loss: 55.4820\n",
      "Epoch [85/300], Step [49/172], Loss: 20.9510\n",
      "Epoch [85/300], Step [50/172], Loss: 52.6350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/300], Step [51/172], Loss: 7.5605\n",
      "Epoch [85/300], Step [52/172], Loss: 19.5580\n",
      "Epoch [85/300], Step [53/172], Loss: 25.7363\n",
      "Epoch [85/300], Step [54/172], Loss: 12.4664\n",
      "Epoch [85/300], Step [55/172], Loss: 13.1214\n",
      "Epoch [85/300], Step [56/172], Loss: 10.3947\n",
      "Epoch [85/300], Step [57/172], Loss: 25.8570\n",
      "Epoch [85/300], Step [58/172], Loss: 19.3845\n",
      "Epoch [85/300], Step [59/172], Loss: 32.3115\n",
      "Epoch [85/300], Step [60/172], Loss: 50.5997\n",
      "Epoch [85/300], Step [61/172], Loss: 10.0626\n",
      "Epoch [85/300], Step [62/172], Loss: 24.2782\n",
      "Epoch [85/300], Step [63/172], Loss: 8.9949\n",
      "Epoch [85/300], Step [64/172], Loss: 7.9531\n",
      "Epoch [85/300], Step [65/172], Loss: 22.0527\n",
      "Epoch [85/300], Step [66/172], Loss: 6.2927\n",
      "Epoch [85/300], Step [67/172], Loss: 28.7149\n",
      "Epoch [85/300], Step [68/172], Loss: 7.6124\n",
      "Epoch [85/300], Step [69/172], Loss: 68.2361\n",
      "Epoch [85/300], Step [70/172], Loss: 60.4290\n",
      "Epoch [85/300], Step [71/172], Loss: 55.1301\n",
      "Epoch [85/300], Step [72/172], Loss: 60.5044\n",
      "Epoch [85/300], Step [73/172], Loss: 64.6429\n",
      "Epoch [85/300], Step [74/172], Loss: 37.6978\n",
      "Epoch [85/300], Step [75/172], Loss: 33.6528\n",
      "Epoch [85/300], Step [76/172], Loss: 40.4512\n",
      "Epoch [85/300], Step [77/172], Loss: 62.2031\n",
      "Epoch [85/300], Step [78/172], Loss: 51.8219\n",
      "Epoch [85/300], Step [79/172], Loss: 50.3262\n",
      "Epoch [85/300], Step [80/172], Loss: 58.2335\n",
      "Epoch [85/300], Step [81/172], Loss: 44.0519\n",
      "Epoch [85/300], Step [82/172], Loss: 41.1897\n",
      "Epoch [85/300], Step [83/172], Loss: 51.6958\n",
      "Epoch [85/300], Step [84/172], Loss: 39.8247\n",
      "Epoch [85/300], Step [85/172], Loss: 43.5834\n",
      "Epoch [85/300], Step [86/172], Loss: 35.5328\n",
      "Epoch [85/300], Step [87/172], Loss: 29.7995\n",
      "Epoch [85/300], Step [88/172], Loss: 31.6718\n",
      "Epoch [85/300], Step [89/172], Loss: 28.3471\n",
      "Epoch [85/300], Step [90/172], Loss: 27.3606\n",
      "Epoch [85/300], Step [91/172], Loss: 30.8300\n",
      "Epoch [85/300], Step [92/172], Loss: 23.0031\n",
      "Epoch [85/300], Step [93/172], Loss: 22.6965\n",
      "Epoch [85/300], Step [94/172], Loss: 30.1504\n",
      "Epoch [85/300], Step [95/172], Loss: 24.2936\n",
      "Epoch [85/300], Step [96/172], Loss: 19.9116\n",
      "Epoch [85/300], Step [97/172], Loss: 26.9479\n",
      "Epoch [85/300], Step [98/172], Loss: 21.3138\n",
      "Epoch [85/300], Step [99/172], Loss: 18.9733\n",
      "Epoch [85/300], Step [100/172], Loss: 17.4927\n",
      "Epoch [85/300], Step [101/172], Loss: 19.0114\n",
      "Epoch [85/300], Step [102/172], Loss: 17.4459\n",
      "Epoch [85/300], Step [103/172], Loss: 15.7347\n",
      "Epoch [85/300], Step [104/172], Loss: 16.8987\n",
      "Epoch [85/300], Step [105/172], Loss: 19.2012\n",
      "Epoch [85/300], Step [106/172], Loss: 18.1630\n",
      "Epoch [85/300], Step [107/172], Loss: 16.2165\n",
      "Epoch [85/300], Step [108/172], Loss: 18.0984\n",
      "Epoch [85/300], Step [109/172], Loss: 19.5789\n",
      "Epoch [85/300], Step [110/172], Loss: 17.1930\n",
      "Epoch [85/300], Step [111/172], Loss: 15.6485\n",
      "Epoch [85/300], Step [112/172], Loss: 20.1810\n",
      "Epoch [85/300], Step [113/172], Loss: 16.3742\n",
      "Epoch [85/300], Step [114/172], Loss: 15.7479\n",
      "Epoch [85/300], Step [115/172], Loss: 23.4369\n",
      "Epoch [85/300], Step [116/172], Loss: 16.1243\n",
      "Epoch [85/300], Step [117/172], Loss: 13.5486\n",
      "Epoch [85/300], Step [118/172], Loss: 16.7104\n",
      "Epoch [85/300], Step [119/172], Loss: 16.2377\n",
      "Epoch [85/300], Step [120/172], Loss: 12.4919\n",
      "Epoch [85/300], Step [121/172], Loss: 12.6221\n",
      "Epoch [85/300], Step [122/172], Loss: 11.9527\n",
      "Epoch [85/300], Step [123/172], Loss: 11.5917\n",
      "Epoch [85/300], Step [124/172], Loss: 9.4405\n",
      "Epoch [85/300], Step [125/172], Loss: 14.1622\n",
      "Epoch [85/300], Step [126/172], Loss: 11.9808\n",
      "Epoch [85/300], Step [127/172], Loss: 14.0150\n",
      "Epoch [85/300], Step [128/172], Loss: 14.3668\n",
      "Epoch [85/300], Step [129/172], Loss: 10.2888\n",
      "Epoch [85/300], Step [130/172], Loss: 12.5144\n",
      "Epoch [85/300], Step [131/172], Loss: 10.3656\n",
      "Epoch [85/300], Step [132/172], Loss: 10.2901\n",
      "Epoch [85/300], Step [133/172], Loss: 11.4515\n",
      "Epoch [85/300], Step [134/172], Loss: 12.8707\n",
      "Epoch [85/300], Step [135/172], Loss: 9.9092\n",
      "Epoch [85/300], Step [136/172], Loss: 9.6174\n",
      "Epoch [85/300], Step [137/172], Loss: 11.3765\n",
      "Epoch [85/300], Step [138/172], Loss: 9.6050\n",
      "Epoch [85/300], Step [139/172], Loss: 10.8922\n",
      "Epoch [85/300], Step [140/172], Loss: 10.8350\n",
      "Epoch [85/300], Step [141/172], Loss: 13.4689\n",
      "Epoch [85/300], Step [142/172], Loss: 14.7807\n",
      "Epoch [85/300], Step [143/172], Loss: 10.1460\n",
      "Epoch [85/300], Step [144/172], Loss: 9.8197\n",
      "Epoch [85/300], Step [145/172], Loss: 10.7363\n",
      "Epoch [85/300], Step [146/172], Loss: 10.8447\n",
      "Epoch [85/300], Step [147/172], Loss: 6.4618\n",
      "Epoch [85/300], Step [148/172], Loss: 7.6061\n",
      "Epoch [85/300], Step [149/172], Loss: 9.3412\n",
      "Epoch [85/300], Step [150/172], Loss: 9.1522\n",
      "Epoch [85/300], Step [151/172], Loss: 8.2372\n",
      "Epoch [85/300], Step [152/172], Loss: 8.5232\n",
      "Epoch [85/300], Step [153/172], Loss: 8.0982\n",
      "Epoch [85/300], Step [154/172], Loss: 8.8009\n",
      "Epoch [85/300], Step [155/172], Loss: 7.9637\n",
      "Epoch [85/300], Step [156/172], Loss: 12.6743\n",
      "Epoch [85/300], Step [157/172], Loss: 11.0585\n",
      "Epoch [85/300], Step [158/172], Loss: 8.7120\n",
      "Epoch [85/300], Step [159/172], Loss: 10.1783\n",
      "Epoch [85/300], Step [160/172], Loss: 10.4511\n",
      "Epoch [85/300], Step [161/172], Loss: 7.8678\n",
      "Epoch [85/300], Step [162/172], Loss: 8.2775\n",
      "Epoch [85/300], Step [163/172], Loss: 7.3212\n",
      "Epoch [85/300], Step [164/172], Loss: 10.5635\n",
      "Epoch [85/300], Step [165/172], Loss: 7.1312\n",
      "Epoch [85/300], Step [166/172], Loss: 7.6252\n",
      "Epoch [85/300], Step [167/172], Loss: 9.0735\n",
      "Epoch [85/300], Step [168/172], Loss: 7.5766\n",
      "Epoch [85/300], Step [169/172], Loss: 7.8434\n",
      "Epoch [85/300], Step [170/172], Loss: 6.5495\n",
      "Epoch [85/300], Step [171/172], Loss: 6.6551\n",
      "Epoch [85/300], Step [172/172], Loss: 6.0034\n",
      "Epoch [86/300], Step [1/172], Loss: 87.6729\n",
      "Epoch [86/300], Step [2/172], Loss: 87.8997\n",
      "Epoch [86/300], Step [3/172], Loss: 82.8408\n",
      "Epoch [86/300], Step [4/172], Loss: 51.4382\n",
      "Epoch [86/300], Step [5/172], Loss: 72.0842\n",
      "Epoch [86/300], Step [6/172], Loss: 22.2742\n",
      "Epoch [86/300], Step [7/172], Loss: 28.9782\n",
      "Epoch [86/300], Step [8/172], Loss: 7.1516\n",
      "Epoch [86/300], Step [9/172], Loss: 45.7456\n",
      "Epoch [86/300], Step [10/172], Loss: 49.6620\n",
      "Epoch [86/300], Step [11/172], Loss: 91.4668\n",
      "Epoch [86/300], Step [12/172], Loss: 84.4720\n",
      "Epoch [86/300], Step [13/172], Loss: 42.2135\n",
      "Epoch [86/300], Step [14/172], Loss: 92.4046\n",
      "Epoch [86/300], Step [15/172], Loss: 79.0463\n",
      "Epoch [86/300], Step [16/172], Loss: 28.7868\n",
      "Epoch [86/300], Step [17/172], Loss: 59.1031\n",
      "Epoch [86/300], Step [18/172], Loss: 62.7411\n",
      "Epoch [86/300], Step [19/172], Loss: 82.9108\n",
      "Epoch [86/300], Step [20/172], Loss: 80.9372\n",
      "Epoch [86/300], Step [21/172], Loss: 95.2403\n",
      "Epoch [86/300], Step [22/172], Loss: 82.6161\n",
      "Epoch [86/300], Step [23/172], Loss: 6.1702\n",
      "Epoch [86/300], Step [24/172], Loss: 73.5926\n",
      "Epoch [86/300], Step [25/172], Loss: 48.3861\n",
      "Epoch [86/300], Step [26/172], Loss: 61.1617\n",
      "Epoch [86/300], Step [27/172], Loss: 81.0862\n",
      "Epoch [86/300], Step [28/172], Loss: 42.2317\n",
      "Epoch [86/300], Step [29/172], Loss: 32.4079\n",
      "Epoch [86/300], Step [30/172], Loss: 81.9350\n",
      "Epoch [86/300], Step [31/172], Loss: 45.1464\n",
      "Epoch [86/300], Step [32/172], Loss: 40.8296\n",
      "Epoch [86/300], Step [33/172], Loss: 72.4421\n",
      "Epoch [86/300], Step [34/172], Loss: 5.1785\n",
      "Epoch [86/300], Step [35/172], Loss: 21.4498\n",
      "Epoch [86/300], Step [36/172], Loss: 22.3545\n",
      "Epoch [86/300], Step [37/172], Loss: 18.3089\n",
      "Epoch [86/300], Step [38/172], Loss: 27.8728\n",
      "Epoch [86/300], Step [39/172], Loss: 50.7198\n",
      "Epoch [86/300], Step [40/172], Loss: 22.6014\n",
      "Epoch [86/300], Step [41/172], Loss: 39.7516\n",
      "Epoch [86/300], Step [42/172], Loss: 43.3646\n",
      "Epoch [86/300], Step [43/172], Loss: 27.5453\n",
      "Epoch [86/300], Step [44/172], Loss: 23.0217\n",
      "Epoch [86/300], Step [45/172], Loss: 22.0876\n",
      "Epoch [86/300], Step [46/172], Loss: 27.2760\n",
      "Epoch [86/300], Step [47/172], Loss: 53.3070\n",
      "Epoch [86/300], Step [48/172], Loss: 55.1087\n",
      "Epoch [86/300], Step [49/172], Loss: 20.8632\n",
      "Epoch [86/300], Step [50/172], Loss: 52.4706\n",
      "Epoch [86/300], Step [51/172], Loss: 7.5191\n",
      "Epoch [86/300], Step [52/172], Loss: 19.4384\n",
      "Epoch [86/300], Step [53/172], Loss: 25.5895\n",
      "Epoch [86/300], Step [54/172], Loss: 12.3839\n",
      "Epoch [86/300], Step [55/172], Loss: 13.0973\n",
      "Epoch [86/300], Step [56/172], Loss: 10.3547\n",
      "Epoch [86/300], Step [57/172], Loss: 25.5199\n",
      "Epoch [86/300], Step [58/172], Loss: 19.3302\n",
      "Epoch [86/300], Step [59/172], Loss: 32.3227\n",
      "Epoch [86/300], Step [60/172], Loss: 50.5601\n",
      "Epoch [86/300], Step [61/172], Loss: 10.0121\n",
      "Epoch [86/300], Step [62/172], Loss: 24.2855\n",
      "Epoch [86/300], Step [63/172], Loss: 8.9666\n",
      "Epoch [86/300], Step [64/172], Loss: 7.9688\n",
      "Epoch [86/300], Step [65/172], Loss: 21.9568\n",
      "Epoch [86/300], Step [66/172], Loss: 6.2886\n",
      "Epoch [86/300], Step [67/172], Loss: 28.7690\n",
      "Epoch [86/300], Step [68/172], Loss: 7.6378\n",
      "Epoch [86/300], Step [69/172], Loss: 67.8645\n",
      "Epoch [86/300], Step [70/172], Loss: 59.9839\n",
      "Epoch [86/300], Step [71/172], Loss: 54.8549\n",
      "Epoch [86/300], Step [72/172], Loss: 60.2674\n",
      "Epoch [86/300], Step [73/172], Loss: 64.3406\n",
      "Epoch [86/300], Step [74/172], Loss: 37.4313\n",
      "Epoch [86/300], Step [75/172], Loss: 33.5466\n",
      "Epoch [86/300], Step [76/172], Loss: 40.1202\n",
      "Epoch [86/300], Step [77/172], Loss: 62.1003\n",
      "Epoch [86/300], Step [78/172], Loss: 51.5356\n",
      "Epoch [86/300], Step [79/172], Loss: 50.1704\n",
      "Epoch [86/300], Step [80/172], Loss: 58.1195\n",
      "Epoch [86/300], Step [81/172], Loss: 43.8225\n",
      "Epoch [86/300], Step [82/172], Loss: 41.0017\n",
      "Epoch [86/300], Step [83/172], Loss: 51.5398\n",
      "Epoch [86/300], Step [84/172], Loss: 39.5885\n",
      "Epoch [86/300], Step [85/172], Loss: 43.2640\n",
      "Epoch [86/300], Step [86/172], Loss: 35.2817\n",
      "Epoch [86/300], Step [87/172], Loss: 29.6556\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/300], Step [88/172], Loss: 31.4979\n",
      "Epoch [86/300], Step [89/172], Loss: 28.0837\n",
      "Epoch [86/300], Step [90/172], Loss: 27.1873\n",
      "Epoch [86/300], Step [91/172], Loss: 30.6873\n",
      "Epoch [86/300], Step [92/172], Loss: 22.8345\n",
      "Epoch [86/300], Step [93/172], Loss: 22.5416\n",
      "Epoch [86/300], Step [94/172], Loss: 30.0327\n",
      "Epoch [86/300], Step [95/172], Loss: 24.1435\n",
      "Epoch [86/300], Step [96/172], Loss: 19.7656\n",
      "Epoch [86/300], Step [97/172], Loss: 26.8003\n",
      "Epoch [86/300], Step [98/172], Loss: 21.1304\n",
      "Epoch [86/300], Step [99/172], Loss: 18.8546\n",
      "Epoch [86/300], Step [100/172], Loss: 17.3314\n",
      "Epoch [86/300], Step [101/172], Loss: 18.8596\n",
      "Epoch [86/300], Step [102/172], Loss: 17.2873\n",
      "Epoch [86/300], Step [103/172], Loss: 15.5870\n",
      "Epoch [86/300], Step [104/172], Loss: 16.7859\n",
      "Epoch [86/300], Step [105/172], Loss: 19.1084\n",
      "Epoch [86/300], Step [106/172], Loss: 18.0706\n",
      "Epoch [86/300], Step [107/172], Loss: 16.1437\n",
      "Epoch [86/300], Step [108/172], Loss: 17.9636\n",
      "Epoch [86/300], Step [109/172], Loss: 19.4432\n",
      "Epoch [86/300], Step [110/172], Loss: 17.0619\n",
      "Epoch [86/300], Step [111/172], Loss: 15.5294\n",
      "Epoch [86/300], Step [112/172], Loss: 20.0192\n",
      "Epoch [86/300], Step [113/172], Loss: 16.2791\n",
      "Epoch [86/300], Step [114/172], Loss: 15.6100\n",
      "Epoch [86/300], Step [115/172], Loss: 23.3161\n",
      "Epoch [86/300], Step [116/172], Loss: 16.0110\n",
      "Epoch [86/300], Step [117/172], Loss: 13.4393\n",
      "Epoch [86/300], Step [118/172], Loss: 16.5807\n",
      "Epoch [86/300], Step [119/172], Loss: 16.1860\n",
      "Epoch [86/300], Step [120/172], Loss: 12.4057\n",
      "Epoch [86/300], Step [121/172], Loss: 12.4947\n",
      "Epoch [86/300], Step [122/172], Loss: 11.8176\n",
      "Epoch [86/300], Step [123/172], Loss: 11.5202\n",
      "Epoch [86/300], Step [124/172], Loss: 9.3610\n",
      "Epoch [86/300], Step [125/172], Loss: 14.0847\n",
      "Epoch [86/300], Step [126/172], Loss: 11.9001\n",
      "Epoch [86/300], Step [127/172], Loss: 13.8740\n",
      "Epoch [86/300], Step [128/172], Loss: 14.2300\n",
      "Epoch [86/300], Step [129/172], Loss: 10.1948\n",
      "Epoch [86/300], Step [130/172], Loss: 12.4399\n",
      "Epoch [86/300], Step [131/172], Loss: 10.2636\n",
      "Epoch [86/300], Step [132/172], Loss: 10.2002\n",
      "Epoch [86/300], Step [133/172], Loss: 11.3347\n",
      "Epoch [86/300], Step [134/172], Loss: 12.8068\n",
      "Epoch [86/300], Step [135/172], Loss: 9.8581\n",
      "Epoch [86/300], Step [136/172], Loss: 9.5351\n",
      "Epoch [86/300], Step [137/172], Loss: 11.2622\n",
      "Epoch [86/300], Step [138/172], Loss: 9.5191\n",
      "Epoch [86/300], Step [139/172], Loss: 10.8142\n",
      "Epoch [86/300], Step [140/172], Loss: 10.7539\n",
      "Epoch [86/300], Step [141/172], Loss: 13.3654\n",
      "Epoch [86/300], Step [142/172], Loss: 14.7108\n",
      "Epoch [86/300], Step [143/172], Loss: 10.0842\n",
      "Epoch [86/300], Step [144/172], Loss: 9.7511\n",
      "Epoch [86/300], Step [145/172], Loss: 10.6893\n",
      "Epoch [86/300], Step [146/172], Loss: 10.7724\n",
      "Epoch [86/300], Step [147/172], Loss: 6.3980\n",
      "Epoch [86/300], Step [148/172], Loss: 7.5417\n",
      "Epoch [86/300], Step [149/172], Loss: 9.2574\n",
      "Epoch [86/300], Step [150/172], Loss: 9.0613\n",
      "Epoch [86/300], Step [151/172], Loss: 8.1654\n",
      "Epoch [86/300], Step [152/172], Loss: 8.4772\n",
      "Epoch [86/300], Step [153/172], Loss: 8.0282\n",
      "Epoch [86/300], Step [154/172], Loss: 8.7081\n",
      "Epoch [86/300], Step [155/172], Loss: 7.8994\n",
      "Epoch [86/300], Step [156/172], Loss: 12.6531\n",
      "Epoch [86/300], Step [157/172], Loss: 10.9843\n",
      "Epoch [86/300], Step [158/172], Loss: 8.6385\n",
      "Epoch [86/300], Step [159/172], Loss: 10.1285\n",
      "Epoch [86/300], Step [160/172], Loss: 10.4113\n",
      "Epoch [86/300], Step [161/172], Loss: 7.8253\n",
      "Epoch [86/300], Step [162/172], Loss: 8.2313\n",
      "Epoch [86/300], Step [163/172], Loss: 7.2556\n",
      "Epoch [86/300], Step [164/172], Loss: 10.5019\n",
      "Epoch [86/300], Step [165/172], Loss: 7.0725\n",
      "Epoch [86/300], Step [166/172], Loss: 7.5634\n",
      "Epoch [86/300], Step [167/172], Loss: 9.0520\n",
      "Epoch [86/300], Step [168/172], Loss: 7.5441\n",
      "Epoch [86/300], Step [169/172], Loss: 7.8006\n",
      "Epoch [86/300], Step [170/172], Loss: 6.4923\n",
      "Epoch [86/300], Step [171/172], Loss: 6.6265\n",
      "Epoch [86/300], Step [172/172], Loss: 5.9750\n",
      "Epoch [87/300], Step [1/172], Loss: 87.3555\n",
      "Epoch [87/300], Step [2/172], Loss: 87.6404\n",
      "Epoch [87/300], Step [3/172], Loss: 82.3760\n",
      "Epoch [87/300], Step [4/172], Loss: 51.1943\n",
      "Epoch [87/300], Step [5/172], Loss: 71.8051\n",
      "Epoch [87/300], Step [6/172], Loss: 22.1436\n",
      "Epoch [87/300], Step [7/172], Loss: 29.0852\n",
      "Epoch [87/300], Step [8/172], Loss: 7.0828\n",
      "Epoch [87/300], Step [9/172], Loss: 45.6269\n",
      "Epoch [87/300], Step [10/172], Loss: 49.4985\n",
      "Epoch [87/300], Step [11/172], Loss: 91.0052\n",
      "Epoch [87/300], Step [12/172], Loss: 84.4128\n",
      "Epoch [87/300], Step [13/172], Loss: 42.2250\n",
      "Epoch [87/300], Step [14/172], Loss: 92.2351\n",
      "Epoch [87/300], Step [15/172], Loss: 78.7185\n",
      "Epoch [87/300], Step [16/172], Loss: 28.1935\n",
      "Epoch [87/300], Step [17/172], Loss: 59.0719\n",
      "Epoch [87/300], Step [18/172], Loss: 62.7526\n",
      "Epoch [87/300], Step [19/172], Loss: 83.0290\n",
      "Epoch [87/300], Step [20/172], Loss: 80.4492\n",
      "Epoch [87/300], Step [21/172], Loss: 95.2580\n",
      "Epoch [87/300], Step [22/172], Loss: 82.4495\n",
      "Epoch [87/300], Step [23/172], Loss: 5.9152\n",
      "Epoch [87/300], Step [24/172], Loss: 73.3613\n",
      "Epoch [87/300], Step [25/172], Loss: 48.2420\n",
      "Epoch [87/300], Step [26/172], Loss: 61.0764\n",
      "Epoch [87/300], Step [27/172], Loss: 80.8316\n",
      "Epoch [87/300], Step [28/172], Loss: 41.7529\n",
      "Epoch [87/300], Step [29/172], Loss: 31.7278\n",
      "Epoch [87/300], Step [30/172], Loss: 82.0424\n",
      "Epoch [87/300], Step [31/172], Loss: 45.0503\n",
      "Epoch [87/300], Step [32/172], Loss: 40.8370\n",
      "Epoch [87/300], Step [33/172], Loss: 72.3569\n",
      "Epoch [87/300], Step [34/172], Loss: 5.1384\n",
      "Epoch [87/300], Step [35/172], Loss: 21.1321\n",
      "Epoch [87/300], Step [36/172], Loss: 22.1387\n",
      "Epoch [87/300], Step [37/172], Loss: 18.2262\n",
      "Epoch [87/300], Step [38/172], Loss: 27.7592\n",
      "Epoch [87/300], Step [39/172], Loss: 50.5891\n",
      "Epoch [87/300], Step [40/172], Loss: 22.3905\n",
      "Epoch [87/300], Step [41/172], Loss: 39.6112\n",
      "Epoch [87/300], Step [42/172], Loss: 43.2013\n",
      "Epoch [87/300], Step [43/172], Loss: 27.4179\n",
      "Epoch [87/300], Step [44/172], Loss: 22.7930\n",
      "Epoch [87/300], Step [45/172], Loss: 21.9793\n",
      "Epoch [87/300], Step [46/172], Loss: 26.9199\n",
      "Epoch [87/300], Step [47/172], Loss: 52.9731\n",
      "Epoch [87/300], Step [48/172], Loss: 54.9834\n",
      "Epoch [87/300], Step [49/172], Loss: 20.6973\n",
      "Epoch [87/300], Step [50/172], Loss: 52.4298\n",
      "Epoch [87/300], Step [51/172], Loss: 7.4672\n",
      "Epoch [87/300], Step [52/172], Loss: 19.2767\n",
      "Epoch [87/300], Step [53/172], Loss: 25.3706\n",
      "Epoch [87/300], Step [54/172], Loss: 12.2937\n",
      "Epoch [87/300], Step [55/172], Loss: 13.0043\n",
      "Epoch [87/300], Step [56/172], Loss: 10.3129\n",
      "Epoch [87/300], Step [57/172], Loss: 25.1371\n",
      "Epoch [87/300], Step [58/172], Loss: 19.2046\n",
      "Epoch [87/300], Step [59/172], Loss: 32.1744\n",
      "Epoch [87/300], Step [60/172], Loss: 49.9990\n",
      "Epoch [87/300], Step [61/172], Loss: 9.9480\n",
      "Epoch [87/300], Step [62/172], Loss: 24.2329\n",
      "Epoch [87/300], Step [63/172], Loss: 8.9286\n",
      "Epoch [87/300], Step [64/172], Loss: 7.9857\n",
      "Epoch [87/300], Step [65/172], Loss: 21.8115\n",
      "Epoch [87/300], Step [66/172], Loss: 6.2587\n",
      "Epoch [87/300], Step [67/172], Loss: 28.7031\n",
      "Epoch [87/300], Step [68/172], Loss: 7.5109\n",
      "Epoch [87/300], Step [69/172], Loss: 67.2964\n",
      "Epoch [87/300], Step [70/172], Loss: 59.6849\n",
      "Epoch [87/300], Step [71/172], Loss: 54.6458\n",
      "Epoch [87/300], Step [72/172], Loss: 60.1427\n",
      "Epoch [87/300], Step [73/172], Loss: 64.2015\n",
      "Epoch [87/300], Step [74/172], Loss: 37.2451\n",
      "Epoch [87/300], Step [75/172], Loss: 33.5630\n",
      "Epoch [87/300], Step [76/172], Loss: 40.0076\n",
      "Epoch [87/300], Step [77/172], Loss: 62.1470\n",
      "Epoch [87/300], Step [78/172], Loss: 51.3562\n",
      "Epoch [87/300], Step [79/172], Loss: 50.1157\n",
      "Epoch [87/300], Step [80/172], Loss: 58.2500\n",
      "Epoch [87/300], Step [81/172], Loss: 43.7185\n",
      "Epoch [87/300], Step [82/172], Loss: 40.9450\n",
      "Epoch [87/300], Step [83/172], Loss: 51.4974\n",
      "Epoch [87/300], Step [84/172], Loss: 39.4719\n",
      "Epoch [87/300], Step [85/172], Loss: 43.0611\n",
      "Epoch [87/300], Step [86/172], Loss: 35.1602\n",
      "Epoch [87/300], Step [87/172], Loss: 29.5767\n",
      "Epoch [87/300], Step [88/172], Loss: 31.4039\n",
      "Epoch [87/300], Step [89/172], Loss: 27.9597\n",
      "Epoch [87/300], Step [90/172], Loss: 27.0731\n",
      "Epoch [87/300], Step [91/172], Loss: 30.6193\n",
      "Epoch [87/300], Step [92/172], Loss: 22.7263\n",
      "Epoch [87/300], Step [93/172], Loss: 22.4284\n",
      "Epoch [87/300], Step [94/172], Loss: 29.9642\n",
      "Epoch [87/300], Step [95/172], Loss: 24.0589\n",
      "Epoch [87/300], Step [96/172], Loss: 19.6832\n",
      "Epoch [87/300], Step [97/172], Loss: 26.7406\n",
      "Epoch [87/300], Step [98/172], Loss: 21.0092\n",
      "Epoch [87/300], Step [99/172], Loss: 18.7737\n",
      "Epoch [87/300], Step [100/172], Loss: 17.2300\n",
      "Epoch [87/300], Step [101/172], Loss: 18.7498\n",
      "Epoch [87/300], Step [102/172], Loss: 17.2253\n",
      "Epoch [87/300], Step [103/172], Loss: 15.4699\n",
      "Epoch [87/300], Step [104/172], Loss: 16.7218\n",
      "Epoch [87/300], Step [105/172], Loss: 19.0825\n",
      "Epoch [87/300], Step [106/172], Loss: 18.0023\n",
      "Epoch [87/300], Step [107/172], Loss: 16.1115\n",
      "Epoch [87/300], Step [108/172], Loss: 17.8733\n",
      "Epoch [87/300], Step [109/172], Loss: 19.3429\n",
      "Epoch [87/300], Step [110/172], Loss: 16.9817\n",
      "Epoch [87/300], Step [111/172], Loss: 15.4368\n",
      "Epoch [87/300], Step [112/172], Loss: 19.9140\n",
      "Epoch [87/300], Step [113/172], Loss: 16.1885\n",
      "Epoch [87/300], Step [114/172], Loss: 15.4998\n",
      "Epoch [87/300], Step [115/172], Loss: 23.2204\n",
      "Epoch [87/300], Step [116/172], Loss: 15.9259\n",
      "Epoch [87/300], Step [117/172], Loss: 13.3425\n",
      "Epoch [87/300], Step [118/172], Loss: 16.4677\n",
      "Epoch [87/300], Step [119/172], Loss: 16.1600\n",
      "Epoch [87/300], Step [120/172], Loss: 12.3427\n",
      "Epoch [87/300], Step [121/172], Loss: 12.3957\n",
      "Epoch [87/300], Step [122/172], Loss: 11.7495\n",
      "Epoch [87/300], Step [123/172], Loss: 11.4601\n",
      "Epoch [87/300], Step [124/172], Loss: 9.3035\n",
      "Epoch [87/300], Step [125/172], Loss: 14.0201\n",
      "Epoch [87/300], Step [126/172], Loss: 11.8376\n",
      "Epoch [87/300], Step [127/172], Loss: 13.7614\n",
      "Epoch [87/300], Step [128/172], Loss: 14.1157\n",
      "Epoch [87/300], Step [129/172], Loss: 10.1257\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/300], Step [130/172], Loss: 12.3825\n",
      "Epoch [87/300], Step [131/172], Loss: 10.1844\n",
      "Epoch [87/300], Step [132/172], Loss: 10.1332\n",
      "Epoch [87/300], Step [133/172], Loss: 11.2552\n",
      "Epoch [87/300], Step [134/172], Loss: 12.7673\n",
      "Epoch [87/300], Step [135/172], Loss: 9.8156\n",
      "Epoch [87/300], Step [136/172], Loss: 9.4821\n",
      "Epoch [87/300], Step [137/172], Loss: 11.1920\n",
      "Epoch [87/300], Step [138/172], Loss: 9.4437\n",
      "Epoch [87/300], Step [139/172], Loss: 10.7691\n",
      "Epoch [87/300], Step [140/172], Loss: 10.7005\n",
      "Epoch [87/300], Step [141/172], Loss: 13.2820\n",
      "Epoch [87/300], Step [142/172], Loss: 14.6605\n",
      "Epoch [87/300], Step [143/172], Loss: 10.0596\n",
      "Epoch [87/300], Step [144/172], Loss: 9.7056\n",
      "Epoch [87/300], Step [145/172], Loss: 10.6660\n",
      "Epoch [87/300], Step [146/172], Loss: 10.7383\n",
      "Epoch [87/300], Step [147/172], Loss: 6.3486\n",
      "Epoch [87/300], Step [148/172], Loss: 7.4944\n",
      "Epoch [87/300], Step [149/172], Loss: 9.1909\n",
      "Epoch [87/300], Step [150/172], Loss: 9.0055\n",
      "Epoch [87/300], Step [151/172], Loss: 8.1193\n",
      "Epoch [87/300], Step [152/172], Loss: 8.4515\n",
      "Epoch [87/300], Step [153/172], Loss: 7.9847\n",
      "Epoch [87/300], Step [154/172], Loss: 8.6573\n",
      "Epoch [87/300], Step [155/172], Loss: 7.8579\n",
      "Epoch [87/300], Step [156/172], Loss: 12.6654\n",
      "Epoch [87/300], Step [157/172], Loss: 10.9575\n",
      "Epoch [87/300], Step [158/172], Loss: 8.5938\n",
      "Epoch [87/300], Step [159/172], Loss: 10.1277\n",
      "Epoch [87/300], Step [160/172], Loss: 10.4085\n",
      "Epoch [87/300], Step [161/172], Loss: 7.8008\n",
      "Epoch [87/300], Step [162/172], Loss: 8.1926\n",
      "Epoch [87/300], Step [163/172], Loss: 7.2144\n",
      "Epoch [87/300], Step [164/172], Loss: 10.4671\n",
      "Epoch [87/300], Step [165/172], Loss: 7.0390\n",
      "Epoch [87/300], Step [166/172], Loss: 7.5296\n",
      "Epoch [87/300], Step [167/172], Loss: 9.0480\n",
      "Epoch [87/300], Step [168/172], Loss: 7.5299\n",
      "Epoch [87/300], Step [169/172], Loss: 7.7709\n",
      "Epoch [87/300], Step [170/172], Loss: 6.4474\n",
      "Epoch [87/300], Step [171/172], Loss: 6.6097\n",
      "Epoch [87/300], Step [172/172], Loss: 5.9564\n",
      "Epoch [88/300], Step [1/172], Loss: 86.9000\n",
      "Epoch [88/300], Step [2/172], Loss: 87.1910\n",
      "Epoch [88/300], Step [3/172], Loss: 81.6616\n",
      "Epoch [88/300], Step [4/172], Loss: 50.8306\n",
      "Epoch [88/300], Step [5/172], Loss: 71.4020\n",
      "Epoch [88/300], Step [6/172], Loss: 21.9524\n",
      "Epoch [88/300], Step [7/172], Loss: 29.0398\n",
      "Epoch [88/300], Step [8/172], Loss: 7.0304\n",
      "Epoch [88/300], Step [9/172], Loss: 45.4633\n",
      "Epoch [88/300], Step [10/172], Loss: 49.2095\n",
      "Epoch [88/300], Step [11/172], Loss: 90.3619\n",
      "Epoch [88/300], Step [12/172], Loss: 84.2677\n",
      "Epoch [88/300], Step [13/172], Loss: 42.0682\n",
      "Epoch [88/300], Step [14/172], Loss: 91.5425\n",
      "Epoch [88/300], Step [15/172], Loss: 78.0761\n",
      "Epoch [88/300], Step [16/172], Loss: 27.2331\n",
      "Epoch [88/300], Step [17/172], Loss: 58.7557\n",
      "Epoch [88/300], Step [18/172], Loss: 62.5683\n",
      "Epoch [88/300], Step [19/172], Loss: 82.8757\n",
      "Epoch [88/300], Step [20/172], Loss: 79.3722\n",
      "Epoch [88/300], Step [21/172], Loss: 94.9097\n",
      "Epoch [88/300], Step [22/172], Loss: 81.8598\n",
      "Epoch [88/300], Step [23/172], Loss: 5.7797\n",
      "Epoch [88/300], Step [24/172], Loss: 73.0394\n",
      "Epoch [88/300], Step [25/172], Loss: 47.9352\n",
      "Epoch [88/300], Step [26/172], Loss: 60.8312\n",
      "Epoch [88/300], Step [27/172], Loss: 80.1679\n",
      "Epoch [88/300], Step [28/172], Loss: 41.2800\n",
      "Epoch [88/300], Step [29/172], Loss: 31.0806\n",
      "Epoch [88/300], Step [30/172], Loss: 82.1968\n",
      "Epoch [88/300], Step [31/172], Loss: 44.9968\n",
      "Epoch [88/300], Step [32/172], Loss: 40.7999\n",
      "Epoch [88/300], Step [33/172], Loss: 72.1979\n",
      "Epoch [88/300], Step [34/172], Loss: 5.0779\n",
      "Epoch [88/300], Step [35/172], Loss: 20.5248\n",
      "Epoch [88/300], Step [36/172], Loss: 22.0376\n",
      "Epoch [88/300], Step [37/172], Loss: 18.2184\n",
      "Epoch [88/300], Step [38/172], Loss: 27.8414\n",
      "Epoch [88/300], Step [39/172], Loss: 50.4422\n",
      "Epoch [88/300], Step [40/172], Loss: 22.3198\n",
      "Epoch [88/300], Step [41/172], Loss: 39.5310\n",
      "Epoch [88/300], Step [42/172], Loss: 43.2194\n",
      "Epoch [88/300], Step [43/172], Loss: 27.4692\n",
      "Epoch [88/300], Step [44/172], Loss: 22.7513\n",
      "Epoch [88/300], Step [45/172], Loss: 22.0246\n",
      "Epoch [88/300], Step [46/172], Loss: 26.7553\n",
      "Epoch [88/300], Step [47/172], Loss: 52.8167\n",
      "Epoch [88/300], Step [48/172], Loss: 54.6564\n",
      "Epoch [88/300], Step [49/172], Loss: 20.7336\n",
      "Epoch [88/300], Step [50/172], Loss: 52.4200\n",
      "Epoch [88/300], Step [51/172], Loss: 7.4723\n",
      "Epoch [88/300], Step [52/172], Loss: 19.2690\n",
      "Epoch [88/300], Step [53/172], Loss: 25.3280\n",
      "Epoch [88/300], Step [54/172], Loss: 12.2859\n",
      "Epoch [88/300], Step [55/172], Loss: 12.9541\n",
      "Epoch [88/300], Step [56/172], Loss: 10.3515\n",
      "Epoch [88/300], Step [57/172], Loss: 24.9322\n",
      "Epoch [88/300], Step [58/172], Loss: 19.1198\n",
      "Epoch [88/300], Step [59/172], Loss: 32.1370\n",
      "Epoch [88/300], Step [60/172], Loss: 50.0278\n",
      "Epoch [88/300], Step [61/172], Loss: 9.8987\n",
      "Epoch [88/300], Step [62/172], Loss: 24.2246\n",
      "Epoch [88/300], Step [63/172], Loss: 8.9397\n",
      "Epoch [88/300], Step [64/172], Loss: 8.0185\n",
      "Epoch [88/300], Step [65/172], Loss: 21.7514\n",
      "Epoch [88/300], Step [66/172], Loss: 6.2723\n",
      "Epoch [88/300], Step [67/172], Loss: 28.7752\n",
      "Epoch [88/300], Step [68/172], Loss: 7.4865\n",
      "Epoch [88/300], Step [69/172], Loss: 66.9897\n",
      "Epoch [88/300], Step [70/172], Loss: 59.4534\n",
      "Epoch [88/300], Step [71/172], Loss: 54.4692\n",
      "Epoch [88/300], Step [72/172], Loss: 59.9891\n",
      "Epoch [88/300], Step [73/172], Loss: 63.9965\n",
      "Epoch [88/300], Step [74/172], Loss: 37.0304\n",
      "Epoch [88/300], Step [75/172], Loss: 33.5113\n",
      "Epoch [88/300], Step [76/172], Loss: 39.8098\n",
      "Epoch [88/300], Step [77/172], Loss: 62.0942\n",
      "Epoch [88/300], Step [78/172], Loss: 51.1598\n",
      "Epoch [88/300], Step [79/172], Loss: 49.9564\n",
      "Epoch [88/300], Step [80/172], Loss: 58.2494\n",
      "Epoch [88/300], Step [81/172], Loss: 43.5059\n",
      "Epoch [88/300], Step [82/172], Loss: 40.8595\n",
      "Epoch [88/300], Step [83/172], Loss: 51.4136\n",
      "Epoch [88/300], Step [84/172], Loss: 39.3456\n",
      "Epoch [88/300], Step [85/172], Loss: 42.8835\n",
      "Epoch [88/300], Step [86/172], Loss: 35.0301\n",
      "Epoch [88/300], Step [87/172], Loss: 29.4678\n",
      "Epoch [88/300], Step [88/172], Loss: 31.2969\n",
      "Epoch [88/300], Step [89/172], Loss: 27.8046\n",
      "Epoch [88/300], Step [90/172], Loss: 26.9540\n",
      "Epoch [88/300], Step [91/172], Loss: 30.5147\n",
      "Epoch [88/300], Step [92/172], Loss: 22.6000\n",
      "Epoch [88/300], Step [93/172], Loss: 22.2835\n",
      "Epoch [88/300], Step [94/172], Loss: 29.8800\n",
      "Epoch [88/300], Step [95/172], Loss: 23.9455\n",
      "Epoch [88/300], Step [96/172], Loss: 19.5846\n",
      "Epoch [88/300], Step [97/172], Loss: 26.6521\n",
      "Epoch [88/300], Step [98/172], Loss: 20.8571\n",
      "Epoch [88/300], Step [99/172], Loss: 18.6752\n",
      "Epoch [88/300], Step [100/172], Loss: 17.1142\n",
      "Epoch [88/300], Step [101/172], Loss: 18.6117\n",
      "Epoch [88/300], Step [102/172], Loss: 17.1362\n",
      "Epoch [88/300], Step [103/172], Loss: 15.3279\n",
      "Epoch [88/300], Step [104/172], Loss: 16.6304\n",
      "Epoch [88/300], Step [105/172], Loss: 18.9837\n",
      "Epoch [88/300], Step [106/172], Loss: 17.8867\n",
      "Epoch [88/300], Step [107/172], Loss: 16.0631\n",
      "Epoch [88/300], Step [108/172], Loss: 17.7796\n",
      "Epoch [88/300], Step [109/172], Loss: 19.2078\n",
      "Epoch [88/300], Step [110/172], Loss: 16.9020\n",
      "Epoch [88/300], Step [111/172], Loss: 15.3281\n",
      "Epoch [88/300], Step [112/172], Loss: 19.7630\n",
      "Epoch [88/300], Step [113/172], Loss: 16.0336\n",
      "Epoch [88/300], Step [114/172], Loss: 15.3531\n",
      "Epoch [88/300], Step [115/172], Loss: 23.0650\n",
      "Epoch [88/300], Step [116/172], Loss: 15.7944\n",
      "Epoch [88/300], Step [117/172], Loss: 13.2175\n",
      "Epoch [88/300], Step [118/172], Loss: 16.3286\n",
      "Epoch [88/300], Step [119/172], Loss: 16.1115\n",
      "Epoch [88/300], Step [120/172], Loss: 12.2393\n",
      "Epoch [88/300], Step [121/172], Loss: 12.2925\n",
      "Epoch [88/300], Step [122/172], Loss: 11.7616\n",
      "Epoch [88/300], Step [123/172], Loss: 11.3605\n",
      "Epoch [88/300], Step [124/172], Loss: 9.2278\n",
      "Epoch [88/300], Step [125/172], Loss: 13.9415\n",
      "Epoch [88/300], Step [126/172], Loss: 11.7523\n",
      "Epoch [88/300], Step [127/172], Loss: 13.6403\n",
      "Epoch [88/300], Step [128/172], Loss: 13.9640\n",
      "Epoch [88/300], Step [129/172], Loss: 10.0382\n",
      "Epoch [88/300], Step [130/172], Loss: 12.2977\n",
      "Epoch [88/300], Step [131/172], Loss: 10.1129\n",
      "Epoch [88/300], Step [132/172], Loss: 10.0481\n",
      "Epoch [88/300], Step [133/172], Loss: 11.2112\n",
      "Epoch [88/300], Step [134/172], Loss: 12.7369\n",
      "Epoch [88/300], Step [135/172], Loss: 9.7617\n",
      "Epoch [88/300], Step [136/172], Loss: 9.4200\n",
      "Epoch [88/300], Step [137/172], Loss: 11.1338\n",
      "Epoch [88/300], Step [138/172], Loss: 9.3821\n",
      "Epoch [88/300], Step [139/172], Loss: 10.7081\n",
      "Epoch [88/300], Step [140/172], Loss: 10.6341\n",
      "Epoch [88/300], Step [141/172], Loss: 13.1655\n",
      "Epoch [88/300], Step [142/172], Loss: 14.6299\n",
      "Epoch [88/300], Step [143/172], Loss: 10.0171\n",
      "Epoch [88/300], Step [144/172], Loss: 9.6659\n",
      "Epoch [88/300], Step [145/172], Loss: 10.6122\n",
      "Epoch [88/300], Step [146/172], Loss: 10.6895\n",
      "Epoch [88/300], Step [147/172], Loss: 6.2944\n",
      "Epoch [88/300], Step [148/172], Loss: 7.4407\n",
      "Epoch [88/300], Step [149/172], Loss: 9.1186\n",
      "Epoch [88/300], Step [150/172], Loss: 8.9293\n",
      "Epoch [88/300], Step [151/172], Loss: 8.0494\n",
      "Epoch [88/300], Step [152/172], Loss: 8.4113\n",
      "Epoch [88/300], Step [153/172], Loss: 7.9218\n",
      "Epoch [88/300], Step [154/172], Loss: 8.6014\n",
      "Epoch [88/300], Step [155/172], Loss: 7.7902\n",
      "Epoch [88/300], Step [156/172], Loss: 12.7085\n",
      "Epoch [88/300], Step [157/172], Loss: 10.9312\n",
      "Epoch [88/300], Step [158/172], Loss: 8.5578\n",
      "Epoch [88/300], Step [159/172], Loss: 10.0833\n",
      "Epoch [88/300], Step [160/172], Loss: 10.4337\n",
      "Epoch [88/300], Step [161/172], Loss: 7.7509\n",
      "Epoch [88/300], Step [162/172], Loss: 8.1535\n",
      "Epoch [88/300], Step [163/172], Loss: 7.1622\n",
      "Epoch [88/300], Step [164/172], Loss: 10.4316\n",
      "Epoch [88/300], Step [165/172], Loss: 7.0003\n",
      "Epoch [88/300], Step [166/172], Loss: 7.4862\n",
      "Epoch [88/300], Step [167/172], Loss: 9.0459\n",
      "Epoch [88/300], Step [168/172], Loss: 7.5018\n",
      "Epoch [88/300], Step [169/172], Loss: 7.7333\n",
      "Epoch [88/300], Step [170/172], Loss: 6.3956\n",
      "Epoch [88/300], Step [171/172], Loss: 6.5841\n",
      "Epoch [88/300], Step [172/172], Loss: 5.9069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/300], Step [1/172], Loss: 86.3919\n",
      "Epoch [89/300], Step [2/172], Loss: 86.5212\n",
      "Epoch [89/300], Step [3/172], Loss: 81.1573\n",
      "Epoch [89/300], Step [4/172], Loss: 50.4247\n",
      "Epoch [89/300], Step [5/172], Loss: 71.1766\n",
      "Epoch [89/300], Step [6/172], Loss: 21.6504\n",
      "Epoch [89/300], Step [7/172], Loss: 28.5007\n",
      "Epoch [89/300], Step [8/172], Loss: 6.8438\n",
      "Epoch [89/300], Step [9/172], Loss: 45.2246\n",
      "Epoch [89/300], Step [10/172], Loss: 48.9563\n",
      "Epoch [89/300], Step [11/172], Loss: 89.8931\n",
      "Epoch [89/300], Step [12/172], Loss: 84.2355\n",
      "Epoch [89/300], Step [13/172], Loss: 42.0555\n",
      "Epoch [89/300], Step [14/172], Loss: 91.3416\n",
      "Epoch [89/300], Step [15/172], Loss: 77.7452\n",
      "Epoch [89/300], Step [16/172], Loss: 26.5793\n",
      "Epoch [89/300], Step [17/172], Loss: 58.6984\n",
      "Epoch [89/300], Step [18/172], Loss: 62.5459\n",
      "Epoch [89/300], Step [19/172], Loss: 82.9662\n",
      "Epoch [89/300], Step [20/172], Loss: 78.8963\n",
      "Epoch [89/300], Step [21/172], Loss: 94.8921\n",
      "Epoch [89/300], Step [22/172], Loss: 81.5719\n",
      "Epoch [89/300], Step [23/172], Loss: 5.5269\n",
      "Epoch [89/300], Step [24/172], Loss: 72.8703\n",
      "Epoch [89/300], Step [25/172], Loss: 47.8913\n",
      "Epoch [89/300], Step [26/172], Loss: 60.6199\n",
      "Epoch [89/300], Step [27/172], Loss: 79.8624\n",
      "Epoch [89/300], Step [28/172], Loss: 40.9197\n",
      "Epoch [89/300], Step [29/172], Loss: 30.6094\n",
      "Epoch [89/300], Step [30/172], Loss: 82.1607\n",
      "Epoch [89/300], Step [31/172], Loss: 45.0567\n",
      "Epoch [89/300], Step [32/172], Loss: 40.8889\n",
      "Epoch [89/300], Step [33/172], Loss: 72.2350\n",
      "Epoch [89/300], Step [34/172], Loss: 5.0927\n",
      "Epoch [89/300], Step [35/172], Loss: 19.9612\n",
      "Epoch [89/300], Step [36/172], Loss: 21.9402\n",
      "Epoch [89/300], Step [37/172], Loss: 18.2070\n",
      "Epoch [89/300], Step [38/172], Loss: 27.8330\n",
      "Epoch [89/300], Step [39/172], Loss: 50.1092\n",
      "Epoch [89/300], Step [40/172], Loss: 22.2035\n",
      "Epoch [89/300], Step [41/172], Loss: 39.4459\n",
      "Epoch [89/300], Step [42/172], Loss: 43.3395\n",
      "Epoch [89/300], Step [43/172], Loss: 27.5171\n",
      "Epoch [89/300], Step [44/172], Loss: 22.6572\n",
      "Epoch [89/300], Step [45/172], Loss: 22.0987\n",
      "Epoch [89/300], Step [46/172], Loss: 26.4033\n",
      "Epoch [89/300], Step [47/172], Loss: 52.5945\n",
      "Epoch [89/300], Step [48/172], Loss: 54.1069\n",
      "Epoch [89/300], Step [49/172], Loss: 20.7901\n",
      "Epoch [89/300], Step [50/172], Loss: 52.3244\n",
      "Epoch [89/300], Step [51/172], Loss: 7.5033\n",
      "Epoch [89/300], Step [52/172], Loss: 19.2830\n",
      "Epoch [89/300], Step [53/172], Loss: 25.2367\n",
      "Epoch [89/300], Step [54/172], Loss: 12.3850\n",
      "Epoch [89/300], Step [55/172], Loss: 12.9863\n",
      "Epoch [89/300], Step [56/172], Loss: 10.3783\n",
      "Epoch [89/300], Step [57/172], Loss: 24.5199\n",
      "Epoch [89/300], Step [58/172], Loss: 19.0622\n",
      "Epoch [89/300], Step [59/172], Loss: 31.9027\n",
      "Epoch [89/300], Step [60/172], Loss: 49.3399\n",
      "Epoch [89/300], Step [61/172], Loss: 9.8196\n",
      "Epoch [89/300], Step [62/172], Loss: 24.1803\n",
      "Epoch [89/300], Step [63/172], Loss: 8.8980\n",
      "Epoch [89/300], Step [64/172], Loss: 8.0391\n",
      "Epoch [89/300], Step [65/172], Loss: 21.6379\n",
      "Epoch [89/300], Step [66/172], Loss: 6.2285\n",
      "Epoch [89/300], Step [67/172], Loss: 28.7460\n",
      "Epoch [89/300], Step [68/172], Loss: 7.5054\n",
      "Epoch [89/300], Step [69/172], Loss: 66.6956\n",
      "Epoch [89/300], Step [70/172], Loss: 59.2430\n",
      "Epoch [89/300], Step [71/172], Loss: 54.4098\n",
      "Epoch [89/300], Step [72/172], Loss: 60.0139\n",
      "Epoch [89/300], Step [73/172], Loss: 63.9466\n",
      "Epoch [89/300], Step [74/172], Loss: 37.0082\n",
      "Epoch [89/300], Step [75/172], Loss: 33.6410\n",
      "Epoch [89/300], Step [76/172], Loss: 39.7528\n",
      "Epoch [89/300], Step [77/172], Loss: 62.3147\n",
      "Epoch [89/300], Step [78/172], Loss: 51.1944\n",
      "Epoch [89/300], Step [79/172], Loss: 50.0655\n",
      "Epoch [89/300], Step [80/172], Loss: 58.5290\n",
      "Epoch [89/300], Step [81/172], Loss: 43.5983\n",
      "Epoch [89/300], Step [82/172], Loss: 40.8754\n",
      "Epoch [89/300], Step [83/172], Loss: 51.5287\n",
      "Epoch [89/300], Step [84/172], Loss: 39.4160\n",
      "Epoch [89/300], Step [85/172], Loss: 42.9585\n",
      "Epoch [89/300], Step [86/172], Loss: 34.9939\n",
      "Epoch [89/300], Step [87/172], Loss: 29.5355\n",
      "Epoch [89/300], Step [88/172], Loss: 31.3113\n",
      "Epoch [89/300], Step [89/172], Loss: 27.8001\n",
      "Epoch [89/300], Step [90/172], Loss: 26.8938\n",
      "Epoch [89/300], Step [91/172], Loss: 30.5209\n",
      "Epoch [89/300], Step [92/172], Loss: 22.5459\n",
      "Epoch [89/300], Step [93/172], Loss: 22.2831\n",
      "Epoch [89/300], Step [94/172], Loss: 29.9568\n",
      "Epoch [89/300], Step [95/172], Loss: 23.9010\n",
      "Epoch [89/300], Step [96/172], Loss: 19.5559\n",
      "Epoch [89/300], Step [97/172], Loss: 26.6327\n",
      "Epoch [89/300], Step [98/172], Loss: 20.7919\n",
      "Epoch [89/300], Step [99/172], Loss: 18.6423\n",
      "Epoch [89/300], Step [100/172], Loss: 17.0475\n",
      "Epoch [89/300], Step [101/172], Loss: 18.5392\n",
      "Epoch [89/300], Step [102/172], Loss: 17.0223\n",
      "Epoch [89/300], Step [103/172], Loss: 15.2529\n",
      "Epoch [89/300], Step [104/172], Loss: 16.5810\n",
      "Epoch [89/300], Step [105/172], Loss: 18.9013\n",
      "Epoch [89/300], Step [106/172], Loss: 17.8727\n",
      "Epoch [89/300], Step [107/172], Loss: 16.0481\n",
      "Epoch [89/300], Step [108/172], Loss: 17.6917\n",
      "Epoch [89/300], Step [109/172], Loss: 19.1382\n",
      "Epoch [89/300], Step [110/172], Loss: 16.8321\n",
      "Epoch [89/300], Step [111/172], Loss: 15.2466\n",
      "Epoch [89/300], Step [112/172], Loss: 19.6972\n",
      "Epoch [89/300], Step [113/172], Loss: 15.9941\n",
      "Epoch [89/300], Step [114/172], Loss: 15.2749\n",
      "Epoch [89/300], Step [115/172], Loss: 23.0505\n",
      "Epoch [89/300], Step [116/172], Loss: 15.7177\n",
      "Epoch [89/300], Step [117/172], Loss: 13.1673\n",
      "Epoch [89/300], Step [118/172], Loss: 16.3425\n",
      "Epoch [89/300], Step [119/172], Loss: 16.0963\n",
      "Epoch [89/300], Step [120/172], Loss: 12.1930\n",
      "Epoch [89/300], Step [121/172], Loss: 12.2057\n",
      "Epoch [89/300], Step [122/172], Loss: 11.6287\n",
      "Epoch [89/300], Step [123/172], Loss: 11.3447\n",
      "Epoch [89/300], Step [124/172], Loss: 9.1681\n",
      "Epoch [89/300], Step [125/172], Loss: 13.8956\n",
      "Epoch [89/300], Step [126/172], Loss: 11.6781\n",
      "Epoch [89/300], Step [127/172], Loss: 13.5181\n",
      "Epoch [89/300], Step [128/172], Loss: 13.8449\n",
      "Epoch [89/300], Step [129/172], Loss: 9.9662\n",
      "Epoch [89/300], Step [130/172], Loss: 12.2570\n",
      "Epoch [89/300], Step [131/172], Loss: 10.0166\n",
      "Epoch [89/300], Step [132/172], Loss: 9.9796\n",
      "Epoch [89/300], Step [133/172], Loss: 11.0946\n",
      "Epoch [89/300], Step [134/172], Loss: 12.6924\n",
      "Epoch [89/300], Step [135/172], Loss: 9.7117\n",
      "Epoch [89/300], Step [136/172], Loss: 9.3432\n",
      "Epoch [89/300], Step [137/172], Loss: 11.0445\n",
      "Epoch [89/300], Step [138/172], Loss: 9.3021\n",
      "Epoch [89/300], Step [139/172], Loss: 10.6423\n",
      "Epoch [89/300], Step [140/172], Loss: 10.5692\n",
      "Epoch [89/300], Step [141/172], Loss: 13.0860\n",
      "Epoch [89/300], Step [142/172], Loss: 14.5368\n",
      "Epoch [89/300], Step [143/172], Loss: 9.9928\n",
      "Epoch [89/300], Step [144/172], Loss: 9.6094\n",
      "Epoch [89/300], Step [145/172], Loss: 10.5864\n",
      "Epoch [89/300], Step [146/172], Loss: 10.6215\n",
      "Epoch [89/300], Step [147/172], Loss: 6.2464\n",
      "Epoch [89/300], Step [148/172], Loss: 7.3944\n",
      "Epoch [89/300], Step [149/172], Loss: 9.0510\n",
      "Epoch [89/300], Step [150/172], Loss: 8.8652\n",
      "Epoch [89/300], Step [151/172], Loss: 8.0049\n",
      "Epoch [89/300], Step [152/172], Loss: 8.3753\n",
      "Epoch [89/300], Step [153/172], Loss: 7.8712\n",
      "Epoch [89/300], Step [154/172], Loss: 8.5657\n",
      "Epoch [89/300], Step [155/172], Loss: 7.7235\n",
      "Epoch [89/300], Step [156/172], Loss: 12.6820\n",
      "Epoch [89/300], Step [157/172], Loss: 10.8915\n",
      "Epoch [89/300], Step [158/172], Loss: 8.4968\n",
      "Epoch [89/300], Step [159/172], Loss: 10.0509\n",
      "Epoch [89/300], Step [160/172], Loss: 10.3956\n",
      "Epoch [89/300], Step [161/172], Loss: 7.7085\n",
      "Epoch [89/300], Step [162/172], Loss: 8.1100\n",
      "Epoch [89/300], Step [163/172], Loss: 7.1305\n",
      "Epoch [89/300], Step [164/172], Loss: 10.3464\n",
      "Epoch [89/300], Step [165/172], Loss: 6.9636\n",
      "Epoch [89/300], Step [166/172], Loss: 7.4174\n",
      "Epoch [89/300], Step [167/172], Loss: 9.0527\n",
      "Epoch [89/300], Step [168/172], Loss: 7.4788\n",
      "Epoch [89/300], Step [169/172], Loss: 7.6934\n",
      "Epoch [89/300], Step [170/172], Loss: 6.3498\n",
      "Epoch [89/300], Step [171/172], Loss: 6.5838\n",
      "Epoch [89/300], Step [172/172], Loss: 5.8629\n",
      "Epoch [90/300], Step [1/172], Loss: 85.8132\n",
      "Epoch [90/300], Step [2/172], Loss: 85.8094\n",
      "Epoch [90/300], Step [3/172], Loss: 80.5871\n",
      "Epoch [90/300], Step [4/172], Loss: 50.0294\n",
      "Epoch [90/300], Step [5/172], Loss: 70.6998\n",
      "Epoch [90/300], Step [6/172], Loss: 21.5785\n",
      "Epoch [90/300], Step [7/172], Loss: 28.8956\n",
      "Epoch [90/300], Step [8/172], Loss: 7.0419\n",
      "Epoch [90/300], Step [9/172], Loss: 45.1314\n",
      "Epoch [90/300], Step [10/172], Loss: 48.8186\n",
      "Epoch [90/300], Step [11/172], Loss: 89.4756\n",
      "Epoch [90/300], Step [12/172], Loss: 84.2867\n",
      "Epoch [90/300], Step [13/172], Loss: 42.0950\n",
      "Epoch [90/300], Step [14/172], Loss: 91.2378\n",
      "Epoch [90/300], Step [15/172], Loss: 77.5122\n",
      "Epoch [90/300], Step [16/172], Loss: 26.0651\n",
      "Epoch [90/300], Step [17/172], Loss: 58.6683\n",
      "Epoch [90/300], Step [18/172], Loss: 62.5734\n",
      "Epoch [90/300], Step [19/172], Loss: 83.1078\n",
      "Epoch [90/300], Step [20/172], Loss: 78.2395\n",
      "Epoch [90/300], Step [21/172], Loss: 94.9540\n",
      "Epoch [90/300], Step [22/172], Loss: 81.5578\n",
      "Epoch [90/300], Step [23/172], Loss: 5.3932\n",
      "Epoch [90/300], Step [24/172], Loss: 72.7209\n",
      "Epoch [90/300], Step [25/172], Loss: 47.8206\n",
      "Epoch [90/300], Step [26/172], Loss: 60.7133\n",
      "Epoch [90/300], Step [27/172], Loss: 79.8758\n",
      "Epoch [90/300], Step [28/172], Loss: 40.4583\n",
      "Epoch [90/300], Step [29/172], Loss: 30.0870\n",
      "Epoch [90/300], Step [30/172], Loss: 82.6040\n",
      "Epoch [90/300], Step [31/172], Loss: 45.1465\n",
      "Epoch [90/300], Step [32/172], Loss: 40.9631\n",
      "Epoch [90/300], Step [33/172], Loss: 72.3318\n",
      "Epoch [90/300], Step [34/172], Loss: 4.8700\n",
      "Epoch [90/300], Step [35/172], Loss: 19.7389\n",
      "Epoch [90/300], Step [36/172], Loss: 21.5600\n",
      "Epoch [90/300], Step [37/172], Loss: 18.1215\n",
      "Epoch [90/300], Step [38/172], Loss: 27.7758\n",
      "Epoch [90/300], Step [39/172], Loss: 49.8122\n",
      "Epoch [90/300], Step [40/172], Loss: 22.0988\n",
      "Epoch [90/300], Step [41/172], Loss: 39.3734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/300], Step [42/172], Loss: 43.2597\n",
      "Epoch [90/300], Step [43/172], Loss: 27.4171\n",
      "Epoch [90/300], Step [44/172], Loss: 22.5495\n",
      "Epoch [90/300], Step [45/172], Loss: 22.0855\n",
      "Epoch [90/300], Step [46/172], Loss: 26.1693\n",
      "Epoch [90/300], Step [47/172], Loss: 52.4617\n",
      "Epoch [90/300], Step [48/172], Loss: 54.0144\n",
      "Epoch [90/300], Step [49/172], Loss: 20.7238\n",
      "Epoch [90/300], Step [50/172], Loss: 52.2557\n",
      "Epoch [90/300], Step [51/172], Loss: 7.4646\n",
      "Epoch [90/300], Step [52/172], Loss: 19.2176\n",
      "Epoch [90/300], Step [53/172], Loss: 25.0492\n",
      "Epoch [90/300], Step [54/172], Loss: 12.1939\n",
      "Epoch [90/300], Step [55/172], Loss: 12.8533\n",
      "Epoch [90/300], Step [56/172], Loss: 10.3265\n",
      "Epoch [90/300], Step [57/172], Loss: 24.0835\n",
      "Epoch [90/300], Step [58/172], Loss: 18.9229\n",
      "Epoch [90/300], Step [59/172], Loss: 31.8254\n",
      "Epoch [90/300], Step [60/172], Loss: 49.1170\n",
      "Epoch [90/300], Step [61/172], Loss: 9.7302\n",
      "Epoch [90/300], Step [62/172], Loss: 24.0473\n",
      "Epoch [90/300], Step [63/172], Loss: 8.8859\n",
      "Epoch [90/300], Step [64/172], Loss: 8.0374\n",
      "Epoch [90/300], Step [65/172], Loss: 21.4972\n",
      "Epoch [90/300], Step [66/172], Loss: 6.1968\n",
      "Epoch [90/300], Step [67/172], Loss: 28.6339\n",
      "Epoch [90/300], Step [68/172], Loss: 7.2955\n",
      "Epoch [90/300], Step [69/172], Loss: 66.2456\n",
      "Epoch [90/300], Step [70/172], Loss: 59.0593\n",
      "Epoch [90/300], Step [71/172], Loss: 54.2296\n",
      "Epoch [90/300], Step [72/172], Loss: 59.8216\n",
      "Epoch [90/300], Step [73/172], Loss: 63.7858\n",
      "Epoch [90/300], Step [74/172], Loss: 36.7903\n",
      "Epoch [90/300], Step [75/172], Loss: 33.4364\n",
      "Epoch [90/300], Step [76/172], Loss: 39.5622\n",
      "Epoch [90/300], Step [77/172], Loss: 62.1750\n",
      "Epoch [90/300], Step [78/172], Loss: 50.8002\n",
      "Epoch [90/300], Step [79/172], Loss: 49.8370\n",
      "Epoch [90/300], Step [80/172], Loss: 58.5034\n",
      "Epoch [90/300], Step [81/172], Loss: 43.3500\n",
      "Epoch [90/300], Step [82/172], Loss: 40.8733\n",
      "Epoch [90/300], Step [83/172], Loss: 51.3931\n",
      "Epoch [90/300], Step [84/172], Loss: 39.2438\n",
      "Epoch [90/300], Step [85/172], Loss: 42.6674\n",
      "Epoch [90/300], Step [86/172], Loss: 34.8140\n",
      "Epoch [90/300], Step [87/172], Loss: 29.3889\n",
      "Epoch [90/300], Step [88/172], Loss: 31.2411\n",
      "Epoch [90/300], Step [89/172], Loss: 27.6628\n",
      "Epoch [90/300], Step [90/172], Loss: 26.7818\n",
      "Epoch [90/300], Step [91/172], Loss: 30.4794\n",
      "Epoch [90/300], Step [92/172], Loss: 22.4858\n",
      "Epoch [90/300], Step [93/172], Loss: 22.1909\n",
      "Epoch [90/300], Step [94/172], Loss: 29.8583\n",
      "Epoch [90/300], Step [95/172], Loss: 23.8343\n",
      "Epoch [90/300], Step [96/172], Loss: 19.5034\n",
      "Epoch [90/300], Step [97/172], Loss: 26.6129\n",
      "Epoch [90/300], Step [98/172], Loss: 20.7111\n",
      "Epoch [90/300], Step [99/172], Loss: 18.5968\n",
      "Epoch [90/300], Step [100/172], Loss: 16.9794\n",
      "Epoch [90/300], Step [101/172], Loss: 18.4569\n",
      "Epoch [90/300], Step [102/172], Loss: 17.0302\n",
      "Epoch [90/300], Step [103/172], Loss: 15.1377\n",
      "Epoch [90/300], Step [104/172], Loss: 16.5412\n",
      "Epoch [90/300], Step [105/172], Loss: 18.9028\n",
      "Epoch [90/300], Step [106/172], Loss: 17.8160\n",
      "Epoch [90/300], Step [107/172], Loss: 16.0316\n",
      "Epoch [90/300], Step [108/172], Loss: 17.6164\n",
      "Epoch [90/300], Step [109/172], Loss: 19.0166\n",
      "Epoch [90/300], Step [110/172], Loss: 16.7768\n",
      "Epoch [90/300], Step [111/172], Loss: 15.1665\n",
      "Epoch [90/300], Step [112/172], Loss: 19.6171\n",
      "Epoch [90/300], Step [113/172], Loss: 15.8983\n",
      "Epoch [90/300], Step [114/172], Loss: 15.1805\n",
      "Epoch [90/300], Step [115/172], Loss: 22.9248\n",
      "Epoch [90/300], Step [116/172], Loss: 15.6733\n",
      "Epoch [90/300], Step [117/172], Loss: 13.0318\n",
      "Epoch [90/300], Step [118/172], Loss: 16.1669\n",
      "Epoch [90/300], Step [119/172], Loss: 16.0963\n",
      "Epoch [90/300], Step [120/172], Loss: 12.1278\n",
      "Epoch [90/300], Step [121/172], Loss: 12.1102\n",
      "Epoch [90/300], Step [122/172], Loss: 11.5741\n",
      "Epoch [90/300], Step [123/172], Loss: 11.2570\n",
      "Epoch [90/300], Step [124/172], Loss: 9.1308\n",
      "Epoch [90/300], Step [125/172], Loss: 13.8307\n",
      "Epoch [90/300], Step [126/172], Loss: 11.6290\n",
      "Epoch [90/300], Step [127/172], Loss: 13.4281\n",
      "Epoch [90/300], Step [128/172], Loss: 13.7779\n",
      "Epoch [90/300], Step [129/172], Loss: 9.9070\n",
      "Epoch [90/300], Step [130/172], Loss: 12.2226\n",
      "Epoch [90/300], Step [131/172], Loss: 9.9518\n",
      "Epoch [90/300], Step [132/172], Loss: 9.9330\n",
      "Epoch [90/300], Step [133/172], Loss: 11.0330\n",
      "Epoch [90/300], Step [134/172], Loss: 12.6515\n",
      "Epoch [90/300], Step [135/172], Loss: 9.6678\n",
      "Epoch [90/300], Step [136/172], Loss: 9.3387\n",
      "Epoch [90/300], Step [137/172], Loss: 11.0067\n",
      "Epoch [90/300], Step [138/172], Loss: 9.2594\n",
      "Epoch [90/300], Step [139/172], Loss: 10.6163\n",
      "Epoch [90/300], Step [140/172], Loss: 10.5334\n",
      "Epoch [90/300], Step [141/172], Loss: 13.0059\n",
      "Epoch [90/300], Step [142/172], Loss: 14.4787\n",
      "Epoch [90/300], Step [143/172], Loss: 9.9766\n",
      "Epoch [90/300], Step [144/172], Loss: 9.5783\n",
      "Epoch [90/300], Step [145/172], Loss: 10.5435\n",
      "Epoch [90/300], Step [146/172], Loss: 10.5982\n",
      "Epoch [90/300], Step [147/172], Loss: 6.2016\n",
      "Epoch [90/300], Step [148/172], Loss: 7.3588\n",
      "Epoch [90/300], Step [149/172], Loss: 8.9914\n",
      "Epoch [90/300], Step [150/172], Loss: 8.8181\n",
      "Epoch [90/300], Step [151/172], Loss: 7.9531\n",
      "Epoch [90/300], Step [152/172], Loss: 8.3569\n",
      "Epoch [90/300], Step [153/172], Loss: 7.8281\n",
      "Epoch [90/300], Step [154/172], Loss: 8.5311\n",
      "Epoch [90/300], Step [155/172], Loss: 7.6768\n",
      "Epoch [90/300], Step [156/172], Loss: 12.6849\n",
      "Epoch [90/300], Step [157/172], Loss: 10.8556\n",
      "Epoch [90/300], Step [158/172], Loss: 8.4404\n",
      "Epoch [90/300], Step [159/172], Loss: 10.0640\n",
      "Epoch [90/300], Step [160/172], Loss: 10.3754\n",
      "Epoch [90/300], Step [161/172], Loss: 7.6930\n",
      "Epoch [90/300], Step [162/172], Loss: 8.0735\n",
      "Epoch [90/300], Step [163/172], Loss: 7.0816\n",
      "Epoch [90/300], Step [164/172], Loss: 10.3353\n",
      "Epoch [90/300], Step [165/172], Loss: 6.9364\n",
      "Epoch [90/300], Step [166/172], Loss: 7.3815\n",
      "Epoch [90/300], Step [167/172], Loss: 9.0533\n",
      "Epoch [90/300], Step [168/172], Loss: 7.4628\n",
      "Epoch [90/300], Step [169/172], Loss: 7.6507\n",
      "Epoch [90/300], Step [170/172], Loss: 6.2929\n",
      "Epoch [90/300], Step [171/172], Loss: 6.5631\n",
      "Epoch [90/300], Step [172/172], Loss: 5.8237\n",
      "Epoch [91/300], Step [1/172], Loss: 85.1606\n",
      "Epoch [91/300], Step [2/172], Loss: 85.4000\n",
      "Epoch [91/300], Step [3/172], Loss: 79.9189\n",
      "Epoch [91/300], Step [4/172], Loss: 49.7041\n",
      "Epoch [91/300], Step [5/172], Loss: 70.2917\n",
      "Epoch [91/300], Step [6/172], Loss: 21.3638\n",
      "Epoch [91/300], Step [7/172], Loss: 28.6882\n",
      "Epoch [91/300], Step [8/172], Loss: 6.7162\n",
      "Epoch [91/300], Step [9/172], Loss: 44.8417\n",
      "Epoch [91/300], Step [10/172], Loss: 48.5668\n",
      "Epoch [91/300], Step [11/172], Loss: 88.8383\n",
      "Epoch [91/300], Step [12/172], Loss: 84.1176\n",
      "Epoch [91/300], Step [13/172], Loss: 41.9152\n",
      "Epoch [91/300], Step [14/172], Loss: 90.4854\n",
      "Epoch [91/300], Step [15/172], Loss: 76.7889\n",
      "Epoch [91/300], Step [16/172], Loss: 25.2153\n",
      "Epoch [91/300], Step [17/172], Loss: 58.4284\n",
      "Epoch [91/300], Step [18/172], Loss: 62.3876\n",
      "Epoch [91/300], Step [19/172], Loss: 83.0169\n",
      "Epoch [91/300], Step [20/172], Loss: 77.0139\n",
      "Epoch [91/300], Step [21/172], Loss: 94.6111\n",
      "Epoch [91/300], Step [22/172], Loss: 81.0371\n",
      "Epoch [91/300], Step [23/172], Loss: 5.2959\n",
      "Epoch [91/300], Step [24/172], Loss: 72.2773\n",
      "Epoch [91/300], Step [25/172], Loss: 47.4421\n",
      "Epoch [91/300], Step [26/172], Loss: 60.2236\n",
      "Epoch [91/300], Step [27/172], Loss: 79.0429\n",
      "Epoch [91/300], Step [28/172], Loss: 39.8475\n",
      "Epoch [91/300], Step [29/172], Loss: 29.4596\n",
      "Epoch [91/300], Step [30/172], Loss: 82.6521\n",
      "Epoch [91/300], Step [31/172], Loss: 44.9930\n",
      "Epoch [91/300], Step [32/172], Loss: 40.7978\n",
      "Epoch [91/300], Step [33/172], Loss: 72.0040\n",
      "Epoch [91/300], Step [34/172], Loss: 4.8856\n",
      "Epoch [91/300], Step [35/172], Loss: 19.2818\n",
      "Epoch [91/300], Step [36/172], Loss: 21.4626\n",
      "Epoch [91/300], Step [37/172], Loss: 18.0329\n",
      "Epoch [91/300], Step [38/172], Loss: 27.6364\n",
      "Epoch [91/300], Step [39/172], Loss: 49.3620\n",
      "Epoch [91/300], Step [40/172], Loss: 21.9596\n",
      "Epoch [91/300], Step [41/172], Loss: 39.2991\n",
      "Epoch [91/300], Step [42/172], Loss: 42.9949\n",
      "Epoch [91/300], Step [43/172], Loss: 27.3448\n",
      "Epoch [91/300], Step [44/172], Loss: 22.4174\n",
      "Epoch [91/300], Step [45/172], Loss: 22.0989\n",
      "Epoch [91/300], Step [46/172], Loss: 25.9586\n",
      "Epoch [91/300], Step [47/172], Loss: 52.1449\n",
      "Epoch [91/300], Step [48/172], Loss: 53.5912\n",
      "Epoch [91/300], Step [49/172], Loss: 20.7180\n",
      "Epoch [91/300], Step [50/172], Loss: 52.3451\n",
      "Epoch [91/300], Step [51/172], Loss: 7.4797\n",
      "Epoch [91/300], Step [52/172], Loss: 19.0920\n",
      "Epoch [91/300], Step [53/172], Loss: 25.0156\n",
      "Epoch [91/300], Step [54/172], Loss: 12.1740\n",
      "Epoch [91/300], Step [55/172], Loss: 12.7876\n",
      "Epoch [91/300], Step [56/172], Loss: 10.3324\n",
      "Epoch [91/300], Step [57/172], Loss: 23.9525\n",
      "Epoch [91/300], Step [58/172], Loss: 18.8472\n",
      "Epoch [91/300], Step [59/172], Loss: 31.6370\n",
      "Epoch [91/300], Step [60/172], Loss: 49.1384\n",
      "Epoch [91/300], Step [61/172], Loss: 9.7274\n",
      "Epoch [91/300], Step [62/172], Loss: 24.0234\n",
      "Epoch [91/300], Step [63/172], Loss: 8.8826\n",
      "Epoch [91/300], Step [64/172], Loss: 8.0763\n",
      "Epoch [91/300], Step [65/172], Loss: 21.4565\n",
      "Epoch [91/300], Step [66/172], Loss: 6.1777\n",
      "Epoch [91/300], Step [67/172], Loss: 28.7482\n",
      "Epoch [91/300], Step [68/172], Loss: 7.1422\n",
      "Epoch [91/300], Step [69/172], Loss: 66.0005\n",
      "Epoch [91/300], Step [70/172], Loss: 58.9413\n",
      "Epoch [91/300], Step [71/172], Loss: 54.0675\n",
      "Epoch [91/300], Step [72/172], Loss: 59.5610\n",
      "Epoch [91/300], Step [73/172], Loss: 63.4782\n",
      "Epoch [91/300], Step [74/172], Loss: 36.5222\n",
      "Epoch [91/300], Step [75/172], Loss: 33.4104\n",
      "Epoch [91/300], Step [76/172], Loss: 39.3129\n",
      "Epoch [91/300], Step [77/172], Loss: 62.0828\n",
      "Epoch [91/300], Step [78/172], Loss: 50.5344\n",
      "Epoch [91/300], Step [79/172], Loss: 49.5587\n",
      "Epoch [91/300], Step [80/172], Loss: 58.2636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/300], Step [81/172], Loss: 43.1083\n",
      "Epoch [91/300], Step [82/172], Loss: 40.6087\n",
      "Epoch [91/300], Step [83/172], Loss: 51.2433\n",
      "Epoch [91/300], Step [84/172], Loss: 39.0328\n",
      "Epoch [91/300], Step [85/172], Loss: 42.5064\n",
      "Epoch [91/300], Step [86/172], Loss: 34.6349\n",
      "Epoch [91/300], Step [87/172], Loss: 29.3117\n",
      "Epoch [91/300], Step [88/172], Loss: 31.1206\n",
      "Epoch [91/300], Step [89/172], Loss: 27.4954\n",
      "Epoch [91/300], Step [90/172], Loss: 26.6618\n",
      "Epoch [91/300], Step [91/172], Loss: 30.3766\n",
      "Epoch [91/300], Step [92/172], Loss: 22.3834\n",
      "Epoch [91/300], Step [93/172], Loss: 22.1168\n",
      "Epoch [91/300], Step [94/172], Loss: 29.8750\n",
      "Epoch [91/300], Step [95/172], Loss: 23.7833\n",
      "Epoch [91/300], Step [96/172], Loss: 19.4224\n",
      "Epoch [91/300], Step [97/172], Loss: 26.5515\n",
      "Epoch [91/300], Step [98/172], Loss: 20.6046\n",
      "Epoch [91/300], Step [99/172], Loss: 18.5295\n",
      "Epoch [91/300], Step [100/172], Loss: 16.8937\n",
      "Epoch [91/300], Step [101/172], Loss: 18.3610\n",
      "Epoch [91/300], Step [102/172], Loss: 16.8867\n",
      "Epoch [91/300], Step [103/172], Loss: 15.0399\n",
      "Epoch [91/300], Step [104/172], Loss: 16.4765\n",
      "Epoch [91/300], Step [105/172], Loss: 18.7884\n",
      "Epoch [91/300], Step [106/172], Loss: 17.7670\n",
      "Epoch [91/300], Step [107/172], Loss: 15.9916\n",
      "Epoch [91/300], Step [108/172], Loss: 17.5139\n",
      "Epoch [91/300], Step [109/172], Loss: 18.9255\n",
      "Epoch [91/300], Step [110/172], Loss: 16.6834\n",
      "Epoch [91/300], Step [111/172], Loss: 15.0987\n",
      "Epoch [91/300], Step [112/172], Loss: 19.5296\n",
      "Epoch [91/300], Step [113/172], Loss: 15.8174\n",
      "Epoch [91/300], Step [114/172], Loss: 15.0654\n",
      "Epoch [91/300], Step [115/172], Loss: 22.8878\n",
      "Epoch [91/300], Step [116/172], Loss: 15.5490\n",
      "Epoch [91/300], Step [117/172], Loss: 12.9569\n",
      "Epoch [91/300], Step [118/172], Loss: 16.1373\n",
      "Epoch [91/300], Step [119/172], Loss: 16.0517\n",
      "Epoch [91/300], Step [120/172], Loss: 12.0566\n",
      "Epoch [91/300], Step [121/172], Loss: 11.9943\n",
      "Epoch [91/300], Step [122/172], Loss: 11.5374\n",
      "Epoch [91/300], Step [123/172], Loss: 11.2070\n",
      "Epoch [91/300], Step [124/172], Loss: 9.0544\n",
      "Epoch [91/300], Step [125/172], Loss: 13.7698\n",
      "Epoch [91/300], Step [126/172], Loss: 11.5393\n",
      "Epoch [91/300], Step [127/172], Loss: 13.2935\n",
      "Epoch [91/300], Step [128/172], Loss: 13.6124\n",
      "Epoch [91/300], Step [129/172], Loss: 9.8216\n",
      "Epoch [91/300], Step [130/172], Loss: 12.1475\n",
      "Epoch [91/300], Step [131/172], Loss: 9.8704\n",
      "Epoch [91/300], Step [132/172], Loss: 9.8384\n",
      "Epoch [91/300], Step [133/172], Loss: 10.9535\n",
      "Epoch [91/300], Step [134/172], Loss: 12.6137\n",
      "Epoch [91/300], Step [135/172], Loss: 9.6069\n",
      "Epoch [91/300], Step [136/172], Loss: 9.2373\n",
      "Epoch [91/300], Step [137/172], Loss: 10.9041\n",
      "Epoch [91/300], Step [138/172], Loss: 9.1677\n",
      "Epoch [91/300], Step [139/172], Loss: 10.5254\n",
      "Epoch [91/300], Step [140/172], Loss: 10.4454\n",
      "Epoch [91/300], Step [141/172], Loss: 12.8824\n",
      "Epoch [91/300], Step [142/172], Loss: 14.4047\n",
      "Epoch [91/300], Step [143/172], Loss: 9.9345\n",
      "Epoch [91/300], Step [144/172], Loss: 9.5093\n",
      "Epoch [91/300], Step [145/172], Loss: 10.4888\n",
      "Epoch [91/300], Step [146/172], Loss: 10.5091\n",
      "Epoch [91/300], Step [147/172], Loss: 6.1441\n",
      "Epoch [91/300], Step [148/172], Loss: 7.3011\n",
      "Epoch [91/300], Step [149/172], Loss: 8.9185\n",
      "Epoch [91/300], Step [150/172], Loss: 8.7367\n",
      "Epoch [91/300], Step [151/172], Loss: 7.8990\n",
      "Epoch [91/300], Step [152/172], Loss: 8.3178\n",
      "Epoch [91/300], Step [153/172], Loss: 7.7571\n",
      "Epoch [91/300], Step [154/172], Loss: 8.4812\n",
      "Epoch [91/300], Step [155/172], Loss: 7.6105\n",
      "Epoch [91/300], Step [156/172], Loss: 12.6963\n",
      "Epoch [91/300], Step [157/172], Loss: 10.8173\n",
      "Epoch [91/300], Step [158/172], Loss: 8.3876\n",
      "Epoch [91/300], Step [159/172], Loss: 9.9867\n",
      "Epoch [91/300], Step [160/172], Loss: 10.3624\n",
      "Epoch [91/300], Step [161/172], Loss: 7.6563\n",
      "Epoch [91/300], Step [162/172], Loss: 8.0375\n",
      "Epoch [91/300], Step [163/172], Loss: 7.0407\n",
      "Epoch [91/300], Step [164/172], Loss: 10.2758\n",
      "Epoch [91/300], Step [165/172], Loss: 6.8963\n",
      "Epoch [91/300], Step [166/172], Loss: 7.3069\n",
      "Epoch [91/300], Step [167/172], Loss: 9.0672\n",
      "Epoch [91/300], Step [168/172], Loss: 7.4436\n",
      "Epoch [91/300], Step [169/172], Loss: 7.6293\n",
      "Epoch [91/300], Step [170/172], Loss: 6.2513\n",
      "Epoch [91/300], Step [171/172], Loss: 6.5521\n",
      "Epoch [91/300], Step [172/172], Loss: 5.7954\n",
      "Epoch [92/300], Step [1/172], Loss: 84.4628\n",
      "Epoch [92/300], Step [2/172], Loss: 84.7273\n",
      "Epoch [92/300], Step [3/172], Loss: 79.6647\n",
      "Epoch [92/300], Step [4/172], Loss: 49.3119\n",
      "Epoch [92/300], Step [5/172], Loss: 69.9452\n",
      "Epoch [92/300], Step [6/172], Loss: 21.0542\n",
      "Epoch [92/300], Step [7/172], Loss: 28.5931\n",
      "Epoch [92/300], Step [8/172], Loss: 6.8340\n",
      "Epoch [92/300], Step [9/172], Loss: 44.7184\n",
      "Epoch [92/300], Step [10/172], Loss: 48.4647\n",
      "Epoch [92/300], Step [11/172], Loss: 88.3883\n",
      "Epoch [92/300], Step [12/172], Loss: 84.1270\n",
      "Epoch [92/300], Step [13/172], Loss: 42.0293\n",
      "Epoch [92/300], Step [14/172], Loss: 90.3577\n",
      "Epoch [92/300], Step [15/172], Loss: 76.5334\n",
      "Epoch [92/300], Step [16/172], Loss: 24.8627\n",
      "Epoch [92/300], Step [17/172], Loss: 58.5822\n",
      "Epoch [92/300], Step [18/172], Loss: 62.4570\n",
      "Epoch [92/300], Step [19/172], Loss: 83.3182\n",
      "Epoch [92/300], Step [20/172], Loss: 76.5502\n",
      "Epoch [92/300], Step [21/172], Loss: 94.9534\n",
      "Epoch [92/300], Step [22/172], Loss: 81.0832\n",
      "Epoch [92/300], Step [23/172], Loss: 5.2104\n",
      "Epoch [92/300], Step [24/172], Loss: 72.3398\n",
      "Epoch [92/300], Step [25/172], Loss: 47.4618\n",
      "Epoch [92/300], Step [26/172], Loss: 60.3620\n",
      "Epoch [92/300], Step [27/172], Loss: 79.0128\n",
      "Epoch [92/300], Step [28/172], Loss: 39.4655\n",
      "Epoch [92/300], Step [29/172], Loss: 29.0493\n",
      "Epoch [92/300], Step [30/172], Loss: 83.1235\n",
      "Epoch [92/300], Step [31/172], Loss: 45.0821\n",
      "Epoch [92/300], Step [32/172], Loss: 40.9284\n",
      "Epoch [92/300], Step [33/172], Loss: 72.1396\n",
      "Epoch [92/300], Step [34/172], Loss: 4.7622\n",
      "Epoch [92/300], Step [35/172], Loss: 18.9604\n",
      "Epoch [92/300], Step [36/172], Loss: 21.3327\n",
      "Epoch [92/300], Step [37/172], Loss: 17.9458\n",
      "Epoch [92/300], Step [38/172], Loss: 27.5532\n",
      "Epoch [92/300], Step [39/172], Loss: 48.9091\n",
      "Epoch [92/300], Step [40/172], Loss: 21.7872\n",
      "Epoch [92/300], Step [41/172], Loss: 39.1019\n",
      "Epoch [92/300], Step [42/172], Loss: 42.8548\n",
      "Epoch [92/300], Step [43/172], Loss: 27.1596\n",
      "Epoch [92/300], Step [44/172], Loss: 22.2331\n",
      "Epoch [92/300], Step [45/172], Loss: 21.9803\n",
      "Epoch [92/300], Step [46/172], Loss: 25.6787\n",
      "Epoch [92/300], Step [47/172], Loss: 51.8503\n",
      "Epoch [92/300], Step [48/172], Loss: 53.6421\n",
      "Epoch [92/300], Step [49/172], Loss: 20.5143\n",
      "Epoch [92/300], Step [50/172], Loss: 52.1604\n",
      "Epoch [92/300], Step [51/172], Loss: 7.4101\n",
      "Epoch [92/300], Step [52/172], Loss: 18.9335\n",
      "Epoch [92/300], Step [53/172], Loss: 24.7606\n",
      "Epoch [92/300], Step [54/172], Loss: 12.0416\n",
      "Epoch [92/300], Step [55/172], Loss: 12.6897\n",
      "Epoch [92/300], Step [56/172], Loss: 10.3163\n",
      "Epoch [92/300], Step [57/172], Loss: 23.4144\n",
      "Epoch [92/300], Step [58/172], Loss: 18.6303\n",
      "Epoch [92/300], Step [59/172], Loss: 31.5176\n",
      "Epoch [92/300], Step [60/172], Loss: 48.9321\n",
      "Epoch [92/300], Step [61/172], Loss: 9.6477\n",
      "Epoch [92/300], Step [62/172], Loss: 24.0134\n",
      "Epoch [92/300], Step [63/172], Loss: 8.8696\n",
      "Epoch [92/300], Step [64/172], Loss: 8.0843\n",
      "Epoch [92/300], Step [65/172], Loss: 21.3054\n",
      "Epoch [92/300], Step [66/172], Loss: 6.1442\n",
      "Epoch [92/300], Step [67/172], Loss: 28.6653\n",
      "Epoch [92/300], Step [68/172], Loss: 7.1019\n",
      "Epoch [92/300], Step [69/172], Loss: 65.5756\n",
      "Epoch [92/300], Step [70/172], Loss: 58.5739\n",
      "Epoch [92/300], Step [71/172], Loss: 53.8671\n",
      "Epoch [92/300], Step [72/172], Loss: 59.4307\n",
      "Epoch [92/300], Step [73/172], Loss: 63.2897\n",
      "Epoch [92/300], Step [74/172], Loss: 36.3951\n",
      "Epoch [92/300], Step [75/172], Loss: 33.4479\n",
      "Epoch [92/300], Step [76/172], Loss: 39.1504\n",
      "Epoch [92/300], Step [77/172], Loss: 62.0558\n",
      "Epoch [92/300], Step [78/172], Loss: 50.2786\n",
      "Epoch [92/300], Step [79/172], Loss: 49.4689\n",
      "Epoch [92/300], Step [80/172], Loss: 58.4164\n",
      "Epoch [92/300], Step [81/172], Loss: 42.9999\n",
      "Epoch [92/300], Step [82/172], Loss: 40.6324\n",
      "Epoch [92/300], Step [83/172], Loss: 51.2057\n",
      "Epoch [92/300], Step [84/172], Loss: 38.9435\n",
      "Epoch [92/300], Step [85/172], Loss: 42.3270\n",
      "Epoch [92/300], Step [86/172], Loss: 34.5336\n",
      "Epoch [92/300], Step [87/172], Loss: 29.2162\n",
      "Epoch [92/300], Step [88/172], Loss: 31.0104\n",
      "Epoch [92/300], Step [89/172], Loss: 27.3757\n",
      "Epoch [92/300], Step [90/172], Loss: 26.4872\n",
      "Epoch [92/300], Step [91/172], Loss: 30.3083\n",
      "Epoch [92/300], Step [92/172], Loss: 22.3284\n",
      "Epoch [92/300], Step [93/172], Loss: 22.0229\n",
      "Epoch [92/300], Step [94/172], Loss: 29.7807\n",
      "Epoch [92/300], Step [95/172], Loss: 23.6989\n",
      "Epoch [92/300], Step [96/172], Loss: 19.3865\n",
      "Epoch [92/300], Step [97/172], Loss: 26.4747\n",
      "Epoch [92/300], Step [98/172], Loss: 20.5109\n",
      "Epoch [92/300], Step [99/172], Loss: 18.4576\n",
      "Epoch [92/300], Step [100/172], Loss: 16.8151\n",
      "Epoch [92/300], Step [101/172], Loss: 18.2754\n",
      "Epoch [92/300], Step [102/172], Loss: 16.8481\n",
      "Epoch [92/300], Step [103/172], Loss: 14.9425\n",
      "Epoch [92/300], Step [104/172], Loss: 16.4400\n",
      "Epoch [92/300], Step [105/172], Loss: 18.8097\n",
      "Epoch [92/300], Step [106/172], Loss: 17.7145\n",
      "Epoch [92/300], Step [107/172], Loss: 15.9726\n",
      "Epoch [92/300], Step [108/172], Loss: 17.4425\n",
      "Epoch [92/300], Step [109/172], Loss: 18.8061\n",
      "Epoch [92/300], Step [110/172], Loss: 16.6389\n",
      "Epoch [92/300], Step [111/172], Loss: 15.0527\n",
      "Epoch [92/300], Step [112/172], Loss: 19.4779\n",
      "Epoch [92/300], Step [113/172], Loss: 15.7654\n",
      "Epoch [92/300], Step [114/172], Loss: 15.0117\n",
      "Epoch [92/300], Step [115/172], Loss: 22.7623\n",
      "Epoch [92/300], Step [116/172], Loss: 15.5334\n",
      "Epoch [92/300], Step [117/172], Loss: 12.8742\n",
      "Epoch [92/300], Step [118/172], Loss: 15.9932\n",
      "Epoch [92/300], Step [119/172], Loss: 16.0781\n",
      "Epoch [92/300], Step [120/172], Loss: 12.0030\n",
      "Epoch [92/300], Step [121/172], Loss: 11.9361\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/300], Step [122/172], Loss: 11.4549\n",
      "Epoch [92/300], Step [123/172], Loss: 11.1688\n",
      "Epoch [92/300], Step [124/172], Loss: 9.0417\n",
      "Epoch [92/300], Step [125/172], Loss: 13.7362\n",
      "Epoch [92/300], Step [126/172], Loss: 11.5203\n",
      "Epoch [92/300], Step [127/172], Loss: 13.2451\n",
      "Epoch [92/300], Step [128/172], Loss: 13.5856\n",
      "Epoch [92/300], Step [129/172], Loss: 9.7905\n",
      "Epoch [92/300], Step [130/172], Loss: 12.1509\n",
      "Epoch [92/300], Step [131/172], Loss: 9.8200\n",
      "Epoch [92/300], Step [132/172], Loss: 9.8052\n",
      "Epoch [92/300], Step [133/172], Loss: 10.8915\n",
      "Epoch [92/300], Step [134/172], Loss: 12.5892\n",
      "Epoch [92/300], Step [135/172], Loss: 9.5941\n",
      "Epoch [92/300], Step [136/172], Loss: 9.2423\n",
      "Epoch [92/300], Step [137/172], Loss: 10.8852\n",
      "Epoch [92/300], Step [138/172], Loss: 9.1617\n",
      "Epoch [92/300], Step [139/172], Loss: 10.5177\n",
      "Epoch [92/300], Step [140/172], Loss: 10.4285\n",
      "Epoch [92/300], Step [141/172], Loss: 12.8208\n",
      "Epoch [92/300], Step [142/172], Loss: 14.3816\n",
      "Epoch [92/300], Step [143/172], Loss: 9.9277\n",
      "Epoch [92/300], Step [144/172], Loss: 9.5024\n",
      "Epoch [92/300], Step [145/172], Loss: 10.4800\n",
      "Epoch [92/300], Step [146/172], Loss: 10.5032\n",
      "Epoch [92/300], Step [147/172], Loss: 6.1191\n",
      "Epoch [92/300], Step [148/172], Loss: 7.2797\n",
      "Epoch [92/300], Step [149/172], Loss: 8.8810\n",
      "Epoch [92/300], Step [150/172], Loss: 8.7133\n",
      "Epoch [92/300], Step [151/172], Loss: 7.8615\n",
      "Epoch [92/300], Step [152/172], Loss: 8.3158\n",
      "Epoch [92/300], Step [153/172], Loss: 7.7284\n",
      "Epoch [92/300], Step [154/172], Loss: 8.4616\n",
      "Epoch [92/300], Step [155/172], Loss: 7.5825\n",
      "Epoch [92/300], Step [156/172], Loss: 12.7048\n",
      "Epoch [92/300], Step [157/172], Loss: 10.7959\n",
      "Epoch [92/300], Step [158/172], Loss: 8.3585\n",
      "Epoch [92/300], Step [159/172], Loss: 10.0006\n",
      "Epoch [92/300], Step [160/172], Loss: 10.3596\n",
      "Epoch [92/300], Step [161/172], Loss: 7.6569\n",
      "Epoch [92/300], Step [162/172], Loss: 8.0277\n",
      "Epoch [92/300], Step [163/172], Loss: 7.0224\n",
      "Epoch [92/300], Step [164/172], Loss: 10.2875\n",
      "Epoch [92/300], Step [165/172], Loss: 6.8796\n",
      "Epoch [92/300], Step [166/172], Loss: 7.2781\n",
      "Epoch [92/300], Step [167/172], Loss: 9.0942\n",
      "Epoch [92/300], Step [168/172], Loss: 7.4473\n",
      "Epoch [92/300], Step [169/172], Loss: 7.6240\n",
      "Epoch [92/300], Step [170/172], Loss: 6.2236\n",
      "Epoch [92/300], Step [171/172], Loss: 6.5640\n",
      "Epoch [92/300], Step [172/172], Loss: 5.7812\n",
      "Epoch [93/300], Step [1/172], Loss: 84.0879\n",
      "Epoch [93/300], Step [2/172], Loss: 84.2981\n",
      "Epoch [93/300], Step [3/172], Loss: 79.0795\n",
      "Epoch [93/300], Step [4/172], Loss: 48.8499\n",
      "Epoch [93/300], Step [5/172], Loss: 69.5750\n",
      "Epoch [93/300], Step [6/172], Loss: 20.8423\n",
      "Epoch [93/300], Step [7/172], Loss: 28.4717\n",
      "Epoch [93/300], Step [8/172], Loss: 6.6108\n",
      "Epoch [93/300], Step [9/172], Loss: 44.4785\n",
      "Epoch [93/300], Step [10/172], Loss: 48.2903\n",
      "Epoch [93/300], Step [11/172], Loss: 87.8645\n",
      "Epoch [93/300], Step [12/172], Loss: 84.0452\n",
      "Epoch [93/300], Step [13/172], Loss: 41.9704\n",
      "Epoch [93/300], Step [14/172], Loss: 89.8031\n",
      "Epoch [93/300], Step [15/172], Loss: 75.9857\n",
      "Epoch [93/300], Step [16/172], Loss: 24.1091\n",
      "Epoch [93/300], Step [17/172], Loss: 58.4491\n",
      "Epoch [93/300], Step [18/172], Loss: 62.3281\n",
      "Epoch [93/300], Step [19/172], Loss: 83.2651\n",
      "Epoch [93/300], Step [20/172], Loss: 75.3352\n",
      "Epoch [93/300], Step [21/172], Loss: 94.7546\n",
      "Epoch [93/300], Step [22/172], Loss: 80.5677\n",
      "Epoch [93/300], Step [23/172], Loss: 4.9567\n",
      "Epoch [93/300], Step [24/172], Loss: 71.9409\n",
      "Epoch [93/300], Step [25/172], Loss: 47.3159\n",
      "Epoch [93/300], Step [26/172], Loss: 60.1161\n",
      "Epoch [93/300], Step [27/172], Loss: 78.2466\n",
      "Epoch [93/300], Step [28/172], Loss: 38.9510\n",
      "Epoch [93/300], Step [29/172], Loss: 28.5856\n",
      "Epoch [93/300], Step [30/172], Loss: 83.1885\n",
      "Epoch [93/300], Step [31/172], Loss: 44.9752\n",
      "Epoch [93/300], Step [32/172], Loss: 40.8926\n",
      "Epoch [93/300], Step [33/172], Loss: 71.8971\n",
      "Epoch [93/300], Step [34/172], Loss: 4.8282\n",
      "Epoch [93/300], Step [35/172], Loss: 18.6682\n",
      "Epoch [93/300], Step [36/172], Loss: 21.3952\n",
      "Epoch [93/300], Step [37/172], Loss: 17.9402\n",
      "Epoch [93/300], Step [38/172], Loss: 27.4788\n",
      "Epoch [93/300], Step [39/172], Loss: 48.6605\n",
      "Epoch [93/300], Step [40/172], Loss: 21.7355\n",
      "Epoch [93/300], Step [41/172], Loss: 39.0854\n",
      "Epoch [93/300], Step [42/172], Loss: 42.8635\n",
      "Epoch [93/300], Step [43/172], Loss: 27.1814\n",
      "Epoch [93/300], Step [44/172], Loss: 22.1462\n",
      "Epoch [93/300], Step [45/172], Loss: 22.0696\n",
      "Epoch [93/300], Step [46/172], Loss: 25.4900\n",
      "Epoch [93/300], Step [47/172], Loss: 51.6398\n",
      "Epoch [93/300], Step [48/172], Loss: 53.4828\n",
      "Epoch [93/300], Step [49/172], Loss: 20.5651\n",
      "Epoch [93/300], Step [50/172], Loss: 52.3179\n",
      "Epoch [93/300], Step [51/172], Loss: 7.4328\n",
      "Epoch [93/300], Step [52/172], Loss: 18.9087\n",
      "Epoch [93/300], Step [53/172], Loss: 24.7071\n",
      "Epoch [93/300], Step [54/172], Loss: 12.0700\n",
      "Epoch [93/300], Step [55/172], Loss: 12.6461\n",
      "Epoch [93/300], Step [56/172], Loss: 10.3026\n",
      "Epoch [93/300], Step [57/172], Loss: 23.1595\n",
      "Epoch [93/300], Step [58/172], Loss: 18.5965\n",
      "Epoch [93/300], Step [59/172], Loss: 31.2696\n",
      "Epoch [93/300], Step [60/172], Loss: 48.6614\n",
      "Epoch [93/300], Step [61/172], Loss: 9.6031\n",
      "Epoch [93/300], Step [62/172], Loss: 23.9219\n",
      "Epoch [93/300], Step [63/172], Loss: 8.8092\n",
      "Epoch [93/300], Step [64/172], Loss: 8.0634\n",
      "Epoch [93/300], Step [65/172], Loss: 21.2426\n",
      "Epoch [93/300], Step [66/172], Loss: 6.1119\n",
      "Epoch [93/300], Step [67/172], Loss: 28.7352\n",
      "Epoch [93/300], Step [68/172], Loss: 6.8712\n",
      "Epoch [93/300], Step [69/172], Loss: 65.3371\n",
      "Epoch [93/300], Step [70/172], Loss: 58.5991\n",
      "Epoch [93/300], Step [71/172], Loss: 53.9253\n",
      "Epoch [93/300], Step [72/172], Loss: 59.4003\n",
      "Epoch [93/300], Step [73/172], Loss: 63.2729\n",
      "Epoch [93/300], Step [74/172], Loss: 36.3222\n",
      "Epoch [93/300], Step [75/172], Loss: 33.5615\n",
      "Epoch [93/300], Step [76/172], Loss: 39.0734\n",
      "Epoch [93/300], Step [77/172], Loss: 62.1488\n",
      "Epoch [93/300], Step [78/172], Loss: 50.1849\n",
      "Epoch [93/300], Step [79/172], Loss: 49.3513\n",
      "Epoch [93/300], Step [80/172], Loss: 58.3553\n",
      "Epoch [93/300], Step [81/172], Loss: 42.8526\n",
      "Epoch [93/300], Step [82/172], Loss: 40.4814\n",
      "Epoch [93/300], Step [83/172], Loss: 51.1387\n",
      "Epoch [93/300], Step [84/172], Loss: 38.7749\n",
      "Epoch [93/300], Step [85/172], Loss: 42.2416\n",
      "Epoch [93/300], Step [86/172], Loss: 34.4386\n",
      "Epoch [93/300], Step [87/172], Loss: 29.1932\n",
      "Epoch [93/300], Step [88/172], Loss: 30.9165\n",
      "Epoch [93/300], Step [89/172], Loss: 27.2582\n",
      "Epoch [93/300], Step [90/172], Loss: 26.4433\n",
      "Epoch [93/300], Step [91/172], Loss: 30.2545\n",
      "Epoch [93/300], Step [92/172], Loss: 22.2712\n",
      "Epoch [93/300], Step [93/172], Loss: 22.0476\n",
      "Epoch [93/300], Step [94/172], Loss: 29.8525\n",
      "Epoch [93/300], Step [95/172], Loss: 23.6140\n",
      "Epoch [93/300], Step [96/172], Loss: 19.3513\n",
      "Epoch [93/300], Step [97/172], Loss: 26.4600\n",
      "Epoch [93/300], Step [98/172], Loss: 20.4380\n",
      "Epoch [93/300], Step [99/172], Loss: 18.4240\n",
      "Epoch [93/300], Step [100/172], Loss: 16.7613\n",
      "Epoch [93/300], Step [101/172], Loss: 18.1900\n",
      "Epoch [93/300], Step [102/172], Loss: 16.7485\n",
      "Epoch [93/300], Step [103/172], Loss: 14.8746\n",
      "Epoch [93/300], Step [104/172], Loss: 16.4130\n",
      "Epoch [93/300], Step [105/172], Loss: 18.7151\n",
      "Epoch [93/300], Step [106/172], Loss: 17.6890\n",
      "Epoch [93/300], Step [107/172], Loss: 15.9677\n",
      "Epoch [93/300], Step [108/172], Loss: 17.3768\n",
      "Epoch [93/300], Step [109/172], Loss: 18.7355\n",
      "Epoch [93/300], Step [110/172], Loss: 16.6133\n",
      "Epoch [93/300], Step [111/172], Loss: 15.0140\n",
      "Epoch [93/300], Step [112/172], Loss: 19.4232\n",
      "Epoch [93/300], Step [113/172], Loss: 15.6880\n",
      "Epoch [93/300], Step [114/172], Loss: 14.9299\n",
      "Epoch [93/300], Step [115/172], Loss: 22.7312\n",
      "Epoch [93/300], Step [116/172], Loss: 15.4474\n",
      "Epoch [93/300], Step [117/172], Loss: 12.8182\n",
      "Epoch [93/300], Step [118/172], Loss: 16.0127\n",
      "Epoch [93/300], Step [119/172], Loss: 16.0615\n",
      "Epoch [93/300], Step [120/172], Loss: 11.9708\n",
      "Epoch [93/300], Step [121/172], Loss: 11.8870\n",
      "Epoch [93/300], Step [122/172], Loss: 11.4114\n",
      "Epoch [93/300], Step [123/172], Loss: 11.1476\n",
      "Epoch [93/300], Step [124/172], Loss: 9.0000\n",
      "Epoch [93/300], Step [125/172], Loss: 13.7091\n",
      "Epoch [93/300], Step [126/172], Loss: 11.4635\n",
      "Epoch [93/300], Step [127/172], Loss: 13.1557\n",
      "Epoch [93/300], Step [128/172], Loss: 13.4934\n",
      "Epoch [93/300], Step [129/172], Loss: 9.7344\n",
      "Epoch [93/300], Step [130/172], Loss: 12.1298\n",
      "Epoch [93/300], Step [131/172], Loss: 9.7690\n",
      "Epoch [93/300], Step [132/172], Loss: 9.7634\n",
      "Epoch [93/300], Step [133/172], Loss: 10.8494\n",
      "Epoch [93/300], Step [134/172], Loss: 12.5734\n",
      "Epoch [93/300], Step [135/172], Loss: 9.5780\n",
      "Epoch [93/300], Step [136/172], Loss: 9.1875\n",
      "Epoch [93/300], Step [137/172], Loss: 10.8265\n",
      "Epoch [93/300], Step [138/172], Loss: 9.0952\n",
      "Epoch [93/300], Step [139/172], Loss: 10.4692\n",
      "Epoch [93/300], Step [140/172], Loss: 10.3775\n",
      "Epoch [93/300], Step [141/172], Loss: 12.7445\n",
      "Epoch [93/300], Step [142/172], Loss: 14.3578\n",
      "Epoch [93/300], Step [143/172], Loss: 9.9189\n",
      "Epoch [93/300], Step [144/172], Loss: 9.4671\n",
      "Epoch [93/300], Step [145/172], Loss: 10.4610\n",
      "Epoch [93/300], Step [146/172], Loss: 10.4727\n",
      "Epoch [93/300], Step [147/172], Loss: 6.0948\n",
      "Epoch [93/300], Step [148/172], Loss: 7.2514\n",
      "Epoch [93/300], Step [149/172], Loss: 8.8132\n",
      "Epoch [93/300], Step [150/172], Loss: 8.6799\n",
      "Epoch [93/300], Step [151/172], Loss: 7.8348\n",
      "Epoch [93/300], Step [152/172], Loss: 8.2882\n",
      "Epoch [93/300], Step [153/172], Loss: 7.6973\n",
      "Epoch [93/300], Step [154/172], Loss: 8.4353\n",
      "Epoch [93/300], Step [155/172], Loss: 7.5560\n",
      "Epoch [93/300], Step [156/172], Loss: 12.7241\n",
      "Epoch [93/300], Step [157/172], Loss: 10.7730\n",
      "Epoch [93/300], Step [158/172], Loss: 8.3309\n",
      "Epoch [93/300], Step [159/172], Loss: 9.9674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/300], Step [160/172], Loss: 10.3730\n",
      "Epoch [93/300], Step [161/172], Loss: 7.6261\n",
      "Epoch [93/300], Step [162/172], Loss: 8.0078\n",
      "Epoch [93/300], Step [163/172], Loss: 7.0088\n",
      "Epoch [93/300], Step [164/172], Loss: 10.2358\n",
      "Epoch [93/300], Step [165/172], Loss: 6.8493\n",
      "Epoch [93/300], Step [166/172], Loss: 7.2293\n",
      "Epoch [93/300], Step [167/172], Loss: 9.1372\n",
      "Epoch [93/300], Step [168/172], Loss: 7.4606\n",
      "Epoch [93/300], Step [169/172], Loss: 7.6416\n",
      "Epoch [93/300], Step [170/172], Loss: 6.2179\n",
      "Epoch [93/300], Step [171/172], Loss: 6.5876\n",
      "Epoch [93/300], Step [172/172], Loss: 5.7726\n",
      "Epoch [94/300], Step [1/172], Loss: 83.6520\n",
      "Epoch [94/300], Step [2/172], Loss: 83.7622\n",
      "Epoch [94/300], Step [3/172], Loss: 78.3027\n",
      "Epoch [94/300], Step [4/172], Loss: 48.4971\n",
      "Epoch [94/300], Step [5/172], Loss: 69.2232\n",
      "Epoch [94/300], Step [6/172], Loss: 20.6286\n",
      "Epoch [94/300], Step [7/172], Loss: 28.4309\n",
      "Epoch [94/300], Step [8/172], Loss: 6.7789\n",
      "Epoch [94/300], Step [9/172], Loss: 44.3419\n",
      "Epoch [94/300], Step [10/172], Loss: 48.1225\n",
      "Epoch [94/300], Step [11/172], Loss: 87.1167\n",
      "Epoch [94/300], Step [12/172], Loss: 83.8892\n",
      "Epoch [94/300], Step [13/172], Loss: 42.0370\n",
      "Epoch [94/300], Step [14/172], Loss: 89.6335\n",
      "Epoch [94/300], Step [15/172], Loss: 75.6739\n",
      "Epoch [94/300], Step [16/172], Loss: 23.7676\n",
      "Epoch [94/300], Step [17/172], Loss: 58.4257\n",
      "Epoch [94/300], Step [18/172], Loss: 62.2930\n",
      "Epoch [94/300], Step [19/172], Loss: 83.3773\n",
      "Epoch [94/300], Step [20/172], Loss: 74.8854\n",
      "Epoch [94/300], Step [21/172], Loss: 94.9376\n",
      "Epoch [94/300], Step [22/172], Loss: 80.5509\n",
      "Epoch [94/300], Step [23/172], Loss: 4.8244\n",
      "Epoch [94/300], Step [24/172], Loss: 71.8188\n",
      "Epoch [94/300], Step [25/172], Loss: 47.1388\n",
      "Epoch [94/300], Step [26/172], Loss: 59.9976\n",
      "Epoch [94/300], Step [27/172], Loss: 78.0970\n",
      "Epoch [94/300], Step [28/172], Loss: 38.7266\n",
      "Epoch [94/300], Step [29/172], Loss: 28.2175\n",
      "Epoch [94/300], Step [30/172], Loss: 83.1908\n",
      "Epoch [94/300], Step [31/172], Loss: 44.9215\n",
      "Epoch [94/300], Step [32/172], Loss: 41.0173\n",
      "Epoch [94/300], Step [33/172], Loss: 71.9882\n",
      "Epoch [94/300], Step [34/172], Loss: 4.6895\n",
      "Epoch [94/300], Step [35/172], Loss: 18.3450\n",
      "Epoch [94/300], Step [36/172], Loss: 21.2680\n",
      "Epoch [94/300], Step [37/172], Loss: 17.8861\n",
      "Epoch [94/300], Step [38/172], Loss: 27.4295\n",
      "Epoch [94/300], Step [39/172], Loss: 48.4153\n",
      "Epoch [94/300], Step [40/172], Loss: 21.4546\n",
      "Epoch [94/300], Step [41/172], Loss: 38.8794\n",
      "Epoch [94/300], Step [42/172], Loss: 42.4365\n",
      "Epoch [94/300], Step [43/172], Loss: 26.9808\n",
      "Epoch [94/300], Step [44/172], Loss: 21.8633\n",
      "Epoch [94/300], Step [45/172], Loss: 21.9143\n",
      "Epoch [94/300], Step [46/172], Loss: 25.1798\n",
      "Epoch [94/300], Step [47/172], Loss: 51.2838\n",
      "Epoch [94/300], Step [48/172], Loss: 53.4934\n",
      "Epoch [94/300], Step [49/172], Loss: 20.3625\n",
      "Epoch [94/300], Step [50/172], Loss: 52.3666\n",
      "Epoch [94/300], Step [51/172], Loss: 7.3845\n",
      "Epoch [94/300], Step [52/172], Loss: 18.7387\n",
      "Epoch [94/300], Step [53/172], Loss: 24.4739\n",
      "Epoch [94/300], Step [54/172], Loss: 11.9449\n",
      "Epoch [94/300], Step [55/172], Loss: 12.5655\n",
      "Epoch [94/300], Step [56/172], Loss: 10.3562\n",
      "Epoch [94/300], Step [57/172], Loss: 22.8220\n",
      "Epoch [94/300], Step [58/172], Loss: 18.3877\n",
      "Epoch [94/300], Step [59/172], Loss: 31.2330\n",
      "Epoch [94/300], Step [60/172], Loss: 47.9499\n",
      "Epoch [94/300], Step [61/172], Loss: 9.5318\n",
      "Epoch [94/300], Step [62/172], Loss: 23.8298\n",
      "Epoch [94/300], Step [63/172], Loss: 8.8519\n",
      "Epoch [94/300], Step [64/172], Loss: 8.1150\n",
      "Epoch [94/300], Step [65/172], Loss: 21.0542\n",
      "Epoch [94/300], Step [66/172], Loss: 6.0933\n",
      "Epoch [94/300], Step [67/172], Loss: 28.5432\n",
      "Epoch [94/300], Step [68/172], Loss: 6.8409\n",
      "Epoch [94/300], Step [69/172], Loss: 64.7273\n",
      "Epoch [94/300], Step [70/172], Loss: 58.1676\n",
      "Epoch [94/300], Step [71/172], Loss: 53.5929\n",
      "Epoch [94/300], Step [72/172], Loss: 59.0523\n",
      "Epoch [94/300], Step [73/172], Loss: 63.0216\n",
      "Epoch [94/300], Step [74/172], Loss: 36.0471\n",
      "Epoch [94/300], Step [75/172], Loss: 33.4453\n",
      "Epoch [94/300], Step [76/172], Loss: 38.8593\n",
      "Epoch [94/300], Step [77/172], Loss: 61.9953\n",
      "Epoch [94/300], Step [78/172], Loss: 49.8459\n",
      "Epoch [94/300], Step [79/172], Loss: 49.1360\n",
      "Epoch [94/300], Step [80/172], Loss: 58.3618\n",
      "Epoch [94/300], Step [81/172], Loss: 42.6407\n",
      "Epoch [94/300], Step [82/172], Loss: 40.4892\n",
      "Epoch [94/300], Step [83/172], Loss: 51.0049\n",
      "Epoch [94/300], Step [84/172], Loss: 38.6294\n",
      "Epoch [94/300], Step [85/172], Loss: 42.0162\n",
      "Epoch [94/300], Step [86/172], Loss: 34.2748\n",
      "Epoch [94/300], Step [87/172], Loss: 29.0580\n",
      "Epoch [94/300], Step [88/172], Loss: 30.7125\n",
      "Epoch [94/300], Step [89/172], Loss: 27.0806\n",
      "Epoch [94/300], Step [90/172], Loss: 26.2796\n",
      "Epoch [94/300], Step [91/172], Loss: 30.1540\n",
      "Epoch [94/300], Step [92/172], Loss: 22.1658\n",
      "Epoch [94/300], Step [93/172], Loss: 21.9065\n",
      "Epoch [94/300], Step [94/172], Loss: 29.7525\n",
      "Epoch [94/300], Step [95/172], Loss: 23.4456\n",
      "Epoch [94/300], Step [96/172], Loss: 19.2733\n",
      "Epoch [94/300], Step [97/172], Loss: 26.3625\n",
      "Epoch [94/300], Step [98/172], Loss: 20.2817\n",
      "Epoch [94/300], Step [99/172], Loss: 18.3267\n",
      "Epoch [94/300], Step [100/172], Loss: 16.6502\n",
      "Epoch [94/300], Step [101/172], Loss: 18.0298\n",
      "Epoch [94/300], Step [102/172], Loss: 16.7491\n",
      "Epoch [94/300], Step [103/172], Loss: 14.7582\n",
      "Epoch [94/300], Step [104/172], Loss: 16.3217\n",
      "Epoch [94/300], Step [105/172], Loss: 18.7026\n",
      "Epoch [94/300], Step [106/172], Loss: 17.5850\n",
      "Epoch [94/300], Step [107/172], Loss: 15.9178\n",
      "Epoch [94/300], Step [108/172], Loss: 17.2681\n",
      "Epoch [94/300], Step [109/172], Loss: 18.6183\n",
      "Epoch [94/300], Step [110/172], Loss: 16.5054\n",
      "Epoch [94/300], Step [111/172], Loss: 14.9090\n",
      "Epoch [94/300], Step [112/172], Loss: 19.3137\n",
      "Epoch [94/300], Step [113/172], Loss: 15.5735\n",
      "Epoch [94/300], Step [114/172], Loss: 14.8308\n",
      "Epoch [94/300], Step [115/172], Loss: 22.6045\n",
      "Epoch [94/300], Step [116/172], Loss: 15.3215\n",
      "Epoch [94/300], Step [117/172], Loss: 12.7244\n",
      "Epoch [94/300], Step [118/172], Loss: 15.9480\n",
      "Epoch [94/300], Step [119/172], Loss: 16.0064\n",
      "Epoch [94/300], Step [120/172], Loss: 11.9077\n",
      "Epoch [94/300], Step [121/172], Loss: 11.7986\n",
      "Epoch [94/300], Step [122/172], Loss: 11.3396\n",
      "Epoch [94/300], Step [123/172], Loss: 11.0791\n",
      "Epoch [94/300], Step [124/172], Loss: 8.9412\n",
      "Epoch [94/300], Step [125/172], Loss: 13.6225\n",
      "Epoch [94/300], Step [126/172], Loss: 11.3855\n",
      "Epoch [94/300], Step [127/172], Loss: 13.0412\n",
      "Epoch [94/300], Step [128/172], Loss: 13.3562\n",
      "Epoch [94/300], Step [129/172], Loss: 9.6630\n",
      "Epoch [94/300], Step [130/172], Loss: 12.0897\n",
      "Epoch [94/300], Step [131/172], Loss: 9.6887\n",
      "Epoch [94/300], Step [132/172], Loss: 9.6923\n",
      "Epoch [94/300], Step [133/172], Loss: 10.7741\n",
      "Epoch [94/300], Step [134/172], Loss: 12.5254\n",
      "Epoch [94/300], Step [135/172], Loss: 9.5434\n",
      "Epoch [94/300], Step [136/172], Loss: 9.1601\n",
      "Epoch [94/300], Step [137/172], Loss: 10.7614\n",
      "Epoch [94/300], Step [138/172], Loss: 9.0324\n",
      "Epoch [94/300], Step [139/172], Loss: 10.4138\n",
      "Epoch [94/300], Step [140/172], Loss: 10.3092\n",
      "Epoch [94/300], Step [141/172], Loss: 12.6562\n",
      "Epoch [94/300], Step [142/172], Loss: 14.2706\n",
      "Epoch [94/300], Step [143/172], Loss: 9.8894\n",
      "Epoch [94/300], Step [144/172], Loss: 9.4387\n",
      "Epoch [94/300], Step [145/172], Loss: 10.3993\n",
      "Epoch [94/300], Step [146/172], Loss: 10.4344\n",
      "Epoch [94/300], Step [147/172], Loss: 6.0469\n",
      "Epoch [94/300], Step [148/172], Loss: 7.1995\n",
      "Epoch [94/300], Step [149/172], Loss: 8.7268\n",
      "Epoch [94/300], Step [150/172], Loss: 8.6244\n",
      "Epoch [94/300], Step [151/172], Loss: 7.7920\n",
      "Epoch [94/300], Step [152/172], Loss: 8.2420\n",
      "Epoch [94/300], Step [153/172], Loss: 7.6576\n",
      "Epoch [94/300], Step [154/172], Loss: 8.3961\n",
      "Epoch [94/300], Step [155/172], Loss: 7.5141\n",
      "Epoch [94/300], Step [156/172], Loss: 12.7131\n",
      "Epoch [94/300], Step [157/172], Loss: 10.7374\n",
      "Epoch [94/300], Step [158/172], Loss: 8.2861\n",
      "Epoch [94/300], Step [159/172], Loss: 9.9354\n",
      "Epoch [94/300], Step [160/172], Loss: 10.3612\n",
      "Epoch [94/300], Step [161/172], Loss: 7.5865\n",
      "Epoch [94/300], Step [162/172], Loss: 7.9511\n",
      "Epoch [94/300], Step [163/172], Loss: 6.9774\n",
      "Epoch [94/300], Step [164/172], Loss: 10.2534\n",
      "Epoch [94/300], Step [165/172], Loss: 6.8088\n",
      "Epoch [94/300], Step [166/172], Loss: 7.1648\n",
      "Epoch [94/300], Step [167/172], Loss: 9.1373\n",
      "Epoch [94/300], Step [168/172], Loss: 7.4313\n",
      "Epoch [94/300], Step [169/172], Loss: 7.6002\n",
      "Epoch [94/300], Step [170/172], Loss: 6.1839\n",
      "Epoch [94/300], Step [171/172], Loss: 6.5766\n",
      "Epoch [94/300], Step [172/172], Loss: 5.7492\n",
      "Epoch [95/300], Step [1/172], Loss: 83.4914\n",
      "Epoch [95/300], Step [2/172], Loss: 83.4250\n",
      "Epoch [95/300], Step [3/172], Loss: 77.8362\n",
      "Epoch [95/300], Step [4/172], Loss: 48.1235\n",
      "Epoch [95/300], Step [5/172], Loss: 68.9212\n",
      "Epoch [95/300], Step [6/172], Loss: 20.5788\n",
      "Epoch [95/300], Step [7/172], Loss: 28.5358\n",
      "Epoch [95/300], Step [8/172], Loss: 6.4655\n",
      "Epoch [95/300], Step [9/172], Loss: 44.1335\n",
      "Epoch [95/300], Step [10/172], Loss: 47.9803\n",
      "Epoch [95/300], Step [11/172], Loss: 86.4429\n",
      "Epoch [95/300], Step [12/172], Loss: 83.8952\n",
      "Epoch [95/300], Step [13/172], Loss: 41.9108\n",
      "Epoch [95/300], Step [14/172], Loss: 89.1284\n",
      "Epoch [95/300], Step [15/172], Loss: 75.2296\n",
      "Epoch [95/300], Step [16/172], Loss: 22.8576\n",
      "Epoch [95/300], Step [17/172], Loss: 58.1190\n",
      "Epoch [95/300], Step [18/172], Loss: 62.1179\n",
      "Epoch [95/300], Step [19/172], Loss: 83.3118\n",
      "Epoch [95/300], Step [20/172], Loss: 73.7785\n",
      "Epoch [95/300], Step [21/172], Loss: 94.8349\n",
      "Epoch [95/300], Step [22/172], Loss: 79.8450\n",
      "Epoch [95/300], Step [23/172], Loss: 4.7476\n",
      "Epoch [95/300], Step [24/172], Loss: 71.5364\n",
      "Epoch [95/300], Step [25/172], Loss: 47.2796\n",
      "Epoch [95/300], Step [26/172], Loss: 59.9498\n",
      "Epoch [95/300], Step [27/172], Loss: 77.4004\n",
      "Epoch [95/300], Step [28/172], Loss: 38.4855\n",
      "Epoch [95/300], Step [29/172], Loss: 27.9344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/300], Step [30/172], Loss: 83.4162\n",
      "Epoch [95/300], Step [31/172], Loss: 45.3054\n",
      "Epoch [95/300], Step [32/172], Loss: 41.2358\n",
      "Epoch [95/300], Step [33/172], Loss: 72.1676\n",
      "Epoch [95/300], Step [34/172], Loss: 4.7367\n",
      "Epoch [95/300], Step [35/172], Loss: 17.8480\n",
      "Epoch [95/300], Step [36/172], Loss: 21.2976\n",
      "Epoch [95/300], Step [37/172], Loss: 18.0811\n",
      "Epoch [95/300], Step [38/172], Loss: 27.6370\n",
      "Epoch [95/300], Step [39/172], Loss: 47.7750\n",
      "Epoch [95/300], Step [40/172], Loss: 21.6869\n",
      "Epoch [95/300], Step [41/172], Loss: 39.0084\n",
      "Epoch [95/300], Step [42/172], Loss: 42.8785\n",
      "Epoch [95/300], Step [43/172], Loss: 27.2343\n",
      "Epoch [95/300], Step [44/172], Loss: 22.0345\n",
      "Epoch [95/300], Step [45/172], Loss: 22.2736\n",
      "Epoch [95/300], Step [46/172], Loss: 25.0360\n",
      "Epoch [95/300], Step [47/172], Loss: 51.4104\n",
      "Epoch [95/300], Step [48/172], Loss: 53.3239\n",
      "Epoch [95/300], Step [49/172], Loss: 20.4890\n",
      "Epoch [95/300], Step [50/172], Loss: 51.7971\n",
      "Epoch [95/300], Step [51/172], Loss: 7.4502\n",
      "Epoch [95/300], Step [52/172], Loss: 18.8717\n",
      "Epoch [95/300], Step [53/172], Loss: 24.7168\n",
      "Epoch [95/300], Step [54/172], Loss: 12.0420\n",
      "Epoch [95/300], Step [55/172], Loss: 12.6648\n",
      "Epoch [95/300], Step [56/172], Loss: 10.3979\n",
      "Epoch [95/300], Step [57/172], Loss: 22.5739\n",
      "Epoch [95/300], Step [58/172], Loss: 18.3123\n",
      "Epoch [95/300], Step [59/172], Loss: 31.1278\n",
      "Epoch [95/300], Step [60/172], Loss: 47.2632\n",
      "Epoch [95/300], Step [61/172], Loss: 9.4023\n",
      "Epoch [95/300], Step [62/172], Loss: 23.7326\n",
      "Epoch [95/300], Step [63/172], Loss: 8.8700\n",
      "Epoch [95/300], Step [64/172], Loss: 8.1398\n",
      "Epoch [95/300], Step [65/172], Loss: 21.0289\n",
      "Epoch [95/300], Step [66/172], Loss: 6.1350\n",
      "Epoch [95/300], Step [67/172], Loss: 28.2266\n",
      "Epoch [95/300], Step [68/172], Loss: 6.8418\n",
      "Epoch [95/300], Step [69/172], Loss: 64.2091\n",
      "Epoch [95/300], Step [70/172], Loss: 57.9592\n",
      "Epoch [95/300], Step [71/172], Loss: 53.3530\n",
      "Epoch [95/300], Step [72/172], Loss: 58.6890\n",
      "Epoch [95/300], Step [73/172], Loss: 62.6319\n",
      "Epoch [95/300], Step [74/172], Loss: 35.7737\n",
      "Epoch [95/300], Step [75/172], Loss: 33.0130\n",
      "Epoch [95/300], Step [76/172], Loss: 38.5204\n",
      "Epoch [95/300], Step [77/172], Loss: 61.8620\n",
      "Epoch [95/300], Step [78/172], Loss: 49.4991\n",
      "Epoch [95/300], Step [79/172], Loss: 48.9008\n",
      "Epoch [95/300], Step [80/172], Loss: 58.0452\n",
      "Epoch [95/300], Step [81/172], Loss: 42.4022\n",
      "Epoch [95/300], Step [82/172], Loss: 40.0410\n",
      "Epoch [95/300], Step [83/172], Loss: 50.8682\n",
      "Epoch [95/300], Step [84/172], Loss: 38.4771\n",
      "Epoch [95/300], Step [85/172], Loss: 41.9437\n",
      "Epoch [95/300], Step [86/172], Loss: 34.1262\n",
      "Epoch [95/300], Step [87/172], Loss: 28.9492\n",
      "Epoch [95/300], Step [88/172], Loss: 30.6834\n",
      "Epoch [95/300], Step [89/172], Loss: 26.9373\n",
      "Epoch [95/300], Step [90/172], Loss: 26.1576\n",
      "Epoch [95/300], Step [91/172], Loss: 30.0684\n",
      "Epoch [95/300], Step [92/172], Loss: 22.1276\n",
      "Epoch [95/300], Step [93/172], Loss: 21.9113\n",
      "Epoch [95/300], Step [94/172], Loss: 29.8164\n",
      "Epoch [95/300], Step [95/172], Loss: 23.4704\n",
      "Epoch [95/300], Step [96/172], Loss: 19.2180\n",
      "Epoch [95/300], Step [97/172], Loss: 26.3352\n",
      "Epoch [95/300], Step [98/172], Loss: 20.1998\n",
      "Epoch [95/300], Step [99/172], Loss: 18.3101\n",
      "Epoch [95/300], Step [100/172], Loss: 16.5615\n",
      "Epoch [95/300], Step [101/172], Loss: 17.9662\n",
      "Epoch [95/300], Step [102/172], Loss: 16.5495\n",
      "Epoch [95/300], Step [103/172], Loss: 14.6537\n",
      "Epoch [95/300], Step [104/172], Loss: 16.2912\n",
      "Epoch [95/300], Step [105/172], Loss: 18.5638\n",
      "Epoch [95/300], Step [106/172], Loss: 17.5699\n",
      "Epoch [95/300], Step [107/172], Loss: 15.9109\n",
      "Epoch [95/300], Step [108/172], Loss: 17.2034\n",
      "Epoch [95/300], Step [109/172], Loss: 18.4992\n",
      "Epoch [95/300], Step [110/172], Loss: 16.4449\n",
      "Epoch [95/300], Step [111/172], Loss: 14.8554\n",
      "Epoch [95/300], Step [112/172], Loss: 19.2529\n",
      "Epoch [95/300], Step [113/172], Loss: 15.4968\n",
      "Epoch [95/300], Step [114/172], Loss: 14.7612\n",
      "Epoch [95/300], Step [115/172], Loss: 22.5431\n",
      "Epoch [95/300], Step [116/172], Loss: 15.2958\n",
      "Epoch [95/300], Step [117/172], Loss: 12.6474\n",
      "Epoch [95/300], Step [118/172], Loss: 15.8985\n",
      "Epoch [95/300], Step [119/172], Loss: 16.0280\n",
      "Epoch [95/300], Step [120/172], Loss: 11.8452\n",
      "Epoch [95/300], Step [121/172], Loss: 11.6977\n",
      "Epoch [95/300], Step [122/172], Loss: 11.2330\n",
      "Epoch [95/300], Step [123/172], Loss: 11.0166\n",
      "Epoch [95/300], Step [124/172], Loss: 8.8908\n",
      "Epoch [95/300], Step [125/172], Loss: 13.5961\n",
      "Epoch [95/300], Step [126/172], Loss: 11.3473\n",
      "Epoch [95/300], Step [127/172], Loss: 12.9385\n",
      "Epoch [95/300], Step [128/172], Loss: 13.2740\n",
      "Epoch [95/300], Step [129/172], Loss: 9.6073\n",
      "Epoch [95/300], Step [130/172], Loss: 12.0571\n",
      "Epoch [95/300], Step [131/172], Loss: 9.6089\n",
      "Epoch [95/300], Step [132/172], Loss: 9.6413\n",
      "Epoch [95/300], Step [133/172], Loss: 10.6823\n",
      "Epoch [95/300], Step [134/172], Loss: 12.4895\n",
      "Epoch [95/300], Step [135/172], Loss: 9.4978\n",
      "Epoch [95/300], Step [136/172], Loss: 9.1261\n",
      "Epoch [95/300], Step [137/172], Loss: 10.7132\n",
      "Epoch [95/300], Step [138/172], Loss: 8.9888\n",
      "Epoch [95/300], Step [139/172], Loss: 10.3898\n",
      "Epoch [95/300], Step [140/172], Loss: 10.2768\n",
      "Epoch [95/300], Step [141/172], Loss: 12.5420\n",
      "Epoch [95/300], Step [142/172], Loss: 14.1954\n",
      "Epoch [95/300], Step [143/172], Loss: 9.8812\n",
      "Epoch [95/300], Step [144/172], Loss: 9.3997\n",
      "Epoch [95/300], Step [145/172], Loss: 10.3419\n",
      "Epoch [95/300], Step [146/172], Loss: 10.3932\n",
      "Epoch [95/300], Step [147/172], Loss: 5.9992\n",
      "Epoch [95/300], Step [148/172], Loss: 7.1609\n",
      "Epoch [95/300], Step [149/172], Loss: 8.6866\n",
      "Epoch [95/300], Step [150/172], Loss: 8.5617\n",
      "Epoch [95/300], Step [151/172], Loss: 7.7318\n",
      "Epoch [95/300], Step [152/172], Loss: 8.2348\n",
      "Epoch [95/300], Step [153/172], Loss: 7.6052\n",
      "Epoch [95/300], Step [154/172], Loss: 8.3586\n",
      "Epoch [95/300], Step [155/172], Loss: 7.4484\n",
      "Epoch [95/300], Step [156/172], Loss: 12.7038\n",
      "Epoch [95/300], Step [157/172], Loss: 10.6970\n",
      "Epoch [95/300], Step [158/172], Loss: 8.2356\n",
      "Epoch [95/300], Step [159/172], Loss: 9.8894\n",
      "Epoch [95/300], Step [160/172], Loss: 10.3386\n",
      "Epoch [95/300], Step [161/172], Loss: 7.5481\n",
      "Epoch [95/300], Step [162/172], Loss: 7.9470\n",
      "Epoch [95/300], Step [163/172], Loss: 6.9418\n",
      "Epoch [95/300], Step [164/172], Loss: 10.1025\n",
      "Epoch [95/300], Step [165/172], Loss: 6.7743\n",
      "Epoch [95/300], Step [166/172], Loss: 7.0660\n",
      "Epoch [95/300], Step [167/172], Loss: 9.1550\n",
      "Epoch [95/300], Step [168/172], Loss: 7.4023\n",
      "Epoch [95/300], Step [169/172], Loss: 7.5408\n",
      "Epoch [95/300], Step [170/172], Loss: 6.1357\n",
      "Epoch [95/300], Step [171/172], Loss: 6.5688\n",
      "Epoch [95/300], Step [172/172], Loss: 5.6941\n",
      "Epoch [96/300], Step [1/172], Loss: 82.8156\n",
      "Epoch [96/300], Step [2/172], Loss: 82.8937\n",
      "Epoch [96/300], Step [3/172], Loss: 77.2325\n",
      "Epoch [96/300], Step [4/172], Loss: 47.6876\n",
      "Epoch [96/300], Step [5/172], Loss: 68.4079\n",
      "Epoch [96/300], Step [6/172], Loss: 20.3443\n",
      "Epoch [96/300], Step [7/172], Loss: 28.8380\n",
      "Epoch [96/300], Step [8/172], Loss: 7.1292\n",
      "Epoch [96/300], Step [9/172], Loss: 44.1793\n",
      "Epoch [96/300], Step [10/172], Loss: 47.6675\n",
      "Epoch [96/300], Step [11/172], Loss: 86.0262\n",
      "Epoch [96/300], Step [12/172], Loss: 83.7073\n",
      "Epoch [96/300], Step [13/172], Loss: 41.8911\n",
      "Epoch [96/300], Step [14/172], Loss: 89.0104\n",
      "Epoch [96/300], Step [15/172], Loss: 75.1220\n",
      "Epoch [96/300], Step [16/172], Loss: 22.5733\n",
      "Epoch [96/300], Step [17/172], Loss: 58.1990\n",
      "Epoch [96/300], Step [18/172], Loss: 62.2364\n",
      "Epoch [96/300], Step [19/172], Loss: 83.4419\n",
      "Epoch [96/300], Step [20/172], Loss: 73.5841\n",
      "Epoch [96/300], Step [21/172], Loss: 95.0009\n",
      "Epoch [96/300], Step [22/172], Loss: 79.8980\n",
      "Epoch [96/300], Step [23/172], Loss: 4.4335\n",
      "Epoch [96/300], Step [24/172], Loss: 71.2360\n",
      "Epoch [96/300], Step [25/172], Loss: 46.8104\n",
      "Epoch [96/300], Step [26/172], Loss: 59.4600\n",
      "Epoch [96/300], Step [27/172], Loss: 77.1550\n",
      "Epoch [96/300], Step [28/172], Loss: 37.8696\n",
      "Epoch [96/300], Step [29/172], Loss: 27.2880\n",
      "Epoch [96/300], Step [30/172], Loss: 83.1262\n",
      "Epoch [96/300], Step [31/172], Loss: 44.8778\n",
      "Epoch [96/300], Step [32/172], Loss: 40.9708\n",
      "Epoch [96/300], Step [33/172], Loss: 71.6894\n",
      "Epoch [96/300], Step [34/172], Loss: 4.5083\n",
      "Epoch [96/300], Step [35/172], Loss: 17.5390\n",
      "Epoch [96/300], Step [36/172], Loss: 21.2024\n",
      "Epoch [96/300], Step [37/172], Loss: 17.7892\n",
      "Epoch [96/300], Step [38/172], Loss: 27.2761\n",
      "Epoch [96/300], Step [39/172], Loss: 47.4724\n",
      "Epoch [96/300], Step [40/172], Loss: 21.2074\n",
      "Epoch [96/300], Step [41/172], Loss: 38.6013\n",
      "Epoch [96/300], Step [42/172], Loss: 42.2957\n",
      "Epoch [96/300], Step [43/172], Loss: 26.8918\n",
      "Epoch [96/300], Step [44/172], Loss: 21.5524\n",
      "Epoch [96/300], Step [45/172], Loss: 21.8921\n",
      "Epoch [96/300], Step [46/172], Loss: 24.6235\n",
      "Epoch [96/300], Step [47/172], Loss: 50.7125\n",
      "Epoch [96/300], Step [48/172], Loss: 52.5351\n",
      "Epoch [96/300], Step [49/172], Loss: 20.1940\n",
      "Epoch [96/300], Step [50/172], Loss: 51.9205\n",
      "Epoch [96/300], Step [51/172], Loss: 7.3437\n",
      "Epoch [96/300], Step [52/172], Loss: 18.5774\n",
      "Epoch [96/300], Step [53/172], Loss: 24.2956\n",
      "Epoch [96/300], Step [54/172], Loss: 11.8217\n",
      "Epoch [96/300], Step [55/172], Loss: 12.4692\n",
      "Epoch [96/300], Step [56/172], Loss: 10.3504\n",
      "Epoch [96/300], Step [57/172], Loss: 22.2009\n",
      "Epoch [96/300], Step [58/172], Loss: 18.1942\n",
      "Epoch [96/300], Step [59/172], Loss: 31.0365\n",
      "Epoch [96/300], Step [60/172], Loss: 47.5722\n",
      "Epoch [96/300], Step [61/172], Loss: 9.4479\n",
      "Epoch [96/300], Step [62/172], Loss: 23.6829\n",
      "Epoch [96/300], Step [63/172], Loss: 8.8318\n",
      "Epoch [96/300], Step [64/172], Loss: 8.1442\n",
      "Epoch [96/300], Step [65/172], Loss: 20.9826\n",
      "Epoch [96/300], Step [66/172], Loss: 6.0849\n",
      "Epoch [96/300], Step [67/172], Loss: 28.4980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/300], Step [68/172], Loss: 6.7950\n",
      "Epoch [96/300], Step [69/172], Loss: 63.9994\n",
      "Epoch [96/300], Step [70/172], Loss: 57.6596\n",
      "Epoch [96/300], Step [71/172], Loss: 53.2674\n",
      "Epoch [96/300], Step [72/172], Loss: 58.6790\n",
      "Epoch [96/300], Step [73/172], Loss: 62.5700\n",
      "Epoch [96/300], Step [74/172], Loss: 35.7481\n",
      "Epoch [96/300], Step [75/172], Loss: 33.3290\n",
      "Epoch [96/300], Step [76/172], Loss: 38.5348\n",
      "Epoch [96/300], Step [77/172], Loss: 61.9687\n",
      "Epoch [96/300], Step [78/172], Loss: 49.5306\n",
      "Epoch [96/300], Step [79/172], Loss: 48.9785\n",
      "Epoch [96/300], Step [80/172], Loss: 58.4433\n",
      "Epoch [96/300], Step [81/172], Loss: 42.4235\n",
      "Epoch [96/300], Step [82/172], Loss: 40.3706\n",
      "Epoch [96/300], Step [83/172], Loss: 50.8503\n",
      "Epoch [96/300], Step [84/172], Loss: 38.4844\n",
      "Epoch [96/300], Step [85/172], Loss: 41.9441\n",
      "Epoch [96/300], Step [86/172], Loss: 34.1841\n",
      "Epoch [96/300], Step [87/172], Loss: 28.9527\n",
      "Epoch [96/300], Step [88/172], Loss: 30.6565\n",
      "Epoch [96/300], Step [89/172], Loss: 26.9322\n",
      "Epoch [96/300], Step [90/172], Loss: 26.1094\n",
      "Epoch [96/300], Step [91/172], Loss: 30.0848\n",
      "Epoch [96/300], Step [92/172], Loss: 22.1141\n",
      "Epoch [96/300], Step [93/172], Loss: 21.8745\n",
      "Epoch [96/300], Step [94/172], Loss: 29.7857\n",
      "Epoch [96/300], Step [95/172], Loss: 23.3997\n",
      "Epoch [96/300], Step [96/172], Loss: 19.2313\n",
      "Epoch [96/300], Step [97/172], Loss: 26.3314\n",
      "Epoch [96/300], Step [98/172], Loss: 20.1417\n",
      "Epoch [96/300], Step [99/172], Loss: 18.2666\n",
      "Epoch [96/300], Step [100/172], Loss: 16.5116\n",
      "Epoch [96/300], Step [101/172], Loss: 17.8874\n",
      "Epoch [96/300], Step [102/172], Loss: 16.6052\n",
      "Epoch [96/300], Step [103/172], Loss: 14.5649\n",
      "Epoch [96/300], Step [104/172], Loss: 16.2576\n",
      "Epoch [96/300], Step [105/172], Loss: 18.6206\n",
      "Epoch [96/300], Step [106/172], Loss: 17.4831\n",
      "Epoch [96/300], Step [107/172], Loss: 15.8842\n",
      "Epoch [96/300], Step [108/172], Loss: 17.1317\n",
      "Epoch [96/300], Step [109/172], Loss: 18.3584\n",
      "Epoch [96/300], Step [110/172], Loss: 16.3925\n",
      "Epoch [96/300], Step [111/172], Loss: 14.8133\n",
      "Epoch [96/300], Step [112/172], Loss: 19.2383\n",
      "Epoch [96/300], Step [113/172], Loss: 15.3906\n",
      "Epoch [96/300], Step [114/172], Loss: 14.7021\n",
      "Epoch [96/300], Step [115/172], Loss: 22.3618\n",
      "Epoch [96/300], Step [116/172], Loss: 15.2380\n",
      "Epoch [96/300], Step [117/172], Loss: 12.5507\n",
      "Epoch [96/300], Step [118/172], Loss: 15.7395\n",
      "Epoch [96/300], Step [119/172], Loss: 16.0303\n",
      "Epoch [96/300], Step [120/172], Loss: 11.7798\n",
      "Epoch [96/300], Step [121/172], Loss: 11.6260\n",
      "Epoch [96/300], Step [122/172], Loss: 11.1669\n",
      "Epoch [96/300], Step [123/172], Loss: 10.9772\n",
      "Epoch [96/300], Step [124/172], Loss: 8.8696\n",
      "Epoch [96/300], Step [125/172], Loss: 13.5231\n",
      "Epoch [96/300], Step [126/172], Loss: 11.3006\n",
      "Epoch [96/300], Step [127/172], Loss: 12.8808\n",
      "Epoch [96/300], Step [128/172], Loss: 13.2318\n",
      "Epoch [96/300], Step [129/172], Loss: 9.5619\n",
      "Epoch [96/300], Step [130/172], Loss: 12.0329\n",
      "Epoch [96/300], Step [131/172], Loss: 9.5537\n",
      "Epoch [96/300], Step [132/172], Loss: 9.6083\n",
      "Epoch [96/300], Step [133/172], Loss: 10.6284\n",
      "Epoch [96/300], Step [134/172], Loss: 12.4682\n",
      "Epoch [96/300], Step [135/172], Loss: 9.4703\n",
      "Epoch [96/300], Step [136/172], Loss: 9.1310\n",
      "Epoch [96/300], Step [137/172], Loss: 10.6864\n",
      "Epoch [96/300], Step [138/172], Loss: 8.9565\n",
      "Epoch [96/300], Step [139/172], Loss: 10.3687\n",
      "Epoch [96/300], Step [140/172], Loss: 10.2348\n",
      "Epoch [96/300], Step [141/172], Loss: 12.4561\n",
      "Epoch [96/300], Step [142/172], Loss: 14.1374\n",
      "Epoch [96/300], Step [143/172], Loss: 9.8614\n",
      "Epoch [96/300], Step [144/172], Loss: 9.3720\n",
      "Epoch [96/300], Step [145/172], Loss: 10.2994\n",
      "Epoch [96/300], Step [146/172], Loss: 10.3735\n",
      "Epoch [96/300], Step [147/172], Loss: 5.9657\n",
      "Epoch [96/300], Step [148/172], Loss: 7.1235\n",
      "Epoch [96/300], Step [149/172], Loss: 8.6159\n",
      "Epoch [96/300], Step [150/172], Loss: 8.5343\n",
      "Epoch [96/300], Step [151/172], Loss: 7.6849\n",
      "Epoch [96/300], Step [152/172], Loss: 8.2094\n",
      "Epoch [96/300], Step [153/172], Loss: 7.5757\n",
      "Epoch [96/300], Step [154/172], Loss: 8.3301\n",
      "Epoch [96/300], Step [155/172], Loss: 7.4172\n",
      "Epoch [96/300], Step [156/172], Loss: 12.7030\n",
      "Epoch [96/300], Step [157/172], Loss: 10.6798\n",
      "Epoch [96/300], Step [158/172], Loss: 8.2040\n",
      "Epoch [96/300], Step [159/172], Loss: 9.8983\n",
      "Epoch [96/300], Step [160/172], Loss: 10.3372\n",
      "Epoch [96/300], Step [161/172], Loss: 7.5330\n",
      "Epoch [96/300], Step [162/172], Loss: 7.8884\n",
      "Epoch [96/300], Step [163/172], Loss: 6.9144\n",
      "Epoch [96/300], Step [164/172], Loss: 10.1464\n",
      "Epoch [96/300], Step [165/172], Loss: 6.7470\n",
      "Epoch [96/300], Step [166/172], Loss: 7.0371\n",
      "Epoch [96/300], Step [167/172], Loss: 9.1801\n",
      "Epoch [96/300], Step [168/172], Loss: 7.3954\n",
      "Epoch [96/300], Step [169/172], Loss: 7.5280\n",
      "Epoch [96/300], Step [170/172], Loss: 6.1055\n",
      "Epoch [96/300], Step [171/172], Loss: 6.5981\n",
      "Epoch [96/300], Step [172/172], Loss: 5.6703\n",
      "Epoch [97/300], Step [1/172], Loss: 82.5381\n",
      "Epoch [97/300], Step [2/172], Loss: 82.2721\n",
      "Epoch [97/300], Step [3/172], Loss: 76.4875\n",
      "Epoch [97/300], Step [4/172], Loss: 47.2625\n",
      "Epoch [97/300], Step [5/172], Loss: 68.0736\n",
      "Epoch [97/300], Step [6/172], Loss: 20.2244\n",
      "Epoch [97/300], Step [7/172], Loss: 28.7260\n",
      "Epoch [97/300], Step [8/172], Loss: 6.4836\n",
      "Epoch [97/300], Step [9/172], Loss: 43.7787\n",
      "Epoch [97/300], Step [10/172], Loss: 47.5460\n",
      "Epoch [97/300], Step [11/172], Loss: 85.4788\n",
      "Epoch [97/300], Step [12/172], Loss: 83.6888\n",
      "Epoch [97/300], Step [13/172], Loss: 41.8025\n",
      "Epoch [97/300], Step [14/172], Loss: 88.4407\n",
      "Epoch [97/300], Step [15/172], Loss: 74.6937\n",
      "Epoch [97/300], Step [16/172], Loss: 21.8229\n",
      "Epoch [97/300], Step [17/172], Loss: 58.0087\n",
      "Epoch [97/300], Step [18/172], Loss: 62.2059\n",
      "Epoch [97/300], Step [19/172], Loss: 83.4186\n",
      "Epoch [97/300], Step [20/172], Loss: 72.1029\n",
      "Epoch [97/300], Step [21/172], Loss: 94.9393\n",
      "Epoch [97/300], Step [22/172], Loss: 79.3428\n",
      "Epoch [97/300], Step [23/172], Loss: 4.3916\n",
      "Epoch [97/300], Step [24/172], Loss: 70.9986\n",
      "Epoch [97/300], Step [25/172], Loss: 46.5800\n",
      "Epoch [97/300], Step [26/172], Loss: 59.3447\n",
      "Epoch [97/300], Step [27/172], Loss: 76.5586\n",
      "Epoch [97/300], Step [28/172], Loss: 37.4425\n",
      "Epoch [97/300], Step [29/172], Loss: 26.8956\n",
      "Epoch [97/300], Step [30/172], Loss: 83.0637\n",
      "Epoch [97/300], Step [31/172], Loss: 44.7179\n",
      "Epoch [97/300], Step [32/172], Loss: 40.9362\n",
      "Epoch [97/300], Step [33/172], Loss: 71.6160\n",
      "Epoch [97/300], Step [34/172], Loss: 4.6228\n",
      "Epoch [97/300], Step [35/172], Loss: 17.5950\n",
      "Epoch [97/300], Step [36/172], Loss: 21.1045\n",
      "Epoch [97/300], Step [37/172], Loss: 17.7918\n",
      "Epoch [97/300], Step [38/172], Loss: 27.3114\n",
      "Epoch [97/300], Step [39/172], Loss: 47.2160\n",
      "Epoch [97/300], Step [40/172], Loss: 21.1888\n",
      "Epoch [97/300], Step [41/172], Loss: 38.6081\n",
      "Epoch [97/300], Step [42/172], Loss: 42.3006\n",
      "Epoch [97/300], Step [43/172], Loss: 26.8989\n",
      "Epoch [97/300], Step [44/172], Loss: 21.6111\n",
      "Epoch [97/300], Step [45/172], Loss: 22.0397\n",
      "Epoch [97/300], Step [46/172], Loss: 24.6526\n",
      "Epoch [97/300], Step [47/172], Loss: 50.6277\n",
      "Epoch [97/300], Step [48/172], Loss: 53.1444\n",
      "Epoch [97/300], Step [49/172], Loss: 20.2174\n",
      "Epoch [97/300], Step [50/172], Loss: 51.9371\n",
      "Epoch [97/300], Step [51/172], Loss: 7.3526\n",
      "Epoch [97/300], Step [52/172], Loss: 18.5310\n",
      "Epoch [97/300], Step [53/172], Loss: 24.1971\n",
      "Epoch [97/300], Step [54/172], Loss: 11.9174\n",
      "Epoch [97/300], Step [55/172], Loss: 12.4944\n",
      "Epoch [97/300], Step [56/172], Loss: 10.3663\n",
      "Epoch [97/300], Step [57/172], Loss: 21.7735\n",
      "Epoch [97/300], Step [58/172], Loss: 18.0720\n",
      "Epoch [97/300], Step [59/172], Loss: 30.9717\n",
      "Epoch [97/300], Step [60/172], Loss: 47.1035\n",
      "Epoch [97/300], Step [61/172], Loss: 9.3480\n",
      "Epoch [97/300], Step [62/172], Loss: 23.5149\n",
      "Epoch [97/300], Step [63/172], Loss: 8.7924\n",
      "Epoch [97/300], Step [64/172], Loss: 8.1351\n",
      "Epoch [97/300], Step [65/172], Loss: 20.8399\n",
      "Epoch [97/300], Step [66/172], Loss: 6.0562\n",
      "Epoch [97/300], Step [67/172], Loss: 28.3446\n",
      "Epoch [97/300], Step [68/172], Loss: 6.7483\n",
      "Epoch [97/300], Step [69/172], Loss: 63.5699\n",
      "Epoch [97/300], Step [70/172], Loss: 57.6141\n",
      "Epoch [97/300], Step [71/172], Loss: 53.2221\n",
      "Epoch [97/300], Step [72/172], Loss: 58.6357\n",
      "Epoch [97/300], Step [73/172], Loss: 62.4692\n",
      "Epoch [97/300], Step [74/172], Loss: 35.6619\n",
      "Epoch [97/300], Step [75/172], Loss: 33.3374\n",
      "Epoch [97/300], Step [76/172], Loss: 38.3452\n",
      "Epoch [97/300], Step [77/172], Loss: 61.9890\n",
      "Epoch [97/300], Step [78/172], Loss: 49.3720\n",
      "Epoch [97/300], Step [79/172], Loss: 48.8568\n",
      "Epoch [97/300], Step [80/172], Loss: 58.3514\n",
      "Epoch [97/300], Step [81/172], Loss: 42.2867\n",
      "Epoch [97/300], Step [82/172], Loss: 40.1301\n",
      "Epoch [97/300], Step [83/172], Loss: 50.6690\n",
      "Epoch [97/300], Step [84/172], Loss: 38.3394\n",
      "Epoch [97/300], Step [85/172], Loss: 41.7989\n",
      "Epoch [97/300], Step [86/172], Loss: 34.0603\n",
      "Epoch [97/300], Step [87/172], Loss: 28.8709\n",
      "Epoch [97/300], Step [88/172], Loss: 30.4834\n",
      "Epoch [97/300], Step [89/172], Loss: 26.8356\n",
      "Epoch [97/300], Step [90/172], Loss: 26.0044\n",
      "Epoch [97/300], Step [91/172], Loss: 29.9800\n",
      "Epoch [97/300], Step [92/172], Loss: 22.0513\n",
      "Epoch [97/300], Step [93/172], Loss: 21.8234\n",
      "Epoch [97/300], Step [94/172], Loss: 29.7678\n",
      "Epoch [97/300], Step [95/172], Loss: 23.2840\n",
      "Epoch [97/300], Step [96/172], Loss: 19.1693\n",
      "Epoch [97/300], Step [97/172], Loss: 26.2693\n",
      "Epoch [97/300], Step [98/172], Loss: 20.0434\n",
      "Epoch [97/300], Step [99/172], Loss: 18.2141\n",
      "Epoch [97/300], Step [100/172], Loss: 16.4310\n",
      "Epoch [97/300], Step [101/172], Loss: 17.7805\n",
      "Epoch [97/300], Step [102/172], Loss: 16.4813\n",
      "Epoch [97/300], Step [103/172], Loss: 14.4696\n",
      "Epoch [97/300], Step [104/172], Loss: 16.2046\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/300], Step [105/172], Loss: 18.5032\n",
      "Epoch [97/300], Step [106/172], Loss: 17.4402\n",
      "Epoch [97/300], Step [107/172], Loss: 15.8586\n",
      "Epoch [97/300], Step [108/172], Loss: 17.0445\n",
      "Epoch [97/300], Step [109/172], Loss: 18.2660\n",
      "Epoch [97/300], Step [110/172], Loss: 16.3390\n",
      "Epoch [97/300], Step [111/172], Loss: 14.7545\n",
      "Epoch [97/300], Step [112/172], Loss: 19.1548\n",
      "Epoch [97/300], Step [113/172], Loss: 15.3033\n",
      "Epoch [97/300], Step [114/172], Loss: 14.6166\n",
      "Epoch [97/300], Step [115/172], Loss: 22.3081\n",
      "Epoch [97/300], Step [116/172], Loss: 15.1560\n",
      "Epoch [97/300], Step [117/172], Loss: 12.4761\n",
      "Epoch [97/300], Step [118/172], Loss: 15.7179\n",
      "Epoch [97/300], Step [119/172], Loss: 16.0104\n",
      "Epoch [97/300], Step [120/172], Loss: 11.7255\n",
      "Epoch [97/300], Step [121/172], Loss: 11.5400\n",
      "Epoch [97/300], Step [122/172], Loss: 11.1259\n",
      "Epoch [97/300], Step [123/172], Loss: 10.9323\n",
      "Epoch [97/300], Step [124/172], Loss: 8.8209\n",
      "Epoch [97/300], Step [125/172], Loss: 13.4764\n",
      "Epoch [97/300], Step [126/172], Loss: 11.2393\n",
      "Epoch [97/300], Step [127/172], Loss: 12.7904\n",
      "Epoch [97/300], Step [128/172], Loss: 13.1279\n",
      "Epoch [97/300], Step [129/172], Loss: 9.4974\n",
      "Epoch [97/300], Step [130/172], Loss: 12.0000\n",
      "Epoch [97/300], Step [131/172], Loss: 9.4886\n",
      "Epoch [97/300], Step [132/172], Loss: 9.5472\n",
      "Epoch [97/300], Step [133/172], Loss: 10.5666\n",
      "Epoch [97/300], Step [134/172], Loss: 12.4368\n",
      "Epoch [97/300], Step [135/172], Loss: 9.4292\n",
      "Epoch [97/300], Step [136/172], Loss: 9.0748\n",
      "Epoch [97/300], Step [137/172], Loss: 10.6171\n",
      "Epoch [97/300], Step [138/172], Loss: 8.8877\n",
      "Epoch [97/300], Step [139/172], Loss: 10.3127\n",
      "Epoch [97/300], Step [140/172], Loss: 10.1778\n",
      "Epoch [97/300], Step [141/172], Loss: 12.3582\n",
      "Epoch [97/300], Step [142/172], Loss: 14.1027\n",
      "Epoch [97/300], Step [143/172], Loss: 9.8439\n",
      "Epoch [97/300], Step [144/172], Loss: 9.3296\n",
      "Epoch [97/300], Step [145/172], Loss: 10.2546\n",
      "Epoch [97/300], Step [146/172], Loss: 10.3167\n",
      "Epoch [97/300], Step [147/172], Loss: 5.9254\n",
      "Epoch [97/300], Step [148/172], Loss: 7.0807\n",
      "Epoch [97/300], Step [149/172], Loss: 8.5502\n",
      "Epoch [97/300], Step [150/172], Loss: 8.4779\n",
      "Epoch [97/300], Step [151/172], Loss: 7.6414\n",
      "Epoch [97/300], Step [152/172], Loss: 8.1765\n",
      "Epoch [97/300], Step [153/172], Loss: 7.5267\n",
      "Epoch [97/300], Step [154/172], Loss: 8.2986\n",
      "Epoch [97/300], Step [155/172], Loss: 7.3610\n",
      "Epoch [97/300], Step [156/172], Loss: 12.7066\n",
      "Epoch [97/300], Step [157/172], Loss: 10.6524\n",
      "Epoch [97/300], Step [158/172], Loss: 8.1773\n",
      "Epoch [97/300], Step [159/172], Loss: 9.8659\n",
      "Epoch [97/300], Step [160/172], Loss: 10.3536\n",
      "Epoch [97/300], Step [161/172], Loss: 7.4980\n",
      "Epoch [97/300], Step [162/172], Loss: 7.8521\n",
      "Epoch [97/300], Step [163/172], Loss: 6.8903\n",
      "Epoch [97/300], Step [164/172], Loss: 10.0813\n",
      "Epoch [97/300], Step [165/172], Loss: 6.7122\n",
      "Epoch [97/300], Step [166/172], Loss: 6.9672\n",
      "Epoch [97/300], Step [167/172], Loss: 9.2055\n",
      "Epoch [97/300], Step [168/172], Loss: 7.3750\n",
      "Epoch [97/300], Step [169/172], Loss: 7.5168\n",
      "Epoch [97/300], Step [170/172], Loss: 6.0715\n",
      "Epoch [97/300], Step [171/172], Loss: 6.5949\n",
      "Epoch [97/300], Step [172/172], Loss: 5.6338\n",
      "Epoch [98/300], Step [1/172], Loss: 82.1019\n",
      "Epoch [98/300], Step [2/172], Loss: 81.7437\n",
      "Epoch [98/300], Step [3/172], Loss: 76.0502\n",
      "Epoch [98/300], Step [4/172], Loss: 46.9483\n",
      "Epoch [98/300], Step [5/172], Loss: 67.6458\n",
      "Epoch [98/300], Step [6/172], Loss: 19.9976\n",
      "Epoch [98/300], Step [7/172], Loss: 28.4968\n",
      "Epoch [98/300], Step [8/172], Loss: 6.6863\n",
      "Epoch [98/300], Step [9/172], Loss: 43.6480\n",
      "Epoch [98/300], Step [10/172], Loss: 47.3917\n",
      "Epoch [98/300], Step [11/172], Loss: 84.9927\n",
      "Epoch [98/300], Step [12/172], Loss: 83.6118\n",
      "Epoch [98/300], Step [13/172], Loss: 41.8953\n",
      "Epoch [98/300], Step [14/172], Loss: 88.3158\n",
      "Epoch [98/300], Step [15/172], Loss: 74.5998\n",
      "Epoch [98/300], Step [16/172], Loss: 21.5275\n",
      "Epoch [98/300], Step [17/172], Loss: 57.9755\n",
      "Epoch [98/300], Step [18/172], Loss: 62.2560\n",
      "Epoch [98/300], Step [19/172], Loss: 83.4922\n",
      "Epoch [98/300], Step [20/172], Loss: 71.7385\n",
      "Epoch [98/300], Step [21/172], Loss: 95.1342\n",
      "Epoch [98/300], Step [22/172], Loss: 79.3466\n",
      "Epoch [98/300], Step [23/172], Loss: 4.2491\n",
      "Epoch [98/300], Step [24/172], Loss: 70.9070\n",
      "Epoch [98/300], Step [25/172], Loss: 46.6485\n",
      "Epoch [98/300], Step [26/172], Loss: 59.3117\n",
      "Epoch [98/300], Step [27/172], Loss: 76.6380\n",
      "Epoch [98/300], Step [28/172], Loss: 37.3020\n",
      "Epoch [98/300], Step [29/172], Loss: 26.6909\n",
      "Epoch [98/300], Step [30/172], Loss: 83.4921\n",
      "Epoch [98/300], Step [31/172], Loss: 44.9399\n",
      "Epoch [98/300], Step [32/172], Loss: 41.1388\n",
      "Epoch [98/300], Step [33/172], Loss: 71.7335\n",
      "Epoch [98/300], Step [34/172], Loss: 4.5055\n",
      "Epoch [98/300], Step [35/172], Loss: 17.1618\n",
      "Epoch [98/300], Step [36/172], Loss: 20.9699\n",
      "Epoch [98/300], Step [37/172], Loss: 17.8072\n",
      "Epoch [98/300], Step [38/172], Loss: 27.4172\n",
      "Epoch [98/300], Step [39/172], Loss: 46.9832\n",
      "Epoch [98/300], Step [40/172], Loss: 21.1056\n",
      "Epoch [98/300], Step [41/172], Loss: 38.5228\n",
      "Epoch [98/300], Step [42/172], Loss: 42.2295\n",
      "Epoch [98/300], Step [43/172], Loss: 26.9065\n",
      "Epoch [98/300], Step [44/172], Loss: 21.4888\n",
      "Epoch [98/300], Step [45/172], Loss: 22.0389\n",
      "Epoch [98/300], Step [46/172], Loss: 24.3927\n",
      "Epoch [98/300], Step [47/172], Loss: 50.4493\n",
      "Epoch [98/300], Step [48/172], Loss: 52.7539\n",
      "Epoch [98/300], Step [49/172], Loss: 19.9903\n",
      "Epoch [98/300], Step [50/172], Loss: 51.8027\n",
      "Epoch [98/300], Step [51/172], Loss: 7.3452\n",
      "Epoch [98/300], Step [52/172], Loss: 18.4504\n",
      "Epoch [98/300], Step [53/172], Loss: 24.0848\n",
      "Epoch [98/300], Step [54/172], Loss: 11.7689\n",
      "Epoch [98/300], Step [55/172], Loss: 12.3678\n",
      "Epoch [98/300], Step [56/172], Loss: 10.3299\n",
      "Epoch [98/300], Step [57/172], Loss: 21.6004\n",
      "Epoch [98/300], Step [58/172], Loss: 17.8903\n",
      "Epoch [98/300], Step [59/172], Loss: 30.7984\n",
      "Epoch [98/300], Step [60/172], Loss: 46.8850\n",
      "Epoch [98/300], Step [61/172], Loss: 9.3206\n",
      "Epoch [98/300], Step [62/172], Loss: 23.4511\n",
      "Epoch [98/300], Step [63/172], Loss: 8.7953\n",
      "Epoch [98/300], Step [64/172], Loss: 8.1383\n",
      "Epoch [98/300], Step [65/172], Loss: 20.7632\n",
      "Epoch [98/300], Step [66/172], Loss: 5.9704\n",
      "Epoch [98/300], Step [67/172], Loss: 28.2934\n",
      "Epoch [98/300], Step [68/172], Loss: 6.5147\n",
      "Epoch [98/300], Step [69/172], Loss: 63.4218\n",
      "Epoch [98/300], Step [70/172], Loss: 57.5988\n",
      "Epoch [98/300], Step [71/172], Loss: 53.1810\n",
      "Epoch [98/300], Step [72/172], Loss: 58.5805\n",
      "Epoch [98/300], Step [73/172], Loss: 62.3881\n",
      "Epoch [98/300], Step [74/172], Loss: 35.5833\n",
      "Epoch [98/300], Step [75/172], Loss: 33.2937\n",
      "Epoch [98/300], Step [76/172], Loss: 38.2369\n",
      "Epoch [98/300], Step [77/172], Loss: 62.0545\n",
      "Epoch [98/300], Step [78/172], Loss: 49.2311\n",
      "Epoch [98/300], Step [79/172], Loss: 48.8015\n",
      "Epoch [98/300], Step [80/172], Loss: 58.5288\n",
      "Epoch [98/300], Step [81/172], Loss: 42.2007\n",
      "Epoch [98/300], Step [82/172], Loss: 40.3021\n",
      "Epoch [98/300], Step [83/172], Loss: 50.5801\n",
      "Epoch [98/300], Step [84/172], Loss: 38.2378\n",
      "Epoch [98/300], Step [85/172], Loss: 41.7155\n",
      "Epoch [98/300], Step [86/172], Loss: 33.9903\n",
      "Epoch [98/300], Step [87/172], Loss: 28.8395\n",
      "Epoch [98/300], Step [88/172], Loss: 30.4647\n",
      "Epoch [98/300], Step [89/172], Loss: 26.7601\n",
      "Epoch [98/300], Step [90/172], Loss: 25.9552\n",
      "Epoch [98/300], Step [91/172], Loss: 29.9116\n",
      "Epoch [98/300], Step [92/172], Loss: 22.0294\n",
      "Epoch [98/300], Step [93/172], Loss: 21.8010\n",
      "Epoch [98/300], Step [94/172], Loss: 29.7426\n",
      "Epoch [98/300], Step [95/172], Loss: 23.2734\n",
      "Epoch [98/300], Step [96/172], Loss: 19.1343\n",
      "Epoch [98/300], Step [97/172], Loss: 26.2435\n",
      "Epoch [98/300], Step [98/172], Loss: 20.0100\n",
      "Epoch [98/300], Step [99/172], Loss: 18.1921\n",
      "Epoch [98/300], Step [100/172], Loss: 16.3855\n",
      "Epoch [98/300], Step [101/172], Loss: 17.7168\n",
      "Epoch [98/300], Step [102/172], Loss: 16.5554\n",
      "Epoch [98/300], Step [103/172], Loss: 14.4176\n",
      "Epoch [98/300], Step [104/172], Loss: 16.1622\n",
      "Epoch [98/300], Step [105/172], Loss: 18.5647\n",
      "Epoch [98/300], Step [106/172], Loss: 17.3950\n",
      "Epoch [98/300], Step [107/172], Loss: 15.8464\n",
      "Epoch [98/300], Step [108/172], Loss: 16.9917\n",
      "Epoch [98/300], Step [109/172], Loss: 18.1970\n",
      "Epoch [98/300], Step [110/172], Loss: 16.2803\n",
      "Epoch [98/300], Step [111/172], Loss: 14.6848\n",
      "Epoch [98/300], Step [112/172], Loss: 19.1297\n",
      "Epoch [98/300], Step [113/172], Loss: 15.2424\n",
      "Epoch [98/300], Step [114/172], Loss: 14.5627\n",
      "Epoch [98/300], Step [115/172], Loss: 22.2428\n",
      "Epoch [98/300], Step [116/172], Loss: 15.1108\n",
      "Epoch [98/300], Step [117/172], Loss: 12.4038\n",
      "Epoch [98/300], Step [118/172], Loss: 15.6398\n",
      "Epoch [98/300], Step [119/172], Loss: 16.0155\n",
      "Epoch [98/300], Step [120/172], Loss: 11.6928\n",
      "Epoch [98/300], Step [121/172], Loss: 11.4791\n",
      "Epoch [98/300], Step [122/172], Loss: 11.0203\n",
      "Epoch [98/300], Step [123/172], Loss: 10.8935\n",
      "Epoch [98/300], Step [124/172], Loss: 8.7820\n",
      "Epoch [98/300], Step [125/172], Loss: 13.4217\n",
      "Epoch [98/300], Step [126/172], Loss: 11.1989\n",
      "Epoch [98/300], Step [127/172], Loss: 12.6900\n",
      "Epoch [98/300], Step [128/172], Loss: 13.0351\n",
      "Epoch [98/300], Step [129/172], Loss: 9.4455\n",
      "Epoch [98/300], Step [130/172], Loss: 11.9658\n",
      "Epoch [98/300], Step [131/172], Loss: 9.4204\n",
      "Epoch [98/300], Step [132/172], Loss: 9.5046\n",
      "Epoch [98/300], Step [133/172], Loss: 10.4786\n",
      "Epoch [98/300], Step [134/172], Loss: 12.3982\n",
      "Epoch [98/300], Step [135/172], Loss: 9.4124\n",
      "Epoch [98/300], Step [136/172], Loss: 9.0423\n",
      "Epoch [98/300], Step [137/172], Loss: 10.5735\n",
      "Epoch [98/300], Step [138/172], Loss: 8.8476\n",
      "Epoch [98/300], Step [139/172], Loss: 10.2854\n",
      "Epoch [98/300], Step [140/172], Loss: 10.1299\n",
      "Epoch [98/300], Step [141/172], Loss: 12.3030\n",
      "Epoch [98/300], Step [142/172], Loss: 14.0164\n",
      "Epoch [98/300], Step [143/172], Loss: 9.8308\n",
      "Epoch [98/300], Step [144/172], Loss: 9.3133\n",
      "Epoch [98/300], Step [145/172], Loss: 10.2105\n",
      "Epoch [98/300], Step [146/172], Loss: 10.2954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/300], Step [147/172], Loss: 5.8971\n",
      "Epoch [98/300], Step [148/172], Loss: 7.0472\n",
      "Epoch [98/300], Step [149/172], Loss: 8.4974\n",
      "Epoch [98/300], Step [150/172], Loss: 8.4334\n",
      "Epoch [98/300], Step [151/172], Loss: 7.6210\n",
      "Epoch [98/300], Step [152/172], Loss: 8.1603\n",
      "Epoch [98/300], Step [153/172], Loss: 7.5046\n",
      "Epoch [98/300], Step [154/172], Loss: 8.2765\n",
      "Epoch [98/300], Step [155/172], Loss: 7.3376\n",
      "Epoch [98/300], Step [156/172], Loss: 12.6423\n",
      "Epoch [98/300], Step [157/172], Loss: 10.5936\n",
      "Epoch [98/300], Step [158/172], Loss: 8.1161\n",
      "Epoch [98/300], Step [159/172], Loss: 9.8371\n",
      "Epoch [98/300], Step [160/172], Loss: 10.3312\n",
      "Epoch [98/300], Step [161/172], Loss: 7.4972\n",
      "Epoch [98/300], Step [162/172], Loss: 7.8041\n",
      "Epoch [98/300], Step [163/172], Loss: 6.8856\n",
      "Epoch [98/300], Step [164/172], Loss: 10.1315\n",
      "Epoch [98/300], Step [165/172], Loss: 6.7034\n",
      "Epoch [98/300], Step [166/172], Loss: 6.9367\n",
      "Epoch [98/300], Step [167/172], Loss: 9.2335\n",
      "Epoch [98/300], Step [168/172], Loss: 7.3762\n",
      "Epoch [98/300], Step [169/172], Loss: 7.5118\n",
      "Epoch [98/300], Step [170/172], Loss: 6.0513\n",
      "Epoch [98/300], Step [171/172], Loss: 6.6200\n",
      "Epoch [98/300], Step [172/172], Loss: 5.6179\n",
      "Epoch [99/300], Step [1/172], Loss: 81.6167\n",
      "Epoch [99/300], Step [2/172], Loss: 81.2257\n",
      "Epoch [99/300], Step [3/172], Loss: 75.4457\n",
      "Epoch [99/300], Step [4/172], Loss: 46.5245\n",
      "Epoch [99/300], Step [5/172], Loss: 67.3977\n",
      "Epoch [99/300], Step [6/172], Loss: 20.0071\n",
      "Epoch [99/300], Step [7/172], Loss: 28.8313\n",
      "Epoch [99/300], Step [8/172], Loss: 6.3000\n",
      "Epoch [99/300], Step [9/172], Loss: 43.3329\n",
      "Epoch [99/300], Step [10/172], Loss: 47.2099\n",
      "Epoch [99/300], Step [11/172], Loss: 84.2688\n",
      "Epoch [99/300], Step [12/172], Loss: 83.4441\n",
      "Epoch [99/300], Step [13/172], Loss: 41.7062\n",
      "Epoch [99/300], Step [14/172], Loss: 87.5899\n",
      "Epoch [99/300], Step [15/172], Loss: 74.0682\n",
      "Epoch [99/300], Step [16/172], Loss: 20.7542\n",
      "Epoch [99/300], Step [17/172], Loss: 57.6385\n",
      "Epoch [99/300], Step [18/172], Loss: 62.0990\n",
      "Epoch [99/300], Step [19/172], Loss: 83.4168\n",
      "Epoch [99/300], Step [20/172], Loss: 70.4142\n",
      "Epoch [99/300], Step [21/172], Loss: 94.9587\n",
      "Epoch [99/300], Step [22/172], Loss: 78.5719\n",
      "Epoch [99/300], Step [23/172], Loss: 4.3191\n",
      "Epoch [99/300], Step [24/172], Loss: 70.5753\n",
      "Epoch [99/300], Step [25/172], Loss: 46.5773\n",
      "Epoch [99/300], Step [26/172], Loss: 59.0673\n",
      "Epoch [99/300], Step [27/172], Loss: 75.8287\n",
      "Epoch [99/300], Step [28/172], Loss: 36.9723\n",
      "Epoch [99/300], Step [29/172], Loss: 26.3788\n",
      "Epoch [99/300], Step [30/172], Loss: 83.2918\n",
      "Epoch [99/300], Step [31/172], Loss: 44.9895\n",
      "Epoch [99/300], Step [32/172], Loss: 41.2155\n",
      "Epoch [99/300], Step [33/172], Loss: 71.7600\n",
      "Epoch [99/300], Step [34/172], Loss: 4.5338\n",
      "Epoch [99/300], Step [35/172], Loss: 16.9562\n",
      "Epoch [99/300], Step [36/172], Loss: 21.0522\n",
      "Epoch [99/300], Step [37/172], Loss: 17.9212\n",
      "Epoch [99/300], Step [38/172], Loss: 27.5534\n",
      "Epoch [99/300], Step [39/172], Loss: 46.6267\n",
      "Epoch [99/300], Step [40/172], Loss: 21.1229\n",
      "Epoch [99/300], Step [41/172], Loss: 38.4696\n",
      "Epoch [99/300], Step [42/172], Loss: 42.5259\n",
      "Epoch [99/300], Step [43/172], Loss: 26.9662\n",
      "Epoch [99/300], Step [44/172], Loss: 21.4859\n",
      "Epoch [99/300], Step [45/172], Loss: 22.1374\n",
      "Epoch [99/300], Step [46/172], Loss: 24.2486\n",
      "Epoch [99/300], Step [47/172], Loss: 50.3460\n",
      "Epoch [99/300], Step [48/172], Loss: 52.7201\n",
      "Epoch [99/300], Step [49/172], Loss: 20.1129\n",
      "Epoch [99/300], Step [50/172], Loss: 51.6395\n",
      "Epoch [99/300], Step [51/172], Loss: 7.3550\n",
      "Epoch [99/300], Step [52/172], Loss: 18.4508\n",
      "Epoch [99/300], Step [53/172], Loss: 23.9725\n",
      "Epoch [99/300], Step [54/172], Loss: 11.6711\n",
      "Epoch [99/300], Step [55/172], Loss: 12.2787\n",
      "Epoch [99/300], Step [56/172], Loss: 10.2963\n",
      "Epoch [99/300], Step [57/172], Loss: 21.2756\n",
      "Epoch [99/300], Step [58/172], Loss: 17.7910\n",
      "Epoch [99/300], Step [59/172], Loss: 30.6232\n",
      "Epoch [99/300], Step [60/172], Loss: 46.5960\n",
      "Epoch [99/300], Step [61/172], Loss: 9.1861\n",
      "Epoch [99/300], Step [62/172], Loss: 23.2620\n",
      "Epoch [99/300], Step [63/172], Loss: 8.7306\n",
      "Epoch [99/300], Step [64/172], Loss: 8.0834\n",
      "Epoch [99/300], Step [65/172], Loss: 20.7315\n",
      "Epoch [99/300], Step [66/172], Loss: 5.9325\n",
      "Epoch [99/300], Step [67/172], Loss: 27.9096\n",
      "Epoch [99/300], Step [68/172], Loss: 6.4469\n",
      "Epoch [99/300], Step [69/172], Loss: 63.0162\n",
      "Epoch [99/300], Step [70/172], Loss: 57.4848\n",
      "Epoch [99/300], Step [71/172], Loss: 53.1415\n",
      "Epoch [99/300], Step [72/172], Loss: 58.4706\n",
      "Epoch [99/300], Step [73/172], Loss: 62.3392\n",
      "Epoch [99/300], Step [74/172], Loss: 35.4386\n",
      "Epoch [99/300], Step [75/172], Loss: 33.2554\n",
      "Epoch [99/300], Step [76/172], Loss: 38.1358\n",
      "Epoch [99/300], Step [77/172], Loss: 62.0683\n",
      "Epoch [99/300], Step [78/172], Loss: 49.0518\n",
      "Epoch [99/300], Step [79/172], Loss: 48.7134\n",
      "Epoch [99/300], Step [80/172], Loss: 58.3709\n",
      "Epoch [99/300], Step [81/172], Loss: 42.0657\n",
      "Epoch [99/300], Step [82/172], Loss: 39.9055\n",
      "Epoch [99/300], Step [83/172], Loss: 50.4166\n",
      "Epoch [99/300], Step [84/172], Loss: 38.1041\n",
      "Epoch [99/300], Step [85/172], Loss: 41.6272\n",
      "Epoch [99/300], Step [86/172], Loss: 33.8338\n",
      "Epoch [99/300], Step [87/172], Loss: 28.7245\n",
      "Epoch [99/300], Step [88/172], Loss: 30.3504\n",
      "Epoch [99/300], Step [89/172], Loss: 26.6026\n",
      "Epoch [99/300], Step [90/172], Loss: 25.8305\n",
      "Epoch [99/300], Step [91/172], Loss: 29.8295\n",
      "Epoch [99/300], Step [92/172], Loss: 21.9470\n",
      "Epoch [99/300], Step [93/172], Loss: 21.7388\n",
      "Epoch [99/300], Step [94/172], Loss: 29.7165\n",
      "Epoch [99/300], Step [95/172], Loss: 23.2039\n",
      "Epoch [99/300], Step [96/172], Loss: 19.0864\n",
      "Epoch [99/300], Step [97/172], Loss: 26.1886\n",
      "Epoch [99/300], Step [98/172], Loss: 19.8767\n",
      "Epoch [99/300], Step [99/172], Loss: 18.1504\n",
      "Epoch [99/300], Step [100/172], Loss: 16.2985\n",
      "Epoch [99/300], Step [101/172], Loss: 17.6092\n",
      "Epoch [99/300], Step [102/172], Loss: 16.3551\n",
      "Epoch [99/300], Step [103/172], Loss: 14.2981\n",
      "Epoch [99/300], Step [104/172], Loss: 16.1170\n",
      "Epoch [99/300], Step [105/172], Loss: 18.3611\n",
      "Epoch [99/300], Step [106/172], Loss: 17.3245\n",
      "Epoch [99/300], Step [107/172], Loss: 15.7798\n",
      "Epoch [99/300], Step [108/172], Loss: 16.8967\n",
      "Epoch [99/300], Step [109/172], Loss: 18.0251\n",
      "Epoch [99/300], Step [110/172], Loss: 16.2179\n",
      "Epoch [99/300], Step [111/172], Loss: 14.6286\n",
      "Epoch [99/300], Step [112/172], Loss: 19.0314\n",
      "Epoch [99/300], Step [113/172], Loss: 15.1054\n",
      "Epoch [99/300], Step [114/172], Loss: 14.4744\n",
      "Epoch [99/300], Step [115/172], Loss: 22.0841\n",
      "Epoch [99/300], Step [116/172], Loss: 15.0152\n",
      "Epoch [99/300], Step [117/172], Loss: 12.3240\n",
      "Epoch [99/300], Step [118/172], Loss: 15.5498\n",
      "Epoch [99/300], Step [119/172], Loss: 16.0055\n",
      "Epoch [99/300], Step [120/172], Loss: 11.6299\n",
      "Epoch [99/300], Step [121/172], Loss: 11.4153\n",
      "Epoch [99/300], Step [122/172], Loss: 10.9644\n",
      "Epoch [99/300], Step [123/172], Loss: 10.8470\n",
      "Epoch [99/300], Step [124/172], Loss: 8.7439\n",
      "Epoch [99/300], Step [125/172], Loss: 13.4207\n",
      "Epoch [99/300], Step [126/172], Loss: 11.1495\n",
      "Epoch [99/300], Step [127/172], Loss: 12.6113\n",
      "Epoch [99/300], Step [128/172], Loss: 12.9488\n",
      "Epoch [99/300], Step [129/172], Loss: 9.3976\n",
      "Epoch [99/300], Step [130/172], Loss: 11.9271\n",
      "Epoch [99/300], Step [131/172], Loss: 9.3613\n",
      "Epoch [99/300], Step [132/172], Loss: 9.4677\n",
      "Epoch [99/300], Step [133/172], Loss: 10.4104\n",
      "Epoch [99/300], Step [134/172], Loss: 12.3729\n",
      "Epoch [99/300], Step [135/172], Loss: 9.3797\n",
      "Epoch [99/300], Step [136/172], Loss: 9.0056\n",
      "Epoch [99/300], Step [137/172], Loss: 10.5373\n",
      "Epoch [99/300], Step [138/172], Loss: 8.8031\n",
      "Epoch [99/300], Step [139/172], Loss: 10.2594\n",
      "Epoch [99/300], Step [140/172], Loss: 10.0847\n",
      "Epoch [99/300], Step [141/172], Loss: 12.1901\n",
      "Epoch [99/300], Step [142/172], Loss: 13.9775\n",
      "Epoch [99/300], Step [143/172], Loss: 9.8098\n",
      "Epoch [99/300], Step [144/172], Loss: 9.2663\n",
      "Epoch [99/300], Step [145/172], Loss: 10.1973\n",
      "Epoch [99/300], Step [146/172], Loss: 10.2280\n",
      "Epoch [99/300], Step [147/172], Loss: 5.8706\n",
      "Epoch [99/300], Step [148/172], Loss: 7.0169\n",
      "Epoch [99/300], Step [149/172], Loss: 8.4429\n",
      "Epoch [99/300], Step [150/172], Loss: 8.3872\n",
      "Epoch [99/300], Step [151/172], Loss: 7.5662\n",
      "Epoch [99/300], Step [152/172], Loss: 8.1340\n",
      "Epoch [99/300], Step [153/172], Loss: 7.4656\n",
      "Epoch [99/300], Step [154/172], Loss: 8.2481\n",
      "Epoch [99/300], Step [155/172], Loss: 7.2930\n",
      "Epoch [99/300], Step [156/172], Loss: 12.6134\n",
      "Epoch [99/300], Step [157/172], Loss: 10.5531\n",
      "Epoch [99/300], Step [158/172], Loss: 8.0776\n",
      "Epoch [99/300], Step [159/172], Loss: 9.7994\n",
      "Epoch [99/300], Step [160/172], Loss: 10.3283\n",
      "Epoch [99/300], Step [161/172], Loss: 7.4530\n",
      "Epoch [99/300], Step [162/172], Loss: 7.7737\n",
      "Epoch [99/300], Step [163/172], Loss: 6.8503\n",
      "Epoch [99/300], Step [164/172], Loss: 9.9995\n",
      "Epoch [99/300], Step [165/172], Loss: 6.6632\n",
      "Epoch [99/300], Step [166/172], Loss: 6.8692\n",
      "Epoch [99/300], Step [167/172], Loss: 9.2687\n",
      "Epoch [99/300], Step [168/172], Loss: 7.3711\n",
      "Epoch [99/300], Step [169/172], Loss: 7.5095\n",
      "Epoch [99/300], Step [170/172], Loss: 6.0217\n",
      "Epoch [99/300], Step [171/172], Loss: 6.6259\n",
      "Epoch [99/300], Step [172/172], Loss: 5.5993\n",
      "Epoch [100/300], Step [1/172], Loss: 81.2070\n",
      "Epoch [100/300], Step [2/172], Loss: 80.6140\n",
      "Epoch [100/300], Step [3/172], Loss: 75.0158\n",
      "Epoch [100/300], Step [4/172], Loss: 46.2672\n",
      "Epoch [100/300], Step [5/172], Loss: 66.9557\n",
      "Epoch [100/300], Step [6/172], Loss: 19.8744\n",
      "Epoch [100/300], Step [7/172], Loss: 28.7497\n",
      "Epoch [100/300], Step [8/172], Loss: 6.7554\n",
      "Epoch [100/300], Step [9/172], Loss: 43.2971\n",
      "Epoch [100/300], Step [10/172], Loss: 47.1648\n",
      "Epoch [100/300], Step [11/172], Loss: 83.9123\n",
      "Epoch [100/300], Step [12/172], Loss: 83.4795\n",
      "Epoch [100/300], Step [13/172], Loss: 42.0232\n",
      "Epoch [100/300], Step [14/172], Loss: 87.7505\n",
      "Epoch [100/300], Step [15/172], Loss: 74.1414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/300], Step [16/172], Loss: 20.7831\n",
      "Epoch [100/300], Step [17/172], Loss: 57.8473\n",
      "Epoch [100/300], Step [18/172], Loss: 62.3194\n",
      "Epoch [100/300], Step [19/172], Loss: 83.8571\n",
      "Epoch [100/300], Step [20/172], Loss: 70.3055\n",
      "Epoch [100/300], Step [21/172], Loss: 95.5281\n",
      "Epoch [100/300], Step [22/172], Loss: 78.9180\n",
      "Epoch [100/300], Step [23/172], Loss: 4.1997\n",
      "Epoch [100/300], Step [24/172], Loss: 70.6738\n",
      "Epoch [100/300], Step [25/172], Loss: 46.5457\n",
      "Epoch [100/300], Step [26/172], Loss: 59.0081\n",
      "Epoch [100/300], Step [27/172], Loss: 76.2477\n",
      "Epoch [100/300], Step [28/172], Loss: 36.7718\n",
      "Epoch [100/300], Step [29/172], Loss: 26.1719\n",
      "Epoch [100/300], Step [30/172], Loss: 83.3673\n",
      "Epoch [100/300], Step [31/172], Loss: 45.2228\n",
      "Epoch [100/300], Step [32/172], Loss: 41.4406\n",
      "Epoch [100/300], Step [33/172], Loss: 72.0078\n",
      "Epoch [100/300], Step [34/172], Loss: 4.4618\n",
      "Epoch [100/300], Step [35/172], Loss: 16.6410\n",
      "Epoch [100/300], Step [36/172], Loss: 20.8136\n",
      "Epoch [100/300], Step [37/172], Loss: 17.9173\n",
      "Epoch [100/300], Step [38/172], Loss: 27.5706\n",
      "Epoch [100/300], Step [39/172], Loss: 46.1367\n",
      "Epoch [100/300], Step [40/172], Loss: 21.0005\n",
      "Epoch [100/300], Step [41/172], Loss: 38.3759\n",
      "Epoch [100/300], Step [42/172], Loss: 42.4138\n",
      "Epoch [100/300], Step [43/172], Loss: 26.8892\n",
      "Epoch [100/300], Step [44/172], Loss: 21.3107\n",
      "Epoch [100/300], Step [45/172], Loss: 22.0570\n",
      "Epoch [100/300], Step [46/172], Loss: 23.9308\n",
      "Epoch [100/300], Step [47/172], Loss: 50.0637\n",
      "Epoch [100/300], Step [48/172], Loss: 52.3799\n",
      "Epoch [100/300], Step [49/172], Loss: 19.9869\n",
      "Epoch [100/300], Step [50/172], Loss: 51.4037\n",
      "Epoch [100/300], Step [51/172], Loss: 7.3203\n",
      "Epoch [100/300], Step [52/172], Loss: 18.3542\n",
      "Epoch [100/300], Step [53/172], Loss: 23.7907\n",
      "Epoch [100/300], Step [54/172], Loss: 11.6312\n",
      "Epoch [100/300], Step [55/172], Loss: 12.2485\n",
      "Epoch [100/300], Step [56/172], Loss: 10.3675\n",
      "Epoch [100/300], Step [57/172], Loss: 20.8956\n",
      "Epoch [100/300], Step [58/172], Loss: 17.6146\n",
      "Epoch [100/300], Step [59/172], Loss: 30.5217\n",
      "Epoch [100/300], Step [60/172], Loss: 45.9865\n",
      "Epoch [100/300], Step [61/172], Loss: 9.1257\n",
      "Epoch [100/300], Step [62/172], Loss: 23.1761\n",
      "Epoch [100/300], Step [63/172], Loss: 8.7850\n",
      "Epoch [100/300], Step [64/172], Loss: 8.1514\n",
      "Epoch [100/300], Step [65/172], Loss: 20.6647\n",
      "Epoch [100/300], Step [66/172], Loss: 5.9273\n",
      "Epoch [100/300], Step [67/172], Loss: 27.8221\n",
      "Epoch [100/300], Step [68/172], Loss: 6.4415\n",
      "Epoch [100/300], Step [69/172], Loss: 62.6921\n",
      "Epoch [100/300], Step [70/172], Loss: 56.9429\n",
      "Epoch [100/300], Step [71/172], Loss: 52.8494\n",
      "Epoch [100/300], Step [72/172], Loss: 58.1538\n",
      "Epoch [100/300], Step [73/172], Loss: 62.0442\n",
      "Epoch [100/300], Step [74/172], Loss: 35.2593\n",
      "Epoch [100/300], Step [75/172], Loss: 33.1620\n",
      "Epoch [100/300], Step [76/172], Loss: 37.8049\n",
      "Epoch [100/300], Step [77/172], Loss: 61.9645\n",
      "Epoch [100/300], Step [78/172], Loss: 48.8694\n",
      "Epoch [100/300], Step [79/172], Loss: 48.5617\n",
      "Epoch [100/300], Step [80/172], Loss: 58.5512\n",
      "Epoch [100/300], Step [81/172], Loss: 41.9648\n",
      "Epoch [100/300], Step [82/172], Loss: 40.0905\n",
      "Epoch [100/300], Step [83/172], Loss: 50.2615\n",
      "Epoch [100/300], Step [84/172], Loss: 38.0169\n",
      "Epoch [100/300], Step [85/172], Loss: 41.5355\n",
      "Epoch [100/300], Step [86/172], Loss: 33.7952\n",
      "Epoch [100/300], Step [87/172], Loss: 28.6622\n",
      "Epoch [100/300], Step [88/172], Loss: 30.2717\n",
      "Epoch [100/300], Step [89/172], Loss: 26.5405\n",
      "Epoch [100/300], Step [90/172], Loss: 25.7629\n",
      "Epoch [100/300], Step [91/172], Loss: 29.7958\n",
      "Epoch [100/300], Step [92/172], Loss: 21.8971\n",
      "Epoch [100/300], Step [93/172], Loss: 21.7001\n",
      "Epoch [100/300], Step [94/172], Loss: 29.6873\n",
      "Epoch [100/300], Step [95/172], Loss: 23.1525\n",
      "Epoch [100/300], Step [96/172], Loss: 19.0639\n",
      "Epoch [100/300], Step [97/172], Loss: 26.1800\n",
      "Epoch [100/300], Step [98/172], Loss: 19.8175\n",
      "Epoch [100/300], Step [99/172], Loss: 18.1492\n",
      "Epoch [100/300], Step [100/172], Loss: 16.2301\n",
      "Epoch [100/300], Step [101/172], Loss: 17.5441\n",
      "Epoch [100/300], Step [102/172], Loss: 16.4358\n",
      "Epoch [100/300], Step [103/172], Loss: 14.2143\n",
      "Epoch [100/300], Step [104/172], Loss: 16.0928\n",
      "Epoch [100/300], Step [105/172], Loss: 18.4178\n",
      "Epoch [100/300], Step [106/172], Loss: 17.2480\n",
      "Epoch [100/300], Step [107/172], Loss: 15.7728\n",
      "Epoch [100/300], Step [108/172], Loss: 16.8202\n",
      "Epoch [100/300], Step [109/172], Loss: 17.9304\n",
      "Epoch [100/300], Step [110/172], Loss: 16.1432\n",
      "Epoch [100/300], Step [111/172], Loss: 14.5587\n",
      "Epoch [100/300], Step [112/172], Loss: 18.9540\n",
      "Epoch [100/300], Step [113/172], Loss: 15.0162\n",
      "Epoch [100/300], Step [114/172], Loss: 14.3682\n",
      "Epoch [100/300], Step [115/172], Loss: 21.9896\n",
      "Epoch [100/300], Step [116/172], Loss: 14.9290\n",
      "Epoch [100/300], Step [117/172], Loss: 12.2336\n",
      "Epoch [100/300], Step [118/172], Loss: 15.4790\n",
      "Epoch [100/300], Step [119/172], Loss: 15.9830\n",
      "Epoch [100/300], Step [120/172], Loss: 11.5516\n",
      "Epoch [100/300], Step [121/172], Loss: 11.3336\n",
      "Epoch [100/300], Step [122/172], Loss: 10.8879\n",
      "Epoch [100/300], Step [123/172], Loss: 10.8002\n",
      "Epoch [100/300], Step [124/172], Loss: 8.6934\n",
      "Epoch [100/300], Step [125/172], Loss: 13.3446\n",
      "Epoch [100/300], Step [126/172], Loss: 11.0873\n",
      "Epoch [100/300], Step [127/172], Loss: 12.4848\n",
      "Epoch [100/300], Step [128/172], Loss: 12.8276\n",
      "Epoch [100/300], Step [129/172], Loss: 9.3222\n",
      "Epoch [100/300], Step [130/172], Loss: 11.8902\n",
      "Epoch [100/300], Step [131/172], Loss: 9.2797\n",
      "Epoch [100/300], Step [132/172], Loss: 9.4111\n",
      "Epoch [100/300], Step [133/172], Loss: 10.3110\n",
      "Epoch [100/300], Step [134/172], Loss: 12.3397\n",
      "Epoch [100/300], Step [135/172], Loss: 9.3477\n",
      "Epoch [100/300], Step [136/172], Loss: 8.9677\n",
      "Epoch [100/300], Step [137/172], Loss: 10.4825\n",
      "Epoch [100/300], Step [138/172], Loss: 8.7384\n",
      "Epoch [100/300], Step [139/172], Loss: 10.2144\n",
      "Epoch [100/300], Step [140/172], Loss: 10.0374\n",
      "Epoch [100/300], Step [141/172], Loss: 12.1214\n",
      "Epoch [100/300], Step [142/172], Loss: 13.9060\n",
      "Epoch [100/300], Step [143/172], Loss: 9.7848\n",
      "Epoch [100/300], Step [144/172], Loss: 9.2214\n",
      "Epoch [100/300], Step [145/172], Loss: 10.1546\n",
      "Epoch [100/300], Step [146/172], Loss: 10.1854\n",
      "Epoch [100/300], Step [147/172], Loss: 5.8193\n",
      "Epoch [100/300], Step [148/172], Loss: 6.9616\n",
      "Epoch [100/300], Step [149/172], Loss: 8.3761\n",
      "Epoch [100/300], Step [150/172], Loss: 8.3252\n",
      "Epoch [100/300], Step [151/172], Loss: 7.5092\n",
      "Epoch [100/300], Step [152/172], Loss: 8.0995\n",
      "Epoch [100/300], Step [153/172], Loss: 7.4276\n",
      "Epoch [100/300], Step [154/172], Loss: 8.2129\n",
      "Epoch [100/300], Step [155/172], Loss: 7.2443\n",
      "Epoch [100/300], Step [156/172], Loss: 12.5575\n",
      "Epoch [100/300], Step [157/172], Loss: 10.4985\n",
      "Epoch [100/300], Step [158/172], Loss: 8.0127\n",
      "Epoch [100/300], Step [159/172], Loss: 9.8264\n",
      "Epoch [100/300], Step [160/172], Loss: 10.2646\n",
      "Epoch [100/300], Step [161/172], Loss: 7.4263\n",
      "Epoch [100/300], Step [162/172], Loss: 7.6962\n",
      "Epoch [100/300], Step [163/172], Loss: 6.8205\n",
      "Epoch [100/300], Step [164/172], Loss: 10.0367\n",
      "Epoch [100/300], Step [165/172], Loss: 6.6276\n",
      "Epoch [100/300], Step [166/172], Loss: 6.8257\n",
      "Epoch [100/300], Step [167/172], Loss: 9.2637\n",
      "Epoch [100/300], Step [168/172], Loss: 7.3455\n",
      "Epoch [100/300], Step [169/172], Loss: 7.4727\n",
      "Epoch [100/300], Step [170/172], Loss: 5.9867\n",
      "Epoch [100/300], Step [171/172], Loss: 6.6263\n",
      "Epoch [100/300], Step [172/172], Loss: 5.5906\n",
      "Epoch [101/300], Step [1/172], Loss: 80.7560\n",
      "Epoch [101/300], Step [2/172], Loss: 80.1391\n",
      "Epoch [101/300], Step [3/172], Loss: 74.0828\n",
      "Epoch [101/300], Step [4/172], Loss: 45.9824\n",
      "Epoch [101/300], Step [5/172], Loss: 66.4030\n",
      "Epoch [101/300], Step [6/172], Loss: 19.9207\n",
      "Epoch [101/300], Step [7/172], Loss: 28.9836\n",
      "Epoch [101/300], Step [8/172], Loss: 6.2505\n",
      "Epoch [101/300], Step [9/172], Loss: 42.8787\n",
      "Epoch [101/300], Step [10/172], Loss: 46.9920\n",
      "Epoch [101/300], Step [11/172], Loss: 83.4062\n",
      "Epoch [101/300], Step [12/172], Loss: 83.3452\n",
      "Epoch [101/300], Step [13/172], Loss: 41.7999\n",
      "Epoch [101/300], Step [14/172], Loss: 87.1263\n",
      "Epoch [101/300], Step [15/172], Loss: 73.6073\n",
      "Epoch [101/300], Step [16/172], Loss: 20.0503\n",
      "Epoch [101/300], Step [17/172], Loss: 57.6192\n",
      "Epoch [101/300], Step [18/172], Loss: 62.2165\n",
      "Epoch [101/300], Step [19/172], Loss: 83.8052\n",
      "Epoch [101/300], Step [20/172], Loss: 69.4733\n",
      "Epoch [101/300], Step [21/172], Loss: 95.3472\n",
      "Epoch [101/300], Step [22/172], Loss: 78.3388\n",
      "Epoch [101/300], Step [23/172], Loss: 4.1709\n",
      "Epoch [101/300], Step [24/172], Loss: 70.3994\n",
      "Epoch [101/300], Step [25/172], Loss: 46.5280\n",
      "Epoch [101/300], Step [26/172], Loss: 58.8903\n",
      "Epoch [101/300], Step [27/172], Loss: 75.5771\n",
      "Epoch [101/300], Step [28/172], Loss: 36.3850\n",
      "Epoch [101/300], Step [29/172], Loss: 25.8552\n",
      "Epoch [101/300], Step [30/172], Loss: 83.2495\n",
      "Epoch [101/300], Step [31/172], Loss: 45.2807\n",
      "Epoch [101/300], Step [32/172], Loss: 41.4716\n",
      "Epoch [101/300], Step [33/172], Loss: 71.9469\n",
      "Epoch [101/300], Step [34/172], Loss: 4.6921\n",
      "Epoch [101/300], Step [35/172], Loss: 16.3689\n",
      "Epoch [101/300], Step [36/172], Loss: 20.9531\n",
      "Epoch [101/300], Step [37/172], Loss: 17.9863\n",
      "Epoch [101/300], Step [38/172], Loss: 27.5749\n",
      "Epoch [101/300], Step [39/172], Loss: 45.8815\n",
      "Epoch [101/300], Step [40/172], Loss: 21.0957\n",
      "Epoch [101/300], Step [41/172], Loss: 38.5321\n",
      "Epoch [101/300], Step [42/172], Loss: 42.5583\n",
      "Epoch [101/300], Step [43/172], Loss: 26.9519\n",
      "Epoch [101/300], Step [44/172], Loss: 21.3514\n",
      "Epoch [101/300], Step [45/172], Loss: 22.2842\n",
      "Epoch [101/300], Step [46/172], Loss: 23.8893\n",
      "Epoch [101/300], Step [47/172], Loss: 50.0054\n",
      "Epoch [101/300], Step [48/172], Loss: 52.5201\n",
      "Epoch [101/300], Step [49/172], Loss: 20.1792\n",
      "Epoch [101/300], Step [50/172], Loss: 51.3230\n",
      "Epoch [101/300], Step [51/172], Loss: 7.3387\n",
      "Epoch [101/300], Step [52/172], Loss: 18.2683\n",
      "Epoch [101/300], Step [53/172], Loss: 23.7058\n",
      "Epoch [101/300], Step [54/172], Loss: 11.6164\n",
      "Epoch [101/300], Step [55/172], Loss: 12.1900\n",
      "Epoch [101/300], Step [56/172], Loss: 10.3397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/300], Step [57/172], Loss: 20.6034\n",
      "Epoch [101/300], Step [58/172], Loss: 17.5194\n",
      "Epoch [101/300], Step [59/172], Loss: 30.4833\n",
      "Epoch [101/300], Step [60/172], Loss: 46.1794\n",
      "Epoch [101/300], Step [61/172], Loss: 9.0567\n",
      "Epoch [101/300], Step [62/172], Loss: 23.0703\n",
      "Epoch [101/300], Step [63/172], Loss: 8.6801\n",
      "Epoch [101/300], Step [64/172], Loss: 8.0815\n",
      "Epoch [101/300], Step [65/172], Loss: 20.6213\n",
      "Epoch [101/300], Step [66/172], Loss: 5.9166\n",
      "Epoch [101/300], Step [67/172], Loss: 27.6742\n",
      "Epoch [101/300], Step [68/172], Loss: 6.1679\n",
      "Epoch [101/300], Step [69/172], Loss: 62.3516\n",
      "Epoch [101/300], Step [70/172], Loss: 56.9869\n",
      "Epoch [101/300], Step [71/172], Loss: 52.8164\n",
      "Epoch [101/300], Step [72/172], Loss: 57.9995\n",
      "Epoch [101/300], Step [73/172], Loss: 62.0353\n",
      "Epoch [101/300], Step [74/172], Loss: 35.0729\n",
      "Epoch [101/300], Step [75/172], Loss: 33.0657\n",
      "Epoch [101/300], Step [76/172], Loss: 37.6530\n",
      "Epoch [101/300], Step [77/172], Loss: 62.0134\n",
      "Epoch [101/300], Step [78/172], Loss: 48.6658\n",
      "Epoch [101/300], Step [79/172], Loss: 48.2443\n",
      "Epoch [101/300], Step [80/172], Loss: 58.0908\n",
      "Epoch [101/300], Step [81/172], Loss: 41.8364\n",
      "Epoch [101/300], Step [82/172], Loss: 39.5745\n",
      "Epoch [101/300], Step [83/172], Loss: 50.0945\n",
      "Epoch [101/300], Step [84/172], Loss: 37.8040\n",
      "Epoch [101/300], Step [85/172], Loss: 41.4372\n",
      "Epoch [101/300], Step [86/172], Loss: 33.6609\n",
      "Epoch [101/300], Step [87/172], Loss: 28.5653\n",
      "Epoch [101/300], Step [88/172], Loss: 30.0829\n",
      "Epoch [101/300], Step [89/172], Loss: 26.4011\n",
      "Epoch [101/300], Step [90/172], Loss: 25.7009\n",
      "Epoch [101/300], Step [91/172], Loss: 29.5876\n",
      "Epoch [101/300], Step [92/172], Loss: 21.7965\n",
      "Epoch [101/300], Step [93/172], Loss: 21.5623\n",
      "Epoch [101/300], Step [94/172], Loss: 29.6878\n",
      "Epoch [101/300], Step [95/172], Loss: 23.1253\n",
      "Epoch [101/300], Step [96/172], Loss: 18.9880\n",
      "Epoch [101/300], Step [97/172], Loss: 26.1220\n",
      "Epoch [101/300], Step [98/172], Loss: 19.7336\n",
      "Epoch [101/300], Step [99/172], Loss: 18.0916\n",
      "Epoch [101/300], Step [100/172], Loss: 16.1563\n",
      "Epoch [101/300], Step [101/172], Loss: 17.4779\n",
      "Epoch [101/300], Step [102/172], Loss: 16.2190\n",
      "Epoch [101/300], Step [103/172], Loss: 14.1248\n",
      "Epoch [101/300], Step [104/172], Loss: 16.0532\n",
      "Epoch [101/300], Step [105/172], Loss: 18.2199\n",
      "Epoch [101/300], Step [106/172], Loss: 17.1990\n",
      "Epoch [101/300], Step [107/172], Loss: 15.7384\n",
      "Epoch [101/300], Step [108/172], Loss: 16.7436\n",
      "Epoch [101/300], Step [109/172], Loss: 17.8633\n",
      "Epoch [101/300], Step [110/172], Loss: 16.1051\n",
      "Epoch [101/300], Step [111/172], Loss: 14.4981\n",
      "Epoch [101/300], Step [112/172], Loss: 18.8575\n",
      "Epoch [101/300], Step [113/172], Loss: 14.9530\n",
      "Epoch [101/300], Step [114/172], Loss: 14.2840\n",
      "Epoch [101/300], Step [115/172], Loss: 21.9468\n",
      "Epoch [101/300], Step [116/172], Loss: 14.8535\n",
      "Epoch [101/300], Step [117/172], Loss: 12.1765\n",
      "Epoch [101/300], Step [118/172], Loss: 15.4979\n",
      "Epoch [101/300], Step [119/172], Loss: 15.9729\n",
      "Epoch [101/300], Step [120/172], Loss: 11.4827\n",
      "Epoch [101/300], Step [121/172], Loss: 11.2541\n",
      "Epoch [101/300], Step [122/172], Loss: 10.8196\n",
      "Epoch [101/300], Step [123/172], Loss: 10.7405\n",
      "Epoch [101/300], Step [124/172], Loss: 8.6299\n",
      "Epoch [101/300], Step [125/172], Loss: 13.3370\n",
      "Epoch [101/300], Step [126/172], Loss: 11.0355\n",
      "Epoch [101/300], Step [127/172], Loss: 12.3812\n",
      "Epoch [101/300], Step [128/172], Loss: 12.7323\n",
      "Epoch [101/300], Step [129/172], Loss: 9.2554\n",
      "Epoch [101/300], Step [130/172], Loss: 11.8535\n",
      "Epoch [101/300], Step [131/172], Loss: 9.2075\n",
      "Epoch [101/300], Step [132/172], Loss: 9.3382\n",
      "Epoch [101/300], Step [133/172], Loss: 10.2498\n",
      "Epoch [101/300], Step [134/172], Loss: 12.3234\n",
      "Epoch [101/300], Step [135/172], Loss: 9.3045\n",
      "Epoch [101/300], Step [136/172], Loss: 8.8581\n",
      "Epoch [101/300], Step [137/172], Loss: 10.4004\n",
      "Epoch [101/300], Step [138/172], Loss: 8.6629\n",
      "Epoch [101/300], Step [139/172], Loss: 10.1580\n",
      "Epoch [101/300], Step [140/172], Loss: 9.9668\n",
      "Epoch [101/300], Step [141/172], Loss: 12.0153\n",
      "Epoch [101/300], Step [142/172], Loss: 13.8757\n",
      "Epoch [101/300], Step [143/172], Loss: 9.7577\n",
      "Epoch [101/300], Step [144/172], Loss: 9.1732\n",
      "Epoch [101/300], Step [145/172], Loss: 10.1186\n",
      "Epoch [101/300], Step [146/172], Loss: 10.1230\n",
      "Epoch [101/300], Step [147/172], Loss: 5.7791\n",
      "Epoch [101/300], Step [148/172], Loss: 6.9109\n",
      "Epoch [101/300], Step [149/172], Loss: 8.3205\n",
      "Epoch [101/300], Step [150/172], Loss: 8.2582\n",
      "Epoch [101/300], Step [151/172], Loss: 7.4670\n",
      "Epoch [101/300], Step [152/172], Loss: 8.0588\n",
      "Epoch [101/300], Step [153/172], Loss: 7.3679\n",
      "Epoch [101/300], Step [154/172], Loss: 8.1809\n",
      "Epoch [101/300], Step [155/172], Loss: 7.2017\n",
      "Epoch [101/300], Step [156/172], Loss: 12.5305\n",
      "Epoch [101/300], Step [157/172], Loss: 10.4494\n",
      "Epoch [101/300], Step [158/172], Loss: 7.9565\n",
      "Epoch [101/300], Step [159/172], Loss: 9.7725\n",
      "Epoch [101/300], Step [160/172], Loss: 10.2384\n",
      "Epoch [101/300], Step [161/172], Loss: 7.4014\n",
      "Epoch [101/300], Step [162/172], Loss: 7.6624\n",
      "Epoch [101/300], Step [163/172], Loss: 6.7887\n",
      "Epoch [101/300], Step [164/172], Loss: 9.8930\n",
      "Epoch [101/300], Step [165/172], Loss: 6.5771\n",
      "Epoch [101/300], Step [166/172], Loss: 6.7403\n",
      "Epoch [101/300], Step [167/172], Loss: 9.2762\n",
      "Epoch [101/300], Step [168/172], Loss: 7.3412\n",
      "Epoch [101/300], Step [169/172], Loss: 7.4527\n",
      "Epoch [101/300], Step [170/172], Loss: 5.9470\n",
      "Epoch [101/300], Step [171/172], Loss: 6.6190\n",
      "Epoch [101/300], Step [172/172], Loss: 5.5732\n",
      "Epoch [102/300], Step [1/172], Loss: 80.3388\n",
      "Epoch [102/300], Step [2/172], Loss: 79.7490\n",
      "Epoch [102/300], Step [3/172], Loss: 73.5139\n",
      "Epoch [102/300], Step [4/172], Loss: 45.6573\n",
      "Epoch [102/300], Step [5/172], Loss: 66.1536\n",
      "Epoch [102/300], Step [6/172], Loss: 19.8182\n",
      "Epoch [102/300], Step [7/172], Loss: 28.9999\n",
      "Epoch [102/300], Step [8/172], Loss: 6.9034\n",
      "Epoch [102/300], Step [9/172], Loss: 43.0201\n",
      "Epoch [102/300], Step [10/172], Loss: 46.8735\n",
      "Epoch [102/300], Step [11/172], Loss: 83.1202\n",
      "Epoch [102/300], Step [12/172], Loss: 83.4764\n",
      "Epoch [102/300], Step [13/172], Loss: 42.0658\n",
      "Epoch [102/300], Step [14/172], Loss: 87.3100\n",
      "Epoch [102/300], Step [15/172], Loss: 73.6224\n",
      "Epoch [102/300], Step [16/172], Loss: 20.0877\n",
      "Epoch [102/300], Step [17/172], Loss: 57.8548\n",
      "Epoch [102/300], Step [18/172], Loss: 62.4759\n",
      "Epoch [102/300], Step [19/172], Loss: 84.0624\n",
      "Epoch [102/300], Step [20/172], Loss: 69.3879\n",
      "Epoch [102/300], Step [21/172], Loss: 95.7717\n",
      "Epoch [102/300], Step [22/172], Loss: 78.4590\n",
      "Epoch [102/300], Step [23/172], Loss: 3.7817\n",
      "Epoch [102/300], Step [24/172], Loss: 70.2778\n",
      "Epoch [102/300], Step [25/172], Loss: 46.5237\n",
      "Epoch [102/300], Step [26/172], Loss: 58.6934\n",
      "Epoch [102/300], Step [27/172], Loss: 75.7737\n",
      "Epoch [102/300], Step [28/172], Loss: 36.1531\n",
      "Epoch [102/300], Step [29/172], Loss: 25.7404\n",
      "Epoch [102/300], Step [30/172], Loss: 83.1784\n",
      "Epoch [102/300], Step [31/172], Loss: 45.3032\n",
      "Epoch [102/300], Step [32/172], Loss: 41.6175\n",
      "Epoch [102/300], Step [33/172], Loss: 72.0313\n",
      "Epoch [102/300], Step [34/172], Loss: 4.5481\n",
      "Epoch [102/300], Step [35/172], Loss: 16.1061\n",
      "Epoch [102/300], Step [36/172], Loss: 20.9249\n",
      "Epoch [102/300], Step [37/172], Loss: 17.9283\n",
      "Epoch [102/300], Step [38/172], Loss: 27.5106\n",
      "Epoch [102/300], Step [39/172], Loss: 45.5533\n",
      "Epoch [102/300], Step [40/172], Loss: 20.9645\n",
      "Epoch [102/300], Step [41/172], Loss: 38.4500\n",
      "Epoch [102/300], Step [42/172], Loss: 42.4696\n",
      "Epoch [102/300], Step [43/172], Loss: 26.9817\n",
      "Epoch [102/300], Step [44/172], Loss: 21.1129\n",
      "Epoch [102/300], Step [45/172], Loss: 22.2187\n",
      "Epoch [102/300], Step [46/172], Loss: 23.4791\n",
      "Epoch [102/300], Step [47/172], Loss: 49.7525\n",
      "Epoch [102/300], Step [48/172], Loss: 52.1540\n",
      "Epoch [102/300], Step [49/172], Loss: 19.9777\n",
      "Epoch [102/300], Step [50/172], Loss: 50.9955\n",
      "Epoch [102/300], Step [51/172], Loss: 7.3561\n",
      "Epoch [102/300], Step [52/172], Loss: 18.2421\n",
      "Epoch [102/300], Step [53/172], Loss: 23.5163\n",
      "Epoch [102/300], Step [54/172], Loss: 11.6026\n",
      "Epoch [102/300], Step [55/172], Loss: 12.1610\n",
      "Epoch [102/300], Step [56/172], Loss: 10.4347\n",
      "Epoch [102/300], Step [57/172], Loss: 20.4271\n",
      "Epoch [102/300], Step [58/172], Loss: 17.2607\n",
      "Epoch [102/300], Step [59/172], Loss: 30.4092\n",
      "Epoch [102/300], Step [60/172], Loss: 45.7311\n",
      "Epoch [102/300], Step [61/172], Loss: 9.0102\n",
      "Epoch [102/300], Step [62/172], Loss: 23.1215\n",
      "Epoch [102/300], Step [63/172], Loss: 8.7562\n",
      "Epoch [102/300], Step [64/172], Loss: 8.1781\n",
      "Epoch [102/300], Step [65/172], Loss: 20.5610\n",
      "Epoch [102/300], Step [66/172], Loss: 5.8675\n",
      "Epoch [102/300], Step [67/172], Loss: 27.5175\n",
      "Epoch [102/300], Step [68/172], Loss: 6.1573\n",
      "Epoch [102/300], Step [69/172], Loss: 62.0023\n",
      "Epoch [102/300], Step [70/172], Loss: 56.6811\n",
      "Epoch [102/300], Step [71/172], Loss: 52.6288\n",
      "Epoch [102/300], Step [72/172], Loss: 57.7974\n",
      "Epoch [102/300], Step [73/172], Loss: 61.7895\n",
      "Epoch [102/300], Step [74/172], Loss: 34.9747\n",
      "Epoch [102/300], Step [75/172], Loss: 33.1288\n",
      "Epoch [102/300], Step [76/172], Loss: 37.6311\n",
      "Epoch [102/300], Step [77/172], Loss: 61.9419\n",
      "Epoch [102/300], Step [78/172], Loss: 48.4726\n",
      "Epoch [102/300], Step [79/172], Loss: 48.2254\n",
      "Epoch [102/300], Step [80/172], Loss: 58.2910\n",
      "Epoch [102/300], Step [81/172], Loss: 41.7215\n",
      "Epoch [102/300], Step [82/172], Loss: 39.7580\n",
      "Epoch [102/300], Step [83/172], Loss: 49.9305\n",
      "Epoch [102/300], Step [84/172], Loss: 37.7163\n",
      "Epoch [102/300], Step [85/172], Loss: 41.2965\n",
      "Epoch [102/300], Step [86/172], Loss: 33.6514\n",
      "Epoch [102/300], Step [87/172], Loss: 28.5023\n",
      "Epoch [102/300], Step [88/172], Loss: 30.0218\n",
      "Epoch [102/300], Step [89/172], Loss: 26.3355\n",
      "Epoch [102/300], Step [90/172], Loss: 25.6492\n",
      "Epoch [102/300], Step [91/172], Loss: 29.5848\n",
      "Epoch [102/300], Step [92/172], Loss: 21.7883\n",
      "Epoch [102/300], Step [93/172], Loss: 21.5423\n",
      "Epoch [102/300], Step [94/172], Loss: 29.6674\n",
      "Epoch [102/300], Step [95/172], Loss: 23.1063\n",
      "Epoch [102/300], Step [96/172], Loss: 18.9997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [102/300], Step [97/172], Loss: 26.0982\n",
      "Epoch [102/300], Step [98/172], Loss: 19.6749\n",
      "Epoch [102/300], Step [99/172], Loss: 18.0649\n",
      "Epoch [102/300], Step [100/172], Loss: 16.1021\n",
      "Epoch [102/300], Step [101/172], Loss: 17.4209\n",
      "Epoch [102/300], Step [102/172], Loss: 16.3213\n",
      "Epoch [102/300], Step [103/172], Loss: 14.0731\n",
      "Epoch [102/300], Step [104/172], Loss: 16.0311\n",
      "Epoch [102/300], Step [105/172], Loss: 18.3376\n",
      "Epoch [102/300], Step [106/172], Loss: 17.1599\n",
      "Epoch [102/300], Step [107/172], Loss: 15.7243\n",
      "Epoch [102/300], Step [108/172], Loss: 16.7050\n",
      "Epoch [102/300], Step [109/172], Loss: 17.8011\n",
      "Epoch [102/300], Step [110/172], Loss: 16.0955\n",
      "Epoch [102/300], Step [111/172], Loss: 14.4868\n",
      "Epoch [102/300], Step [112/172], Loss: 18.8485\n",
      "Epoch [102/300], Step [113/172], Loss: 14.9173\n",
      "Epoch [102/300], Step [114/172], Loss: 14.2513\n",
      "Epoch [102/300], Step [115/172], Loss: 21.8917\n",
      "Epoch [102/300], Step [116/172], Loss: 14.8466\n",
      "Epoch [102/300], Step [117/172], Loss: 12.1227\n",
      "Epoch [102/300], Step [118/172], Loss: 15.4809\n",
      "Epoch [102/300], Step [119/172], Loss: 15.9964\n",
      "Epoch [102/300], Step [120/172], Loss: 11.4636\n",
      "Epoch [102/300], Step [121/172], Loss: 11.1934\n",
      "Epoch [102/300], Step [122/172], Loss: 10.8258\n",
      "Epoch [102/300], Step [123/172], Loss: 10.7591\n",
      "Epoch [102/300], Step [124/172], Loss: 8.6290\n",
      "Epoch [102/300], Step [125/172], Loss: 13.3101\n",
      "Epoch [102/300], Step [126/172], Loss: 11.0101\n",
      "Epoch [102/300], Step [127/172], Loss: 12.3770\n",
      "Epoch [102/300], Step [128/172], Loss: 12.7127\n",
      "Epoch [102/300], Step [129/172], Loss: 9.2197\n",
      "Epoch [102/300], Step [130/172], Loss: 11.8745\n",
      "Epoch [102/300], Step [131/172], Loss: 9.1854\n",
      "Epoch [102/300], Step [132/172], Loss: 9.3082\n",
      "Epoch [102/300], Step [133/172], Loss: 10.2441\n",
      "Epoch [102/300], Step [134/172], Loss: 12.3556\n",
      "Epoch [102/300], Step [135/172], Loss: 9.3011\n",
      "Epoch [102/300], Step [136/172], Loss: 8.8589\n",
      "Epoch [102/300], Step [137/172], Loss: 10.3720\n",
      "Epoch [102/300], Step [138/172], Loss: 8.6366\n",
      "Epoch [102/300], Step [139/172], Loss: 10.1407\n",
      "Epoch [102/300], Step [140/172], Loss: 9.9267\n",
      "Epoch [102/300], Step [141/172], Loss: 11.9588\n",
      "Epoch [102/300], Step [142/172], Loss: 13.8856\n",
      "Epoch [102/300], Step [143/172], Loss: 9.7519\n",
      "Epoch [102/300], Step [144/172], Loss: 9.1498\n",
      "Epoch [102/300], Step [145/172], Loss: 10.1013\n",
      "Epoch [102/300], Step [146/172], Loss: 10.0826\n",
      "Epoch [102/300], Step [147/172], Loss: 5.7549\n",
      "Epoch [102/300], Step [148/172], Loss: 6.8838\n",
      "Epoch [102/300], Step [149/172], Loss: 8.2699\n",
      "Epoch [102/300], Step [150/172], Loss: 8.2362\n",
      "Epoch [102/300], Step [151/172], Loss: 7.4428\n",
      "Epoch [102/300], Step [152/172], Loss: 8.0560\n",
      "Epoch [102/300], Step [153/172], Loss: 7.3421\n",
      "Epoch [102/300], Step [154/172], Loss: 8.1708\n",
      "Epoch [102/300], Step [155/172], Loss: 7.1747\n",
      "Epoch [102/300], Step [156/172], Loss: 12.5546\n",
      "Epoch [102/300], Step [157/172], Loss: 10.4527\n",
      "Epoch [102/300], Step [158/172], Loss: 7.9454\n",
      "Epoch [102/300], Step [159/172], Loss: 9.7462\n",
      "Epoch [102/300], Step [160/172], Loss: 10.2670\n",
      "Epoch [102/300], Step [161/172], Loss: 7.3881\n",
      "Epoch [102/300], Step [162/172], Loss: 7.6100\n",
      "Epoch [102/300], Step [163/172], Loss: 6.7642\n",
      "Epoch [102/300], Step [164/172], Loss: 9.9602\n",
      "Epoch [102/300], Step [165/172], Loss: 6.5517\n",
      "Epoch [102/300], Step [166/172], Loss: 6.7075\n",
      "Epoch [102/300], Step [167/172], Loss: 9.3184\n",
      "Epoch [102/300], Step [168/172], Loss: 7.3112\n",
      "Epoch [102/300], Step [169/172], Loss: 7.4589\n",
      "Epoch [102/300], Step [170/172], Loss: 5.9164\n",
      "Epoch [102/300], Step [171/172], Loss: 6.6474\n",
      "Epoch [102/300], Step [172/172], Loss: 5.5472\n",
      "Epoch [103/300], Step [1/172], Loss: 79.8352\n",
      "Epoch [103/300], Step [2/172], Loss: 79.0761\n",
      "Epoch [103/300], Step [3/172], Loss: 73.0465\n",
      "Epoch [103/300], Step [4/172], Loss: 45.2619\n",
      "Epoch [103/300], Step [5/172], Loss: 65.7671\n",
      "Epoch [103/300], Step [6/172], Loss: 19.6105\n",
      "Epoch [103/300], Step [7/172], Loss: 28.2971\n",
      "Epoch [103/300], Step [8/172], Loss: 6.2440\n",
      "Epoch [103/300], Step [9/172], Loss: 42.5886\n",
      "Epoch [103/300], Step [10/172], Loss: 46.8005\n",
      "Epoch [103/300], Step [11/172], Loss: 82.6626\n",
      "Epoch [103/300], Step [12/172], Loss: 83.4042\n",
      "Epoch [103/300], Step [13/172], Loss: 41.9969\n",
      "Epoch [103/300], Step [14/172], Loss: 86.7110\n",
      "Epoch [103/300], Step [15/172], Loss: 73.3011\n",
      "Epoch [103/300], Step [16/172], Loss: 19.4710\n",
      "Epoch [103/300], Step [17/172], Loss: 57.6902\n",
      "Epoch [103/300], Step [18/172], Loss: 62.4634\n",
      "Epoch [103/300], Step [19/172], Loss: 84.0683\n",
      "Epoch [103/300], Step [20/172], Loss: 67.8767\n",
      "Epoch [103/300], Step [21/172], Loss: 95.5618\n",
      "Epoch [103/300], Step [22/172], Loss: 77.6420\n",
      "Epoch [103/300], Step [23/172], Loss: 3.7479\n",
      "Epoch [103/300], Step [24/172], Loss: 70.0147\n",
      "Epoch [103/300], Step [25/172], Loss: 46.3243\n",
      "Epoch [103/300], Step [26/172], Loss: 58.5684\n",
      "Epoch [103/300], Step [27/172], Loss: 75.0537\n",
      "Epoch [103/300], Step [28/172], Loss: 35.7644\n",
      "Epoch [103/300], Step [29/172], Loss: 25.5037\n",
      "Epoch [103/300], Step [30/172], Loss: 83.0567\n",
      "Epoch [103/300], Step [31/172], Loss: 45.0914\n",
      "Epoch [103/300], Step [32/172], Loss: 41.5246\n",
      "Epoch [103/300], Step [33/172], Loss: 71.9315\n",
      "Epoch [103/300], Step [34/172], Loss: 4.7179\n",
      "Epoch [103/300], Step [35/172], Loss: 16.0127\n",
      "Epoch [103/300], Step [36/172], Loss: 21.0909\n",
      "Epoch [103/300], Step [37/172], Loss: 17.9156\n",
      "Epoch [103/300], Step [38/172], Loss: 27.5103\n",
      "Epoch [103/300], Step [39/172], Loss: 45.4222\n",
      "Epoch [103/300], Step [40/172], Loss: 20.9287\n",
      "Epoch [103/300], Step [41/172], Loss: 38.3190\n",
      "Epoch [103/300], Step [42/172], Loss: 42.1442\n",
      "Epoch [103/300], Step [43/172], Loss: 26.8763\n",
      "Epoch [103/300], Step [44/172], Loss: 21.1524\n",
      "Epoch [103/300], Step [45/172], Loss: 22.3380\n",
      "Epoch [103/300], Step [46/172], Loss: 23.4918\n",
      "Epoch [103/300], Step [47/172], Loss: 49.6860\n",
      "Epoch [103/300], Step [48/172], Loss: 52.6647\n",
      "Epoch [103/300], Step [49/172], Loss: 19.9951\n",
      "Epoch [103/300], Step [50/172], Loss: 51.1109\n",
      "Epoch [103/300], Step [51/172], Loss: 7.3638\n",
      "Epoch [103/300], Step [52/172], Loss: 18.1560\n",
      "Epoch [103/300], Step [53/172], Loss: 23.5522\n",
      "Epoch [103/300], Step [54/172], Loss: 11.6958\n",
      "Epoch [103/300], Step [55/172], Loss: 12.1821\n",
      "Epoch [103/300], Step [56/172], Loss: 10.4980\n",
      "Epoch [103/300], Step [57/172], Loss: 20.1076\n",
      "Epoch [103/300], Step [58/172], Loss: 17.2414\n",
      "Epoch [103/300], Step [59/172], Loss: 30.3757\n",
      "Epoch [103/300], Step [60/172], Loss: 45.7458\n",
      "Epoch [103/300], Step [61/172], Loss: 8.9510\n",
      "Epoch [103/300], Step [62/172], Loss: 23.1061\n",
      "Epoch [103/300], Step [63/172], Loss: 8.7207\n",
      "Epoch [103/300], Step [64/172], Loss: 8.1909\n",
      "Epoch [103/300], Step [65/172], Loss: 20.5122\n",
      "Epoch [103/300], Step [66/172], Loss: 5.9149\n",
      "Epoch [103/300], Step [67/172], Loss: 27.4652\n",
      "Epoch [103/300], Step [68/172], Loss: 6.2424\n",
      "Epoch [103/300], Step [69/172], Loss: 61.5684\n",
      "Epoch [103/300], Step [70/172], Loss: 56.5230\n",
      "Epoch [103/300], Step [71/172], Loss: 52.5035\n",
      "Epoch [103/300], Step [72/172], Loss: 57.6541\n",
      "Epoch [103/300], Step [73/172], Loss: 61.7175\n",
      "Epoch [103/300], Step [74/172], Loss: 34.8919\n",
      "Epoch [103/300], Step [75/172], Loss: 32.9903\n",
      "Epoch [103/300], Step [76/172], Loss: 37.6423\n",
      "Epoch [103/300], Step [77/172], Loss: 62.0077\n",
      "Epoch [103/300], Step [78/172], Loss: 48.4214\n",
      "Epoch [103/300], Step [79/172], Loss: 48.1539\n",
      "Epoch [103/300], Step [80/172], Loss: 58.2725\n",
      "Epoch [103/300], Step [81/172], Loss: 41.7121\n",
      "Epoch [103/300], Step [82/172], Loss: 39.4992\n",
      "Epoch [103/300], Step [83/172], Loss: 49.9458\n",
      "Epoch [103/300], Step [84/172], Loss: 37.6889\n",
      "Epoch [103/300], Step [85/172], Loss: 41.3779\n",
      "Epoch [103/300], Step [86/172], Loss: 33.5906\n",
      "Epoch [103/300], Step [87/172], Loss: 28.4597\n",
      "Epoch [103/300], Step [88/172], Loss: 29.9204\n",
      "Epoch [103/300], Step [89/172], Loss: 26.2024\n",
      "Epoch [103/300], Step [90/172], Loss: 25.5918\n",
      "Epoch [103/300], Step [91/172], Loss: 29.4870\n",
      "Epoch [103/300], Step [92/172], Loss: 21.6959\n",
      "Epoch [103/300], Step [93/172], Loss: 21.4887\n",
      "Epoch [103/300], Step [94/172], Loss: 29.7040\n",
      "Epoch [103/300], Step [95/172], Loss: 23.0689\n",
      "Epoch [103/300], Step [96/172], Loss: 18.9536\n",
      "Epoch [103/300], Step [97/172], Loss: 26.0620\n",
      "Epoch [103/300], Step [98/172], Loss: 19.5756\n",
      "Epoch [103/300], Step [99/172], Loss: 18.0176\n",
      "Epoch [103/300], Step [100/172], Loss: 15.9988\n",
      "Epoch [103/300], Step [101/172], Loss: 17.3480\n",
      "Epoch [103/300], Step [102/172], Loss: 16.1554\n",
      "Epoch [103/300], Step [103/172], Loss: 13.9617\n",
      "Epoch [103/300], Step [104/172], Loss: 15.9625\n",
      "Epoch [103/300], Step [105/172], Loss: 18.1512\n",
      "Epoch [103/300], Step [106/172], Loss: 17.0943\n",
      "Epoch [103/300], Step [107/172], Loss: 15.6519\n",
      "Epoch [103/300], Step [108/172], Loss: 16.6033\n",
      "Epoch [103/300], Step [109/172], Loss: 17.6616\n",
      "Epoch [103/300], Step [110/172], Loss: 15.9950\n",
      "Epoch [103/300], Step [111/172], Loss: 14.3819\n",
      "Epoch [103/300], Step [112/172], Loss: 18.7187\n",
      "Epoch [103/300], Step [113/172], Loss: 14.7819\n",
      "Epoch [103/300], Step [114/172], Loss: 14.1356\n",
      "Epoch [103/300], Step [115/172], Loss: 21.7680\n",
      "Epoch [103/300], Step [116/172], Loss: 14.7630\n",
      "Epoch [103/300], Step [117/172], Loss: 11.9935\n",
      "Epoch [103/300], Step [118/172], Loss: 15.3730\n",
      "Epoch [103/300], Step [119/172], Loss: 15.9635\n",
      "Epoch [103/300], Step [120/172], Loss: 11.3674\n",
      "Epoch [103/300], Step [121/172], Loss: 11.0882\n",
      "Epoch [103/300], Step [122/172], Loss: 10.7189\n",
      "Epoch [103/300], Step [123/172], Loss: 10.6448\n",
      "Epoch [103/300], Step [124/172], Loss: 8.5666\n",
      "Epoch [103/300], Step [125/172], Loss: 13.2464\n",
      "Epoch [103/300], Step [126/172], Loss: 10.9508\n",
      "Epoch [103/300], Step [127/172], Loss: 12.2581\n",
      "Epoch [103/300], Step [128/172], Loss: 12.5884\n",
      "Epoch [103/300], Step [129/172], Loss: 9.1410\n",
      "Epoch [103/300], Step [130/172], Loss: 11.8149\n",
      "Epoch [103/300], Step [131/172], Loss: 9.0855\n",
      "Epoch [103/300], Step [132/172], Loss: 9.2374\n",
      "Epoch [103/300], Step [133/172], Loss: 10.1334\n",
      "Epoch [103/300], Step [134/172], Loss: 12.2810\n",
      "Epoch [103/300], Step [135/172], Loss: 9.2480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [103/300], Step [136/172], Loss: 8.8018\n",
      "Epoch [103/300], Step [137/172], Loss: 10.2948\n",
      "Epoch [103/300], Step [138/172], Loss: 8.5551\n",
      "Epoch [103/300], Step [139/172], Loss: 10.0958\n",
      "Epoch [103/300], Step [140/172], Loss: 9.8768\n",
      "Epoch [103/300], Step [141/172], Loss: 11.8347\n",
      "Epoch [103/300], Step [142/172], Loss: 13.7641\n",
      "Epoch [103/300], Step [143/172], Loss: 9.7227\n",
      "Epoch [103/300], Step [144/172], Loss: 9.0961\n",
      "Epoch [103/300], Step [145/172], Loss: 10.0143\n",
      "Epoch [103/300], Step [146/172], Loss: 10.0042\n",
      "Epoch [103/300], Step [147/172], Loss: 5.7066\n",
      "Epoch [103/300], Step [148/172], Loss: 6.8279\n",
      "Epoch [103/300], Step [149/172], Loss: 8.1974\n",
      "Epoch [103/300], Step [150/172], Loss: 8.1570\n",
      "Epoch [103/300], Step [151/172], Loss: 7.3821\n",
      "Epoch [103/300], Step [152/172], Loss: 8.0118\n",
      "Epoch [103/300], Step [153/172], Loss: 7.2836\n",
      "Epoch [103/300], Step [154/172], Loss: 8.1150\n",
      "Epoch [103/300], Step [155/172], Loss: 7.1089\n",
      "Epoch [103/300], Step [156/172], Loss: 12.5069\n",
      "Epoch [103/300], Step [157/172], Loss: 10.3783\n",
      "Epoch [103/300], Step [158/172], Loss: 7.8699\n",
      "Epoch [103/300], Step [159/172], Loss: 9.7158\n",
      "Epoch [103/300], Step [160/172], Loss: 10.1989\n",
      "Epoch [103/300], Step [161/172], Loss: 7.3214\n",
      "Epoch [103/300], Step [162/172], Loss: 7.5672\n",
      "Epoch [103/300], Step [163/172], Loss: 6.7109\n",
      "Epoch [103/300], Step [164/172], Loss: 9.8448\n",
      "Epoch [103/300], Step [165/172], Loss: 6.5163\n",
      "Epoch [103/300], Step [166/172], Loss: 6.6229\n",
      "Epoch [103/300], Step [167/172], Loss: 9.2889\n",
      "Epoch [103/300], Step [168/172], Loss: 7.2859\n",
      "Epoch [103/300], Step [169/172], Loss: 7.3876\n",
      "Epoch [103/300], Step [170/172], Loss: 5.8520\n",
      "Epoch [103/300], Step [171/172], Loss: 6.6066\n",
      "Epoch [103/300], Step [172/172], Loss: 5.4913\n",
      "Epoch [104/300], Step [1/172], Loss: 79.4373\n",
      "Epoch [104/300], Step [2/172], Loss: 78.7484\n",
      "Epoch [104/300], Step [3/172], Loss: 72.4411\n",
      "Epoch [104/300], Step [4/172], Loss: 44.9556\n",
      "Epoch [104/300], Step [5/172], Loss: 65.4224\n",
      "Epoch [104/300], Step [6/172], Loss: 19.6342\n",
      "Epoch [104/300], Step [7/172], Loss: 28.8080\n",
      "Epoch [104/300], Step [8/172], Loss: 6.5158\n",
      "Epoch [104/300], Step [9/172], Loss: 42.4502\n",
      "Epoch [104/300], Step [10/172], Loss: 46.6630\n",
      "Epoch [104/300], Step [11/172], Loss: 82.2807\n",
      "Epoch [104/300], Step [12/172], Loss: 83.4142\n",
      "Epoch [104/300], Step [13/172], Loss: 42.0720\n",
      "Epoch [104/300], Step [14/172], Loss: 86.6665\n",
      "Epoch [104/300], Step [15/172], Loss: 73.2405\n",
      "Epoch [104/300], Step [16/172], Loss: 19.2757\n",
      "Epoch [104/300], Step [17/172], Loss: 57.6822\n",
      "Epoch [104/300], Step [18/172], Loss: 62.5736\n",
      "Epoch [104/300], Step [19/172], Loss: 84.2090\n",
      "Epoch [104/300], Step [20/172], Loss: 67.7742\n",
      "Epoch [104/300], Step [21/172], Loss: 95.7057\n",
      "Epoch [104/300], Step [22/172], Loss: 77.4126\n",
      "Epoch [104/300], Step [23/172], Loss: 3.8677\n",
      "Epoch [104/300], Step [24/172], Loss: 70.0041\n",
      "Epoch [104/300], Step [25/172], Loss: 46.2088\n",
      "Epoch [104/300], Step [26/172], Loss: 58.3276\n",
      "Epoch [104/300], Step [27/172], Loss: 75.0773\n",
      "Epoch [104/300], Step [28/172], Loss: 35.4470\n",
      "Epoch [104/300], Step [29/172], Loss: 24.9949\n",
      "Epoch [104/300], Step [30/172], Loss: 82.9681\n",
      "Epoch [104/300], Step [31/172], Loss: 45.1027\n",
      "Epoch [104/300], Step [32/172], Loss: 41.5737\n",
      "Epoch [104/300], Step [33/172], Loss: 71.9189\n",
      "Epoch [104/300], Step [34/172], Loss: 4.5175\n",
      "Epoch [104/300], Step [35/172], Loss: 15.7406\n",
      "Epoch [104/300], Step [36/172], Loss: 20.6534\n",
      "Epoch [104/300], Step [37/172], Loss: 17.7775\n",
      "Epoch [104/300], Step [38/172], Loss: 27.3771\n",
      "Epoch [104/300], Step [39/172], Loss: 44.7617\n",
      "Epoch [104/300], Step [40/172], Loss: 20.7496\n",
      "Epoch [104/300], Step [41/172], Loss: 38.1825\n",
      "Epoch [104/300], Step [42/172], Loss: 41.8300\n",
      "Epoch [104/300], Step [43/172], Loss: 26.7330\n",
      "Epoch [104/300], Step [44/172], Loss: 21.0185\n",
      "Epoch [104/300], Step [45/172], Loss: 22.2494\n",
      "Epoch [104/300], Step [46/172], Loss: 23.1620\n",
      "Epoch [104/300], Step [47/172], Loss: 49.4993\n",
      "Epoch [104/300], Step [48/172], Loss: 52.4441\n",
      "Epoch [104/300], Step [49/172], Loss: 19.8692\n",
      "Epoch [104/300], Step [50/172], Loss: 50.8680\n",
      "Epoch [104/300], Step [51/172], Loss: 7.3376\n",
      "Epoch [104/300], Step [52/172], Loss: 18.0473\n",
      "Epoch [104/300], Step [53/172], Loss: 23.3647\n",
      "Epoch [104/300], Step [54/172], Loss: 11.5873\n",
      "Epoch [104/300], Step [55/172], Loss: 12.1213\n",
      "Epoch [104/300], Step [56/172], Loss: 10.5236\n",
      "Epoch [104/300], Step [57/172], Loss: 20.0407\n",
      "Epoch [104/300], Step [58/172], Loss: 17.1316\n",
      "Epoch [104/300], Step [59/172], Loss: 30.4346\n",
      "Epoch [104/300], Step [60/172], Loss: 45.2198\n",
      "Epoch [104/300], Step [61/172], Loss: 8.8977\n",
      "Epoch [104/300], Step [62/172], Loss: 22.9675\n",
      "Epoch [104/300], Step [63/172], Loss: 8.7761\n",
      "Epoch [104/300], Step [64/172], Loss: 8.2353\n",
      "Epoch [104/300], Step [65/172], Loss: 20.5003\n",
      "Epoch [104/300], Step [66/172], Loss: 5.8941\n",
      "Epoch [104/300], Step [67/172], Loss: 27.4855\n",
      "Epoch [104/300], Step [68/172], Loss: 6.1480\n",
      "Epoch [104/300], Step [69/172], Loss: 61.1281\n",
      "Epoch [104/300], Step [70/172], Loss: 56.1417\n",
      "Epoch [104/300], Step [71/172], Loss: 52.2191\n",
      "Epoch [104/300], Step [72/172], Loss: 57.4239\n",
      "Epoch [104/300], Step [73/172], Loss: 61.3697\n",
      "Epoch [104/300], Step [74/172], Loss: 34.6770\n",
      "Epoch [104/300], Step [75/172], Loss: 32.7449\n",
      "Epoch [104/300], Step [76/172], Loss: 37.2099\n",
      "Epoch [104/300], Step [77/172], Loss: 61.7322\n",
      "Epoch [104/300], Step [78/172], Loss: 48.1846\n",
      "Epoch [104/300], Step [79/172], Loss: 47.8998\n",
      "Epoch [104/300], Step [80/172], Loss: 58.2398\n",
      "Epoch [104/300], Step [81/172], Loss: 41.5405\n",
      "Epoch [104/300], Step [82/172], Loss: 39.5623\n",
      "Epoch [104/300], Step [83/172], Loss: 49.6628\n",
      "Epoch [104/300], Step [84/172], Loss: 37.5658\n",
      "Epoch [104/300], Step [85/172], Loss: 41.2287\n",
      "Epoch [104/300], Step [86/172], Loss: 33.4875\n",
      "Epoch [104/300], Step [87/172], Loss: 28.3359\n",
      "Epoch [104/300], Step [88/172], Loss: 29.8143\n",
      "Epoch [104/300], Step [89/172], Loss: 26.1105\n",
      "Epoch [104/300], Step [90/172], Loss: 25.4528\n",
      "Epoch [104/300], Step [91/172], Loss: 29.3956\n",
      "Epoch [104/300], Step [92/172], Loss: 21.6477\n",
      "Epoch [104/300], Step [93/172], Loss: 21.4068\n",
      "Epoch [104/300], Step [94/172], Loss: 29.6378\n",
      "Epoch [104/300], Step [95/172], Loss: 22.9950\n",
      "Epoch [104/300], Step [96/172], Loss: 18.9578\n",
      "Epoch [104/300], Step [97/172], Loss: 26.0380\n",
      "Epoch [104/300], Step [98/172], Loss: 19.5000\n",
      "Epoch [104/300], Step [99/172], Loss: 18.0121\n",
      "Epoch [104/300], Step [100/172], Loss: 15.9310\n",
      "Epoch [104/300], Step [101/172], Loss: 17.2988\n",
      "Epoch [104/300], Step [102/172], Loss: 16.1605\n",
      "Epoch [104/300], Step [103/172], Loss: 13.8852\n",
      "Epoch [104/300], Step [104/172], Loss: 15.9499\n",
      "Epoch [104/300], Step [105/172], Loss: 18.1588\n",
      "Epoch [104/300], Step [106/172], Loss: 17.0579\n",
      "Epoch [104/300], Step [107/172], Loss: 15.6006\n",
      "Epoch [104/300], Step [108/172], Loss: 16.5586\n",
      "Epoch [104/300], Step [109/172], Loss: 17.5224\n",
      "Epoch [104/300], Step [110/172], Loss: 15.9531\n",
      "Epoch [104/300], Step [111/172], Loss: 14.3421\n",
      "Epoch [104/300], Step [112/172], Loss: 18.6718\n",
      "Epoch [104/300], Step [113/172], Loss: 14.6926\n",
      "Epoch [104/300], Step [114/172], Loss: 14.0915\n",
      "Epoch [104/300], Step [115/172], Loss: 21.6259\n",
      "Epoch [104/300], Step [116/172], Loss: 14.7502\n",
      "Epoch [104/300], Step [117/172], Loss: 11.9098\n",
      "Epoch [104/300], Step [118/172], Loss: 15.2529\n",
      "Epoch [104/300], Step [119/172], Loss: 15.9828\n",
      "Epoch [104/300], Step [120/172], Loss: 11.3079\n",
      "Epoch [104/300], Step [121/172], Loss: 11.0383\n",
      "Epoch [104/300], Step [122/172], Loss: 10.7196\n",
      "Epoch [104/300], Step [123/172], Loss: 10.6296\n",
      "Epoch [104/300], Step [124/172], Loss: 8.5745\n",
      "Epoch [104/300], Step [125/172], Loss: 13.2362\n",
      "Epoch [104/300], Step [126/172], Loss: 10.9409\n",
      "Epoch [104/300], Step [127/172], Loss: 12.2189\n",
      "Epoch [104/300], Step [128/172], Loss: 12.5405\n",
      "Epoch [104/300], Step [129/172], Loss: 9.1009\n",
      "Epoch [104/300], Step [130/172], Loss: 11.8068\n",
      "Epoch [104/300], Step [131/172], Loss: 9.0369\n",
      "Epoch [104/300], Step [132/172], Loss: 9.2151\n",
      "Epoch [104/300], Step [133/172], Loss: 10.0975\n",
      "Epoch [104/300], Step [134/172], Loss: 12.2879\n",
      "Epoch [104/300], Step [135/172], Loss: 9.2448\n",
      "Epoch [104/300], Step [136/172], Loss: 8.8109\n",
      "Epoch [104/300], Step [137/172], Loss: 10.2650\n",
      "Epoch [104/300], Step [138/172], Loss: 8.5171\n",
      "Epoch [104/300], Step [139/172], Loss: 10.0909\n",
      "Epoch [104/300], Step [140/172], Loss: 9.8496\n",
      "Epoch [104/300], Step [141/172], Loss: 11.7501\n",
      "Epoch [104/300], Step [142/172], Loss: 13.7286\n",
      "Epoch [104/300], Step [143/172], Loss: 9.7225\n",
      "Epoch [104/300], Step [144/172], Loss: 9.0766\n",
      "Epoch [104/300], Step [145/172], Loss: 9.9800\n",
      "Epoch [104/300], Step [146/172], Loss: 9.9434\n",
      "Epoch [104/300], Step [147/172], Loss: 5.6829\n",
      "Epoch [104/300], Step [148/172], Loss: 6.8068\n",
      "Epoch [104/300], Step [149/172], Loss: 8.1339\n",
      "Epoch [104/300], Step [150/172], Loss: 8.1151\n",
      "Epoch [104/300], Step [151/172], Loss: 7.3275\n",
      "Epoch [104/300], Step [152/172], Loss: 7.9914\n",
      "Epoch [104/300], Step [153/172], Loss: 7.2503\n",
      "Epoch [104/300], Step [154/172], Loss: 8.0721\n",
      "Epoch [104/300], Step [155/172], Loss: 7.0741\n",
      "Epoch [104/300], Step [156/172], Loss: 12.5127\n",
      "Epoch [104/300], Step [157/172], Loss: 10.3478\n",
      "Epoch [104/300], Step [158/172], Loss: 7.8365\n",
      "Epoch [104/300], Step [159/172], Loss: 9.6639\n",
      "Epoch [104/300], Step [160/172], Loss: 10.1946\n",
      "Epoch [104/300], Step [161/172], Loss: 7.2771\n",
      "Epoch [104/300], Step [162/172], Loss: 7.5189\n",
      "Epoch [104/300], Step [163/172], Loss: 6.6592\n",
      "Epoch [104/300], Step [164/172], Loss: 9.8220\n",
      "Epoch [104/300], Step [165/172], Loss: 6.5000\n",
      "Epoch [104/300], Step [166/172], Loss: 6.5550\n",
      "Epoch [104/300], Step [167/172], Loss: 9.2701\n",
      "Epoch [104/300], Step [168/172], Loss: 7.2554\n",
      "Epoch [104/300], Step [169/172], Loss: 7.3214\n",
      "Epoch [104/300], Step [170/172], Loss: 5.8022\n",
      "Epoch [104/300], Step [171/172], Loss: 6.5834\n",
      "Epoch [104/300], Step [172/172], Loss: 5.4309\n",
      "Epoch [105/300], Step [1/172], Loss: 78.8439\n",
      "Epoch [105/300], Step [2/172], Loss: 78.3068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [105/300], Step [3/172], Loss: 71.7057\n",
      "Epoch [105/300], Step [4/172], Loss: 44.5528\n",
      "Epoch [105/300], Step [5/172], Loss: 65.1797\n",
      "Epoch [105/300], Step [6/172], Loss: 19.5185\n",
      "Epoch [105/300], Step [7/172], Loss: 28.5004\n",
      "Epoch [105/300], Step [8/172], Loss: 6.1338\n",
      "Epoch [105/300], Step [9/172], Loss: 42.0533\n",
      "Epoch [105/300], Step [10/172], Loss: 46.5762\n",
      "Epoch [105/300], Step [11/172], Loss: 81.7385\n",
      "Epoch [105/300], Step [12/172], Loss: 83.1429\n",
      "Epoch [105/300], Step [13/172], Loss: 41.8439\n",
      "Epoch [105/300], Step [14/172], Loss: 85.7517\n",
      "Epoch [105/300], Step [15/172], Loss: 72.5236\n",
      "Epoch [105/300], Step [16/172], Loss: 18.5756\n",
      "Epoch [105/300], Step [17/172], Loss: 57.1605\n",
      "Epoch [105/300], Step [18/172], Loss: 62.2510\n",
      "Epoch [105/300], Step [19/172], Loss: 83.8217\n",
      "Epoch [105/300], Step [20/172], Loss: 67.0752\n",
      "Epoch [105/300], Step [21/172], Loss: 95.0577\n",
      "Epoch [105/300], Step [22/172], Loss: 76.5991\n",
      "Epoch [105/300], Step [23/172], Loss: 3.6021\n",
      "Epoch [105/300], Step [24/172], Loss: 69.6479\n",
      "Epoch [105/300], Step [25/172], Loss: 46.0948\n",
      "Epoch [105/300], Step [26/172], Loss: 58.0962\n",
      "Epoch [105/300], Step [27/172], Loss: 74.3586\n",
      "Epoch [105/300], Step [28/172], Loss: 35.0236\n",
      "Epoch [105/300], Step [29/172], Loss: 24.6914\n",
      "Epoch [105/300], Step [30/172], Loss: 82.7413\n",
      "Epoch [105/300], Step [31/172], Loss: 45.1332\n",
      "Epoch [105/300], Step [32/172], Loss: 41.6025\n",
      "Epoch [105/300], Step [33/172], Loss: 71.8500\n",
      "Epoch [105/300], Step [34/172], Loss: 4.6662\n",
      "Epoch [105/300], Step [35/172], Loss: 15.4931\n",
      "Epoch [105/300], Step [36/172], Loss: 20.8745\n",
      "Epoch [105/300], Step [37/172], Loss: 17.8090\n",
      "Epoch [105/300], Step [38/172], Loss: 27.4142\n",
      "Epoch [105/300], Step [39/172], Loss: 44.3283\n",
      "Epoch [105/300], Step [40/172], Loss: 20.8936\n",
      "Epoch [105/300], Step [41/172], Loss: 38.2124\n",
      "Epoch [105/300], Step [42/172], Loss: 42.1260\n",
      "Epoch [105/300], Step [43/172], Loss: 26.8505\n",
      "Epoch [105/300], Step [44/172], Loss: 21.0634\n",
      "Epoch [105/300], Step [45/172], Loss: 22.4907\n",
      "Epoch [105/300], Step [46/172], Loss: 23.0728\n",
      "Epoch [105/300], Step [47/172], Loss: 49.5190\n",
      "Epoch [105/300], Step [48/172], Loss: 52.5039\n",
      "Epoch [105/300], Step [49/172], Loss: 20.0585\n",
      "Epoch [105/300], Step [50/172], Loss: 50.4563\n",
      "Epoch [105/300], Step [51/172], Loss: 7.3959\n",
      "Epoch [105/300], Step [52/172], Loss: 18.1197\n",
      "Epoch [105/300], Step [53/172], Loss: 23.4410\n",
      "Epoch [105/300], Step [54/172], Loss: 11.7939\n",
      "Epoch [105/300], Step [55/172], Loss: 12.2217\n",
      "Epoch [105/300], Step [56/172], Loss: 10.6264\n",
      "Epoch [105/300], Step [57/172], Loss: 19.8636\n",
      "Epoch [105/300], Step [58/172], Loss: 17.0474\n",
      "Epoch [105/300], Step [59/172], Loss: 30.3094\n",
      "Epoch [105/300], Step [60/172], Loss: 44.3341\n",
      "Epoch [105/300], Step [61/172], Loss: 8.8070\n",
      "Epoch [105/300], Step [62/172], Loss: 22.8601\n",
      "Epoch [105/300], Step [63/172], Loss: 8.7657\n",
      "Epoch [105/300], Step [64/172], Loss: 8.2523\n",
      "Epoch [105/300], Step [65/172], Loss: 20.5133\n",
      "Epoch [105/300], Step [66/172], Loss: 5.8514\n",
      "Epoch [105/300], Step [67/172], Loss: 27.1900\n",
      "Epoch [105/300], Step [68/172], Loss: 6.1302\n",
      "Epoch [105/300], Step [69/172], Loss: 60.7806\n",
      "Epoch [105/300], Step [70/172], Loss: 55.9872\n",
      "Epoch [105/300], Step [71/172], Loss: 52.2175\n",
      "Epoch [105/300], Step [72/172], Loss: 57.3086\n",
      "Epoch [105/300], Step [73/172], Loss: 61.3273\n",
      "Epoch [105/300], Step [74/172], Loss: 34.6112\n",
      "Epoch [105/300], Step [75/172], Loss: 32.7607\n",
      "Epoch [105/300], Step [76/172], Loss: 37.2836\n",
      "Epoch [105/300], Step [77/172], Loss: 61.8177\n",
      "Epoch [105/300], Step [78/172], Loss: 48.1775\n",
      "Epoch [105/300], Step [79/172], Loss: 48.0065\n",
      "Epoch [105/300], Step [80/172], Loss: 58.2516\n",
      "Epoch [105/300], Step [81/172], Loss: 41.6503\n",
      "Epoch [105/300], Step [82/172], Loss: 39.4189\n",
      "Epoch [105/300], Step [83/172], Loss: 49.6758\n",
      "Epoch [105/300], Step [84/172], Loss: 37.6490\n",
      "Epoch [105/300], Step [85/172], Loss: 41.3818\n",
      "Epoch [105/300], Step [86/172], Loss: 33.5569\n",
      "Epoch [105/300], Step [87/172], Loss: 28.3758\n",
      "Epoch [105/300], Step [88/172], Loss: 29.8802\n",
      "Epoch [105/300], Step [89/172], Loss: 26.1258\n",
      "Epoch [105/300], Step [90/172], Loss: 25.4125\n",
      "Epoch [105/300], Step [91/172], Loss: 29.3603\n",
      "Epoch [105/300], Step [92/172], Loss: 21.6335\n",
      "Epoch [105/300], Step [93/172], Loss: 21.3928\n",
      "Epoch [105/300], Step [94/172], Loss: 29.7012\n",
      "Epoch [105/300], Step [95/172], Loss: 23.0046\n",
      "Epoch [105/300], Step [96/172], Loss: 18.9548\n",
      "Epoch [105/300], Step [97/172], Loss: 26.0853\n",
      "Epoch [105/300], Step [98/172], Loss: 19.4884\n",
      "Epoch [105/300], Step [99/172], Loss: 18.0593\n",
      "Epoch [105/300], Step [100/172], Loss: 15.9238\n",
      "Epoch [105/300], Step [101/172], Loss: 17.2958\n",
      "Epoch [105/300], Step [102/172], Loss: 16.0550\n",
      "Epoch [105/300], Step [103/172], Loss: 13.8334\n",
      "Epoch [105/300], Step [104/172], Loss: 15.9658\n",
      "Epoch [105/300], Step [105/172], Loss: 18.0300\n",
      "Epoch [105/300], Step [106/172], Loss: 17.0314\n",
      "Epoch [105/300], Step [107/172], Loss: 15.5845\n",
      "Epoch [105/300], Step [108/172], Loss: 16.5325\n",
      "Epoch [105/300], Step [109/172], Loss: 17.4139\n",
      "Epoch [105/300], Step [110/172], Loss: 15.9376\n",
      "Epoch [105/300], Step [111/172], Loss: 14.2881\n",
      "Epoch [105/300], Step [112/172], Loss: 18.5796\n",
      "Epoch [105/300], Step [113/172], Loss: 14.5959\n",
      "Epoch [105/300], Step [114/172], Loss: 14.0619\n",
      "Epoch [105/300], Step [115/172], Loss: 21.4892\n",
      "Epoch [105/300], Step [116/172], Loss: 14.7023\n",
      "Epoch [105/300], Step [117/172], Loss: 11.8353\n",
      "Epoch [105/300], Step [118/172], Loss: 15.2017\n",
      "Epoch [105/300], Step [119/172], Loss: 16.0027\n",
      "Epoch [105/300], Step [120/172], Loss: 11.2314\n",
      "Epoch [105/300], Step [121/172], Loss: 11.0023\n",
      "Epoch [105/300], Step [122/172], Loss: 10.6597\n",
      "Epoch [105/300], Step [123/172], Loss: 10.5348\n",
      "Epoch [105/300], Step [124/172], Loss: 8.5414\n",
      "Epoch [105/300], Step [125/172], Loss: 13.2517\n",
      "Epoch [105/300], Step [126/172], Loss: 10.9052\n",
      "Epoch [105/300], Step [127/172], Loss: 12.1505\n",
      "Epoch [105/300], Step [128/172], Loss: 12.4861\n",
      "Epoch [105/300], Step [129/172], Loss: 9.0711\n",
      "Epoch [105/300], Step [130/172], Loss: 11.7940\n",
      "Epoch [105/300], Step [131/172], Loss: 8.9741\n",
      "Epoch [105/300], Step [132/172], Loss: 9.1816\n",
      "Epoch [105/300], Step [133/172], Loss: 10.0099\n",
      "Epoch [105/300], Step [134/172], Loss: 12.2905\n",
      "Epoch [105/300], Step [135/172], Loss: 9.2543\n",
      "Epoch [105/300], Step [136/172], Loss: 8.7503\n",
      "Epoch [105/300], Step [137/172], Loss: 10.2291\n",
      "Epoch [105/300], Step [138/172], Loss: 8.4845\n",
      "Epoch [105/300], Step [139/172], Loss: 10.0804\n",
      "Epoch [105/300], Step [140/172], Loss: 9.8147\n",
      "Epoch [105/300], Step [141/172], Loss: 11.6579\n",
      "Epoch [105/300], Step [142/172], Loss: 13.6654\n",
      "Epoch [105/300], Step [143/172], Loss: 9.7240\n",
      "Epoch [105/300], Step [144/172], Loss: 9.0603\n",
      "Epoch [105/300], Step [145/172], Loss: 9.9434\n",
      "Epoch [105/300], Step [146/172], Loss: 9.9133\n",
      "Epoch [105/300], Step [147/172], Loss: 5.6700\n",
      "Epoch [105/300], Step [148/172], Loss: 6.7880\n",
      "Epoch [105/300], Step [149/172], Loss: 8.1136\n",
      "Epoch [105/300], Step [150/172], Loss: 8.0678\n",
      "Epoch [105/300], Step [151/172], Loss: 7.2947\n",
      "Epoch [105/300], Step [152/172], Loss: 7.9831\n",
      "Epoch [105/300], Step [153/172], Loss: 7.2261\n",
      "Epoch [105/300], Step [154/172], Loss: 8.0590\n",
      "Epoch [105/300], Step [155/172], Loss: 7.0521\n",
      "Epoch [105/300], Step [156/172], Loss: 12.4659\n",
      "Epoch [105/300], Step [157/172], Loss: 10.2937\n",
      "Epoch [105/300], Step [158/172], Loss: 7.7869\n",
      "Epoch [105/300], Step [159/172], Loss: 9.6059\n",
      "Epoch [105/300], Step [160/172], Loss: 10.1430\n",
      "Epoch [105/300], Step [161/172], Loss: 7.2819\n",
      "Epoch [105/300], Step [162/172], Loss: 7.4933\n",
      "Epoch [105/300], Step [163/172], Loss: 6.6356\n",
      "Epoch [105/300], Step [164/172], Loss: 9.6813\n",
      "Epoch [105/300], Step [165/172], Loss: 6.4815\n",
      "Epoch [105/300], Step [166/172], Loss: 6.4847\n",
      "Epoch [105/300], Step [167/172], Loss: 9.2850\n",
      "Epoch [105/300], Step [168/172], Loss: 7.2662\n",
      "Epoch [105/300], Step [169/172], Loss: 7.2954\n",
      "Epoch [105/300], Step [170/172], Loss: 5.7804\n",
      "Epoch [105/300], Step [171/172], Loss: 6.5946\n",
      "Epoch [105/300], Step [172/172], Loss: 5.4113\n",
      "Epoch [106/300], Step [1/172], Loss: 78.2464\n",
      "Epoch [106/300], Step [2/172], Loss: 77.5678\n",
      "Epoch [106/300], Step [3/172], Loss: 71.4693\n",
      "Epoch [106/300], Step [4/172], Loss: 44.3142\n",
      "Epoch [106/300], Step [5/172], Loss: 65.0449\n",
      "Epoch [106/300], Step [6/172], Loss: 19.5941\n",
      "Epoch [106/300], Step [7/172], Loss: 29.1262\n",
      "Epoch [106/300], Step [8/172], Loss: 6.8867\n",
      "Epoch [106/300], Step [9/172], Loss: 42.1277\n",
      "Epoch [106/300], Step [10/172], Loss: 46.5590\n",
      "Epoch [106/300], Step [11/172], Loss: 81.4293\n",
      "Epoch [106/300], Step [12/172], Loss: 83.1319\n",
      "Epoch [106/300], Step [13/172], Loss: 41.9328\n",
      "Epoch [106/300], Step [14/172], Loss: 85.6756\n",
      "Epoch [106/300], Step [15/172], Loss: 72.4850\n",
      "Epoch [106/300], Step [16/172], Loss: 18.5185\n",
      "Epoch [106/300], Step [17/172], Loss: 57.3302\n",
      "Epoch [106/300], Step [18/172], Loss: 62.3766\n",
      "Epoch [106/300], Step [19/172], Loss: 84.0067\n",
      "Epoch [106/300], Step [20/172], Loss: 66.7439\n",
      "Epoch [106/300], Step [21/172], Loss: 95.2257\n",
      "Epoch [106/300], Step [22/172], Loss: 76.5937\n",
      "Epoch [106/300], Step [23/172], Loss: 3.2882\n",
      "Epoch [106/300], Step [24/172], Loss: 69.4016\n",
      "Epoch [106/300], Step [25/172], Loss: 45.8953\n",
      "Epoch [106/300], Step [26/172], Loss: 57.9327\n",
      "Epoch [106/300], Step [27/172], Loss: 74.3111\n",
      "Epoch [106/300], Step [28/172], Loss: 34.5349\n",
      "Epoch [106/300], Step [29/172], Loss: 24.3955\n",
      "Epoch [106/300], Step [30/172], Loss: 82.3807\n",
      "Epoch [106/300], Step [31/172], Loss: 44.6730\n",
      "Epoch [106/300], Step [32/172], Loss: 41.4681\n",
      "Epoch [106/300], Step [33/172], Loss: 71.6047\n",
      "Epoch [106/300], Step [34/172], Loss: 4.5243\n",
      "Epoch [106/300], Step [35/172], Loss: 15.4719\n",
      "Epoch [106/300], Step [36/172], Loss: 20.6933\n",
      "Epoch [106/300], Step [37/172], Loss: 17.5641\n",
      "Epoch [106/300], Step [38/172], Loss: 27.1331\n",
      "Epoch [106/300], Step [39/172], Loss: 44.1640\n",
      "Epoch [106/300], Step [40/172], Loss: 20.5378\n",
      "Epoch [106/300], Step [41/172], Loss: 37.8179\n",
      "Epoch [106/300], Step [42/172], Loss: 41.3952\n",
      "Epoch [106/300], Step [43/172], Loss: 26.5178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [106/300], Step [44/172], Loss: 20.6604\n",
      "Epoch [106/300], Step [45/172], Loss: 22.2005\n",
      "Epoch [106/300], Step [46/172], Loss: 22.8523\n",
      "Epoch [106/300], Step [47/172], Loss: 49.2181\n",
      "Epoch [106/300], Step [48/172], Loss: 52.2322\n",
      "Epoch [106/300], Step [49/172], Loss: 19.8255\n",
      "Epoch [106/300], Step [50/172], Loss: 50.6042\n",
      "Epoch [106/300], Step [51/172], Loss: 7.3502\n",
      "Epoch [106/300], Step [52/172], Loss: 17.8805\n",
      "Epoch [106/300], Step [53/172], Loss: 23.1773\n",
      "Epoch [106/300], Step [54/172], Loss: 11.5902\n",
      "Epoch [106/300], Step [55/172], Loss: 12.0197\n",
      "Epoch [106/300], Step [56/172], Loss: 10.6763\n",
      "Epoch [106/300], Step [57/172], Loss: 19.7953\n",
      "Epoch [106/300], Step [58/172], Loss: 16.8621\n",
      "Epoch [106/300], Step [59/172], Loss: 30.1172\n",
      "Epoch [106/300], Step [60/172], Loss: 45.0234\n",
      "Epoch [106/300], Step [61/172], Loss: 8.7752\n",
      "Epoch [106/300], Step [62/172], Loss: 22.9842\n",
      "Epoch [106/300], Step [63/172], Loss: 8.9304\n",
      "Epoch [106/300], Step [64/172], Loss: 8.3974\n",
      "Epoch [106/300], Step [65/172], Loss: 20.5715\n",
      "Epoch [106/300], Step [66/172], Loss: 5.8962\n",
      "Epoch [106/300], Step [67/172], Loss: 27.2793\n",
      "Epoch [106/300], Step [68/172], Loss: 6.0088\n",
      "Epoch [106/300], Step [69/172], Loss: 60.5405\n",
      "Epoch [106/300], Step [70/172], Loss: 55.6449\n",
      "Epoch [106/300], Step [71/172], Loss: 51.7889\n",
      "Epoch [106/300], Step [72/172], Loss: 57.0095\n",
      "Epoch [106/300], Step [73/172], Loss: 60.9879\n",
      "Epoch [106/300], Step [74/172], Loss: 34.3756\n",
      "Epoch [106/300], Step [75/172], Loss: 32.6805\n",
      "Epoch [106/300], Step [76/172], Loss: 37.1215\n",
      "Epoch [106/300], Step [77/172], Loss: 61.6273\n",
      "Epoch [106/300], Step [78/172], Loss: 47.8669\n",
      "Epoch [106/300], Step [79/172], Loss: 47.6325\n",
      "Epoch [106/300], Step [80/172], Loss: 58.1745\n",
      "Epoch [106/300], Step [81/172], Loss: 41.4447\n",
      "Epoch [106/300], Step [82/172], Loss: 39.5717\n",
      "Epoch [106/300], Step [83/172], Loss: 49.5443\n",
      "Epoch [106/300], Step [84/172], Loss: 37.4643\n",
      "Epoch [106/300], Step [85/172], Loss: 41.2308\n",
      "Epoch [106/300], Step [86/172], Loss: 33.4615\n",
      "Epoch [106/300], Step [87/172], Loss: 28.3218\n",
      "Epoch [106/300], Step [88/172], Loss: 29.8230\n",
      "Epoch [106/300], Step [89/172], Loss: 26.1439\n",
      "Epoch [106/300], Step [90/172], Loss: 25.4308\n",
      "Epoch [106/300], Step [91/172], Loss: 29.3453\n",
      "Epoch [106/300], Step [92/172], Loss: 21.6414\n",
      "Epoch [106/300], Step [93/172], Loss: 21.3356\n",
      "Epoch [106/300], Step [94/172], Loss: 29.6401\n",
      "Epoch [106/300], Step [95/172], Loss: 22.9645\n",
      "Epoch [106/300], Step [96/172], Loss: 18.9806\n",
      "Epoch [106/300], Step [97/172], Loss: 26.1427\n",
      "Epoch [106/300], Step [98/172], Loss: 19.5240\n",
      "Epoch [106/300], Step [99/172], Loss: 18.0907\n",
      "Epoch [106/300], Step [100/172], Loss: 15.9217\n",
      "Epoch [106/300], Step [101/172], Loss: 17.2865\n",
      "Epoch [106/300], Step [102/172], Loss: 16.1931\n",
      "Epoch [106/300], Step [103/172], Loss: 13.8532\n",
      "Epoch [106/300], Step [104/172], Loss: 15.9631\n",
      "Epoch [106/300], Step [105/172], Loss: 18.1684\n",
      "Epoch [106/300], Step [106/172], Loss: 17.0222\n",
      "Epoch [106/300], Step [107/172], Loss: 15.5747\n",
      "Epoch [106/300], Step [108/172], Loss: 16.5160\n",
      "Epoch [106/300], Step [109/172], Loss: 17.4516\n",
      "Epoch [106/300], Step [110/172], Loss: 15.9289\n",
      "Epoch [106/300], Step [111/172], Loss: 14.2285\n",
      "Epoch [106/300], Step [112/172], Loss: 18.5431\n",
      "Epoch [106/300], Step [113/172], Loss: 14.5957\n",
      "Epoch [106/300], Step [114/172], Loss: 14.0396\n",
      "Epoch [106/300], Step [115/172], Loss: 21.4924\n",
      "Epoch [106/300], Step [116/172], Loss: 14.6931\n",
      "Epoch [106/300], Step [117/172], Loss: 11.7758\n",
      "Epoch [106/300], Step [118/172], Loss: 15.1519\n",
      "Epoch [106/300], Step [119/172], Loss: 15.9993\n",
      "Epoch [106/300], Step [120/172], Loss: 11.2506\n",
      "Epoch [106/300], Step [121/172], Loss: 10.9556\n",
      "Epoch [106/300], Step [122/172], Loss: 10.5962\n",
      "Epoch [106/300], Step [123/172], Loss: 10.5528\n",
      "Epoch [106/300], Step [124/172], Loss: 8.5313\n",
      "Epoch [106/300], Step [125/172], Loss: 13.1940\n",
      "Epoch [106/300], Step [126/172], Loss: 10.8771\n",
      "Epoch [106/300], Step [127/172], Loss: 12.0992\n",
      "Epoch [106/300], Step [128/172], Loss: 12.4353\n",
      "Epoch [106/300], Step [129/172], Loss: 9.0330\n",
      "Epoch [106/300], Step [130/172], Loss: 11.7875\n",
      "Epoch [106/300], Step [131/172], Loss: 8.9378\n",
      "Epoch [106/300], Step [132/172], Loss: 9.1600\n",
      "Epoch [106/300], Step [133/172], Loss: 9.9694\n",
      "Epoch [106/300], Step [134/172], Loss: 12.2872\n",
      "Epoch [106/300], Step [135/172], Loss: 9.2494\n",
      "Epoch [106/300], Step [136/172], Loss: 8.7420\n",
      "Epoch [106/300], Step [137/172], Loss: 10.1673\n",
      "Epoch [106/300], Step [138/172], Loss: 8.4408\n",
      "Epoch [106/300], Step [139/172], Loss: 10.0357\n",
      "Epoch [106/300], Step [140/172], Loss: 9.7784\n",
      "Epoch [106/300], Step [141/172], Loss: 11.6348\n",
      "Epoch [106/300], Step [142/172], Loss: 13.6196\n",
      "Epoch [106/300], Step [143/172], Loss: 9.7214\n",
      "Epoch [106/300], Step [144/172], Loss: 9.0347\n",
      "Epoch [106/300], Step [145/172], Loss: 9.9245\n",
      "Epoch [106/300], Step [146/172], Loss: 9.8715\n",
      "Epoch [106/300], Step [147/172], Loss: 5.6371\n",
      "Epoch [106/300], Step [148/172], Loss: 6.7566\n",
      "Epoch [106/300], Step [149/172], Loss: 8.0508\n",
      "Epoch [106/300], Step [150/172], Loss: 8.0276\n",
      "Epoch [106/300], Step [151/172], Loss: 7.3050\n",
      "Epoch [106/300], Step [152/172], Loss: 7.9685\n",
      "Epoch [106/300], Step [153/172], Loss: 7.1976\n",
      "Epoch [106/300], Step [154/172], Loss: 8.0070\n",
      "Epoch [106/300], Step [155/172], Loss: 7.0107\n",
      "Epoch [106/300], Step [156/172], Loss: 12.4456\n",
      "Epoch [106/300], Step [157/172], Loss: 10.2534\n",
      "Epoch [106/300], Step [158/172], Loss: 7.7545\n",
      "Epoch [106/300], Step [159/172], Loss: 9.5688\n",
      "Epoch [106/300], Step [160/172], Loss: 10.1290\n",
      "Epoch [106/300], Step [161/172], Loss: 7.2595\n",
      "Epoch [106/300], Step [162/172], Loss: 7.4302\n",
      "Epoch [106/300], Step [163/172], Loss: 6.6178\n",
      "Epoch [106/300], Step [164/172], Loss: 9.7951\n",
      "Epoch [106/300], Step [165/172], Loss: 6.4663\n",
      "Epoch [106/300], Step [166/172], Loss: 6.4327\n",
      "Epoch [106/300], Step [167/172], Loss: 9.2966\n",
      "Epoch [106/300], Step [168/172], Loss: 7.2236\n",
      "Epoch [106/300], Step [169/172], Loss: 7.2555\n",
      "Epoch [106/300], Step [170/172], Loss: 5.7497\n",
      "Epoch [106/300], Step [171/172], Loss: 6.5859\n",
      "Epoch [106/300], Step [172/172], Loss: 5.3732\n",
      "Epoch [107/300], Step [1/172], Loss: 77.6374\n",
      "Epoch [107/300], Step [2/172], Loss: 77.1409\n",
      "Epoch [107/300], Step [3/172], Loss: 70.9410\n",
      "Epoch [107/300], Step [4/172], Loss: 43.9352\n",
      "Epoch [107/300], Step [5/172], Loss: 64.8112\n",
      "Epoch [107/300], Step [6/172], Loss: 19.4875\n",
      "Epoch [107/300], Step [7/172], Loss: 28.8824\n",
      "Epoch [107/300], Step [8/172], Loss: 6.1246\n",
      "Epoch [107/300], Step [9/172], Loss: 41.7114\n",
      "Epoch [107/300], Step [10/172], Loss: 46.5425\n",
      "Epoch [107/300], Step [11/172], Loss: 80.9694\n",
      "Epoch [107/300], Step [12/172], Loss: 83.0791\n",
      "Epoch [107/300], Step [13/172], Loss: 41.8164\n",
      "Epoch [107/300], Step [14/172], Loss: 85.1884\n",
      "Epoch [107/300], Step [15/172], Loss: 72.2186\n",
      "Epoch [107/300], Step [16/172], Loss: 17.9060\n",
      "Epoch [107/300], Step [17/172], Loss: 57.0405\n",
      "Epoch [107/300], Step [18/172], Loss: 62.3288\n",
      "Epoch [107/300], Step [19/172], Loss: 84.1714\n",
      "Epoch [107/300], Step [20/172], Loss: 65.7502\n",
      "Epoch [107/300], Step [21/172], Loss: 95.1132\n",
      "Epoch [107/300], Step [22/172], Loss: 75.6617\n",
      "Epoch [107/300], Step [23/172], Loss: 3.6986\n",
      "Epoch [107/300], Step [24/172], Loss: 69.1669\n",
      "Epoch [107/300], Step [25/172], Loss: 45.8898\n",
      "Epoch [107/300], Step [26/172], Loss: 57.7062\n",
      "Epoch [107/300], Step [27/172], Loss: 73.5350\n",
      "Epoch [107/300], Step [28/172], Loss: 34.1758\n",
      "Epoch [107/300], Step [29/172], Loss: 24.1100\n",
      "Epoch [107/300], Step [30/172], Loss: 82.1179\n",
      "Epoch [107/300], Step [31/172], Loss: 44.6281\n",
      "Epoch [107/300], Step [32/172], Loss: 41.3215\n",
      "Epoch [107/300], Step [33/172], Loss: 71.3800\n",
      "Epoch [107/300], Step [34/172], Loss: 4.7157\n",
      "Epoch [107/300], Step [35/172], Loss: 15.1166\n",
      "Epoch [107/300], Step [36/172], Loss: 20.6726\n",
      "Epoch [107/300], Step [37/172], Loss: 17.5274\n",
      "Epoch [107/300], Step [38/172], Loss: 26.9799\n",
      "Epoch [107/300], Step [39/172], Loss: 43.5289\n",
      "Epoch [107/300], Step [40/172], Loss: 20.4758\n",
      "Epoch [107/300], Step [41/172], Loss: 37.7284\n",
      "Epoch [107/300], Step [42/172], Loss: 41.2567\n",
      "Epoch [107/300], Step [43/172], Loss: 26.4218\n",
      "Epoch [107/300], Step [44/172], Loss: 20.6843\n",
      "Epoch [107/300], Step [45/172], Loss: 22.2573\n",
      "Epoch [107/300], Step [46/172], Loss: 22.6500\n",
      "Epoch [107/300], Step [47/172], Loss: 49.1010\n",
      "Epoch [107/300], Step [48/172], Loss: 52.4718\n",
      "Epoch [107/300], Step [49/172], Loss: 19.8599\n",
      "Epoch [107/300], Step [50/172], Loss: 50.2605\n",
      "Epoch [107/300], Step [51/172], Loss: 7.3254\n",
      "Epoch [107/300], Step [52/172], Loss: 17.8621\n",
      "Epoch [107/300], Step [53/172], Loss: 23.1072\n",
      "Epoch [107/300], Step [54/172], Loss: 11.6074\n",
      "Epoch [107/300], Step [55/172], Loss: 12.0931\n",
      "Epoch [107/300], Step [56/172], Loss: 10.6524\n",
      "Epoch [107/300], Step [57/172], Loss: 19.6750\n",
      "Epoch [107/300], Step [58/172], Loss: 16.8830\n",
      "Epoch [107/300], Step [59/172], Loss: 30.2440\n",
      "Epoch [107/300], Step [60/172], Loss: 44.4875\n",
      "Epoch [107/300], Step [61/172], Loss: 8.7055\n",
      "Epoch [107/300], Step [62/172], Loss: 22.8867\n",
      "Epoch [107/300], Step [63/172], Loss: 8.8101\n",
      "Epoch [107/300], Step [64/172], Loss: 8.3402\n",
      "Epoch [107/300], Step [65/172], Loss: 20.5349\n",
      "Epoch [107/300], Step [66/172], Loss: 5.8606\n",
      "Epoch [107/300], Step [67/172], Loss: 27.2187\n",
      "Epoch [107/300], Step [68/172], Loss: 6.0331\n",
      "Epoch [107/300], Step [69/172], Loss: 60.1625\n",
      "Epoch [107/300], Step [70/172], Loss: 55.4912\n",
      "Epoch [107/300], Step [71/172], Loss: 51.8384\n",
      "Epoch [107/300], Step [72/172], Loss: 56.8933\n",
      "Epoch [107/300], Step [73/172], Loss: 60.8749\n",
      "Epoch [107/300], Step [74/172], Loss: 34.2969\n",
      "Epoch [107/300], Step [75/172], Loss: 32.6711\n",
      "Epoch [107/300], Step [76/172], Loss: 36.9564\n",
      "Epoch [107/300], Step [77/172], Loss: 61.5302\n",
      "Epoch [107/300], Step [78/172], Loss: 47.8009\n",
      "Epoch [107/300], Step [79/172], Loss: 47.5256\n",
      "Epoch [107/300], Step [80/172], Loss: 57.9586\n",
      "Epoch [107/300], Step [81/172], Loss: 41.4589\n",
      "Epoch [107/300], Step [82/172], Loss: 39.2773\n",
      "Epoch [107/300], Step [83/172], Loss: 49.4402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [107/300], Step [84/172], Loss: 37.3329\n",
      "Epoch [107/300], Step [85/172], Loss: 41.1571\n",
      "Epoch [107/300], Step [86/172], Loss: 33.4718\n",
      "Epoch [107/300], Step [87/172], Loss: 28.2675\n",
      "Epoch [107/300], Step [88/172], Loss: 29.7013\n",
      "Epoch [107/300], Step [89/172], Loss: 26.0670\n",
      "Epoch [107/300], Step [90/172], Loss: 25.3314\n",
      "Epoch [107/300], Step [91/172], Loss: 29.1859\n",
      "Epoch [107/300], Step [92/172], Loss: 21.5966\n",
      "Epoch [107/300], Step [93/172], Loss: 21.3339\n",
      "Epoch [107/300], Step [94/172], Loss: 29.7169\n",
      "Epoch [107/300], Step [95/172], Loss: 22.9428\n",
      "Epoch [107/300], Step [96/172], Loss: 18.9778\n",
      "Epoch [107/300], Step [97/172], Loss: 26.0713\n",
      "Epoch [107/300], Step [98/172], Loss: 19.4455\n",
      "Epoch [107/300], Step [99/172], Loss: 18.0850\n",
      "Epoch [107/300], Step [100/172], Loss: 15.8619\n",
      "Epoch [107/300], Step [101/172], Loss: 17.2270\n",
      "Epoch [107/300], Step [102/172], Loss: 16.0374\n",
      "Epoch [107/300], Step [103/172], Loss: 13.7794\n",
      "Epoch [107/300], Step [104/172], Loss: 15.9427\n",
      "Epoch [107/300], Step [105/172], Loss: 18.0308\n",
      "Epoch [107/300], Step [106/172], Loss: 16.9804\n",
      "Epoch [107/300], Step [107/172], Loss: 15.5358\n",
      "Epoch [107/300], Step [108/172], Loss: 16.4592\n",
      "Epoch [107/300], Step [109/172], Loss: 17.3203\n",
      "Epoch [107/300], Step [110/172], Loss: 15.9012\n",
      "Epoch [107/300], Step [111/172], Loss: 14.2243\n",
      "Epoch [107/300], Step [112/172], Loss: 18.4719\n",
      "Epoch [107/300], Step [113/172], Loss: 14.5215\n",
      "Epoch [107/300], Step [114/172], Loss: 14.0029\n",
      "Epoch [107/300], Step [115/172], Loss: 21.3976\n",
      "Epoch [107/300], Step [116/172], Loss: 14.6906\n",
      "Epoch [107/300], Step [117/172], Loss: 11.7377\n",
      "Epoch [107/300], Step [118/172], Loss: 15.1434\n",
      "Epoch [107/300], Step [119/172], Loss: 15.9809\n",
      "Epoch [107/300], Step [120/172], Loss: 11.1914\n",
      "Epoch [107/300], Step [121/172], Loss: 10.9210\n",
      "Epoch [107/300], Step [122/172], Loss: 10.5294\n",
      "Epoch [107/300], Step [123/172], Loss: 10.5408\n",
      "Epoch [107/300], Step [124/172], Loss: 8.5238\n",
      "Epoch [107/300], Step [125/172], Loss: 13.2035\n",
      "Epoch [107/300], Step [126/172], Loss: 10.8527\n",
      "Epoch [107/300], Step [127/172], Loss: 12.0538\n",
      "Epoch [107/300], Step [128/172], Loss: 12.4243\n",
      "Epoch [107/300], Step [129/172], Loss: 9.0118\n",
      "Epoch [107/300], Step [130/172], Loss: 11.7923\n",
      "Epoch [107/300], Step [131/172], Loss: 8.8889\n",
      "Epoch [107/300], Step [132/172], Loss: 9.1388\n",
      "Epoch [107/300], Step [133/172], Loss: 9.9232\n",
      "Epoch [107/300], Step [134/172], Loss: 12.3160\n",
      "Epoch [107/300], Step [135/172], Loss: 9.2535\n",
      "Epoch [107/300], Step [136/172], Loss: 8.6987\n",
      "Epoch [107/300], Step [137/172], Loss: 10.1360\n",
      "Epoch [107/300], Step [138/172], Loss: 8.4069\n",
      "Epoch [107/300], Step [139/172], Loss: 10.0290\n",
      "Epoch [107/300], Step [140/172], Loss: 9.7537\n",
      "Epoch [107/300], Step [141/172], Loss: 11.5628\n",
      "Epoch [107/300], Step [142/172], Loss: 13.6377\n",
      "Epoch [107/300], Step [143/172], Loss: 9.7121\n",
      "Epoch [107/300], Step [144/172], Loss: 9.0287\n",
      "Epoch [107/300], Step [145/172], Loss: 9.9404\n",
      "Epoch [107/300], Step [146/172], Loss: 9.8792\n",
      "Epoch [107/300], Step [147/172], Loss: 5.6246\n",
      "Epoch [107/300], Step [148/172], Loss: 6.7404\n",
      "Epoch [107/300], Step [149/172], Loss: 8.0349\n",
      "Epoch [107/300], Step [150/172], Loss: 8.0046\n",
      "Epoch [107/300], Step [151/172], Loss: 7.2778\n",
      "Epoch [107/300], Step [152/172], Loss: 7.9697\n",
      "Epoch [107/300], Step [153/172], Loss: 7.1911\n",
      "Epoch [107/300], Step [154/172], Loss: 8.0002\n",
      "Epoch [107/300], Step [155/172], Loss: 6.9968\n",
      "Epoch [107/300], Step [156/172], Loss: 12.4343\n",
      "Epoch [107/300], Step [157/172], Loss: 10.2345\n",
      "Epoch [107/300], Step [158/172], Loss: 7.7262\n",
      "Epoch [107/300], Step [159/172], Loss: 9.6171\n",
      "Epoch [107/300], Step [160/172], Loss: 10.1233\n",
      "Epoch [107/300], Step [161/172], Loss: 7.2696\n",
      "Epoch [107/300], Step [162/172], Loss: 7.4050\n",
      "Epoch [107/300], Step [163/172], Loss: 6.6185\n",
      "Epoch [107/300], Step [164/172], Loss: 9.6830\n",
      "Epoch [107/300], Step [165/172], Loss: 6.4541\n",
      "Epoch [107/300], Step [166/172], Loss: 6.4009\n",
      "Epoch [107/300], Step [167/172], Loss: 9.3452\n",
      "Epoch [107/300], Step [168/172], Loss: 7.2600\n",
      "Epoch [107/300], Step [169/172], Loss: 7.2568\n",
      "Epoch [107/300], Step [170/172], Loss: 5.7557\n",
      "Epoch [107/300], Step [171/172], Loss: 6.6672\n",
      "Epoch [107/300], Step [172/172], Loss: 5.3871\n",
      "Epoch [108/300], Step [1/172], Loss: 77.1423\n",
      "Epoch [108/300], Step [2/172], Loss: 76.7242\n",
      "Epoch [108/300], Step [3/172], Loss: 70.3088\n",
      "Epoch [108/300], Step [4/172], Loss: 43.5574\n",
      "Epoch [108/300], Step [5/172], Loss: 64.2466\n",
      "Epoch [108/300], Step [6/172], Loss: 19.5548\n",
      "Epoch [108/300], Step [7/172], Loss: 29.3318\n",
      "Epoch [108/300], Step [8/172], Loss: 6.6687\n",
      "Epoch [108/300], Step [9/172], Loss: 41.5882\n",
      "Epoch [108/300], Step [10/172], Loss: 46.3751\n",
      "Epoch [108/300], Step [11/172], Loss: 80.5748\n",
      "Epoch [108/300], Step [12/172], Loss: 82.9956\n",
      "Epoch [108/300], Step [13/172], Loss: 41.8830\n",
      "Epoch [108/300], Step [14/172], Loss: 85.0714\n",
      "Epoch [108/300], Step [15/172], Loss: 72.0881\n",
      "Epoch [108/300], Step [16/172], Loss: 17.8223\n",
      "Epoch [108/300], Step [17/172], Loss: 57.1221\n",
      "Epoch [108/300], Step [18/172], Loss: 62.5085\n",
      "Epoch [108/300], Step [19/172], Loss: 84.3188\n",
      "Epoch [108/300], Step [20/172], Loss: 65.8053\n",
      "Epoch [108/300], Step [21/172], Loss: 95.2575\n",
      "Epoch [108/300], Step [22/172], Loss: 75.7842\n",
      "Epoch [108/300], Step [23/172], Loss: 3.3565\n",
      "Epoch [108/300], Step [24/172], Loss: 69.2118\n",
      "Epoch [108/300], Step [25/172], Loss: 45.6842\n",
      "Epoch [108/300], Step [26/172], Loss: 57.6689\n",
      "Epoch [108/300], Step [27/172], Loss: 73.8892\n",
      "Epoch [108/300], Step [28/172], Loss: 34.0893\n",
      "Epoch [108/300], Step [29/172], Loss: 23.9958\n",
      "Epoch [108/300], Step [30/172], Loss: 82.2578\n",
      "Epoch [108/300], Step [31/172], Loss: 44.3952\n",
      "Epoch [108/300], Step [32/172], Loss: 41.3950\n",
      "Epoch [108/300], Step [33/172], Loss: 71.4979\n",
      "Epoch [108/300], Step [34/172], Loss: 4.4980\n",
      "Epoch [108/300], Step [35/172], Loss: 15.3001\n",
      "Epoch [108/300], Step [36/172], Loss: 20.6379\n",
      "Epoch [108/300], Step [37/172], Loss: 17.5170\n",
      "Epoch [108/300], Step [38/172], Loss: 27.0433\n",
      "Epoch [108/300], Step [39/172], Loss: 43.4627\n",
      "Epoch [108/300], Step [40/172], Loss: 20.3234\n",
      "Epoch [108/300], Step [41/172], Loss: 37.6912\n",
      "Epoch [108/300], Step [42/172], Loss: 41.0344\n",
      "Epoch [108/300], Step [43/172], Loss: 26.3230\n",
      "Epoch [108/300], Step [44/172], Loss: 20.6150\n",
      "Epoch [108/300], Step [45/172], Loss: 22.1621\n",
      "Epoch [108/300], Step [46/172], Loss: 22.5051\n",
      "Epoch [108/300], Step [47/172], Loss: 48.8771\n",
      "Epoch [108/300], Step [48/172], Loss: 52.7325\n",
      "Epoch [108/300], Step [49/172], Loss: 19.7807\n",
      "Epoch [108/300], Step [50/172], Loss: 50.5227\n",
      "Epoch [108/300], Step [51/172], Loss: 7.3498\n",
      "Epoch [108/300], Step [52/172], Loss: 17.7098\n",
      "Epoch [108/300], Step [53/172], Loss: 23.0184\n",
      "Epoch [108/300], Step [54/172], Loss: 11.5274\n",
      "Epoch [108/300], Step [55/172], Loss: 12.0113\n",
      "Epoch [108/300], Step [56/172], Loss: 10.7077\n",
      "Epoch [108/300], Step [57/172], Loss: 19.6029\n",
      "Epoch [108/300], Step [58/172], Loss: 16.8155\n",
      "Epoch [108/300], Step [59/172], Loss: 30.2064\n",
      "Epoch [108/300], Step [60/172], Loss: 44.2604\n",
      "Epoch [108/300], Step [61/172], Loss: 8.6736\n",
      "Epoch [108/300], Step [62/172], Loss: 22.9355\n",
      "Epoch [108/300], Step [63/172], Loss: 8.9302\n",
      "Epoch [108/300], Step [64/172], Loss: 8.3964\n",
      "Epoch [108/300], Step [65/172], Loss: 20.6102\n",
      "Epoch [108/300], Step [66/172], Loss: 5.8061\n",
      "Epoch [108/300], Step [67/172], Loss: 27.2223\n",
      "Epoch [108/300], Step [68/172], Loss: 5.8347\n",
      "Epoch [108/300], Step [69/172], Loss: 59.8951\n",
      "Epoch [108/300], Step [70/172], Loss: 55.2093\n",
      "Epoch [108/300], Step [71/172], Loss: 51.5945\n",
      "Epoch [108/300], Step [72/172], Loss: 56.7590\n",
      "Epoch [108/300], Step [73/172], Loss: 60.7614\n",
      "Epoch [108/300], Step [74/172], Loss: 34.0752\n",
      "Epoch [108/300], Step [75/172], Loss: 32.6103\n",
      "Epoch [108/300], Step [76/172], Loss: 36.7988\n",
      "Epoch [108/300], Step [77/172], Loss: 61.4683\n",
      "Epoch [108/300], Step [78/172], Loss: 47.7340\n",
      "Epoch [108/300], Step [79/172], Loss: 47.3262\n",
      "Epoch [108/300], Step [80/172], Loss: 58.1126\n",
      "Epoch [108/300], Step [81/172], Loss: 41.2723\n",
      "Epoch [108/300], Step [82/172], Loss: 39.5432\n",
      "Epoch [108/300], Step [83/172], Loss: 49.1252\n",
      "Epoch [108/300], Step [84/172], Loss: 37.0791\n",
      "Epoch [108/300], Step [85/172], Loss: 40.9395\n",
      "Epoch [108/300], Step [86/172], Loss: 33.3269\n",
      "Epoch [108/300], Step [87/172], Loss: 28.1030\n",
      "Epoch [108/300], Step [88/172], Loss: 29.5805\n",
      "Epoch [108/300], Step [89/172], Loss: 25.9416\n",
      "Epoch [108/300], Step [90/172], Loss: 25.1619\n",
      "Epoch [108/300], Step [91/172], Loss: 29.0818\n",
      "Epoch [108/300], Step [92/172], Loss: 21.5196\n",
      "Epoch [108/300], Step [93/172], Loss: 21.2064\n",
      "Epoch [108/300], Step [94/172], Loss: 29.5175\n",
      "Epoch [108/300], Step [95/172], Loss: 22.8008\n",
      "Epoch [108/300], Step [96/172], Loss: 18.9306\n",
      "Epoch [108/300], Step [97/172], Loss: 26.0280\n",
      "Epoch [108/300], Step [98/172], Loss: 19.3468\n",
      "Epoch [108/300], Step [99/172], Loss: 18.0641\n",
      "Epoch [108/300], Step [100/172], Loss: 15.8064\n",
      "Epoch [108/300], Step [101/172], Loss: 17.1571\n",
      "Epoch [108/300], Step [102/172], Loss: 16.1425\n",
      "Epoch [108/300], Step [103/172], Loss: 13.7079\n",
      "Epoch [108/300], Step [104/172], Loss: 15.9293\n",
      "Epoch [108/300], Step [105/172], Loss: 18.0741\n",
      "Epoch [108/300], Step [106/172], Loss: 16.9134\n",
      "Epoch [108/300], Step [107/172], Loss: 15.5203\n",
      "Epoch [108/300], Step [108/172], Loss: 16.4383\n",
      "Epoch [108/300], Step [109/172], Loss: 17.2077\n",
      "Epoch [108/300], Step [110/172], Loss: 15.8782\n",
      "Epoch [108/300], Step [111/172], Loss: 14.1525\n",
      "Epoch [108/300], Step [112/172], Loss: 18.3846\n",
      "Epoch [108/300], Step [113/172], Loss: 14.3941\n",
      "Epoch [108/300], Step [114/172], Loss: 13.9555\n",
      "Epoch [108/300], Step [115/172], Loss: 21.2438\n",
      "Epoch [108/300], Step [116/172], Loss: 14.6292\n",
      "Epoch [108/300], Step [117/172], Loss: 11.6457\n",
      "Epoch [108/300], Step [118/172], Loss: 14.9720\n",
      "Epoch [108/300], Step [119/172], Loss: 15.9744\n",
      "Epoch [108/300], Step [120/172], Loss: 11.0943\n",
      "Epoch [108/300], Step [121/172], Loss: 10.8897\n",
      "Epoch [108/300], Step [122/172], Loss: 10.5383\n",
      "Epoch [108/300], Step [123/172], Loss: 10.4642\n",
      "Epoch [108/300], Step [124/172], Loss: 8.4926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [108/300], Step [125/172], Loss: 13.1445\n",
      "Epoch [108/300], Step [126/172], Loss: 10.7987\n",
      "Epoch [108/300], Step [127/172], Loss: 11.9806\n",
      "Epoch [108/300], Step [128/172], Loss: 12.3227\n",
      "Epoch [108/300], Step [129/172], Loss: 8.9755\n",
      "Epoch [108/300], Step [130/172], Loss: 11.7444\n",
      "Epoch [108/300], Step [131/172], Loss: 8.8485\n",
      "Epoch [108/300], Step [132/172], Loss: 9.0948\n",
      "Epoch [108/300], Step [133/172], Loss: 9.8965\n",
      "Epoch [108/300], Step [134/172], Loss: 12.3136\n",
      "Epoch [108/300], Step [135/172], Loss: 9.2480\n",
      "Epoch [108/300], Step [136/172], Loss: 8.6763\n",
      "Epoch [108/300], Step [137/172], Loss: 10.1143\n",
      "Epoch [108/300], Step [138/172], Loss: 8.3888\n",
      "Epoch [108/300], Step [139/172], Loss: 10.0090\n",
      "Epoch [108/300], Step [140/172], Loss: 9.7141\n",
      "Epoch [108/300], Step [141/172], Loss: 11.4941\n",
      "Epoch [108/300], Step [142/172], Loss: 13.5810\n",
      "Epoch [108/300], Step [143/172], Loss: 9.7011\n",
      "Epoch [108/300], Step [144/172], Loss: 9.0267\n",
      "Epoch [108/300], Step [145/172], Loss: 9.8895\n",
      "Epoch [108/300], Step [146/172], Loss: 9.8636\n",
      "Epoch [108/300], Step [147/172], Loss: 5.6114\n",
      "Epoch [108/300], Step [148/172], Loss: 6.7195\n",
      "Epoch [108/300], Step [149/172], Loss: 7.9884\n",
      "Epoch [108/300], Step [150/172], Loss: 7.9604\n",
      "Epoch [108/300], Step [151/172], Loss: 7.2580\n",
      "Epoch [108/300], Step [152/172], Loss: 7.9615\n",
      "Epoch [108/300], Step [153/172], Loss: 7.1768\n",
      "Epoch [108/300], Step [154/172], Loss: 7.9801\n",
      "Epoch [108/300], Step [155/172], Loss: 6.9821\n",
      "Epoch [108/300], Step [156/172], Loss: 12.4331\n",
      "Epoch [108/300], Step [157/172], Loss: 10.1971\n",
      "Epoch [108/300], Step [158/172], Loss: 7.7077\n",
      "Epoch [108/300], Step [159/172], Loss: 9.5644\n",
      "Epoch [108/300], Step [160/172], Loss: 10.1244\n",
      "Epoch [108/300], Step [161/172], Loss: 7.2783\n",
      "Epoch [108/300], Step [162/172], Loss: 7.3557\n",
      "Epoch [108/300], Step [163/172], Loss: 6.6105\n",
      "Epoch [108/300], Step [164/172], Loss: 9.7417\n",
      "Epoch [108/300], Step [165/172], Loss: 6.4451\n",
      "Epoch [108/300], Step [166/172], Loss: 6.3764\n",
      "Epoch [108/300], Step [167/172], Loss: 9.3431\n",
      "Epoch [108/300], Step [168/172], Loss: 7.2616\n",
      "Epoch [108/300], Step [169/172], Loss: 7.2262\n",
      "Epoch [108/300], Step [170/172], Loss: 5.7342\n",
      "Epoch [108/300], Step [171/172], Loss: 6.6843\n",
      "Epoch [108/300], Step [172/172], Loss: 5.3743\n",
      "Epoch [109/300], Step [1/172], Loss: 76.7694\n",
      "Epoch [109/300], Step [2/172], Loss: 76.2471\n",
      "Epoch [109/300], Step [3/172], Loss: 70.3008\n",
      "Epoch [109/300], Step [4/172], Loss: 43.2677\n",
      "Epoch [109/300], Step [5/172], Loss: 64.0852\n",
      "Epoch [109/300], Step [6/172], Loss: 19.4378\n",
      "Epoch [109/300], Step [7/172], Loss: 28.9409\n",
      "Epoch [109/300], Step [8/172], Loss: 6.1375\n",
      "Epoch [109/300], Step [9/172], Loss: 41.2176\n",
      "Epoch [109/300], Step [10/172], Loss: 46.2766\n",
      "Epoch [109/300], Step [11/172], Loss: 80.0297\n",
      "Epoch [109/300], Step [12/172], Loss: 82.7577\n",
      "Epoch [109/300], Step [13/172], Loss: 41.7534\n",
      "Epoch [109/300], Step [14/172], Loss: 84.3039\n",
      "Epoch [109/300], Step [15/172], Loss: 71.6690\n",
      "Epoch [109/300], Step [16/172], Loss: 17.2091\n",
      "Epoch [109/300], Step [17/172], Loss: 56.6782\n",
      "Epoch [109/300], Step [18/172], Loss: 62.2796\n",
      "Epoch [109/300], Step [19/172], Loss: 84.0753\n",
      "Epoch [109/300], Step [20/172], Loss: 64.7567\n",
      "Epoch [109/300], Step [21/172], Loss: 94.7880\n",
      "Epoch [109/300], Step [22/172], Loss: 74.9140\n",
      "Epoch [109/300], Step [23/172], Loss: 3.1999\n",
      "Epoch [109/300], Step [24/172], Loss: 68.7074\n",
      "Epoch [109/300], Step [25/172], Loss: 45.7248\n",
      "Epoch [109/300], Step [26/172], Loss: 57.5459\n",
      "Epoch [109/300], Step [27/172], Loss: 73.0997\n",
      "Epoch [109/300], Step [28/172], Loss: 33.7159\n",
      "Epoch [109/300], Step [29/172], Loss: 23.7957\n",
      "Epoch [109/300], Step [30/172], Loss: 82.1120\n",
      "Epoch [109/300], Step [31/172], Loss: 44.3524\n",
      "Epoch [109/300], Step [32/172], Loss: 41.4749\n",
      "Epoch [109/300], Step [33/172], Loss: 71.4513\n",
      "Epoch [109/300], Step [34/172], Loss: 4.6449\n",
      "Epoch [109/300], Step [35/172], Loss: 15.0398\n",
      "Epoch [109/300], Step [36/172], Loss: 21.0046\n",
      "Epoch [109/300], Step [37/172], Loss: 17.5687\n",
      "Epoch [109/300], Step [38/172], Loss: 27.1076\n",
      "Epoch [109/300], Step [39/172], Loss: 43.0385\n",
      "Epoch [109/300], Step [40/172], Loss: 20.3155\n",
      "Epoch [109/300], Step [41/172], Loss: 37.4901\n",
      "Epoch [109/300], Step [42/172], Loss: 41.1466\n",
      "Epoch [109/300], Step [43/172], Loss: 26.2885\n",
      "Epoch [109/300], Step [44/172], Loss: 20.5777\n",
      "Epoch [109/300], Step [45/172], Loss: 22.1609\n",
      "Epoch [109/300], Step [46/172], Loss: 22.2214\n",
      "Epoch [109/300], Step [47/172], Loss: 48.6473\n",
      "Epoch [109/300], Step [48/172], Loss: 52.5654\n",
      "Epoch [109/300], Step [49/172], Loss: 19.6732\n",
      "Epoch [109/300], Step [50/172], Loss: 50.1935\n",
      "Epoch [109/300], Step [51/172], Loss: 7.2691\n",
      "Epoch [109/300], Step [52/172], Loss: 17.6838\n",
      "Epoch [109/300], Step [53/172], Loss: 22.9446\n",
      "Epoch [109/300], Step [54/172], Loss: 11.5892\n",
      "Epoch [109/300], Step [55/172], Loss: 12.0646\n",
      "Epoch [109/300], Step [56/172], Loss: 10.6802\n",
      "Epoch [109/300], Step [57/172], Loss: 19.4384\n",
      "Epoch [109/300], Step [58/172], Loss: 16.8682\n",
      "Epoch [109/300], Step [59/172], Loss: 30.1050\n",
      "Epoch [109/300], Step [60/172], Loss: 44.1590\n",
      "Epoch [109/300], Step [61/172], Loss: 8.5980\n",
      "Epoch [109/300], Step [62/172], Loss: 22.8330\n",
      "Epoch [109/300], Step [63/172], Loss: 8.7937\n",
      "Epoch [109/300], Step [64/172], Loss: 8.3250\n",
      "Epoch [109/300], Step [65/172], Loss: 20.5224\n",
      "Epoch [109/300], Step [66/172], Loss: 5.7368\n",
      "Epoch [109/300], Step [67/172], Loss: 27.2276\n",
      "Epoch [109/300], Step [68/172], Loss: 5.9770\n",
      "Epoch [109/300], Step [69/172], Loss: 59.7041\n",
      "Epoch [109/300], Step [70/172], Loss: 55.1608\n",
      "Epoch [109/300], Step [71/172], Loss: 51.7532\n",
      "Epoch [109/300], Step [72/172], Loss: 56.6624\n",
      "Epoch [109/300], Step [73/172], Loss: 60.6069\n",
      "Epoch [109/300], Step [74/172], Loss: 34.1596\n",
      "Epoch [109/300], Step [75/172], Loss: 32.7876\n",
      "Epoch [109/300], Step [76/172], Loss: 36.6773\n",
      "Epoch [109/300], Step [77/172], Loss: 61.4300\n",
      "Epoch [109/300], Step [78/172], Loss: 47.5796\n",
      "Epoch [109/300], Step [79/172], Loss: 47.3385\n",
      "Epoch [109/300], Step [80/172], Loss: 57.9379\n",
      "Epoch [109/300], Step [81/172], Loss: 41.3616\n",
      "Epoch [109/300], Step [82/172], Loss: 39.3046\n",
      "Epoch [109/300], Step [83/172], Loss: 49.2212\n",
      "Epoch [109/300], Step [84/172], Loss: 37.0794\n",
      "Epoch [109/300], Step [85/172], Loss: 41.0912\n",
      "Epoch [109/300], Step [86/172], Loss: 33.4350\n",
      "Epoch [109/300], Step [87/172], Loss: 28.2300\n",
      "Epoch [109/300], Step [88/172], Loss: 29.6627\n",
      "Epoch [109/300], Step [89/172], Loss: 26.0567\n",
      "Epoch [109/300], Step [90/172], Loss: 25.2224\n",
      "Epoch [109/300], Step [91/172], Loss: 29.0433\n",
      "Epoch [109/300], Step [92/172], Loss: 21.5617\n",
      "Epoch [109/300], Step [93/172], Loss: 21.3011\n",
      "Epoch [109/300], Step [94/172], Loss: 29.6659\n",
      "Epoch [109/300], Step [95/172], Loss: 22.8069\n",
      "Epoch [109/300], Step [96/172], Loss: 18.9898\n",
      "Epoch [109/300], Step [97/172], Loss: 26.1153\n",
      "Epoch [109/300], Step [98/172], Loss: 19.4251\n",
      "Epoch [109/300], Step [99/172], Loss: 18.1281\n",
      "Epoch [109/300], Step [100/172], Loss: 15.8336\n",
      "Epoch [109/300], Step [101/172], Loss: 17.1857\n",
      "Epoch [109/300], Step [102/172], Loss: 16.0567\n",
      "Epoch [109/300], Step [103/172], Loss: 13.7153\n",
      "Epoch [109/300], Step [104/172], Loss: 15.9707\n",
      "Epoch [109/300], Step [105/172], Loss: 18.0012\n",
      "Epoch [109/300], Step [106/172], Loss: 16.9081\n",
      "Epoch [109/300], Step [107/172], Loss: 15.4784\n",
      "Epoch [109/300], Step [108/172], Loss: 16.4031\n",
      "Epoch [109/300], Step [109/172], Loss: 17.1973\n",
      "Epoch [109/300], Step [110/172], Loss: 15.8683\n",
      "Epoch [109/300], Step [111/172], Loss: 14.1439\n",
      "Epoch [109/300], Step [112/172], Loss: 18.3554\n",
      "Epoch [109/300], Step [113/172], Loss: 14.3763\n",
      "Epoch [109/300], Step [114/172], Loss: 13.9311\n",
      "Epoch [109/300], Step [115/172], Loss: 21.2278\n",
      "Epoch [109/300], Step [116/172], Loss: 14.6046\n",
      "Epoch [109/300], Step [117/172], Loss: 11.6383\n",
      "Epoch [109/300], Step [118/172], Loss: 15.0138\n",
      "Epoch [109/300], Step [119/172], Loss: 15.9390\n",
      "Epoch [109/300], Step [120/172], Loss: 11.0600\n",
      "Epoch [109/300], Step [121/172], Loss: 10.8685\n",
      "Epoch [109/300], Step [122/172], Loss: 10.3962\n",
      "Epoch [109/300], Step [123/172], Loss: 10.4063\n",
      "Epoch [109/300], Step [124/172], Loss: 8.4631\n",
      "Epoch [109/300], Step [125/172], Loss: 13.1371\n",
      "Epoch [109/300], Step [126/172], Loss: 10.7583\n",
      "Epoch [109/300], Step [127/172], Loss: 11.9092\n",
      "Epoch [109/300], Step [128/172], Loss: 12.2698\n",
      "Epoch [109/300], Step [129/172], Loss: 8.9294\n",
      "Epoch [109/300], Step [130/172], Loss: 11.7356\n",
      "Epoch [109/300], Step [131/172], Loss: 8.7776\n",
      "Epoch [109/300], Step [132/172], Loss: 9.0530\n",
      "Epoch [109/300], Step [133/172], Loss: 9.8158\n",
      "Epoch [109/300], Step [134/172], Loss: 12.2992\n",
      "Epoch [109/300], Step [135/172], Loss: 9.2363\n",
      "Epoch [109/300], Step [136/172], Loss: 8.6104\n",
      "Epoch [109/300], Step [137/172], Loss: 10.0202\n",
      "Epoch [109/300], Step [138/172], Loss: 8.2864\n",
      "Epoch [109/300], Step [139/172], Loss: 9.9650\n",
      "Epoch [109/300], Step [140/172], Loss: 9.6562\n",
      "Epoch [109/300], Step [141/172], Loss: 11.4555\n",
      "Epoch [109/300], Step [142/172], Loss: 13.5328\n",
      "Epoch [109/300], Step [143/172], Loss: 9.6781\n",
      "Epoch [109/300], Step [144/172], Loss: 8.9793\n",
      "Epoch [109/300], Step [145/172], Loss: 9.8641\n",
      "Epoch [109/300], Step [146/172], Loss: 9.8285\n",
      "Epoch [109/300], Step [147/172], Loss: 5.5724\n",
      "Epoch [109/300], Step [148/172], Loss: 6.6810\n",
      "Epoch [109/300], Step [149/172], Loss: 7.9210\n",
      "Epoch [109/300], Step [150/172], Loss: 7.9003\n",
      "Epoch [109/300], Step [151/172], Loss: 7.2304\n",
      "Epoch [109/300], Step [152/172], Loss: 7.9197\n",
      "Epoch [109/300], Step [153/172], Loss: 7.1508\n",
      "Epoch [109/300], Step [154/172], Loss: 7.9364\n",
      "Epoch [109/300], Step [155/172], Loss: 6.9542\n",
      "Epoch [109/300], Step [156/172], Loss: 12.3896\n",
      "Epoch [109/300], Step [157/172], Loss: 10.1515\n",
      "Epoch [109/300], Step [158/172], Loss: 7.6469\n",
      "Epoch [109/300], Step [159/172], Loss: 9.5357\n",
      "Epoch [109/300], Step [160/172], Loss: 10.0666\n",
      "Epoch [109/300], Step [161/172], Loss: 7.2360\n",
      "Epoch [109/300], Step [162/172], Loss: 7.3164\n",
      "Epoch [109/300], Step [163/172], Loss: 6.5996\n",
      "Epoch [109/300], Step [164/172], Loss: 9.6509\n",
      "Epoch [109/300], Step [165/172], Loss: 6.4003\n",
      "Epoch [109/300], Step [166/172], Loss: 6.3188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [109/300], Step [167/172], Loss: 9.3535\n",
      "Epoch [109/300], Step [168/172], Loss: 7.2644\n",
      "Epoch [109/300], Step [169/172], Loss: 7.1971\n",
      "Epoch [109/300], Step [170/172], Loss: 5.7241\n",
      "Epoch [109/300], Step [171/172], Loss: 6.7192\n",
      "Epoch [109/300], Step [172/172], Loss: 5.3747\n",
      "Epoch [110/300], Step [1/172], Loss: 76.2487\n",
      "Epoch [110/300], Step [2/172], Loss: 76.0838\n",
      "Epoch [110/300], Step [3/172], Loss: 69.8595\n",
      "Epoch [110/300], Step [4/172], Loss: 42.9785\n",
      "Epoch [110/300], Step [5/172], Loss: 63.9425\n",
      "Epoch [110/300], Step [6/172], Loss: 19.5295\n",
      "Epoch [110/300], Step [7/172], Loss: 29.6791\n",
      "Epoch [110/300], Step [8/172], Loss: 6.6081\n",
      "Epoch [110/300], Step [9/172], Loss: 41.2311\n",
      "Epoch [110/300], Step [10/172], Loss: 46.1988\n",
      "Epoch [110/300], Step [11/172], Loss: 79.5981\n",
      "Epoch [110/300], Step [12/172], Loss: 82.8246\n",
      "Epoch [110/300], Step [13/172], Loss: 41.8727\n",
      "Epoch [110/300], Step [14/172], Loss: 84.4885\n",
      "Epoch [110/300], Step [15/172], Loss: 71.7342\n",
      "Epoch [110/300], Step [16/172], Loss: 17.2805\n",
      "Epoch [110/300], Step [17/172], Loss: 56.8526\n",
      "Epoch [110/300], Step [18/172], Loss: 62.5583\n",
      "Epoch [110/300], Step [19/172], Loss: 84.5180\n",
      "Epoch [110/300], Step [20/172], Loss: 64.5090\n",
      "Epoch [110/300], Step [21/172], Loss: 95.2511\n",
      "Epoch [110/300], Step [22/172], Loss: 75.0698\n",
      "Epoch [110/300], Step [23/172], Loss: 3.2709\n",
      "Epoch [110/300], Step [24/172], Loss: 68.7700\n",
      "Epoch [110/300], Step [25/172], Loss: 45.6747\n",
      "Epoch [110/300], Step [26/172], Loss: 57.5816\n",
      "Epoch [110/300], Step [27/172], Loss: 73.3294\n",
      "Epoch [110/300], Step [28/172], Loss: 33.7034\n",
      "Epoch [110/300], Step [29/172], Loss: 23.6743\n",
      "Epoch [110/300], Step [30/172], Loss: 82.2039\n",
      "Epoch [110/300], Step [31/172], Loss: 44.2818\n",
      "Epoch [110/300], Step [32/172], Loss: 41.5723\n",
      "Epoch [110/300], Step [33/172], Loss: 71.5725\n",
      "Epoch [110/300], Step [34/172], Loss: 4.5663\n",
      "Epoch [110/300], Step [35/172], Loss: 15.1187\n",
      "Epoch [110/300], Step [36/172], Loss: 20.6850\n",
      "Epoch [110/300], Step [37/172], Loss: 17.6213\n",
      "Epoch [110/300], Step [38/172], Loss: 27.2386\n",
      "Epoch [110/300], Step [39/172], Loss: 42.8731\n",
      "Epoch [110/300], Step [40/172], Loss: 20.1806\n",
      "Epoch [110/300], Step [41/172], Loss: 37.5100\n",
      "Epoch [110/300], Step [42/172], Loss: 41.0911\n",
      "Epoch [110/300], Step [43/172], Loss: 26.2901\n",
      "Epoch [110/300], Step [44/172], Loss: 20.5426\n",
      "Epoch [110/300], Step [45/172], Loss: 22.1456\n",
      "Epoch [110/300], Step [46/172], Loss: 21.9916\n",
      "Epoch [110/300], Step [47/172], Loss: 48.5696\n",
      "Epoch [110/300], Step [48/172], Loss: 52.5466\n",
      "Epoch [110/300], Step [49/172], Loss: 19.5567\n",
      "Epoch [110/300], Step [50/172], Loss: 50.2027\n",
      "Epoch [110/300], Step [51/172], Loss: 7.2900\n",
      "Epoch [110/300], Step [52/172], Loss: 17.5810\n",
      "Epoch [110/300], Step [53/172], Loss: 22.8015\n",
      "Epoch [110/300], Step [54/172], Loss: 11.4605\n",
      "Epoch [110/300], Step [55/172], Loss: 11.9299\n",
      "Epoch [110/300], Step [56/172], Loss: 10.7508\n",
      "Epoch [110/300], Step [57/172], Loss: 19.2047\n",
      "Epoch [110/300], Step [58/172], Loss: 16.7031\n",
      "Epoch [110/300], Step [59/172], Loss: 30.0626\n",
      "Epoch [110/300], Step [60/172], Loss: 43.6432\n",
      "Epoch [110/300], Step [61/172], Loss: 8.4934\n",
      "Epoch [110/300], Step [62/172], Loss: 22.8163\n",
      "Epoch [110/300], Step [63/172], Loss: 8.8571\n",
      "Epoch [110/300], Step [64/172], Loss: 8.3702\n",
      "Epoch [110/300], Step [65/172], Loss: 20.4798\n",
      "Epoch [110/300], Step [66/172], Loss: 5.7337\n",
      "Epoch [110/300], Step [67/172], Loss: 27.0650\n",
      "Epoch [110/300], Step [68/172], Loss: 5.7508\n",
      "Epoch [110/300], Step [69/172], Loss: 59.2552\n",
      "Epoch [110/300], Step [70/172], Loss: 54.8368\n",
      "Epoch [110/300], Step [71/172], Loss: 51.4168\n",
      "Epoch [110/300], Step [72/172], Loss: 56.3779\n",
      "Epoch [110/300], Step [73/172], Loss: 60.5607\n",
      "Epoch [110/300], Step [74/172], Loss: 33.8697\n",
      "Epoch [110/300], Step [75/172], Loss: 32.5032\n",
      "Epoch [110/300], Step [76/172], Loss: 36.5710\n",
      "Epoch [110/300], Step [77/172], Loss: 61.3473\n",
      "Epoch [110/300], Step [78/172], Loss: 47.4705\n",
      "Epoch [110/300], Step [79/172], Loss: 47.0523\n",
      "Epoch [110/300], Step [80/172], Loss: 57.9086\n",
      "Epoch [110/300], Step [81/172], Loss: 41.1450\n",
      "Epoch [110/300], Step [82/172], Loss: 39.3933\n",
      "Epoch [110/300], Step [83/172], Loss: 48.9728\n",
      "Epoch [110/300], Step [84/172], Loss: 36.8527\n",
      "Epoch [110/300], Step [85/172], Loss: 40.8565\n",
      "Epoch [110/300], Step [86/172], Loss: 33.3040\n",
      "Epoch [110/300], Step [87/172], Loss: 28.0506\n",
      "Epoch [110/300], Step [88/172], Loss: 29.4425\n",
      "Epoch [110/300], Step [89/172], Loss: 25.8833\n",
      "Epoch [110/300], Step [90/172], Loss: 25.0753\n",
      "Epoch [110/300], Step [91/172], Loss: 28.8916\n",
      "Epoch [110/300], Step [92/172], Loss: 21.4060\n",
      "Epoch [110/300], Step [93/172], Loss: 21.1351\n",
      "Epoch [110/300], Step [94/172], Loss: 29.5337\n",
      "Epoch [110/300], Step [95/172], Loss: 22.6968\n",
      "Epoch [110/300], Step [96/172], Loss: 18.9264\n",
      "Epoch [110/300], Step [97/172], Loss: 26.0125\n",
      "Epoch [110/300], Step [98/172], Loss: 19.2759\n",
      "Epoch [110/300], Step [99/172], Loss: 18.0359\n",
      "Epoch [110/300], Step [100/172], Loss: 15.6961\n",
      "Epoch [110/300], Step [101/172], Loss: 17.0363\n",
      "Epoch [110/300], Step [102/172], Loss: 16.1012\n",
      "Epoch [110/300], Step [103/172], Loss: 13.6125\n",
      "Epoch [110/300], Step [104/172], Loss: 15.8850\n",
      "Epoch [110/300], Step [105/172], Loss: 17.9969\n",
      "Epoch [110/300], Step [106/172], Loss: 16.8053\n",
      "Epoch [110/300], Step [107/172], Loss: 15.4181\n",
      "Epoch [110/300], Step [108/172], Loss: 16.3392\n",
      "Epoch [110/300], Step [109/172], Loss: 17.1068\n",
      "Epoch [110/300], Step [110/172], Loss: 15.8110\n",
      "Epoch [110/300], Step [111/172], Loss: 14.0552\n",
      "Epoch [110/300], Step [112/172], Loss: 18.2756\n",
      "Epoch [110/300], Step [113/172], Loss: 14.2503\n",
      "Epoch [110/300], Step [114/172], Loss: 13.8365\n",
      "Epoch [110/300], Step [115/172], Loss: 21.1168\n",
      "Epoch [110/300], Step [116/172], Loss: 14.5236\n",
      "Epoch [110/300], Step [117/172], Loss: 11.5775\n",
      "Epoch [110/300], Step [118/172], Loss: 14.9734\n",
      "Epoch [110/300], Step [119/172], Loss: 15.8823\n",
      "Epoch [110/300], Step [120/172], Loss: 11.0046\n",
      "Epoch [110/300], Step [121/172], Loss: 10.8044\n",
      "Epoch [110/300], Step [122/172], Loss: 10.4626\n",
      "Epoch [110/300], Step [123/172], Loss: 10.4030\n",
      "Epoch [110/300], Step [124/172], Loss: 8.4171\n",
      "Epoch [110/300], Step [125/172], Loss: 13.0519\n",
      "Epoch [110/300], Step [126/172], Loss: 10.6954\n",
      "Epoch [110/300], Step [127/172], Loss: 11.8619\n",
      "Epoch [110/300], Step [128/172], Loss: 12.1792\n",
      "Epoch [110/300], Step [129/172], Loss: 8.8771\n",
      "Epoch [110/300], Step [130/172], Loss: 11.7031\n",
      "Epoch [110/300], Step [131/172], Loss: 8.7564\n",
      "Epoch [110/300], Step [132/172], Loss: 9.0083\n",
      "Epoch [110/300], Step [133/172], Loss: 9.8054\n",
      "Epoch [110/300], Step [134/172], Loss: 12.3273\n",
      "Epoch [110/300], Step [135/172], Loss: 9.2055\n",
      "Epoch [110/300], Step [136/172], Loss: 8.5841\n",
      "Epoch [110/300], Step [137/172], Loss: 9.9668\n",
      "Epoch [110/300], Step [138/172], Loss: 8.2122\n",
      "Epoch [110/300], Step [139/172], Loss: 9.9207\n",
      "Epoch [110/300], Step [140/172], Loss: 9.6032\n",
      "Epoch [110/300], Step [141/172], Loss: 11.3789\n",
      "Epoch [110/300], Step [142/172], Loss: 13.5562\n",
      "Epoch [110/300], Step [143/172], Loss: 9.6608\n",
      "Epoch [110/300], Step [144/172], Loss: 8.9461\n",
      "Epoch [110/300], Step [145/172], Loss: 9.8584\n",
      "Epoch [110/300], Step [146/172], Loss: 9.7695\n",
      "Epoch [110/300], Step [147/172], Loss: 5.5362\n",
      "Epoch [110/300], Step [148/172], Loss: 6.6398\n",
      "Epoch [110/300], Step [149/172], Loss: 7.8474\n",
      "Epoch [110/300], Step [150/172], Loss: 7.8389\n",
      "Epoch [110/300], Step [151/172], Loss: 7.1880\n",
      "Epoch [110/300], Step [152/172], Loss: 7.8841\n",
      "Epoch [110/300], Step [153/172], Loss: 7.1114\n",
      "Epoch [110/300], Step [154/172], Loss: 7.8920\n",
      "Epoch [110/300], Step [155/172], Loss: 6.9015\n",
      "Epoch [110/300], Step [156/172], Loss: 12.4072\n",
      "Epoch [110/300], Step [157/172], Loss: 10.1349\n",
      "Epoch [110/300], Step [158/172], Loss: 7.6373\n",
      "Epoch [110/300], Step [159/172], Loss: 9.5318\n",
      "Epoch [110/300], Step [160/172], Loss: 10.0796\n",
      "Epoch [110/300], Step [161/172], Loss: 7.1895\n",
      "Epoch [110/300], Step [162/172], Loss: 7.2562\n",
      "Epoch [110/300], Step [163/172], Loss: 6.5723\n",
      "Epoch [110/300], Step [164/172], Loss: 9.7098\n",
      "Epoch [110/300], Step [165/172], Loss: 6.3778\n",
      "Epoch [110/300], Step [166/172], Loss: 6.2738\n",
      "Epoch [110/300], Step [167/172], Loss: 9.3776\n",
      "Epoch [110/300], Step [168/172], Loss: 7.2564\n",
      "Epoch [110/300], Step [169/172], Loss: 7.1611\n",
      "Epoch [110/300], Step [170/172], Loss: 5.7024\n",
      "Epoch [110/300], Step [171/172], Loss: 6.7112\n",
      "Epoch [110/300], Step [172/172], Loss: 5.3505\n",
      "Epoch [111/300], Step [1/172], Loss: 75.7463\n",
      "Epoch [111/300], Step [2/172], Loss: 75.4722\n",
      "Epoch [111/300], Step [3/172], Loss: 69.3730\n",
      "Epoch [111/300], Step [4/172], Loss: 42.6028\n",
      "Epoch [111/300], Step [5/172], Loss: 63.7104\n",
      "Epoch [111/300], Step [6/172], Loss: 19.4451\n",
      "Epoch [111/300], Step [7/172], Loss: 29.2141\n",
      "Epoch [111/300], Step [8/172], Loss: 6.0638\n",
      "Epoch [111/300], Step [9/172], Loss: 40.9444\n",
      "Epoch [111/300], Step [10/172], Loss: 46.2915\n",
      "Epoch [111/300], Step [11/172], Loss: 79.1021\n",
      "Epoch [111/300], Step [12/172], Loss: 82.8291\n",
      "Epoch [111/300], Step [13/172], Loss: 41.7914\n",
      "Epoch [111/300], Step [14/172], Loss: 84.0520\n",
      "Epoch [111/300], Step [15/172], Loss: 71.4250\n",
      "Epoch [111/300], Step [16/172], Loss: 16.8005\n",
      "Epoch [111/300], Step [17/172], Loss: 56.6489\n",
      "Epoch [111/300], Step [18/172], Loss: 62.5931\n",
      "Epoch [111/300], Step [19/172], Loss: 84.6442\n",
      "Epoch [111/300], Step [20/172], Loss: 63.4939\n",
      "Epoch [111/300], Step [21/172], Loss: 95.0995\n",
      "Epoch [111/300], Step [22/172], Loss: 74.4794\n",
      "Epoch [111/300], Step [23/172], Loss: 3.1735\n",
      "Epoch [111/300], Step [24/172], Loss: 68.5490\n",
      "Epoch [111/300], Step [25/172], Loss: 45.8105\n",
      "Epoch [111/300], Step [26/172], Loss: 57.5582\n",
      "Epoch [111/300], Step [27/172], Loss: 72.8279\n",
      "Epoch [111/300], Step [28/172], Loss: 33.3405\n",
      "Epoch [111/300], Step [29/172], Loss: 23.5180\n",
      "Epoch [111/300], Step [30/172], Loss: 82.2458\n",
      "Epoch [111/300], Step [31/172], Loss: 44.4247\n",
      "Epoch [111/300], Step [32/172], Loss: 41.6002\n",
      "Epoch [111/300], Step [33/172], Loss: 71.5203\n",
      "Epoch [111/300], Step [34/172], Loss: 4.7183\n",
      "Epoch [111/300], Step [35/172], Loss: 14.9958\n",
      "Epoch [111/300], Step [36/172], Loss: 20.9465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [111/300], Step [37/172], Loss: 17.6617\n",
      "Epoch [111/300], Step [38/172], Loss: 27.2905\n",
      "Epoch [111/300], Step [39/172], Loss: 42.4984\n",
      "Epoch [111/300], Step [40/172], Loss: 20.2482\n",
      "Epoch [111/300], Step [41/172], Loss: 37.4064\n",
      "Epoch [111/300], Step [42/172], Loss: 41.1281\n",
      "Epoch [111/300], Step [43/172], Loss: 26.2962\n",
      "Epoch [111/300], Step [44/172], Loss: 20.5155\n",
      "Epoch [111/300], Step [45/172], Loss: 22.2516\n",
      "Epoch [111/300], Step [46/172], Loss: 21.8226\n",
      "Epoch [111/300], Step [47/172], Loss: 48.4456\n",
      "Epoch [111/300], Step [48/172], Loss: 52.7279\n",
      "Epoch [111/300], Step [49/172], Loss: 19.5390\n",
      "Epoch [111/300], Step [50/172], Loss: 50.0211\n",
      "Epoch [111/300], Step [51/172], Loss: 7.2778\n",
      "Epoch [111/300], Step [52/172], Loss: 17.5529\n",
      "Epoch [111/300], Step [53/172], Loss: 22.7480\n",
      "Epoch [111/300], Step [54/172], Loss: 11.4679\n",
      "Epoch [111/300], Step [55/172], Loss: 11.9999\n",
      "Epoch [111/300], Step [56/172], Loss: 10.8112\n",
      "Epoch [111/300], Step [57/172], Loss: 18.9597\n",
      "Epoch [111/300], Step [58/172], Loss: 16.6730\n",
      "Epoch [111/300], Step [59/172], Loss: 30.0541\n",
      "Epoch [111/300], Step [60/172], Loss: 43.3662\n",
      "Epoch [111/300], Step [61/172], Loss: 8.3759\n",
      "Epoch [111/300], Step [62/172], Loss: 22.8348\n",
      "Epoch [111/300], Step [63/172], Loss: 8.7813\n",
      "Epoch [111/300], Step [64/172], Loss: 8.3301\n",
      "Epoch [111/300], Step [65/172], Loss: 20.4238\n",
      "Epoch [111/300], Step [66/172], Loss: 5.6944\n",
      "Epoch [111/300], Step [67/172], Loss: 27.0353\n",
      "Epoch [111/300], Step [68/172], Loss: 5.7629\n",
      "Epoch [111/300], Step [69/172], Loss: 59.0382\n",
      "Epoch [111/300], Step [70/172], Loss: 54.6649\n",
      "Epoch [111/300], Step [71/172], Loss: 51.4616\n",
      "Epoch [111/300], Step [72/172], Loss: 56.2878\n",
      "Epoch [111/300], Step [73/172], Loss: 60.3389\n",
      "Epoch [111/300], Step [74/172], Loss: 33.8763\n",
      "Epoch [111/300], Step [75/172], Loss: 32.6591\n",
      "Epoch [111/300], Step [76/172], Loss: 36.4746\n",
      "Epoch [111/300], Step [77/172], Loss: 61.3732\n",
      "Epoch [111/300], Step [78/172], Loss: 47.4017\n",
      "Epoch [111/300], Step [79/172], Loss: 47.0892\n",
      "Epoch [111/300], Step [80/172], Loss: 57.8110\n",
      "Epoch [111/300], Step [81/172], Loss: 41.2322\n",
      "Epoch [111/300], Step [82/172], Loss: 39.1841\n",
      "Epoch [111/300], Step [83/172], Loss: 48.9514\n",
      "Epoch [111/300], Step [84/172], Loss: 36.8359\n",
      "Epoch [111/300], Step [85/172], Loss: 40.9445\n",
      "Epoch [111/300], Step [86/172], Loss: 33.3416\n",
      "Epoch [111/300], Step [87/172], Loss: 28.0877\n",
      "Epoch [111/300], Step [88/172], Loss: 29.5119\n",
      "Epoch [111/300], Step [89/172], Loss: 25.8914\n",
      "Epoch [111/300], Step [90/172], Loss: 25.0086\n",
      "Epoch [111/300], Step [91/172], Loss: 28.8116\n",
      "Epoch [111/300], Step [92/172], Loss: 21.4012\n",
      "Epoch [111/300], Step [93/172], Loss: 21.1458\n",
      "Epoch [111/300], Step [94/172], Loss: 29.5662\n",
      "Epoch [111/300], Step [95/172], Loss: 22.6626\n",
      "Epoch [111/300], Step [96/172], Loss: 18.9494\n",
      "Epoch [111/300], Step [97/172], Loss: 25.9976\n",
      "Epoch [111/300], Step [98/172], Loss: 19.2378\n",
      "Epoch [111/300], Step [99/172], Loss: 18.0657\n",
      "Epoch [111/300], Step [100/172], Loss: 15.6781\n",
      "Epoch [111/300], Step [101/172], Loss: 17.0217\n",
      "Epoch [111/300], Step [102/172], Loss: 15.9528\n",
      "Epoch [111/300], Step [103/172], Loss: 13.5512\n",
      "Epoch [111/300], Step [104/172], Loss: 15.9208\n",
      "Epoch [111/300], Step [105/172], Loss: 17.8462\n",
      "Epoch [111/300], Step [106/172], Loss: 16.7610\n",
      "Epoch [111/300], Step [107/172], Loss: 15.3430\n",
      "Epoch [111/300], Step [108/172], Loss: 16.2936\n",
      "Epoch [111/300], Step [109/172], Loss: 16.9635\n",
      "Epoch [111/300], Step [110/172], Loss: 15.7635\n",
      "Epoch [111/300], Step [111/172], Loss: 14.0423\n",
      "Epoch [111/300], Step [112/172], Loss: 18.1830\n",
      "Epoch [111/300], Step [113/172], Loss: 14.1513\n",
      "Epoch [111/300], Step [114/172], Loss: 13.7977\n",
      "Epoch [111/300], Step [115/172], Loss: 20.9989\n",
      "Epoch [111/300], Step [116/172], Loss: 14.5048\n",
      "Epoch [111/300], Step [117/172], Loss: 11.5016\n",
      "Epoch [111/300], Step [118/172], Loss: 14.8753\n",
      "Epoch [111/300], Step [119/172], Loss: 15.8652\n",
      "Epoch [111/300], Step [120/172], Loss: 10.8806\n",
      "Epoch [111/300], Step [121/172], Loss: 10.7738\n",
      "Epoch [111/300], Step [122/172], Loss: 10.3475\n",
      "Epoch [111/300], Step [123/172], Loss: 10.3337\n",
      "Epoch [111/300], Step [124/172], Loss: 8.3980\n",
      "Epoch [111/300], Step [125/172], Loss: 13.0552\n",
      "Epoch [111/300], Step [126/172], Loss: 10.6712\n",
      "Epoch [111/300], Step [127/172], Loss: 11.7917\n",
      "Epoch [111/300], Step [128/172], Loss: 12.1420\n",
      "Epoch [111/300], Step [129/172], Loss: 8.8488\n",
      "Epoch [111/300], Step [130/172], Loss: 11.6681\n",
      "Epoch [111/300], Step [131/172], Loss: 8.6852\n",
      "Epoch [111/300], Step [132/172], Loss: 8.9690\n",
      "Epoch [111/300], Step [133/172], Loss: 9.7261\n",
      "Epoch [111/300], Step [134/172], Loss: 12.2807\n",
      "Epoch [111/300], Step [135/172], Loss: 9.1969\n",
      "Epoch [111/300], Step [136/172], Loss: 8.5532\n",
      "Epoch [111/300], Step [137/172], Loss: 9.9249\n",
      "Epoch [111/300], Step [138/172], Loss: 8.1771\n",
      "Epoch [111/300], Step [139/172], Loss: 9.9420\n",
      "Epoch [111/300], Step [140/172], Loss: 9.5972\n",
      "Epoch [111/300], Step [141/172], Loss: 11.3275\n",
      "Epoch [111/300], Step [142/172], Loss: 13.5023\n",
      "Epoch [111/300], Step [143/172], Loss: 9.6671\n",
      "Epoch [111/300], Step [144/172], Loss: 8.9468\n",
      "Epoch [111/300], Step [145/172], Loss: 9.8285\n",
      "Epoch [111/300], Step [146/172], Loss: 9.7583\n",
      "Epoch [111/300], Step [147/172], Loss: 5.5253\n",
      "Epoch [111/300], Step [148/172], Loss: 6.6282\n",
      "Epoch [111/300], Step [149/172], Loss: 7.8287\n",
      "Epoch [111/300], Step [150/172], Loss: 7.8209\n",
      "Epoch [111/300], Step [151/172], Loss: 7.1484\n",
      "Epoch [111/300], Step [152/172], Loss: 7.8936\n",
      "Epoch [111/300], Step [153/172], Loss: 7.1158\n",
      "Epoch [111/300], Step [154/172], Loss: 7.8704\n",
      "Epoch [111/300], Step [155/172], Loss: 6.9122\n",
      "Epoch [111/300], Step [156/172], Loss: 12.3355\n",
      "Epoch [111/300], Step [157/172], Loss: 10.0609\n",
      "Epoch [111/300], Step [158/172], Loss: 7.5930\n",
      "Epoch [111/300], Step [159/172], Loss: 9.5615\n",
      "Epoch [111/300], Step [160/172], Loss: 10.0008\n",
      "Epoch [111/300], Step [161/172], Loss: 7.1962\n",
      "Epoch [111/300], Step [162/172], Loss: 7.2400\n",
      "Epoch [111/300], Step [163/172], Loss: 6.5496\n",
      "Epoch [111/300], Step [164/172], Loss: 9.5430\n",
      "Epoch [111/300], Step [165/172], Loss: 6.3657\n",
      "Epoch [111/300], Step [166/172], Loss: 6.2189\n",
      "Epoch [111/300], Step [167/172], Loss: 9.3802\n",
      "Epoch [111/300], Step [168/172], Loss: 7.2764\n",
      "Epoch [111/300], Step [169/172], Loss: 7.1303\n",
      "Epoch [111/300], Step [170/172], Loss: 5.6973\n",
      "Epoch [111/300], Step [171/172], Loss: 6.7426\n",
      "Epoch [111/300], Step [172/172], Loss: 5.3736\n",
      "Epoch [112/300], Step [1/172], Loss: 74.9641\n",
      "Epoch [112/300], Step [2/172], Loss: 75.0681\n",
      "Epoch [112/300], Step [3/172], Loss: 68.6745\n",
      "Epoch [112/300], Step [4/172], Loss: 42.2992\n",
      "Epoch [112/300], Step [5/172], Loss: 63.3352\n",
      "Epoch [112/300], Step [6/172], Loss: 19.6399\n",
      "Epoch [112/300], Step [7/172], Loss: 29.8670\n",
      "Epoch [112/300], Step [8/172], Loss: 6.8326\n",
      "Epoch [112/300], Step [9/172], Loss: 40.9710\n",
      "Epoch [112/300], Step [10/172], Loss: 46.1524\n",
      "Epoch [112/300], Step [11/172], Loss: 78.7636\n",
      "Epoch [112/300], Step [12/172], Loss: 82.6980\n",
      "Epoch [112/300], Step [13/172], Loss: 41.8671\n",
      "Epoch [112/300], Step [14/172], Loss: 83.7102\n",
      "Epoch [112/300], Step [15/172], Loss: 71.2852\n",
      "Epoch [112/300], Step [16/172], Loss: 16.7288\n",
      "Epoch [112/300], Step [17/172], Loss: 56.7254\n",
      "Epoch [112/300], Step [18/172], Loss: 62.7403\n",
      "Epoch [112/300], Step [19/172], Loss: 84.8303\n",
      "Epoch [112/300], Step [20/172], Loss: 63.6557\n",
      "Epoch [112/300], Step [21/172], Loss: 95.2621\n",
      "Epoch [112/300], Step [22/172], Loss: 74.8604\n",
      "Epoch [112/300], Step [23/172], Loss: 2.9666\n",
      "Epoch [112/300], Step [24/172], Loss: 68.5773\n",
      "Epoch [112/300], Step [25/172], Loss: 45.6620\n",
      "Epoch [112/300], Step [26/172], Loss: 57.5066\n",
      "Epoch [112/300], Step [27/172], Loss: 73.2925\n",
      "Epoch [112/300], Step [28/172], Loss: 33.2753\n",
      "Epoch [112/300], Step [29/172], Loss: 23.2803\n",
      "Epoch [112/300], Step [30/172], Loss: 82.1021\n",
      "Epoch [112/300], Step [31/172], Loss: 44.2889\n",
      "Epoch [112/300], Step [32/172], Loss: 41.7309\n",
      "Epoch [112/300], Step [33/172], Loss: 71.7436\n",
      "Epoch [112/300], Step [34/172], Loss: 4.4929\n",
      "Epoch [112/300], Step [35/172], Loss: 15.0765\n",
      "Epoch [112/300], Step [36/172], Loss: 20.6476\n",
      "Epoch [112/300], Step [37/172], Loss: 17.6175\n",
      "Epoch [112/300], Step [38/172], Loss: 27.3523\n",
      "Epoch [112/300], Step [39/172], Loss: 42.3590\n",
      "Epoch [112/300], Step [40/172], Loss: 20.1686\n",
      "Epoch [112/300], Step [41/172], Loss: 37.4975\n",
      "Epoch [112/300], Step [42/172], Loss: 41.0063\n",
      "Epoch [112/300], Step [43/172], Loss: 26.3284\n",
      "Epoch [112/300], Step [44/172], Loss: 20.4528\n",
      "Epoch [112/300], Step [45/172], Loss: 22.2000\n",
      "Epoch [112/300], Step [46/172], Loss: 21.6870\n",
      "Epoch [112/300], Step [47/172], Loss: 48.2924\n",
      "Epoch [112/300], Step [48/172], Loss: 52.8238\n",
      "Epoch [112/300], Step [49/172], Loss: 19.4742\n",
      "Epoch [112/300], Step [50/172], Loss: 49.9709\n",
      "Epoch [112/300], Step [51/172], Loss: 7.3398\n",
      "Epoch [112/300], Step [52/172], Loss: 17.4547\n",
      "Epoch [112/300], Step [53/172], Loss: 22.6895\n",
      "Epoch [112/300], Step [54/172], Loss: 11.4203\n",
      "Epoch [112/300], Step [55/172], Loss: 11.9168\n",
      "Epoch [112/300], Step [56/172], Loss: 10.8979\n",
      "Epoch [112/300], Step [57/172], Loss: 18.9066\n",
      "Epoch [112/300], Step [58/172], Loss: 16.5194\n",
      "Epoch [112/300], Step [59/172], Loss: 29.8976\n",
      "Epoch [112/300], Step [60/172], Loss: 42.9924\n",
      "Epoch [112/300], Step [61/172], Loss: 8.2518\n",
      "Epoch [112/300], Step [62/172], Loss: 22.8459\n",
      "Epoch [112/300], Step [63/172], Loss: 8.8602\n",
      "Epoch [112/300], Step [64/172], Loss: 8.3682\n",
      "Epoch [112/300], Step [65/172], Loss: 20.3999\n",
      "Epoch [112/300], Step [66/172], Loss: 5.6721\n",
      "Epoch [112/300], Step [67/172], Loss: 26.8574\n",
      "Epoch [112/300], Step [68/172], Loss: 5.5603\n",
      "Epoch [112/300], Step [69/172], Loss: 58.7841\n",
      "Epoch [112/300], Step [70/172], Loss: 54.4819\n",
      "Epoch [112/300], Step [71/172], Loss: 51.2240\n",
      "Epoch [112/300], Step [72/172], Loss: 56.1434\n",
      "Epoch [112/300], Step [73/172], Loss: 60.2367\n",
      "Epoch [112/300], Step [74/172], Loss: 33.6816\n",
      "Epoch [112/300], Step [75/172], Loss: 32.5165\n",
      "Epoch [112/300], Step [76/172], Loss: 36.2742\n",
      "Epoch [112/300], Step [77/172], Loss: 61.2963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [112/300], Step [78/172], Loss: 47.2673\n",
      "Epoch [112/300], Step [79/172], Loss: 46.8327\n",
      "Epoch [112/300], Step [80/172], Loss: 57.9201\n",
      "Epoch [112/300], Step [81/172], Loss: 41.0377\n",
      "Epoch [112/300], Step [82/172], Loss: 39.4400\n",
      "Epoch [112/300], Step [83/172], Loss: 48.7742\n",
      "Epoch [112/300], Step [84/172], Loss: 36.6646\n",
      "Epoch [112/300], Step [85/172], Loss: 40.7299\n",
      "Epoch [112/300], Step [86/172], Loss: 33.2648\n",
      "Epoch [112/300], Step [87/172], Loss: 27.9501\n",
      "Epoch [112/300], Step [88/172], Loss: 29.3553\n",
      "Epoch [112/300], Step [89/172], Loss: 25.7500\n",
      "Epoch [112/300], Step [90/172], Loss: 24.9916\n",
      "Epoch [112/300], Step [91/172], Loss: 28.6924\n",
      "Epoch [112/300], Step [92/172], Loss: 21.3365\n",
      "Epoch [112/300], Step [93/172], Loss: 21.0611\n",
      "Epoch [112/300], Step [94/172], Loss: 29.5077\n",
      "Epoch [112/300], Step [95/172], Loss: 22.6033\n",
      "Epoch [112/300], Step [96/172], Loss: 18.8977\n",
      "Epoch [112/300], Step [97/172], Loss: 25.9529\n",
      "Epoch [112/300], Step [98/172], Loss: 19.1522\n",
      "Epoch [112/300], Step [99/172], Loss: 18.0298\n",
      "Epoch [112/300], Step [100/172], Loss: 15.5834\n",
      "Epoch [112/300], Step [101/172], Loss: 16.9470\n",
      "Epoch [112/300], Step [102/172], Loss: 16.0915\n",
      "Epoch [112/300], Step [103/172], Loss: 13.4909\n",
      "Epoch [112/300], Step [104/172], Loss: 15.8753\n",
      "Epoch [112/300], Step [105/172], Loss: 17.9486\n",
      "Epoch [112/300], Step [106/172], Loss: 16.7066\n",
      "Epoch [112/300], Step [107/172], Loss: 15.2908\n",
      "Epoch [112/300], Step [108/172], Loss: 16.2542\n",
      "Epoch [112/300], Step [109/172], Loss: 16.9575\n",
      "Epoch [112/300], Step [110/172], Loss: 15.7303\n",
      "Epoch [112/300], Step [111/172], Loss: 13.9715\n",
      "Epoch [112/300], Step [112/172], Loss: 18.1703\n",
      "Epoch [112/300], Step [113/172], Loss: 14.1349\n",
      "Epoch [112/300], Step [114/172], Loss: 13.7570\n",
      "Epoch [112/300], Step [115/172], Loss: 20.9932\n",
      "Epoch [112/300], Step [116/172], Loss: 14.4820\n",
      "Epoch [112/300], Step [117/172], Loss: 11.4570\n",
      "Epoch [112/300], Step [118/172], Loss: 14.8741\n",
      "Epoch [112/300], Step [119/172], Loss: 15.8232\n",
      "Epoch [112/300], Step [120/172], Loss: 10.8409\n",
      "Epoch [112/300], Step [121/172], Loss: 10.6870\n",
      "Epoch [112/300], Step [122/172], Loss: 10.3444\n",
      "Epoch [112/300], Step [123/172], Loss: 10.3115\n",
      "Epoch [112/300], Step [124/172], Loss: 8.3655\n",
      "Epoch [112/300], Step [125/172], Loss: 12.9621\n",
      "Epoch [112/300], Step [126/172], Loss: 10.6369\n",
      "Epoch [112/300], Step [127/172], Loss: 11.7427\n",
      "Epoch [112/300], Step [128/172], Loss: 12.0844\n",
      "Epoch [112/300], Step [129/172], Loss: 8.8036\n",
      "Epoch [112/300], Step [130/172], Loss: 11.6751\n",
      "Epoch [112/300], Step [131/172], Loss: 8.6432\n",
      "Epoch [112/300], Step [132/172], Loss: 8.9323\n",
      "Epoch [112/300], Step [133/172], Loss: 9.6817\n",
      "Epoch [112/300], Step [134/172], Loss: 12.2803\n",
      "Epoch [112/300], Step [135/172], Loss: 9.1741\n",
      "Epoch [112/300], Step [136/172], Loss: 8.5233\n",
      "Epoch [112/300], Step [137/172], Loss: 9.8792\n",
      "Epoch [112/300], Step [138/172], Loss: 8.1303\n",
      "Epoch [112/300], Step [139/172], Loss: 9.9240\n",
      "Epoch [112/300], Step [140/172], Loss: 9.5605\n",
      "Epoch [112/300], Step [141/172], Loss: 11.2923\n",
      "Epoch [112/300], Step [142/172], Loss: 13.4981\n",
      "Epoch [112/300], Step [143/172], Loss: 9.6865\n",
      "Epoch [112/300], Step [144/172], Loss: 8.9056\n",
      "Epoch [112/300], Step [145/172], Loss: 9.8050\n",
      "Epoch [112/300], Step [146/172], Loss: 9.7018\n",
      "Epoch [112/300], Step [147/172], Loss: 5.4864\n",
      "Epoch [112/300], Step [148/172], Loss: 6.5968\n",
      "Epoch [112/300], Step [149/172], Loss: 7.7625\n",
      "Epoch [112/300], Step [150/172], Loss: 7.7528\n",
      "Epoch [112/300], Step [151/172], Loss: 7.1107\n",
      "Epoch [112/300], Step [152/172], Loss: 7.8861\n",
      "Epoch [112/300], Step [153/172], Loss: 7.0710\n",
      "Epoch [112/300], Step [154/172], Loss: 7.8411\n",
      "Epoch [112/300], Step [155/172], Loss: 6.8491\n",
      "Epoch [112/300], Step [156/172], Loss: 12.3255\n",
      "Epoch [112/300], Step [157/172], Loss: 10.0333\n",
      "Epoch [112/300], Step [158/172], Loss: 7.5626\n",
      "Epoch [112/300], Step [159/172], Loss: 9.5082\n",
      "Epoch [112/300], Step [160/172], Loss: 9.9834\n",
      "Epoch [112/300], Step [161/172], Loss: 7.1599\n",
      "Epoch [112/300], Step [162/172], Loss: 7.1513\n",
      "Epoch [112/300], Step [163/172], Loss: 6.5309\n",
      "Epoch [112/300], Step [164/172], Loss: 9.6009\n",
      "Epoch [112/300], Step [165/172], Loss: 6.3351\n",
      "Epoch [112/300], Step [166/172], Loss: 6.1433\n",
      "Epoch [112/300], Step [167/172], Loss: 9.3749\n",
      "Epoch [112/300], Step [168/172], Loss: 7.2274\n",
      "Epoch [112/300], Step [169/172], Loss: 7.0741\n",
      "Epoch [112/300], Step [170/172], Loss: 5.6458\n",
      "Epoch [112/300], Step [171/172], Loss: 6.7136\n",
      "Epoch [112/300], Step [172/172], Loss: 5.3111\n",
      "Epoch [113/300], Step [1/172], Loss: 74.3925\n",
      "Epoch [113/300], Step [2/172], Loss: 74.4654\n",
      "Epoch [113/300], Step [3/172], Loss: 68.2304\n",
      "Epoch [113/300], Step [4/172], Loss: 42.0178\n",
      "Epoch [113/300], Step [5/172], Loss: 63.1861\n",
      "Epoch [113/300], Step [6/172], Loss: 19.6279\n",
      "Epoch [113/300], Step [7/172], Loss: 29.8835\n",
      "Epoch [113/300], Step [8/172], Loss: 6.1097\n",
      "Epoch [113/300], Step [9/172], Loss: 40.5625\n",
      "Epoch [113/300], Step [10/172], Loss: 46.0788\n",
      "Epoch [113/300], Step [11/172], Loss: 78.1343\n",
      "Epoch [113/300], Step [12/172], Loss: 82.4299\n",
      "Epoch [113/300], Step [13/172], Loss: 41.5531\n",
      "Epoch [113/300], Step [14/172], Loss: 82.9127\n",
      "Epoch [113/300], Step [15/172], Loss: 70.7363\n",
      "Epoch [113/300], Step [16/172], Loss: 16.1202\n",
      "Epoch [113/300], Step [17/172], Loss: 56.2092\n",
      "Epoch [113/300], Step [18/172], Loss: 62.5139\n",
      "Epoch [113/300], Step [19/172], Loss: 84.5882\n",
      "Epoch [113/300], Step [20/172], Loss: 62.4563\n",
      "Epoch [113/300], Step [21/172], Loss: 94.8025\n",
      "Epoch [113/300], Step [22/172], Loss: 73.9745\n",
      "Epoch [113/300], Step [23/172], Loss: 2.9952\n",
      "Epoch [113/300], Step [24/172], Loss: 68.0678\n",
      "Epoch [113/300], Step [25/172], Loss: 45.5066\n",
      "Epoch [113/300], Step [26/172], Loss: 57.1869\n",
      "Epoch [113/300], Step [27/172], Loss: 72.3977\n",
      "Epoch [113/300], Step [28/172], Loss: 32.8661\n",
      "Epoch [113/300], Step [29/172], Loss: 23.0702\n",
      "Epoch [113/300], Step [30/172], Loss: 81.6854\n",
      "Epoch [113/300], Step [31/172], Loss: 44.2230\n",
      "Epoch [113/300], Step [32/172], Loss: 41.6618\n",
      "Epoch [113/300], Step [33/172], Loss: 71.6031\n",
      "Epoch [113/300], Step [34/172], Loss: 4.7049\n",
      "Epoch [113/300], Step [35/172], Loss: 14.8938\n",
      "Epoch [113/300], Step [36/172], Loss: 20.8156\n",
      "Epoch [113/300], Step [37/172], Loss: 17.6048\n",
      "Epoch [113/300], Step [38/172], Loss: 27.3149\n",
      "Epoch [113/300], Step [39/172], Loss: 42.1305\n",
      "Epoch [113/300], Step [40/172], Loss: 20.2038\n",
      "Epoch [113/300], Step [41/172], Loss: 37.4012\n",
      "Epoch [113/300], Step [42/172], Loss: 41.0929\n",
      "Epoch [113/300], Step [43/172], Loss: 26.3757\n",
      "Epoch [113/300], Step [44/172], Loss: 20.4441\n",
      "Epoch [113/300], Step [45/172], Loss: 22.3636\n",
      "Epoch [113/300], Step [46/172], Loss: 21.5129\n",
      "Epoch [113/300], Step [47/172], Loss: 48.1997\n",
      "Epoch [113/300], Step [48/172], Loss: 52.8741\n",
      "Epoch [113/300], Step [49/172], Loss: 19.5433\n",
      "Epoch [113/300], Step [50/172], Loss: 49.8817\n",
      "Epoch [113/300], Step [51/172], Loss: 7.2663\n",
      "Epoch [113/300], Step [52/172], Loss: 17.4414\n",
      "Epoch [113/300], Step [53/172], Loss: 22.5749\n",
      "Epoch [113/300], Step [54/172], Loss: 11.4936\n",
      "Epoch [113/300], Step [55/172], Loss: 11.9636\n",
      "Epoch [113/300], Step [56/172], Loss: 10.8883\n",
      "Epoch [113/300], Step [57/172], Loss: 18.7514\n",
      "Epoch [113/300], Step [58/172], Loss: 16.5359\n",
      "Epoch [113/300], Step [59/172], Loss: 29.9293\n",
      "Epoch [113/300], Step [60/172], Loss: 42.6416\n",
      "Epoch [113/300], Step [61/172], Loss: 8.1006\n",
      "Epoch [113/300], Step [62/172], Loss: 22.5905\n",
      "Epoch [113/300], Step [63/172], Loss: 8.6596\n",
      "Epoch [113/300], Step [64/172], Loss: 8.2726\n",
      "Epoch [113/300], Step [65/172], Loss: 20.1959\n",
      "Epoch [113/300], Step [66/172], Loss: 5.6256\n",
      "Epoch [113/300], Step [67/172], Loss: 26.7472\n",
      "Epoch [113/300], Step [68/172], Loss: 5.6614\n",
      "Epoch [113/300], Step [69/172], Loss: 58.2089\n",
      "Epoch [113/300], Step [70/172], Loss: 54.5074\n",
      "Epoch [113/300], Step [71/172], Loss: 51.4081\n",
      "Epoch [113/300], Step [72/172], Loss: 55.9844\n",
      "Epoch [113/300], Step [73/172], Loss: 60.3182\n",
      "Epoch [113/300], Step [74/172], Loss: 33.7261\n",
      "Epoch [113/300], Step [75/172], Loss: 32.5396\n",
      "Epoch [113/300], Step [76/172], Loss: 36.2350\n",
      "Epoch [113/300], Step [77/172], Loss: 61.2176\n",
      "Epoch [113/300], Step [78/172], Loss: 47.2092\n",
      "Epoch [113/300], Step [79/172], Loss: 46.8159\n",
      "Epoch [113/300], Step [80/172], Loss: 57.6713\n",
      "Epoch [113/300], Step [81/172], Loss: 41.0244\n",
      "Epoch [113/300], Step [82/172], Loss: 39.2576\n",
      "Epoch [113/300], Step [83/172], Loss: 48.5681\n",
      "Epoch [113/300], Step [84/172], Loss: 36.6298\n",
      "Epoch [113/300], Step [85/172], Loss: 40.8336\n",
      "Epoch [113/300], Step [86/172], Loss: 33.3261\n",
      "Epoch [113/300], Step [87/172], Loss: 28.0195\n",
      "Epoch [113/300], Step [88/172], Loss: 29.4330\n",
      "Epoch [113/300], Step [89/172], Loss: 25.8458\n",
      "Epoch [113/300], Step [90/172], Loss: 24.9910\n",
      "Epoch [113/300], Step [91/172], Loss: 28.6511\n",
      "Epoch [113/300], Step [92/172], Loss: 21.3808\n",
      "Epoch [113/300], Step [93/172], Loss: 21.1530\n",
      "Epoch [113/300], Step [94/172], Loss: 29.6736\n",
      "Epoch [113/300], Step [95/172], Loss: 22.6030\n",
      "Epoch [113/300], Step [96/172], Loss: 18.9815\n",
      "Epoch [113/300], Step [97/172], Loss: 26.0389\n",
      "Epoch [113/300], Step [98/172], Loss: 19.2588\n",
      "Epoch [113/300], Step [99/172], Loss: 18.1438\n",
      "Epoch [113/300], Step [100/172], Loss: 15.6572\n",
      "Epoch [113/300], Step [101/172], Loss: 17.0150\n",
      "Epoch [113/300], Step [102/172], Loss: 16.0696\n",
      "Epoch [113/300], Step [103/172], Loss: 13.4891\n",
      "Epoch [113/300], Step [104/172], Loss: 15.9868\n",
      "Epoch [113/300], Step [105/172], Loss: 17.9103\n",
      "Epoch [113/300], Step [106/172], Loss: 16.6922\n",
      "Epoch [113/300], Step [107/172], Loss: 15.2206\n",
      "Epoch [113/300], Step [108/172], Loss: 16.2598\n",
      "Epoch [113/300], Step [109/172], Loss: 16.8369\n",
      "Epoch [113/300], Step [110/172], Loss: 15.7436\n",
      "Epoch [113/300], Step [111/172], Loss: 13.9984\n",
      "Epoch [113/300], Step [112/172], Loss: 18.2132\n",
      "Epoch [113/300], Step [113/172], Loss: 14.0343\n",
      "Epoch [113/300], Step [114/172], Loss: 13.7738\n",
      "Epoch [113/300], Step [115/172], Loss: 20.8388\n",
      "Epoch [113/300], Step [116/172], Loss: 14.4810\n",
      "Epoch [113/300], Step [117/172], Loss: 11.4307\n",
      "Epoch [113/300], Step [118/172], Loss: 14.7719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [113/300], Step [119/172], Loss: 15.7174\n",
      "Epoch [113/300], Step [120/172], Loss: 10.7286\n",
      "Epoch [113/300], Step [121/172], Loss: 10.6898\n",
      "Epoch [113/300], Step [122/172], Loss: 10.2785\n",
      "Epoch [113/300], Step [123/172], Loss: 10.2829\n",
      "Epoch [113/300], Step [124/172], Loss: 8.3749\n",
      "Epoch [113/300], Step [125/172], Loss: 12.9951\n",
      "Epoch [113/300], Step [126/172], Loss: 10.6330\n",
      "Epoch [113/300], Step [127/172], Loss: 11.7246\n",
      "Epoch [113/300], Step [128/172], Loss: 12.0484\n",
      "Epoch [113/300], Step [129/172], Loss: 8.7972\n",
      "Epoch [113/300], Step [130/172], Loss: 11.6616\n",
      "Epoch [113/300], Step [131/172], Loss: 8.5908\n",
      "Epoch [113/300], Step [132/172], Loss: 8.9169\n",
      "Epoch [113/300], Step [133/172], Loss: 9.6187\n",
      "Epoch [113/300], Step [134/172], Loss: 12.2129\n",
      "Epoch [113/300], Step [135/172], Loss: 9.1585\n",
      "Epoch [113/300], Step [136/172], Loss: 8.5192\n",
      "Epoch [113/300], Step [137/172], Loss: 9.8695\n",
      "Epoch [113/300], Step [138/172], Loss: 8.0770\n",
      "Epoch [113/300], Step [139/172], Loss: 9.9087\n",
      "Epoch [113/300], Step [140/172], Loss: 9.5509\n",
      "Epoch [113/300], Step [141/172], Loss: 11.2248\n",
      "Epoch [113/300], Step [142/172], Loss: 13.4497\n",
      "Epoch [113/300], Step [143/172], Loss: 9.7017\n",
      "Epoch [113/300], Step [144/172], Loss: 8.8928\n",
      "Epoch [113/300], Step [145/172], Loss: 9.7734\n",
      "Epoch [113/300], Step [146/172], Loss: 9.6680\n",
      "Epoch [113/300], Step [147/172], Loss: 5.4793\n",
      "Epoch [113/300], Step [148/172], Loss: 6.5844\n",
      "Epoch [113/300], Step [149/172], Loss: 7.7279\n",
      "Epoch [113/300], Step [150/172], Loss: 7.7330\n",
      "Epoch [113/300], Step [151/172], Loss: 7.0403\n",
      "Epoch [113/300], Step [152/172], Loss: 7.8917\n",
      "Epoch [113/300], Step [153/172], Loss: 7.0683\n",
      "Epoch [113/300], Step [154/172], Loss: 7.8310\n",
      "Epoch [113/300], Step [155/172], Loss: 6.8611\n",
      "Epoch [113/300], Step [156/172], Loss: 12.2833\n",
      "Epoch [113/300], Step [157/172], Loss: 9.9876\n",
      "Epoch [113/300], Step [158/172], Loss: 7.5389\n",
      "Epoch [113/300], Step [159/172], Loss: 9.5284\n",
      "Epoch [113/300], Step [160/172], Loss: 9.9332\n",
      "Epoch [113/300], Step [161/172], Loss: 7.1632\n",
      "Epoch [113/300], Step [162/172], Loss: 7.0968\n",
      "Epoch [113/300], Step [163/172], Loss: 6.5069\n",
      "Epoch [113/300], Step [164/172], Loss: 9.4342\n",
      "Epoch [113/300], Step [165/172], Loss: 6.3157\n",
      "Epoch [113/300], Step [166/172], Loss: 6.0853\n",
      "Epoch [113/300], Step [167/172], Loss: 9.3950\n",
      "Epoch [113/300], Step [168/172], Loss: 7.2381\n",
      "Epoch [113/300], Step [169/172], Loss: 7.0418\n",
      "Epoch [113/300], Step [170/172], Loss: 5.6300\n",
      "Epoch [113/300], Step [171/172], Loss: 6.7255\n",
      "Epoch [113/300], Step [172/172], Loss: 5.3233\n",
      "Epoch [114/300], Step [1/172], Loss: 73.6964\n",
      "Epoch [114/300], Step [2/172], Loss: 73.8057\n",
      "Epoch [114/300], Step [3/172], Loss: 67.6925\n",
      "Epoch [114/300], Step [4/172], Loss: 41.7463\n",
      "Epoch [114/300], Step [5/172], Loss: 62.6375\n",
      "Epoch [114/300], Step [6/172], Loss: 19.8353\n",
      "Epoch [114/300], Step [7/172], Loss: 30.3397\n",
      "Epoch [114/300], Step [8/172], Loss: 6.6146\n",
      "Epoch [114/300], Step [9/172], Loss: 40.6094\n",
      "Epoch [114/300], Step [10/172], Loss: 46.1496\n",
      "Epoch [114/300], Step [11/172], Loss: 77.8970\n",
      "Epoch [114/300], Step [12/172], Loss: 82.3617\n",
      "Epoch [114/300], Step [13/172], Loss: 41.5193\n",
      "Epoch [114/300], Step [14/172], Loss: 82.4324\n",
      "Epoch [114/300], Step [15/172], Loss: 70.5119\n",
      "Epoch [114/300], Step [16/172], Loss: 15.9417\n",
      "Epoch [114/300], Step [17/172], Loss: 56.1883\n",
      "Epoch [114/300], Step [18/172], Loss: 62.6629\n",
      "Epoch [114/300], Step [19/172], Loss: 84.7648\n",
      "Epoch [114/300], Step [20/172], Loss: 62.7092\n",
      "Epoch [114/300], Step [21/172], Loss: 94.9196\n",
      "Epoch [114/300], Step [22/172], Loss: 74.0703\n",
      "Epoch [114/300], Step [23/172], Loss: 2.8984\n",
      "Epoch [114/300], Step [24/172], Loss: 68.2046\n",
      "Epoch [114/300], Step [25/172], Loss: 45.6088\n",
      "Epoch [114/300], Step [26/172], Loss: 57.3674\n",
      "Epoch [114/300], Step [27/172], Loss: 72.7116\n",
      "Epoch [114/300], Step [28/172], Loss: 32.8393\n",
      "Epoch [114/300], Step [29/172], Loss: 23.0625\n",
      "Epoch [114/300], Step [30/172], Loss: 81.8728\n",
      "Epoch [114/300], Step [31/172], Loss: 44.3122\n",
      "Epoch [114/300], Step [32/172], Loss: 42.0309\n",
      "Epoch [114/300], Step [33/172], Loss: 72.0885\n",
      "Epoch [114/300], Step [34/172], Loss: 4.5346\n",
      "Epoch [114/300], Step [35/172], Loss: 14.8609\n",
      "Epoch [114/300], Step [36/172], Loss: 20.7100\n",
      "Epoch [114/300], Step [37/172], Loss: 17.6594\n",
      "Epoch [114/300], Step [38/172], Loss: 27.5604\n",
      "Epoch [114/300], Step [39/172], Loss: 41.9387\n",
      "Epoch [114/300], Step [40/172], Loss: 20.3369\n",
      "Epoch [114/300], Step [41/172], Loss: 37.6744\n",
      "Epoch [114/300], Step [42/172], Loss: 41.4088\n",
      "Epoch [114/300], Step [43/172], Loss: 26.5945\n",
      "Epoch [114/300], Step [44/172], Loss: 20.5453\n",
      "Epoch [114/300], Step [45/172], Loss: 22.4899\n",
      "Epoch [114/300], Step [46/172], Loss: 21.5088\n",
      "Epoch [114/300], Step [47/172], Loss: 48.4346\n",
      "Epoch [114/300], Step [48/172], Loss: 53.0448\n",
      "Epoch [114/300], Step [49/172], Loss: 19.6731\n",
      "Epoch [114/300], Step [50/172], Loss: 49.8218\n",
      "Epoch [114/300], Step [51/172], Loss: 7.4113\n",
      "Epoch [114/300], Step [52/172], Loss: 17.4946\n",
      "Epoch [114/300], Step [53/172], Loss: 22.7356\n",
      "Epoch [114/300], Step [54/172], Loss: 11.4977\n",
      "Epoch [114/300], Step [55/172], Loss: 12.0585\n",
      "Epoch [114/300], Step [56/172], Loss: 11.0971\n",
      "Epoch [114/300], Step [57/172], Loss: 18.7679\n",
      "Epoch [114/300], Step [58/172], Loss: 16.5505\n",
      "Epoch [114/300], Step [59/172], Loss: 29.9065\n",
      "Epoch [114/300], Step [60/172], Loss: 42.3130\n",
      "Epoch [114/300], Step [61/172], Loss: 8.0862\n",
      "Epoch [114/300], Step [62/172], Loss: 22.6260\n",
      "Epoch [114/300], Step [63/172], Loss: 8.8237\n",
      "Epoch [114/300], Step [64/172], Loss: 8.3705\n",
      "Epoch [114/300], Step [65/172], Loss: 20.2008\n",
      "Epoch [114/300], Step [66/172], Loss: 5.6440\n",
      "Epoch [114/300], Step [67/172], Loss: 26.7826\n",
      "Epoch [114/300], Step [68/172], Loss: 5.3963\n",
      "Epoch [114/300], Step [69/172], Loss: 57.8643\n",
      "Epoch [114/300], Step [70/172], Loss: 54.2870\n",
      "Epoch [114/300], Step [71/172], Loss: 51.1108\n",
      "Epoch [114/300], Step [72/172], Loss: 55.7501\n",
      "Epoch [114/300], Step [73/172], Loss: 60.1376\n",
      "Epoch [114/300], Step [74/172], Loss: 33.5016\n",
      "Epoch [114/300], Step [75/172], Loss: 32.4112\n",
      "Epoch [114/300], Step [76/172], Loss: 36.0718\n",
      "Epoch [114/300], Step [77/172], Loss: 61.1359\n",
      "Epoch [114/300], Step [78/172], Loss: 46.9902\n",
      "Epoch [114/300], Step [79/172], Loss: 46.5219\n",
      "Epoch [114/300], Step [80/172], Loss: 57.5945\n",
      "Epoch [114/300], Step [81/172], Loss: 40.8382\n",
      "Epoch [114/300], Step [82/172], Loss: 39.3500\n",
      "Epoch [114/300], Step [83/172], Loss: 48.2486\n",
      "Epoch [114/300], Step [84/172], Loss: 36.4174\n",
      "Epoch [114/300], Step [85/172], Loss: 40.6446\n",
      "Epoch [114/300], Step [86/172], Loss: 33.1984\n",
      "Epoch [114/300], Step [87/172], Loss: 27.9263\n",
      "Epoch [114/300], Step [88/172], Loss: 29.4239\n",
      "Epoch [114/300], Step [89/172], Loss: 25.7991\n",
      "Epoch [114/300], Step [90/172], Loss: 24.9486\n",
      "Epoch [114/300], Step [91/172], Loss: 28.5698\n",
      "Epoch [114/300], Step [92/172], Loss: 21.3592\n",
      "Epoch [114/300], Step [93/172], Loss: 21.1272\n",
      "Epoch [114/300], Step [94/172], Loss: 29.6500\n",
      "Epoch [114/300], Step [95/172], Loss: 22.6543\n",
      "Epoch [114/300], Step [96/172], Loss: 18.9917\n",
      "Epoch [114/300], Step [97/172], Loss: 26.1034\n",
      "Epoch [114/300], Step [98/172], Loss: 19.2900\n",
      "Epoch [114/300], Step [99/172], Loss: 18.1868\n",
      "Epoch [114/300], Step [100/172], Loss: 15.6461\n",
      "Epoch [114/300], Step [101/172], Loss: 17.0311\n",
      "Epoch [114/300], Step [102/172], Loss: 16.1767\n",
      "Epoch [114/300], Step [103/172], Loss: 13.4989\n",
      "Epoch [114/300], Step [104/172], Loss: 16.0419\n",
      "Epoch [114/300], Step [105/172], Loss: 17.9715\n",
      "Epoch [114/300], Step [106/172], Loss: 16.6754\n",
      "Epoch [114/300], Step [107/172], Loss: 15.1749\n",
      "Epoch [114/300], Step [108/172], Loss: 16.2707\n",
      "Epoch [114/300], Step [109/172], Loss: 16.8846\n",
      "Epoch [114/300], Step [110/172], Loss: 15.7824\n",
      "Epoch [114/300], Step [111/172], Loss: 13.9761\n",
      "Epoch [114/300], Step [112/172], Loss: 18.2318\n",
      "Epoch [114/300], Step [113/172], Loss: 14.0501\n",
      "Epoch [114/300], Step [114/172], Loss: 13.7890\n",
      "Epoch [114/300], Step [115/172], Loss: 20.8485\n",
      "Epoch [114/300], Step [116/172], Loss: 14.5238\n",
      "Epoch [114/300], Step [117/172], Loss: 11.4413\n",
      "Epoch [114/300], Step [118/172], Loss: 14.7098\n",
      "Epoch [114/300], Step [119/172], Loss: 15.6549\n",
      "Epoch [114/300], Step [120/172], Loss: 10.6757\n",
      "Epoch [114/300], Step [121/172], Loss: 10.6608\n",
      "Epoch [114/300], Step [122/172], Loss: 10.2769\n",
      "Epoch [114/300], Step [123/172], Loss: 10.2961\n",
      "Epoch [114/300], Step [124/172], Loss: 8.3677\n",
      "Epoch [114/300], Step [125/172], Loss: 12.9577\n",
      "Epoch [114/300], Step [126/172], Loss: 10.6198\n",
      "Epoch [114/300], Step [127/172], Loss: 11.6867\n",
      "Epoch [114/300], Step [128/172], Loss: 12.0076\n",
      "Epoch [114/300], Step [129/172], Loss: 8.7779\n",
      "Epoch [114/300], Step [130/172], Loss: 11.6603\n",
      "Epoch [114/300], Step [131/172], Loss: 8.5605\n",
      "Epoch [114/300], Step [132/172], Loss: 8.8992\n",
      "Epoch [114/300], Step [133/172], Loss: 9.5893\n",
      "Epoch [114/300], Step [134/172], Loss: 12.1693\n",
      "Epoch [114/300], Step [135/172], Loss: 9.1275\n",
      "Epoch [114/300], Step [136/172], Loss: 8.5115\n",
      "Epoch [114/300], Step [137/172], Loss: 9.8187\n",
      "Epoch [114/300], Step [138/172], Loss: 8.0384\n",
      "Epoch [114/300], Step [139/172], Loss: 9.8664\n",
      "Epoch [114/300], Step [140/172], Loss: 9.5363\n",
      "Epoch [114/300], Step [141/172], Loss: 11.2100\n",
      "Epoch [114/300], Step [142/172], Loss: 13.4933\n",
      "Epoch [114/300], Step [143/172], Loss: 9.7191\n",
      "Epoch [114/300], Step [144/172], Loss: 8.8680\n",
      "Epoch [114/300], Step [145/172], Loss: 9.8056\n",
      "Epoch [114/300], Step [146/172], Loss: 9.6466\n",
      "Epoch [114/300], Step [147/172], Loss: 5.4550\n",
      "Epoch [114/300], Step [148/172], Loss: 6.5712\n",
      "Epoch [114/300], Step [149/172], Loss: 7.6933\n",
      "Epoch [114/300], Step [150/172], Loss: 7.6877\n",
      "Epoch [114/300], Step [151/172], Loss: 7.0031\n",
      "Epoch [114/300], Step [152/172], Loss: 7.8935\n",
      "Epoch [114/300], Step [153/172], Loss: 7.0382\n",
      "Epoch [114/300], Step [154/172], Loss: 7.7961\n",
      "Epoch [114/300], Step [155/172], Loss: 6.8270\n",
      "Epoch [114/300], Step [156/172], Loss: 12.2610\n",
      "Epoch [114/300], Step [157/172], Loss: 9.9563\n",
      "Epoch [114/300], Step [158/172], Loss: 7.5218\n",
      "Epoch [114/300], Step [159/172], Loss: 9.5191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [114/300], Step [160/172], Loss: 9.9016\n",
      "Epoch [114/300], Step [161/172], Loss: 7.1694\n",
      "Epoch [114/300], Step [162/172], Loss: 7.0316\n",
      "Epoch [114/300], Step [163/172], Loss: 6.4974\n",
      "Epoch [114/300], Step [164/172], Loss: 9.4360\n",
      "Epoch [114/300], Step [165/172], Loss: 6.3094\n",
      "Epoch [114/300], Step [166/172], Loss: 6.0226\n",
      "Epoch [114/300], Step [167/172], Loss: 9.3794\n",
      "Epoch [114/300], Step [168/172], Loss: 7.2174\n",
      "Epoch [114/300], Step [169/172], Loss: 7.0232\n",
      "Epoch [114/300], Step [170/172], Loss: 5.6096\n",
      "Epoch [114/300], Step [171/172], Loss: 6.7220\n",
      "Epoch [114/300], Step [172/172], Loss: 5.3209\n",
      "Epoch [115/300], Step [1/172], Loss: 72.8589\n",
      "Epoch [115/300], Step [2/172], Loss: 73.2700\n",
      "Epoch [115/300], Step [3/172], Loss: 67.1377\n",
      "Epoch [115/300], Step [4/172], Loss: 41.5225\n",
      "Epoch [115/300], Step [5/172], Loss: 62.7521\n",
      "Epoch [115/300], Step [6/172], Loss: 19.8500\n",
      "Epoch [115/300], Step [7/172], Loss: 30.1967\n",
      "Epoch [115/300], Step [8/172], Loss: 6.3266\n",
      "Epoch [115/300], Step [9/172], Loss: 40.3287\n",
      "Epoch [115/300], Step [10/172], Loss: 46.0688\n",
      "Epoch [115/300], Step [11/172], Loss: 77.3182\n",
      "Epoch [115/300], Step [12/172], Loss: 82.0750\n",
      "Epoch [115/300], Step [13/172], Loss: 41.1605\n",
      "Epoch [115/300], Step [14/172], Loss: 81.9225\n",
      "Epoch [115/300], Step [15/172], Loss: 70.0122\n",
      "Epoch [115/300], Step [16/172], Loss: 15.5696\n",
      "Epoch [115/300], Step [17/172], Loss: 55.9051\n",
      "Epoch [115/300], Step [18/172], Loss: 62.5566\n",
      "Epoch [115/300], Step [19/172], Loss: 84.6677\n",
      "Epoch [115/300], Step [20/172], Loss: 61.2966\n",
      "Epoch [115/300], Step [21/172], Loss: 94.5347\n",
      "Epoch [115/300], Step [22/172], Loss: 73.5177\n",
      "Epoch [115/300], Step [23/172], Loss: 2.8668\n",
      "Epoch [115/300], Step [24/172], Loss: 67.7497\n",
      "Epoch [115/300], Step [25/172], Loss: 45.4053\n",
      "Epoch [115/300], Step [26/172], Loss: 57.0652\n",
      "Epoch [115/300], Step [27/172], Loss: 72.1240\n",
      "Epoch [115/300], Step [28/172], Loss: 32.4322\n",
      "Epoch [115/300], Step [29/172], Loss: 22.7932\n",
      "Epoch [115/300], Step [30/172], Loss: 81.4820\n",
      "Epoch [115/300], Step [31/172], Loss: 44.0084\n",
      "Epoch [115/300], Step [32/172], Loss: 41.8535\n",
      "Epoch [115/300], Step [33/172], Loss: 71.8661\n",
      "Epoch [115/300], Step [34/172], Loss: 4.6795\n",
      "Epoch [115/300], Step [35/172], Loss: 14.7738\n",
      "Epoch [115/300], Step [36/172], Loss: 20.6624\n",
      "Epoch [115/300], Step [37/172], Loss: 17.5756\n",
      "Epoch [115/300], Step [38/172], Loss: 27.4824\n",
      "Epoch [115/300], Step [39/172], Loss: 41.6476\n",
      "Epoch [115/300], Step [40/172], Loss: 20.2485\n",
      "Epoch [115/300], Step [41/172], Loss: 37.5252\n",
      "Epoch [115/300], Step [42/172], Loss: 41.1130\n",
      "Epoch [115/300], Step [43/172], Loss: 26.4918\n",
      "Epoch [115/300], Step [44/172], Loss: 20.4691\n",
      "Epoch [115/300], Step [45/172], Loss: 22.5621\n",
      "Epoch [115/300], Step [46/172], Loss: 21.3653\n",
      "Epoch [115/300], Step [47/172], Loss: 48.2011\n",
      "Epoch [115/300], Step [48/172], Loss: 53.1253\n",
      "Epoch [115/300], Step [49/172], Loss: 19.7465\n",
      "Epoch [115/300], Step [50/172], Loss: 49.6798\n",
      "Epoch [115/300], Step [51/172], Loss: 7.4394\n",
      "Epoch [115/300], Step [52/172], Loss: 17.4596\n",
      "Epoch [115/300], Step [53/172], Loss: 22.5477\n",
      "Epoch [115/300], Step [54/172], Loss: 11.5727\n",
      "Epoch [115/300], Step [55/172], Loss: 12.0610\n",
      "Epoch [115/300], Step [56/172], Loss: 11.1951\n",
      "Epoch [115/300], Step [57/172], Loss: 18.5981\n",
      "Epoch [115/300], Step [58/172], Loss: 16.5208\n",
      "Epoch [115/300], Step [59/172], Loss: 29.8488\n",
      "Epoch [115/300], Step [60/172], Loss: 42.1926\n",
      "Epoch [115/300], Step [61/172], Loss: 7.9693\n",
      "Epoch [115/300], Step [62/172], Loss: 22.4516\n",
      "Epoch [115/300], Step [63/172], Loss: 8.8420\n",
      "Epoch [115/300], Step [64/172], Loss: 8.3810\n",
      "Epoch [115/300], Step [65/172], Loss: 20.2010\n",
      "Epoch [115/300], Step [66/172], Loss: 5.6215\n",
      "Epoch [115/300], Step [67/172], Loss: 26.7520\n",
      "Epoch [115/300], Step [68/172], Loss: 5.4827\n",
      "Epoch [115/300], Step [69/172], Loss: 57.6236\n",
      "Epoch [115/300], Step [70/172], Loss: 54.1511\n",
      "Epoch [115/300], Step [71/172], Loss: 50.9817\n",
      "Epoch [115/300], Step [72/172], Loss: 55.5251\n",
      "Epoch [115/300], Step [73/172], Loss: 60.1340\n",
      "Epoch [115/300], Step [74/172], Loss: 33.3999\n",
      "Epoch [115/300], Step [75/172], Loss: 32.2007\n",
      "Epoch [115/300], Step [76/172], Loss: 35.9978\n",
      "Epoch [115/300], Step [77/172], Loss: 61.0793\n",
      "Epoch [115/300], Step [78/172], Loss: 46.9994\n",
      "Epoch [115/300], Step [79/172], Loss: 46.4361\n",
      "Epoch [115/300], Step [80/172], Loss: 57.5363\n",
      "Epoch [115/300], Step [81/172], Loss: 40.8308\n",
      "Epoch [115/300], Step [82/172], Loss: 39.3252\n",
      "Epoch [115/300], Step [83/172], Loss: 48.1102\n",
      "Epoch [115/300], Step [84/172], Loss: 36.3851\n",
      "Epoch [115/300], Step [85/172], Loss: 40.6764\n",
      "Epoch [115/300], Step [86/172], Loss: 33.3045\n",
      "Epoch [115/300], Step [87/172], Loss: 27.9118\n",
      "Epoch [115/300], Step [88/172], Loss: 29.3760\n",
      "Epoch [115/300], Step [89/172], Loss: 25.8064\n",
      "Epoch [115/300], Step [90/172], Loss: 24.9827\n",
      "Epoch [115/300], Step [91/172], Loss: 28.4342\n",
      "Epoch [115/300], Step [92/172], Loss: 21.3305\n",
      "Epoch [115/300], Step [93/172], Loss: 21.1585\n",
      "Epoch [115/300], Step [94/172], Loss: 29.7003\n",
      "Epoch [115/300], Step [95/172], Loss: 22.6278\n",
      "Epoch [115/300], Step [96/172], Loss: 18.9927\n",
      "Epoch [115/300], Step [97/172], Loss: 26.0892\n",
      "Epoch [115/300], Step [98/172], Loss: 19.2560\n",
      "Epoch [115/300], Step [99/172], Loss: 18.1744\n",
      "Epoch [115/300], Step [100/172], Loss: 15.5936\n",
      "Epoch [115/300], Step [101/172], Loss: 16.9816\n",
      "Epoch [115/300], Step [102/172], Loss: 16.1461\n",
      "Epoch [115/300], Step [103/172], Loss: 13.4278\n",
      "Epoch [115/300], Step [104/172], Loss: 16.0242\n",
      "Epoch [115/300], Step [105/172], Loss: 17.9100\n",
      "Epoch [115/300], Step [106/172], Loss: 16.5943\n",
      "Epoch [115/300], Step [107/172], Loss: 15.0842\n",
      "Epoch [115/300], Step [108/172], Loss: 16.2251\n",
      "Epoch [115/300], Step [109/172], Loss: 16.7846\n",
      "Epoch [115/300], Step [110/172], Loss: 15.7370\n",
      "Epoch [115/300], Step [111/172], Loss: 13.9334\n",
      "Epoch [115/300], Step [112/172], Loss: 18.1995\n",
      "Epoch [115/300], Step [113/172], Loss: 13.9464\n",
      "Epoch [115/300], Step [114/172], Loss: 13.7265\n",
      "Epoch [115/300], Step [115/172], Loss: 20.7840\n",
      "Epoch [115/300], Step [116/172], Loss: 14.4590\n",
      "Epoch [115/300], Step [117/172], Loss: 11.3758\n",
      "Epoch [115/300], Step [118/172], Loss: 14.7024\n",
      "Epoch [115/300], Step [119/172], Loss: 15.5068\n",
      "Epoch [115/300], Step [120/172], Loss: 10.5740\n",
      "Epoch [115/300], Step [121/172], Loss: 10.5846\n",
      "Epoch [115/300], Step [122/172], Loss: 10.2939\n",
      "Epoch [115/300], Step [123/172], Loss: 10.2503\n",
      "Epoch [115/300], Step [124/172], Loss: 8.3262\n",
      "Epoch [115/300], Step [125/172], Loss: 12.8923\n",
      "Epoch [115/300], Step [126/172], Loss: 10.5624\n",
      "Epoch [115/300], Step [127/172], Loss: 11.6267\n",
      "Epoch [115/300], Step [128/172], Loss: 11.9344\n",
      "Epoch [115/300], Step [129/172], Loss: 8.7252\n",
      "Epoch [115/300], Step [130/172], Loss: 11.6237\n",
      "Epoch [115/300], Step [131/172], Loss: 8.5050\n",
      "Epoch [115/300], Step [132/172], Loss: 8.8471\n",
      "Epoch [115/300], Step [133/172], Loss: 9.5572\n",
      "Epoch [115/300], Step [134/172], Loss: 12.0495\n",
      "Epoch [115/300], Step [135/172], Loss: 9.0368\n",
      "Epoch [115/300], Step [136/172], Loss: 8.4688\n",
      "Epoch [115/300], Step [137/172], Loss: 9.8014\n",
      "Epoch [115/300], Step [138/172], Loss: 7.9945\n",
      "Epoch [115/300], Step [139/172], Loss: 9.8080\n",
      "Epoch [115/300], Step [140/172], Loss: 9.5048\n",
      "Epoch [115/300], Step [141/172], Loss: 11.1410\n",
      "Epoch [115/300], Step [142/172], Loss: 13.4758\n",
      "Epoch [115/300], Step [143/172], Loss: 9.7175\n",
      "Epoch [115/300], Step [144/172], Loss: 8.8500\n",
      "Epoch [115/300], Step [145/172], Loss: 9.7736\n",
      "Epoch [115/300], Step [146/172], Loss: 9.6097\n",
      "Epoch [115/300], Step [147/172], Loss: 5.4270\n",
      "Epoch [115/300], Step [148/172], Loss: 6.5512\n",
      "Epoch [115/300], Step [149/172], Loss: 7.6505\n",
      "Epoch [115/300], Step [150/172], Loss: 7.6513\n",
      "Epoch [115/300], Step [151/172], Loss: 6.9101\n",
      "Epoch [115/300], Step [152/172], Loss: 7.8495\n",
      "Epoch [115/300], Step [153/172], Loss: 7.0116\n",
      "Epoch [115/300], Step [154/172], Loss: 7.7999\n",
      "Epoch [115/300], Step [155/172], Loss: 6.7900\n",
      "Epoch [115/300], Step [156/172], Loss: 12.2452\n",
      "Epoch [115/300], Step [157/172], Loss: 9.9194\n",
      "Epoch [115/300], Step [158/172], Loss: 7.4919\n",
      "Epoch [115/300], Step [159/172], Loss: 9.5057\n",
      "Epoch [115/300], Step [160/172], Loss: 9.8767\n",
      "Epoch [115/300], Step [161/172], Loss: 7.1398\n",
      "Epoch [115/300], Step [162/172], Loss: 6.9797\n",
      "Epoch [115/300], Step [163/172], Loss: 6.4877\n",
      "Epoch [115/300], Step [164/172], Loss: 9.2899\n",
      "Epoch [115/300], Step [165/172], Loss: 6.2947\n",
      "Epoch [115/300], Step [166/172], Loss: 5.9805\n",
      "Epoch [115/300], Step [167/172], Loss: 9.3851\n",
      "Epoch [115/300], Step [168/172], Loss: 7.1988\n",
      "Epoch [115/300], Step [169/172], Loss: 6.9805\n",
      "Epoch [115/300], Step [170/172], Loss: 5.5799\n",
      "Epoch [115/300], Step [171/172], Loss: 6.7208\n",
      "Epoch [115/300], Step [172/172], Loss: 5.2879\n",
      "Epoch [116/300], Step [1/172], Loss: 72.2129\n",
      "Epoch [116/300], Step [2/172], Loss: 72.5967\n",
      "Epoch [116/300], Step [3/172], Loss: 66.5890\n",
      "Epoch [116/300], Step [4/172], Loss: 41.1880\n",
      "Epoch [116/300], Step [5/172], Loss: 62.2568\n",
      "Epoch [116/300], Step [6/172], Loss: 19.9688\n",
      "Epoch [116/300], Step [7/172], Loss: 30.4055\n",
      "Epoch [116/300], Step [8/172], Loss: 6.6489\n",
      "Epoch [116/300], Step [9/172], Loss: 40.3615\n",
      "Epoch [116/300], Step [10/172], Loss: 46.1466\n",
      "Epoch [116/300], Step [11/172], Loss: 77.1582\n",
      "Epoch [116/300], Step [12/172], Loss: 82.2655\n",
      "Epoch [116/300], Step [13/172], Loss: 41.3197\n",
      "Epoch [116/300], Step [14/172], Loss: 81.6856\n",
      "Epoch [116/300], Step [15/172], Loss: 69.8996\n",
      "Epoch [116/300], Step [16/172], Loss: 15.4676\n",
      "Epoch [116/300], Step [17/172], Loss: 55.9997\n",
      "Epoch [116/300], Step [18/172], Loss: 62.8172\n",
      "Epoch [116/300], Step [19/172], Loss: 85.0571\n",
      "Epoch [116/300], Step [20/172], Loss: 61.1949\n",
      "Epoch [116/300], Step [21/172], Loss: 94.8827\n",
      "Epoch [116/300], Step [22/172], Loss: 73.5336\n",
      "Epoch [116/300], Step [23/172], Loss: 2.7790\n",
      "Epoch [116/300], Step [24/172], Loss: 67.8051\n",
      "Epoch [116/300], Step [25/172], Loss: 45.4159\n",
      "Epoch [116/300], Step [26/172], Loss: 56.9378\n",
      "Epoch [116/300], Step [27/172], Loss: 72.1178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [116/300], Step [28/172], Loss: 32.3954\n",
      "Epoch [116/300], Step [29/172], Loss: 22.7093\n",
      "Epoch [116/300], Step [30/172], Loss: 81.4335\n",
      "Epoch [116/300], Step [31/172], Loss: 44.0001\n",
      "Epoch [116/300], Step [32/172], Loss: 41.9432\n",
      "Epoch [116/300], Step [33/172], Loss: 71.8247\n",
      "Epoch [116/300], Step [34/172], Loss: 4.6212\n",
      "Epoch [116/300], Step [35/172], Loss: 14.5323\n",
      "Epoch [116/300], Step [36/172], Loss: 20.4315\n",
      "Epoch [116/300], Step [37/172], Loss: 17.5247\n",
      "Epoch [116/300], Step [38/172], Loss: 27.4950\n",
      "Epoch [116/300], Step [39/172], Loss: 41.1983\n",
      "Epoch [116/300], Step [40/172], Loss: 20.1976\n",
      "Epoch [116/300], Step [41/172], Loss: 37.4355\n",
      "Epoch [116/300], Step [42/172], Loss: 40.9570\n",
      "Epoch [116/300], Step [43/172], Loss: 26.4896\n",
      "Epoch [116/300], Step [44/172], Loss: 20.2972\n",
      "Epoch [116/300], Step [45/172], Loss: 22.5307\n",
      "Epoch [116/300], Step [46/172], Loss: 21.1757\n",
      "Epoch [116/300], Step [47/172], Loss: 48.0157\n",
      "Epoch [116/300], Step [48/172], Loss: 52.7267\n",
      "Epoch [116/300], Step [49/172], Loss: 19.6497\n",
      "Epoch [116/300], Step [50/172], Loss: 49.3600\n",
      "Epoch [116/300], Step [51/172], Loss: 7.4695\n",
      "Epoch [116/300], Step [52/172], Loss: 17.4480\n",
      "Epoch [116/300], Step [53/172], Loss: 22.5465\n",
      "Epoch [116/300], Step [54/172], Loss: 11.5773\n",
      "Epoch [116/300], Step [55/172], Loss: 12.0743\n",
      "Epoch [116/300], Step [56/172], Loss: 11.4285\n",
      "Epoch [116/300], Step [57/172], Loss: 18.4718\n",
      "Epoch [116/300], Step [58/172], Loss: 16.5013\n",
      "Epoch [116/300], Step [59/172], Loss: 29.7533\n",
      "Epoch [116/300], Step [60/172], Loss: 41.5399\n",
      "Epoch [116/300], Step [61/172], Loss: 7.9046\n",
      "Epoch [116/300], Step [62/172], Loss: 22.4238\n",
      "Epoch [116/300], Step [63/172], Loss: 8.9653\n",
      "Epoch [116/300], Step [64/172], Loss: 8.4826\n",
      "Epoch [116/300], Step [65/172], Loss: 20.1914\n",
      "Epoch [116/300], Step [66/172], Loss: 5.6558\n",
      "Epoch [116/300], Step [67/172], Loss: 26.6002\n",
      "Epoch [116/300], Step [68/172], Loss: 5.4328\n",
      "Epoch [116/300], Step [69/172], Loss: 56.9882\n",
      "Epoch [116/300], Step [70/172], Loss: 53.7355\n",
      "Epoch [116/300], Step [71/172], Loss: 50.5910\n",
      "Epoch [116/300], Step [72/172], Loss: 55.0833\n",
      "Epoch [116/300], Step [73/172], Loss: 59.7984\n",
      "Epoch [116/300], Step [74/172], Loss: 33.1535\n",
      "Epoch [116/300], Step [75/172], Loss: 32.0394\n",
      "Epoch [116/300], Step [76/172], Loss: 35.7888\n",
      "Epoch [116/300], Step [77/172], Loss: 60.9032\n",
      "Epoch [116/300], Step [78/172], Loss: 46.7841\n",
      "Epoch [116/300], Step [79/172], Loss: 46.1984\n",
      "Epoch [116/300], Step [80/172], Loss: 57.4595\n",
      "Epoch [116/300], Step [81/172], Loss: 40.6687\n",
      "Epoch [116/300], Step [82/172], Loss: 39.4202\n",
      "Epoch [116/300], Step [83/172], Loss: 47.8260\n",
      "Epoch [116/300], Step [84/172], Loss: 36.2393\n",
      "Epoch [116/300], Step [85/172], Loss: 40.4836\n",
      "Epoch [116/300], Step [86/172], Loss: 33.2547\n",
      "Epoch [116/300], Step [87/172], Loss: 27.7943\n",
      "Epoch [116/300], Step [88/172], Loss: 29.2352\n",
      "Epoch [116/300], Step [89/172], Loss: 25.6731\n",
      "Epoch [116/300], Step [90/172], Loss: 24.9328\n",
      "Epoch [116/300], Step [91/172], Loss: 28.2818\n",
      "Epoch [116/300], Step [92/172], Loss: 21.2678\n",
      "Epoch [116/300], Step [93/172], Loss: 21.0907\n",
      "Epoch [116/300], Step [94/172], Loss: 29.6519\n",
      "Epoch [116/300], Step [95/172], Loss: 22.6157\n",
      "Epoch [116/300], Step [96/172], Loss: 18.9586\n",
      "Epoch [116/300], Step [97/172], Loss: 26.0375\n",
      "Epoch [116/300], Step [98/172], Loss: 19.1495\n",
      "Epoch [116/300], Step [99/172], Loss: 18.1306\n",
      "Epoch [116/300], Step [100/172], Loss: 15.4997\n",
      "Epoch [116/300], Step [101/172], Loss: 16.9079\n",
      "Epoch [116/300], Step [102/172], Loss: 16.2529\n",
      "Epoch [116/300], Step [103/172], Loss: 13.3571\n",
      "Epoch [116/300], Step [104/172], Loss: 15.9984\n",
      "Epoch [116/300], Step [105/172], Loss: 17.9780\n",
      "Epoch [116/300], Step [106/172], Loss: 16.5358\n",
      "Epoch [116/300], Step [107/172], Loss: 15.0336\n",
      "Epoch [116/300], Step [108/172], Loss: 16.1935\n",
      "Epoch [116/300], Step [109/172], Loss: 16.7314\n",
      "Epoch [116/300], Step [110/172], Loss: 15.7169\n",
      "Epoch [116/300], Step [111/172], Loss: 13.9081\n",
      "Epoch [116/300], Step [112/172], Loss: 18.2236\n",
      "Epoch [116/300], Step [113/172], Loss: 13.8942\n",
      "Epoch [116/300], Step [114/172], Loss: 13.7297\n",
      "Epoch [116/300], Step [115/172], Loss: 20.7525\n",
      "Epoch [116/300], Step [116/172], Loss: 14.4504\n",
      "Epoch [116/300], Step [117/172], Loss: 11.3315\n",
      "Epoch [116/300], Step [118/172], Loss: 14.6901\n",
      "Epoch [116/300], Step [119/172], Loss: 15.4544\n",
      "Epoch [116/300], Step [120/172], Loss: 10.5251\n",
      "Epoch [116/300], Step [121/172], Loss: 10.5259\n",
      "Epoch [116/300], Step [122/172], Loss: 10.3753\n",
      "Epoch [116/300], Step [123/172], Loss: 10.2525\n",
      "Epoch [116/300], Step [124/172], Loss: 8.3209\n",
      "Epoch [116/300], Step [125/172], Loss: 12.8410\n",
      "Epoch [116/300], Step [126/172], Loss: 10.5383\n",
      "Epoch [116/300], Step [127/172], Loss: 11.6385\n",
      "Epoch [116/300], Step [128/172], Loss: 11.9323\n",
      "Epoch [116/300], Step [129/172], Loss: 8.7048\n",
      "Epoch [116/300], Step [130/172], Loss: 11.6401\n",
      "Epoch [116/300], Step [131/172], Loss: 8.4997\n",
      "Epoch [116/300], Step [132/172], Loss: 8.8154\n",
      "Epoch [116/300], Step [133/172], Loss: 9.5940\n",
      "Epoch [116/300], Step [134/172], Loss: 12.0435\n",
      "Epoch [116/300], Step [135/172], Loss: 8.9947\n",
      "Epoch [116/300], Step [136/172], Loss: 8.4668\n",
      "Epoch [116/300], Step [137/172], Loss: 9.8080\n",
      "Epoch [116/300], Step [138/172], Loss: 7.9807\n",
      "Epoch [116/300], Step [139/172], Loss: 9.7791\n",
      "Epoch [116/300], Step [140/172], Loss: 9.4856\n",
      "Epoch [116/300], Step [141/172], Loss: 11.1093\n",
      "Epoch [116/300], Step [142/172], Loss: 13.5448\n",
      "Epoch [116/300], Step [143/172], Loss: 9.7424\n",
      "Epoch [116/300], Step [144/172], Loss: 8.8535\n",
      "Epoch [116/300], Step [145/172], Loss: 9.7786\n",
      "Epoch [116/300], Step [146/172], Loss: 9.6185\n",
      "Epoch [116/300], Step [147/172], Loss: 5.4144\n",
      "Epoch [116/300], Step [148/172], Loss: 6.5449\n",
      "Epoch [116/300], Step [149/172], Loss: 7.6362\n",
      "Epoch [116/300], Step [150/172], Loss: 7.6451\n",
      "Epoch [116/300], Step [151/172], Loss: 6.8777\n",
      "Epoch [116/300], Step [152/172], Loss: 7.8433\n",
      "Epoch [116/300], Step [153/172], Loss: 6.9979\n",
      "Epoch [116/300], Step [154/172], Loss: 7.8087\n",
      "Epoch [116/300], Step [155/172], Loss: 6.7948\n",
      "Epoch [116/300], Step [156/172], Loss: 12.2935\n",
      "Epoch [116/300], Step [157/172], Loss: 9.9455\n",
      "Epoch [116/300], Step [158/172], Loss: 7.5070\n",
      "Epoch [116/300], Step [159/172], Loss: 9.4996\n",
      "Epoch [116/300], Step [160/172], Loss: 9.9314\n",
      "Epoch [116/300], Step [161/172], Loss: 7.1699\n",
      "Epoch [116/300], Step [162/172], Loss: 6.9367\n",
      "Epoch [116/300], Step [163/172], Loss: 6.4981\n",
      "Epoch [116/300], Step [164/172], Loss: 9.3328\n",
      "Epoch [116/300], Step [165/172], Loss: 6.2891\n",
      "Epoch [116/300], Step [166/172], Loss: 5.9734\n",
      "Epoch [116/300], Step [167/172], Loss: 9.4398\n",
      "Epoch [116/300], Step [168/172], Loss: 7.2068\n",
      "Epoch [116/300], Step [169/172], Loss: 6.9870\n",
      "Epoch [116/300], Step [170/172], Loss: 5.5711\n",
      "Epoch [116/300], Step [171/172], Loss: 6.7612\n",
      "Epoch [116/300], Step [172/172], Loss: 5.2948\n",
      "Epoch [117/300], Step [1/172], Loss: 71.7505\n",
      "Epoch [117/300], Step [2/172], Loss: 71.9674\n",
      "Epoch [117/300], Step [3/172], Loss: 66.3297\n",
      "Epoch [117/300], Step [4/172], Loss: 40.7709\n",
      "Epoch [117/300], Step [5/172], Loss: 61.9580\n",
      "Epoch [117/300], Step [6/172], Loss: 19.7845\n",
      "Epoch [117/300], Step [7/172], Loss: 29.9371\n",
      "Epoch [117/300], Step [8/172], Loss: 6.1674\n",
      "Epoch [117/300], Step [9/172], Loss: 40.0628\n",
      "Epoch [117/300], Step [10/172], Loss: 46.0174\n",
      "Epoch [117/300], Step [11/172], Loss: 76.4378\n",
      "Epoch [117/300], Step [12/172], Loss: 81.9175\n",
      "Epoch [117/300], Step [13/172], Loss: 40.9737\n",
      "Epoch [117/300], Step [14/172], Loss: 80.7647\n",
      "Epoch [117/300], Step [15/172], Loss: 69.4393\n",
      "Epoch [117/300], Step [16/172], Loss: 15.1567\n",
      "Epoch [117/300], Step [17/172], Loss: 55.5046\n",
      "Epoch [117/300], Step [18/172], Loss: 62.5774\n",
      "Epoch [117/300], Step [19/172], Loss: 84.7376\n",
      "Epoch [117/300], Step [20/172], Loss: 59.9969\n",
      "Epoch [117/300], Step [21/172], Loss: 94.1328\n",
      "Epoch [117/300], Step [22/172], Loss: 72.8270\n",
      "Epoch [117/300], Step [23/172], Loss: 2.7658\n",
      "Epoch [117/300], Step [24/172], Loss: 67.3881\n",
      "Epoch [117/300], Step [25/172], Loss: 45.2447\n",
      "Epoch [117/300], Step [26/172], Loss: 56.8354\n",
      "Epoch [117/300], Step [27/172], Loss: 71.4504\n",
      "Epoch [117/300], Step [28/172], Loss: 31.9869\n",
      "Epoch [117/300], Step [29/172], Loss: 22.6413\n",
      "Epoch [117/300], Step [30/172], Loss: 80.8291\n",
      "Epoch [117/300], Step [31/172], Loss: 43.7313\n",
      "Epoch [117/300], Step [32/172], Loss: 41.9501\n",
      "Epoch [117/300], Step [33/172], Loss: 71.9226\n",
      "Epoch [117/300], Step [34/172], Loss: 4.7304\n",
      "Epoch [117/300], Step [35/172], Loss: 14.5994\n",
      "Epoch [117/300], Step [36/172], Loss: 20.8122\n",
      "Epoch [117/300], Step [37/172], Loss: 17.5474\n",
      "Epoch [117/300], Step [38/172], Loss: 27.6267\n",
      "Epoch [117/300], Step [39/172], Loss: 41.0981\n",
      "Epoch [117/300], Step [40/172], Loss: 20.2450\n",
      "Epoch [117/300], Step [41/172], Loss: 37.4384\n",
      "Epoch [117/300], Step [42/172], Loss: 41.0595\n",
      "Epoch [117/300], Step [43/172], Loss: 26.5095\n",
      "Epoch [117/300], Step [44/172], Loss: 20.3645\n",
      "Epoch [117/300], Step [45/172], Loss: 22.6204\n",
      "Epoch [117/300], Step [46/172], Loss: 21.1651\n",
      "Epoch [117/300], Step [47/172], Loss: 47.9690\n",
      "Epoch [117/300], Step [48/172], Loss: 53.4162\n",
      "Epoch [117/300], Step [49/172], Loss: 19.7330\n",
      "Epoch [117/300], Step [50/172], Loss: 49.3809\n",
      "Epoch [117/300], Step [51/172], Loss: 7.4514\n",
      "Epoch [117/300], Step [52/172], Loss: 17.4293\n",
      "Epoch [117/300], Step [53/172], Loss: 22.3989\n",
      "Epoch [117/300], Step [54/172], Loss: 11.7140\n",
      "Epoch [117/300], Step [55/172], Loss: 12.1712\n",
      "Epoch [117/300], Step [56/172], Loss: 11.5554\n",
      "Epoch [117/300], Step [57/172], Loss: 18.3491\n",
      "Epoch [117/300], Step [58/172], Loss: 16.5013\n",
      "Epoch [117/300], Step [59/172], Loss: 29.8079\n",
      "Epoch [117/300], Step [60/172], Loss: 41.3489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [117/300], Step [61/172], Loss: 7.7861\n",
      "Epoch [117/300], Step [62/172], Loss: 22.3516\n",
      "Epoch [117/300], Step [63/172], Loss: 8.8582\n",
      "Epoch [117/300], Step [64/172], Loss: 8.4263\n",
      "Epoch [117/300], Step [65/172], Loss: 20.0815\n",
      "Epoch [117/300], Step [66/172], Loss: 5.6203\n",
      "Epoch [117/300], Step [67/172], Loss: 26.5648\n",
      "Epoch [117/300], Step [68/172], Loss: 5.5858\n",
      "Epoch [117/300], Step [69/172], Loss: 56.5672\n",
      "Epoch [117/300], Step [70/172], Loss: 53.6729\n",
      "Epoch [117/300], Step [71/172], Loss: 50.6922\n",
      "Epoch [117/300], Step [72/172], Loss: 54.8976\n",
      "Epoch [117/300], Step [73/172], Loss: 59.8417\n",
      "Epoch [117/300], Step [74/172], Loss: 33.2098\n",
      "Epoch [117/300], Step [75/172], Loss: 31.9587\n",
      "Epoch [117/300], Step [76/172], Loss: 35.7684\n",
      "Epoch [117/300], Step [77/172], Loss: 61.0371\n",
      "Epoch [117/300], Step [78/172], Loss: 46.9135\n",
      "Epoch [117/300], Step [79/172], Loss: 46.3064\n",
      "Epoch [117/300], Step [80/172], Loss: 57.4380\n",
      "Epoch [117/300], Step [81/172], Loss: 40.7832\n",
      "Epoch [117/300], Step [82/172], Loss: 39.3408\n",
      "Epoch [117/300], Step [83/172], Loss: 47.8217\n",
      "Epoch [117/300], Step [84/172], Loss: 36.3143\n",
      "Epoch [117/300], Step [85/172], Loss: 40.6035\n",
      "Epoch [117/300], Step [86/172], Loss: 33.3664\n",
      "Epoch [117/300], Step [87/172], Loss: 27.8061\n",
      "Epoch [117/300], Step [88/172], Loss: 29.2701\n",
      "Epoch [117/300], Step [89/172], Loss: 25.7064\n",
      "Epoch [117/300], Step [90/172], Loss: 24.9810\n",
      "Epoch [117/300], Step [91/172], Loss: 28.1808\n",
      "Epoch [117/300], Step [92/172], Loss: 21.2579\n",
      "Epoch [117/300], Step [93/172], Loss: 21.1177\n",
      "Epoch [117/300], Step [94/172], Loss: 29.7025\n",
      "Epoch [117/300], Step [95/172], Loss: 22.6118\n",
      "Epoch [117/300], Step [96/172], Loss: 18.9699\n",
      "Epoch [117/300], Step [97/172], Loss: 26.0395\n",
      "Epoch [117/300], Step [98/172], Loss: 19.1305\n",
      "Epoch [117/300], Step [99/172], Loss: 18.1201\n",
      "Epoch [117/300], Step [100/172], Loss: 15.4650\n",
      "Epoch [117/300], Step [101/172], Loss: 16.8619\n",
      "Epoch [117/300], Step [102/172], Loss: 16.2204\n",
      "Epoch [117/300], Step [103/172], Loss: 13.3089\n",
      "Epoch [117/300], Step [104/172], Loss: 15.9706\n",
      "Epoch [117/300], Step [105/172], Loss: 17.9140\n",
      "Epoch [117/300], Step [106/172], Loss: 16.4673\n",
      "Epoch [117/300], Step [107/172], Loss: 14.9685\n",
      "Epoch [117/300], Step [108/172], Loss: 16.1646\n",
      "Epoch [117/300], Step [109/172], Loss: 16.6867\n",
      "Epoch [117/300], Step [110/172], Loss: 15.7168\n",
      "Epoch [117/300], Step [111/172], Loss: 13.8799\n",
      "Epoch [117/300], Step [112/172], Loss: 18.1977\n",
      "Epoch [117/300], Step [113/172], Loss: 13.8312\n",
      "Epoch [117/300], Step [114/172], Loss: 13.6851\n",
      "Epoch [117/300], Step [115/172], Loss: 20.6992\n",
      "Epoch [117/300], Step [116/172], Loss: 14.4227\n",
      "Epoch [117/300], Step [117/172], Loss: 11.2894\n",
      "Epoch [117/300], Step [118/172], Loss: 14.6919\n",
      "Epoch [117/300], Step [119/172], Loss: 15.3144\n",
      "Epoch [117/300], Step [120/172], Loss: 10.4881\n",
      "Epoch [117/300], Step [121/172], Loss: 10.4859\n",
      "Epoch [117/300], Step [122/172], Loss: 10.3570\n",
      "Epoch [117/300], Step [123/172], Loss: 10.2358\n",
      "Epoch [117/300], Step [124/172], Loss: 8.2857\n",
      "Epoch [117/300], Step [125/172], Loss: 12.7947\n",
      "Epoch [117/300], Step [126/172], Loss: 10.5033\n",
      "Epoch [117/300], Step [127/172], Loss: 11.5856\n",
      "Epoch [117/300], Step [128/172], Loss: 11.8625\n",
      "Epoch [117/300], Step [129/172], Loss: 8.6520\n",
      "Epoch [117/300], Step [130/172], Loss: 11.5998\n",
      "Epoch [117/300], Step [131/172], Loss: 8.4423\n",
      "Epoch [117/300], Step [132/172], Loss: 8.7733\n",
      "Epoch [117/300], Step [133/172], Loss: 9.5544\n",
      "Epoch [117/300], Step [134/172], Loss: 11.9077\n",
      "Epoch [117/300], Step [135/172], Loss: 8.9170\n",
      "Epoch [117/300], Step [136/172], Loss: 8.4009\n",
      "Epoch [117/300], Step [137/172], Loss: 9.7491\n",
      "Epoch [117/300], Step [138/172], Loss: 7.8981\n",
      "Epoch [117/300], Step [139/172], Loss: 9.6839\n",
      "Epoch [117/300], Step [140/172], Loss: 9.4546\n",
      "Epoch [117/300], Step [141/172], Loss: 11.0705\n",
      "Epoch [117/300], Step [142/172], Loss: 13.5564\n",
      "Epoch [117/300], Step [143/172], Loss: 9.7237\n",
      "Epoch [117/300], Step [144/172], Loss: 8.8311\n",
      "Epoch [117/300], Step [145/172], Loss: 9.7776\n",
      "Epoch [117/300], Step [146/172], Loss: 9.5975\n",
      "Epoch [117/300], Step [147/172], Loss: 5.3841\n",
      "Epoch [117/300], Step [148/172], Loss: 6.5080\n",
      "Epoch [117/300], Step [149/172], Loss: 7.5793\n",
      "Epoch [117/300], Step [150/172], Loss: 7.5881\n",
      "Epoch [117/300], Step [151/172], Loss: 6.8203\n",
      "Epoch [117/300], Step [152/172], Loss: 7.7586\n",
      "Epoch [117/300], Step [153/172], Loss: 6.9787\n",
      "Epoch [117/300], Step [154/172], Loss: 7.7727\n",
      "Epoch [117/300], Step [155/172], Loss: 6.7670\n",
      "Epoch [117/300], Step [156/172], Loss: 12.2785\n",
      "Epoch [117/300], Step [157/172], Loss: 9.9078\n",
      "Epoch [117/300], Step [158/172], Loss: 7.4804\n",
      "Epoch [117/300], Step [159/172], Loss: 9.5107\n",
      "Epoch [117/300], Step [160/172], Loss: 9.9231\n",
      "Epoch [117/300], Step [161/172], Loss: 7.1221\n",
      "Epoch [117/300], Step [162/172], Loss: 6.8858\n",
      "Epoch [117/300], Step [163/172], Loss: 6.4747\n",
      "Epoch [117/300], Step [164/172], Loss: 9.2649\n",
      "Epoch [117/300], Step [165/172], Loss: 6.2648\n",
      "Epoch [117/300], Step [166/172], Loss: 5.9400\n",
      "Epoch [117/300], Step [167/172], Loss: 9.4428\n",
      "Epoch [117/300], Step [168/172], Loss: 7.2205\n",
      "Epoch [117/300], Step [169/172], Loss: 6.9711\n",
      "Epoch [117/300], Step [170/172], Loss: 5.5509\n",
      "Epoch [117/300], Step [171/172], Loss: 6.7681\n",
      "Epoch [117/300], Step [172/172], Loss: 5.2971\n",
      "Epoch [118/300], Step [1/172], Loss: 71.1761\n",
      "Epoch [118/300], Step [2/172], Loss: 71.3751\n",
      "Epoch [118/300], Step [3/172], Loss: 65.7626\n",
      "Epoch [118/300], Step [4/172], Loss: 40.5115\n",
      "Epoch [118/300], Step [5/172], Loss: 61.8183\n",
      "Epoch [118/300], Step [6/172], Loss: 19.8967\n",
      "Epoch [118/300], Step [7/172], Loss: 30.1375\n",
      "Epoch [118/300], Step [8/172], Loss: 6.2414\n",
      "Epoch [118/300], Step [9/172], Loss: 39.8456\n",
      "Epoch [118/300], Step [10/172], Loss: 45.8765\n",
      "Epoch [118/300], Step [11/172], Loss: 76.1542\n",
      "Epoch [118/300], Step [12/172], Loss: 81.9556\n",
      "Epoch [118/300], Step [13/172], Loss: 41.0040\n",
      "Epoch [118/300], Step [14/172], Loss: 80.6700\n",
      "Epoch [118/300], Step [15/172], Loss: 69.2810\n",
      "Epoch [118/300], Step [16/172], Loss: 15.0661\n",
      "Epoch [118/300], Step [17/172], Loss: 55.5114\n",
      "Epoch [118/300], Step [18/172], Loss: 62.7620\n",
      "Epoch [118/300], Step [19/172], Loss: 84.9805\n",
      "Epoch [118/300], Step [20/172], Loss: 59.7759\n",
      "Epoch [118/300], Step [21/172], Loss: 94.2622\n",
      "Epoch [118/300], Step [22/172], Loss: 72.9529\n",
      "Epoch [118/300], Step [23/172], Loss: 2.6861\n",
      "Epoch [118/300], Step [24/172], Loss: 67.5243\n",
      "Epoch [118/300], Step [25/172], Loss: 45.1079\n",
      "Epoch [118/300], Step [26/172], Loss: 56.7680\n",
      "Epoch [118/300], Step [27/172], Loss: 71.6892\n",
      "Epoch [118/300], Step [28/172], Loss: 32.0003\n",
      "Epoch [118/300], Step [29/172], Loss: 22.5736\n",
      "Epoch [118/300], Step [30/172], Loss: 80.7663\n",
      "Epoch [118/300], Step [31/172], Loss: 43.6911\n",
      "Epoch [118/300], Step [32/172], Loss: 42.0465\n",
      "Epoch [118/300], Step [33/172], Loss: 71.9440\n",
      "Epoch [118/300], Step [34/172], Loss: 4.6222\n",
      "Epoch [118/300], Step [35/172], Loss: 14.6003\n",
      "Epoch [118/300], Step [36/172], Loss: 20.4904\n",
      "Epoch [118/300], Step [37/172], Loss: 17.5449\n",
      "Epoch [118/300], Step [38/172], Loss: 27.6705\n",
      "Epoch [118/300], Step [39/172], Loss: 40.9429\n",
      "Epoch [118/300], Step [40/172], Loss: 20.1768\n",
      "Epoch [118/300], Step [41/172], Loss: 37.3242\n",
      "Epoch [118/300], Step [42/172], Loss: 40.7916\n",
      "Epoch [118/300], Step [43/172], Loss: 26.4626\n",
      "Epoch [118/300], Step [44/172], Loss: 20.2289\n",
      "Epoch [118/300], Step [45/172], Loss: 22.6873\n",
      "Epoch [118/300], Step [46/172], Loss: 20.9204\n",
      "Epoch [118/300], Step [47/172], Loss: 47.7239\n",
      "Epoch [118/300], Step [48/172], Loss: 53.3107\n",
      "Epoch [118/300], Step [49/172], Loss: 19.7208\n",
      "Epoch [118/300], Step [50/172], Loss: 49.3999\n",
      "Epoch [118/300], Step [51/172], Loss: 7.4849\n",
      "Epoch [118/300], Step [52/172], Loss: 17.3749\n",
      "Epoch [118/300], Step [53/172], Loss: 22.3563\n",
      "Epoch [118/300], Step [54/172], Loss: 11.6186\n",
      "Epoch [118/300], Step [55/172], Loss: 12.0754\n",
      "Epoch [118/300], Step [56/172], Loss: 11.7023\n",
      "Epoch [118/300], Step [57/172], Loss: 18.2159\n",
      "Epoch [118/300], Step [58/172], Loss: 16.4378\n",
      "Epoch [118/300], Step [59/172], Loss: 29.7236\n",
      "Epoch [118/300], Step [60/172], Loss: 41.0977\n",
      "Epoch [118/300], Step [61/172], Loss: 7.7235\n",
      "Epoch [118/300], Step [62/172], Loss: 22.3285\n",
      "Epoch [118/300], Step [63/172], Loss: 8.9541\n",
      "Epoch [118/300], Step [64/172], Loss: 8.4821\n",
      "Epoch [118/300], Step [65/172], Loss: 20.0868\n",
      "Epoch [118/300], Step [66/172], Loss: 5.6622\n",
      "Epoch [118/300], Step [67/172], Loss: 26.4338\n",
      "Epoch [118/300], Step [68/172], Loss: 5.3958\n",
      "Epoch [118/300], Step [69/172], Loss: 56.0746\n",
      "Epoch [118/300], Step [70/172], Loss: 53.4570\n",
      "Epoch [118/300], Step [71/172], Loss: 50.3178\n",
      "Epoch [118/300], Step [72/172], Loss: 54.5953\n",
      "Epoch [118/300], Step [73/172], Loss: 59.5491\n",
      "Epoch [118/300], Step [74/172], Loss: 32.9444\n",
      "Epoch [118/300], Step [75/172], Loss: 31.8472\n",
      "Epoch [118/300], Step [76/172], Loss: 35.5466\n",
      "Epoch [118/300], Step [77/172], Loss: 60.7360\n",
      "Epoch [118/300], Step [78/172], Loss: 46.6064\n",
      "Epoch [118/300], Step [79/172], Loss: 45.8641\n",
      "Epoch [118/300], Step [80/172], Loss: 57.1714\n",
      "Epoch [118/300], Step [81/172], Loss: 40.4946\n",
      "Epoch [118/300], Step [82/172], Loss: 39.2826\n",
      "Epoch [118/300], Step [83/172], Loss: 47.4907\n",
      "Epoch [118/300], Step [84/172], Loss: 36.1029\n",
      "Epoch [118/300], Step [85/172], Loss: 40.3443\n",
      "Epoch [118/300], Step [86/172], Loss: 33.2374\n",
      "Epoch [118/300], Step [87/172], Loss: 27.6807\n",
      "Epoch [118/300], Step [88/172], Loss: 29.1457\n",
      "Epoch [118/300], Step [89/172], Loss: 25.6439\n",
      "Epoch [118/300], Step [90/172], Loss: 24.9254\n",
      "Epoch [118/300], Step [91/172], Loss: 28.0642\n",
      "Epoch [118/300], Step [92/172], Loss: 21.2137\n",
      "Epoch [118/300], Step [93/172], Loss: 21.0373\n",
      "Epoch [118/300], Step [94/172], Loss: 29.6313\n",
      "Epoch [118/300], Step [95/172], Loss: 22.5461\n",
      "Epoch [118/300], Step [96/172], Loss: 18.9532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [118/300], Step [97/172], Loss: 26.0103\n",
      "Epoch [118/300], Step [98/172], Loss: 19.0520\n",
      "Epoch [118/300], Step [99/172], Loss: 18.0872\n",
      "Epoch [118/300], Step [100/172], Loss: 15.3967\n",
      "Epoch [118/300], Step [101/172], Loss: 16.8139\n",
      "Epoch [118/300], Step [102/172], Loss: 16.2418\n",
      "Epoch [118/300], Step [103/172], Loss: 13.2558\n",
      "Epoch [118/300], Step [104/172], Loss: 15.9637\n",
      "Epoch [118/300], Step [105/172], Loss: 17.9258\n",
      "Epoch [118/300], Step [106/172], Loss: 16.4348\n",
      "Epoch [118/300], Step [107/172], Loss: 14.8986\n",
      "Epoch [118/300], Step [108/172], Loss: 16.1332\n",
      "Epoch [118/300], Step [109/172], Loss: 16.6327\n",
      "Epoch [118/300], Step [110/172], Loss: 15.6845\n",
      "Epoch [118/300], Step [111/172], Loss: 13.8631\n",
      "Epoch [118/300], Step [112/172], Loss: 18.2189\n",
      "Epoch [118/300], Step [113/172], Loss: 13.7809\n",
      "Epoch [118/300], Step [114/172], Loss: 13.6803\n",
      "Epoch [118/300], Step [115/172], Loss: 20.6656\n",
      "Epoch [118/300], Step [116/172], Loss: 14.4129\n",
      "Epoch [118/300], Step [117/172], Loss: 11.2666\n",
      "Epoch [118/300], Step [118/172], Loss: 14.6541\n",
      "Epoch [118/300], Step [119/172], Loss: 15.2612\n",
      "Epoch [118/300], Step [120/172], Loss: 10.4177\n",
      "Epoch [118/300], Step [121/172], Loss: 10.4502\n",
      "Epoch [118/300], Step [122/172], Loss: 10.3636\n",
      "Epoch [118/300], Step [123/172], Loss: 10.2327\n",
      "Epoch [118/300], Step [124/172], Loss: 8.2741\n",
      "Epoch [118/300], Step [125/172], Loss: 12.7702\n",
      "Epoch [118/300], Step [126/172], Loss: 10.4894\n",
      "Epoch [118/300], Step [127/172], Loss: 11.5658\n",
      "Epoch [118/300], Step [128/172], Loss: 11.8253\n",
      "Epoch [118/300], Step [129/172], Loss: 8.6262\n",
      "Epoch [118/300], Step [130/172], Loss: 11.5919\n",
      "Epoch [118/300], Step [131/172], Loss: 8.4004\n",
      "Epoch [118/300], Step [132/172], Loss: 8.7553\n",
      "Epoch [118/300], Step [133/172], Loss: 9.5426\n",
      "Epoch [118/300], Step [134/172], Loss: 11.8611\n",
      "Epoch [118/300], Step [135/172], Loss: 8.8771\n",
      "Epoch [118/300], Step [136/172], Loss: 8.4145\n",
      "Epoch [118/300], Step [137/172], Loss: 9.7438\n",
      "Epoch [118/300], Step [138/172], Loss: 7.8757\n",
      "Epoch [118/300], Step [139/172], Loss: 9.6427\n",
      "Epoch [118/300], Step [140/172], Loss: 9.4285\n",
      "Epoch [118/300], Step [141/172], Loss: 11.0122\n",
      "Epoch [118/300], Step [142/172], Loss: 13.5350\n",
      "Epoch [118/300], Step [143/172], Loss: 9.7380\n",
      "Epoch [118/300], Step [144/172], Loss: 8.8205\n",
      "Epoch [118/300], Step [145/172], Loss: 9.7432\n",
      "Epoch [118/300], Step [146/172], Loss: 9.5670\n",
      "Epoch [118/300], Step [147/172], Loss: 5.3613\n",
      "Epoch [118/300], Step [148/172], Loss: 6.4842\n",
      "Epoch [118/300], Step [149/172], Loss: 7.5340\n",
      "Epoch [118/300], Step [150/172], Loss: 7.5499\n",
      "Epoch [118/300], Step [151/172], Loss: 6.7717\n",
      "Epoch [118/300], Step [152/172], Loss: 7.7300\n",
      "Epoch [118/300], Step [153/172], Loss: 6.9468\n",
      "Epoch [118/300], Step [154/172], Loss: 7.7614\n",
      "Epoch [118/300], Step [155/172], Loss: 6.7357\n",
      "Epoch [118/300], Step [156/172], Loss: 12.2781\n",
      "Epoch [118/300], Step [157/172], Loss: 9.9018\n",
      "Epoch [118/300], Step [158/172], Loss: 7.4564\n",
      "Epoch [118/300], Step [159/172], Loss: 9.4939\n",
      "Epoch [118/300], Step [160/172], Loss: 9.9201\n",
      "Epoch [118/300], Step [161/172], Loss: 7.1015\n",
      "Epoch [118/300], Step [162/172], Loss: 6.8341\n",
      "Epoch [118/300], Step [163/172], Loss: 6.4601\n",
      "Epoch [118/300], Step [164/172], Loss: 9.2555\n",
      "Epoch [118/300], Step [165/172], Loss: 6.2423\n",
      "Epoch [118/300], Step [166/172], Loss: 5.9028\n",
      "Epoch [118/300], Step [167/172], Loss: 9.4483\n",
      "Epoch [118/300], Step [168/172], Loss: 7.1751\n",
      "Epoch [118/300], Step [169/172], Loss: 6.9352\n",
      "Epoch [118/300], Step [170/172], Loss: 5.5101\n",
      "Epoch [118/300], Step [171/172], Loss: 6.7558\n",
      "Epoch [118/300], Step [172/172], Loss: 5.2585\n",
      "Epoch [119/300], Step [1/172], Loss: 70.6998\n",
      "Epoch [119/300], Step [2/172], Loss: 70.8827\n",
      "Epoch [119/300], Step [3/172], Loss: 65.3439\n",
      "Epoch [119/300], Step [4/172], Loss: 40.1832\n",
      "Epoch [119/300], Step [5/172], Loss: 61.4799\n",
      "Epoch [119/300], Step [6/172], Loss: 19.7310\n",
      "Epoch [119/300], Step [7/172], Loss: 29.6577\n",
      "Epoch [119/300], Step [8/172], Loss: 6.0013\n",
      "Epoch [119/300], Step [9/172], Loss: 39.5904\n",
      "Epoch [119/300], Step [10/172], Loss: 45.8076\n",
      "Epoch [119/300], Step [11/172], Loss: 75.6440\n",
      "Epoch [119/300], Step [12/172], Loss: 81.7613\n",
      "Epoch [119/300], Step [13/172], Loss: 40.7545\n",
      "Epoch [119/300], Step [14/172], Loss: 79.9809\n",
      "Epoch [119/300], Step [15/172], Loss: 68.9678\n",
      "Epoch [119/300], Step [16/172], Loss: 15.2630\n",
      "Epoch [119/300], Step [17/172], Loss: 55.1374\n",
      "Epoch [119/300], Step [18/172], Loss: 62.7101\n",
      "Epoch [119/300], Step [19/172], Loss: 84.8326\n",
      "Epoch [119/300], Step [20/172], Loss: 58.8915\n",
      "Epoch [119/300], Step [21/172], Loss: 93.8796\n",
      "Epoch [119/300], Step [22/172], Loss: 72.5967\n",
      "Epoch [119/300], Step [23/172], Loss: 2.7124\n",
      "Epoch [119/300], Step [24/172], Loss: 67.3649\n",
      "Epoch [119/300], Step [25/172], Loss: 45.2326\n",
      "Epoch [119/300], Step [26/172], Loss: 56.8027\n",
      "Epoch [119/300], Step [27/172], Loss: 71.4004\n",
      "Epoch [119/300], Step [28/172], Loss: 31.7104\n",
      "Epoch [119/300], Step [29/172], Loss: 22.4319\n",
      "Epoch [119/300], Step [30/172], Loss: 80.7034\n",
      "Epoch [119/300], Step [31/172], Loss: 43.7754\n",
      "Epoch [119/300], Step [32/172], Loss: 42.1546\n",
      "Epoch [119/300], Step [33/172], Loss: 72.0566\n",
      "Epoch [119/300], Step [34/172], Loss: 4.4827\n",
      "Epoch [119/300], Step [35/172], Loss: 14.5074\n",
      "Epoch [119/300], Step [36/172], Loss: 20.5488\n",
      "Epoch [119/300], Step [37/172], Loss: 17.5630\n",
      "Epoch [119/300], Step [38/172], Loss: 27.8073\n",
      "Epoch [119/300], Step [39/172], Loss: 40.8168\n",
      "Epoch [119/300], Step [40/172], Loss: 20.2369\n",
      "Epoch [119/300], Step [41/172], Loss: 37.3857\n",
      "Epoch [119/300], Step [42/172], Loss: 40.9586\n",
      "Epoch [119/300], Step [43/172], Loss: 26.5765\n",
      "Epoch [119/300], Step [44/172], Loss: 20.2402\n",
      "Epoch [119/300], Step [45/172], Loss: 22.8028\n",
      "Epoch [119/300], Step [46/172], Loss: 20.8966\n",
      "Epoch [119/300], Step [47/172], Loss: 47.7825\n",
      "Epoch [119/300], Step [48/172], Loss: 53.5712\n",
      "Epoch [119/300], Step [49/172], Loss: 19.7500\n",
      "Epoch [119/300], Step [50/172], Loss: 49.1832\n",
      "Epoch [119/300], Step [51/172], Loss: 7.5426\n",
      "Epoch [119/300], Step [52/172], Loss: 17.4380\n",
      "Epoch [119/300], Step [53/172], Loss: 22.3436\n",
      "Epoch [119/300], Step [54/172], Loss: 11.6663\n",
      "Epoch [119/300], Step [55/172], Loss: 12.1152\n",
      "Epoch [119/300], Step [56/172], Loss: 11.8952\n",
      "Epoch [119/300], Step [57/172], Loss: 18.1359\n",
      "Epoch [119/300], Step [58/172], Loss: 16.2307\n",
      "Epoch [119/300], Step [59/172], Loss: 29.7180\n",
      "Epoch [119/300], Step [60/172], Loss: 40.6729\n",
      "Epoch [119/300], Step [61/172], Loss: 7.6269\n",
      "Epoch [119/300], Step [62/172], Loss: 22.2757\n",
      "Epoch [119/300], Step [63/172], Loss: 9.0173\n",
      "Epoch [119/300], Step [64/172], Loss: 8.5575\n",
      "Epoch [119/300], Step [65/172], Loss: 20.0948\n",
      "Epoch [119/300], Step [66/172], Loss: 5.6192\n",
      "Epoch [119/300], Step [67/172], Loss: 26.2614\n",
      "Epoch [119/300], Step [68/172], Loss: 5.4089\n",
      "Epoch [119/300], Step [69/172], Loss: 55.4641\n",
      "Epoch [119/300], Step [70/172], Loss: 53.0826\n",
      "Epoch [119/300], Step [71/172], Loss: 50.1689\n",
      "Epoch [119/300], Step [72/172], Loss: 54.2566\n",
      "Epoch [119/300], Step [73/172], Loss: 59.2950\n",
      "Epoch [119/300], Step [74/172], Loss: 32.7884\n",
      "Epoch [119/300], Step [75/172], Loss: 31.8653\n",
      "Epoch [119/300], Step [76/172], Loss: 35.3871\n",
      "Epoch [119/300], Step [77/172], Loss: 60.6706\n",
      "Epoch [119/300], Step [78/172], Loss: 46.4809\n",
      "Epoch [119/300], Step [79/172], Loss: 45.8198\n",
      "Epoch [119/300], Step [80/172], Loss: 57.1212\n",
      "Epoch [119/300], Step [81/172], Loss: 40.4288\n",
      "Epoch [119/300], Step [82/172], Loss: 39.2573\n",
      "Epoch [119/300], Step [83/172], Loss: 47.3721\n",
      "Epoch [119/300], Step [84/172], Loss: 36.0989\n",
      "Epoch [119/300], Step [85/172], Loss: 40.3454\n",
      "Epoch [119/300], Step [86/172], Loss: 33.2858\n",
      "Epoch [119/300], Step [87/172], Loss: 27.6909\n",
      "Epoch [119/300], Step [88/172], Loss: 29.2462\n",
      "Epoch [119/300], Step [89/172], Loss: 25.7288\n",
      "Epoch [119/300], Step [90/172], Loss: 24.9388\n",
      "Epoch [119/300], Step [91/172], Loss: 28.0896\n",
      "Epoch [119/300], Step [92/172], Loss: 21.2618\n",
      "Epoch [119/300], Step [93/172], Loss: 21.0635\n",
      "Epoch [119/300], Step [94/172], Loss: 29.6125\n",
      "Epoch [119/300], Step [95/172], Loss: 22.6342\n",
      "Epoch [119/300], Step [96/172], Loss: 19.0234\n",
      "Epoch [119/300], Step [97/172], Loss: 26.1555\n",
      "Epoch [119/300], Step [98/172], Loss: 19.1178\n",
      "Epoch [119/300], Step [99/172], Loss: 18.1519\n",
      "Epoch [119/300], Step [100/172], Loss: 15.4279\n",
      "Epoch [119/300], Step [101/172], Loss: 16.8561\n",
      "Epoch [119/300], Step [102/172], Loss: 16.3099\n",
      "Epoch [119/300], Step [103/172], Loss: 13.2485\n",
      "Epoch [119/300], Step [104/172], Loss: 16.0367\n",
      "Epoch [119/300], Step [105/172], Loss: 17.9462\n",
      "Epoch [119/300], Step [106/172], Loss: 16.4060\n",
      "Epoch [119/300], Step [107/172], Loss: 14.9388\n",
      "Epoch [119/300], Step [108/172], Loss: 16.1653\n",
      "Epoch [119/300], Step [109/172], Loss: 16.6383\n",
      "Epoch [119/300], Step [110/172], Loss: 15.7095\n",
      "Epoch [119/300], Step [111/172], Loss: 13.8625\n",
      "Epoch [119/300], Step [112/172], Loss: 18.1895\n",
      "Epoch [119/300], Step [113/172], Loss: 13.7350\n",
      "Epoch [119/300], Step [114/172], Loss: 13.6906\n",
      "Epoch [119/300], Step [115/172], Loss: 20.6428\n",
      "Epoch [119/300], Step [116/172], Loss: 14.3835\n",
      "Epoch [119/300], Step [117/172], Loss: 11.2207\n",
      "Epoch [119/300], Step [118/172], Loss: 14.6404\n",
      "Epoch [119/300], Step [119/172], Loss: 15.2565\n",
      "Epoch [119/300], Step [120/172], Loss: 10.3947\n",
      "Epoch [119/300], Step [121/172], Loss: 10.4456\n",
      "Epoch [119/300], Step [122/172], Loss: 10.3788\n",
      "Epoch [119/300], Step [123/172], Loss: 10.2190\n",
      "Epoch [119/300], Step [124/172], Loss: 8.2594\n",
      "Epoch [119/300], Step [125/172], Loss: 12.7640\n",
      "Epoch [119/300], Step [126/172], Loss: 10.4638\n",
      "Epoch [119/300], Step [127/172], Loss: 11.5417\n",
      "Epoch [119/300], Step [128/172], Loss: 11.7845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119/300], Step [129/172], Loss: 8.6083\n",
      "Epoch [119/300], Step [130/172], Loss: 11.5696\n",
      "Epoch [119/300], Step [131/172], Loss: 8.3766\n",
      "Epoch [119/300], Step [132/172], Loss: 8.7316\n",
      "Epoch [119/300], Step [133/172], Loss: 9.5568\n",
      "Epoch [119/300], Step [134/172], Loss: 11.8511\n",
      "Epoch [119/300], Step [135/172], Loss: 8.8740\n",
      "Epoch [119/300], Step [136/172], Loss: 8.4017\n",
      "Epoch [119/300], Step [137/172], Loss: 9.7167\n",
      "Epoch [119/300], Step [138/172], Loss: 7.8439\n",
      "Epoch [119/300], Step [139/172], Loss: 9.6132\n",
      "Epoch [119/300], Step [140/172], Loss: 9.4156\n",
      "Epoch [119/300], Step [141/172], Loss: 10.9974\n",
      "Epoch [119/300], Step [142/172], Loss: 13.5334\n",
      "Epoch [119/300], Step [143/172], Loss: 9.7377\n",
      "Epoch [119/300], Step [144/172], Loss: 8.8451\n",
      "Epoch [119/300], Step [145/172], Loss: 9.7195\n",
      "Epoch [119/300], Step [146/172], Loss: 9.5828\n",
      "Epoch [119/300], Step [147/172], Loss: 5.3536\n",
      "Epoch [119/300], Step [148/172], Loss: 6.4651\n",
      "Epoch [119/300], Step [149/172], Loss: 7.5110\n",
      "Epoch [119/300], Step [150/172], Loss: 7.5263\n",
      "Epoch [119/300], Step [151/172], Loss: 6.7493\n",
      "Epoch [119/300], Step [152/172], Loss: 7.7122\n",
      "Epoch [119/300], Step [153/172], Loss: 6.9459\n",
      "Epoch [119/300], Step [154/172], Loss: 7.7473\n",
      "Epoch [119/300], Step [155/172], Loss: 6.7408\n",
      "Epoch [119/300], Step [156/172], Loss: 12.2963\n",
      "Epoch [119/300], Step [157/172], Loss: 9.8956\n",
      "Epoch [119/300], Step [158/172], Loss: 7.4471\n",
      "Epoch [119/300], Step [159/172], Loss: 9.4896\n",
      "Epoch [119/300], Step [160/172], Loss: 9.9431\n",
      "Epoch [119/300], Step [161/172], Loss: 7.1167\n",
      "Epoch [119/300], Step [162/172], Loss: 6.8156\n",
      "Epoch [119/300], Step [163/172], Loss: 6.4563\n",
      "Epoch [119/300], Step [164/172], Loss: 9.2331\n",
      "Epoch [119/300], Step [165/172], Loss: 6.2371\n",
      "Epoch [119/300], Step [166/172], Loss: 5.8966\n",
      "Epoch [119/300], Step [167/172], Loss: 9.4848\n",
      "Epoch [119/300], Step [168/172], Loss: 7.1921\n",
      "Epoch [119/300], Step [169/172], Loss: 6.9171\n",
      "Epoch [119/300], Step [170/172], Loss: 5.5161\n",
      "Epoch [119/300], Step [171/172], Loss: 6.7815\n",
      "Epoch [119/300], Step [172/172], Loss: 5.2912\n",
      "Epoch [120/300], Step [1/172], Loss: 70.2223\n",
      "Epoch [120/300], Step [2/172], Loss: 70.4234\n",
      "Epoch [120/300], Step [3/172], Loss: 65.0815\n",
      "Epoch [120/300], Step [4/172], Loss: 39.8396\n",
      "Epoch [120/300], Step [5/172], Loss: 61.3271\n",
      "Epoch [120/300], Step [6/172], Loss: 19.6334\n",
      "Epoch [120/300], Step [7/172], Loss: 29.7814\n",
      "Epoch [120/300], Step [8/172], Loss: 5.9979\n",
      "Epoch [120/300], Step [9/172], Loss: 39.4027\n",
      "Epoch [120/300], Step [10/172], Loss: 45.5707\n",
      "Epoch [120/300], Step [11/172], Loss: 75.2071\n",
      "Epoch [120/300], Step [12/172], Loss: 81.6052\n",
      "Epoch [120/300], Step [13/172], Loss: 40.7919\n",
      "Epoch [120/300], Step [14/172], Loss: 79.6480\n",
      "Epoch [120/300], Step [15/172], Loss: 68.8053\n",
      "Epoch [120/300], Step [16/172], Loss: 15.0233\n",
      "Epoch [120/300], Step [17/172], Loss: 54.9411\n",
      "Epoch [120/300], Step [18/172], Loss: 62.6556\n",
      "Epoch [120/300], Step [19/172], Loss: 84.7935\n",
      "Epoch [120/300], Step [20/172], Loss: 58.3180\n",
      "Epoch [120/300], Step [21/172], Loss: 93.5806\n",
      "Epoch [120/300], Step [22/172], Loss: 72.3891\n",
      "Epoch [120/300], Step [23/172], Loss: 2.6621\n",
      "Epoch [120/300], Step [24/172], Loss: 66.9931\n",
      "Epoch [120/300], Step [25/172], Loss: 44.9259\n",
      "Epoch [120/300], Step [26/172], Loss: 56.4238\n",
      "Epoch [120/300], Step [27/172], Loss: 71.0871\n",
      "Epoch [120/300], Step [28/172], Loss: 31.3577\n",
      "Epoch [120/300], Step [29/172], Loss: 22.1975\n",
      "Epoch [120/300], Step [30/172], Loss: 80.1897\n",
      "Epoch [120/300], Step [31/172], Loss: 43.4288\n",
      "Epoch [120/300], Step [32/172], Loss: 41.9549\n",
      "Epoch [120/300], Step [33/172], Loss: 71.7664\n",
      "Epoch [120/300], Step [34/172], Loss: 4.3991\n",
      "Epoch [120/300], Step [35/172], Loss: 14.4208\n",
      "Epoch [120/300], Step [36/172], Loss: 20.4414\n",
      "Epoch [120/300], Step [37/172], Loss: 17.4889\n",
      "Epoch [120/300], Step [38/172], Loss: 27.6894\n",
      "Epoch [120/300], Step [39/172], Loss: 40.5199\n",
      "Epoch [120/300], Step [40/172], Loss: 20.0603\n",
      "Epoch [120/300], Step [41/172], Loss: 37.1744\n",
      "Epoch [120/300], Step [42/172], Loss: 40.4963\n",
      "Epoch [120/300], Step [43/172], Loss: 26.4228\n",
      "Epoch [120/300], Step [44/172], Loss: 20.1146\n",
      "Epoch [120/300], Step [45/172], Loss: 22.6872\n",
      "Epoch [120/300], Step [46/172], Loss: 20.6714\n",
      "Epoch [120/300], Step [47/172], Loss: 47.4052\n",
      "Epoch [120/300], Step [48/172], Loss: 53.5330\n",
      "Epoch [120/300], Step [49/172], Loss: 19.7356\n",
      "Epoch [120/300], Step [50/172], Loss: 49.2182\n",
      "Epoch [120/300], Step [51/172], Loss: 7.5274\n",
      "Epoch [120/300], Step [52/172], Loss: 17.3439\n",
      "Epoch [120/300], Step [53/172], Loss: 22.3306\n",
      "Epoch [120/300], Step [54/172], Loss: 11.6746\n",
      "Epoch [120/300], Step [55/172], Loss: 12.0616\n",
      "Epoch [120/300], Step [56/172], Loss: 11.9569\n",
      "Epoch [120/300], Step [57/172], Loss: 18.1551\n",
      "Epoch [120/300], Step [58/172], Loss: 16.3531\n",
      "Epoch [120/300], Step [59/172], Loss: 29.6546\n",
      "Epoch [120/300], Step [60/172], Loss: 40.6776\n",
      "Epoch [120/300], Step [61/172], Loss: 7.5914\n",
      "Epoch [120/300], Step [62/172], Loss: 22.3561\n",
      "Epoch [120/300], Step [63/172], Loss: 9.0081\n",
      "Epoch [120/300], Step [64/172], Loss: 8.5881\n",
      "Epoch [120/300], Step [65/172], Loss: 20.0503\n",
      "Epoch [120/300], Step [66/172], Loss: 5.6659\n",
      "Epoch [120/300], Step [67/172], Loss: 26.3200\n",
      "Epoch [120/300], Step [68/172], Loss: 5.4016\n",
      "Epoch [120/300], Step [69/172], Loss: 54.8883\n",
      "Epoch [120/300], Step [70/172], Loss: 52.8741\n",
      "Epoch [120/300], Step [71/172], Loss: 49.9412\n",
      "Epoch [120/300], Step [72/172], Loss: 54.0547\n",
      "Epoch [120/300], Step [73/172], Loss: 59.1345\n",
      "Epoch [120/300], Step [74/172], Loss: 32.6741\n",
      "Epoch [120/300], Step [75/172], Loss: 31.7957\n",
      "Epoch [120/300], Step [76/172], Loss: 35.3000\n",
      "Epoch [120/300], Step [77/172], Loss: 60.5900\n",
      "Epoch [120/300], Step [78/172], Loss: 46.3679\n",
      "Epoch [120/300], Step [79/172], Loss: 45.6474\n",
      "Epoch [120/300], Step [80/172], Loss: 57.1378\n",
      "Epoch [120/300], Step [81/172], Loss: 40.3071\n",
      "Epoch [120/300], Step [82/172], Loss: 39.2252\n",
      "Epoch [120/300], Step [83/172], Loss: 47.3247\n",
      "Epoch [120/300], Step [84/172], Loss: 36.0523\n",
      "Epoch [120/300], Step [85/172], Loss: 40.3751\n",
      "Epoch [120/300], Step [86/172], Loss: 33.3233\n",
      "Epoch [120/300], Step [87/172], Loss: 27.6525\n",
      "Epoch [120/300], Step [88/172], Loss: 29.1351\n",
      "Epoch [120/300], Step [89/172], Loss: 25.6722\n",
      "Epoch [120/300], Step [90/172], Loss: 24.9162\n",
      "Epoch [120/300], Step [91/172], Loss: 27.9918\n",
      "Epoch [120/300], Step [92/172], Loss: 21.2176\n",
      "Epoch [120/300], Step [93/172], Loss: 21.0452\n",
      "Epoch [120/300], Step [94/172], Loss: 29.5813\n",
      "Epoch [120/300], Step [95/172], Loss: 22.5835\n",
      "Epoch [120/300], Step [96/172], Loss: 19.0033\n",
      "Epoch [120/300], Step [97/172], Loss: 26.1195\n",
      "Epoch [120/300], Step [98/172], Loss: 19.0322\n",
      "Epoch [120/300], Step [99/172], Loss: 18.1060\n",
      "Epoch [120/300], Step [100/172], Loss: 15.3439\n",
      "Epoch [120/300], Step [101/172], Loss: 16.7960\n",
      "Epoch [120/300], Step [102/172], Loss: 16.2961\n",
      "Epoch [120/300], Step [103/172], Loss: 13.1620\n",
      "Epoch [120/300], Step [104/172], Loss: 15.9742\n",
      "Epoch [120/300], Step [105/172], Loss: 17.8955\n",
      "Epoch [120/300], Step [106/172], Loss: 16.3277\n",
      "Epoch [120/300], Step [107/172], Loss: 14.8902\n",
      "Epoch [120/300], Step [108/172], Loss: 16.1061\n",
      "Epoch [120/300], Step [109/172], Loss: 16.5570\n",
      "Epoch [120/300], Step [110/172], Loss: 15.6469\n",
      "Epoch [120/300], Step [111/172], Loss: 13.8058\n",
      "Epoch [120/300], Step [112/172], Loss: 18.2002\n",
      "Epoch [120/300], Step [113/172], Loss: 13.6514\n",
      "Epoch [120/300], Step [114/172], Loss: 13.6349\n",
      "Epoch [120/300], Step [115/172], Loss: 20.5749\n",
      "Epoch [120/300], Step [116/172], Loss: 14.3456\n",
      "Epoch [120/300], Step [117/172], Loss: 11.1743\n",
      "Epoch [120/300], Step [118/172], Loss: 14.6327\n",
      "Epoch [120/300], Step [119/172], Loss: 15.2188\n",
      "Epoch [120/300], Step [120/172], Loss: 10.3176\n",
      "Epoch [120/300], Step [121/172], Loss: 10.3548\n",
      "Epoch [120/300], Step [122/172], Loss: 10.4175\n",
      "Epoch [120/300], Step [123/172], Loss: 10.2239\n",
      "Epoch [120/300], Step [124/172], Loss: 8.2125\n",
      "Epoch [120/300], Step [125/172], Loss: 12.6961\n",
      "Epoch [120/300], Step [126/172], Loss: 10.4035\n",
      "Epoch [120/300], Step [127/172], Loss: 11.4716\n",
      "Epoch [120/300], Step [128/172], Loss: 11.6912\n",
      "Epoch [120/300], Step [129/172], Loss: 8.5423\n",
      "Epoch [120/300], Step [130/172], Loss: 11.5178\n",
      "Epoch [120/300], Step [131/172], Loss: 8.3157\n",
      "Epoch [120/300], Step [132/172], Loss: 8.6636\n",
      "Epoch [120/300], Step [133/172], Loss: 9.5284\n",
      "Epoch [120/300], Step [134/172], Loss: 11.8218\n",
      "Epoch [120/300], Step [135/172], Loss: 8.8003\n",
      "Epoch [120/300], Step [136/172], Loss: 8.3753\n",
      "Epoch [120/300], Step [137/172], Loss: 9.6671\n",
      "Epoch [120/300], Step [138/172], Loss: 7.7749\n",
      "Epoch [120/300], Step [139/172], Loss: 9.5444\n",
      "Epoch [120/300], Step [140/172], Loss: 9.3567\n",
      "Epoch [120/300], Step [141/172], Loss: 10.8979\n",
      "Epoch [120/300], Step [142/172], Loss: 13.5152\n",
      "Epoch [120/300], Step [143/172], Loss: 9.7181\n",
      "Epoch [120/300], Step [144/172], Loss: 8.7972\n",
      "Epoch [120/300], Step [145/172], Loss: 9.6713\n",
      "Epoch [120/300], Step [146/172], Loss: 9.5255\n",
      "Epoch [120/300], Step [147/172], Loss: 5.3014\n",
      "Epoch [120/300], Step [148/172], Loss: 6.4080\n",
      "Epoch [120/300], Step [149/172], Loss: 7.4388\n",
      "Epoch [120/300], Step [150/172], Loss: 7.4321\n",
      "Epoch [120/300], Step [151/172], Loss: 6.6772\n",
      "Epoch [120/300], Step [152/172], Loss: 7.6516\n",
      "Epoch [120/300], Step [153/172], Loss: 6.8816\n",
      "Epoch [120/300], Step [154/172], Loss: 7.7218\n",
      "Epoch [120/300], Step [155/172], Loss: 6.6577\n",
      "Epoch [120/300], Step [156/172], Loss: 12.2924\n",
      "Epoch [120/300], Step [157/172], Loss: 9.8651\n",
      "Epoch [120/300], Step [158/172], Loss: 7.4065\n",
      "Epoch [120/300], Step [159/172], Loss: 9.4615\n",
      "Epoch [120/300], Step [160/172], Loss: 9.9321\n",
      "Epoch [120/300], Step [161/172], Loss: 7.0595\n",
      "Epoch [120/300], Step [162/172], Loss: 6.7481\n",
      "Epoch [120/300], Step [163/172], Loss: 6.4234\n",
      "Epoch [120/300], Step [164/172], Loss: 9.1894\n",
      "Epoch [120/300], Step [165/172], Loss: 6.1917\n",
      "Epoch [120/300], Step [166/172], Loss: 5.8422\n",
      "Epoch [120/300], Step [167/172], Loss: 9.4506\n",
      "Epoch [120/300], Step [168/172], Loss: 7.1088\n",
      "Epoch [120/300], Step [169/172], Loss: 6.8667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/300], Step [170/172], Loss: 5.4460\n",
      "Epoch [120/300], Step [171/172], Loss: 6.7444\n",
      "Epoch [120/300], Step [172/172], Loss: 5.2213\n",
      "Epoch [121/300], Step [1/172], Loss: 69.7903\n",
      "Epoch [121/300], Step [2/172], Loss: 69.8901\n",
      "Epoch [121/300], Step [3/172], Loss: 64.7259\n",
      "Epoch [121/300], Step [4/172], Loss: 39.5492\n",
      "Epoch [121/300], Step [5/172], Loss: 61.1570\n",
      "Epoch [121/300], Step [6/172], Loss: 19.5317\n",
      "Epoch [121/300], Step [7/172], Loss: 29.4079\n",
      "Epoch [121/300], Step [8/172], Loss: 5.8754\n",
      "Epoch [121/300], Step [9/172], Loss: 39.1532\n",
      "Epoch [121/300], Step [10/172], Loss: 45.4916\n",
      "Epoch [121/300], Step [11/172], Loss: 74.9119\n",
      "Epoch [121/300], Step [12/172], Loss: 81.4817\n",
      "Epoch [121/300], Step [13/172], Loss: 40.6443\n",
      "Epoch [121/300], Step [14/172], Loss: 79.1808\n",
      "Epoch [121/300], Step [15/172], Loss: 68.6611\n",
      "Epoch [121/300], Step [16/172], Loss: 14.9752\n",
      "Epoch [121/300], Step [17/172], Loss: 54.6920\n",
      "Epoch [121/300], Step [18/172], Loss: 62.6474\n",
      "Epoch [121/300], Step [19/172], Loss: 84.7393\n",
      "Epoch [121/300], Step [20/172], Loss: 57.6467\n",
      "Epoch [121/300], Step [21/172], Loss: 93.1722\n",
      "Epoch [121/300], Step [22/172], Loss: 71.9682\n",
      "Epoch [121/300], Step [23/172], Loss: 2.6125\n",
      "Epoch [121/300], Step [24/172], Loss: 66.7774\n",
      "Epoch [121/300], Step [25/172], Loss: 44.8412\n",
      "Epoch [121/300], Step [26/172], Loss: 56.3165\n",
      "Epoch [121/300], Step [27/172], Loss: 70.7001\n",
      "Epoch [121/300], Step [28/172], Loss: 31.0406\n",
      "Epoch [121/300], Step [29/172], Loss: 21.9395\n",
      "Epoch [121/300], Step [30/172], Loss: 79.6352\n",
      "Epoch [121/300], Step [31/172], Loss: 43.0869\n",
      "Epoch [121/300], Step [32/172], Loss: 41.8357\n",
      "Epoch [121/300], Step [33/172], Loss: 71.4739\n",
      "Epoch [121/300], Step [34/172], Loss: 4.3466\n",
      "Epoch [121/300], Step [35/172], Loss: 14.3529\n",
      "Epoch [121/300], Step [36/172], Loss: 20.4863\n",
      "Epoch [121/300], Step [37/172], Loss: 17.4082\n",
      "Epoch [121/300], Step [38/172], Loss: 27.5708\n",
      "Epoch [121/300], Step [39/172], Loss: 40.2512\n",
      "Epoch [121/300], Step [40/172], Loss: 19.9222\n",
      "Epoch [121/300], Step [41/172], Loss: 37.0242\n",
      "Epoch [121/300], Step [42/172], Loss: 40.4512\n",
      "Epoch [121/300], Step [43/172], Loss: 26.3539\n",
      "Epoch [121/300], Step [44/172], Loss: 20.1091\n",
      "Epoch [121/300], Step [45/172], Loss: 22.6429\n",
      "Epoch [121/300], Step [46/172], Loss: 20.5743\n",
      "Epoch [121/300], Step [47/172], Loss: 47.1932\n",
      "Epoch [121/300], Step [48/172], Loss: 53.7194\n",
      "Epoch [121/300], Step [49/172], Loss: 19.6695\n",
      "Epoch [121/300], Step [50/172], Loss: 49.1603\n",
      "Epoch [121/300], Step [51/172], Loss: 7.5112\n",
      "Epoch [121/300], Step [52/172], Loss: 17.2356\n",
      "Epoch [121/300], Step [53/172], Loss: 22.1914\n",
      "Epoch [121/300], Step [54/172], Loss: 11.7238\n",
      "Epoch [121/300], Step [55/172], Loss: 12.1057\n",
      "Epoch [121/300], Step [56/172], Loss: 12.1411\n",
      "Epoch [121/300], Step [57/172], Loss: 17.9566\n",
      "Epoch [121/300], Step [58/172], Loss: 16.3580\n",
      "Epoch [121/300], Step [59/172], Loss: 29.7373\n",
      "Epoch [121/300], Step [60/172], Loss: 40.7832\n",
      "Epoch [121/300], Step [61/172], Loss: 7.5378\n",
      "Epoch [121/300], Step [62/172], Loss: 22.3985\n",
      "Epoch [121/300], Step [63/172], Loss: 9.0138\n",
      "Epoch [121/300], Step [64/172], Loss: 8.5882\n",
      "Epoch [121/300], Step [65/172], Loss: 20.0382\n",
      "Epoch [121/300], Step [66/172], Loss: 5.6173\n",
      "Epoch [121/300], Step [67/172], Loss: 26.4404\n",
      "Epoch [121/300], Step [68/172], Loss: 5.5247\n",
      "Epoch [121/300], Step [69/172], Loss: 54.5399\n",
      "Epoch [121/300], Step [70/172], Loss: 52.6377\n",
      "Epoch [121/300], Step [71/172], Loss: 49.8691\n",
      "Epoch [121/300], Step [72/172], Loss: 53.9732\n",
      "Epoch [121/300], Step [73/172], Loss: 59.1073\n",
      "Epoch [121/300], Step [74/172], Loss: 32.6948\n",
      "Epoch [121/300], Step [75/172], Loss: 32.0315\n",
      "Epoch [121/300], Step [76/172], Loss: 35.2453\n",
      "Epoch [121/300], Step [77/172], Loss: 60.6561\n",
      "Epoch [121/300], Step [78/172], Loss: 46.4377\n",
      "Epoch [121/300], Step [79/172], Loss: 45.6705\n",
      "Epoch [121/300], Step [80/172], Loss: 57.2649\n",
      "Epoch [121/300], Step [81/172], Loss: 40.3783\n",
      "Epoch [121/300], Step [82/172], Loss: 39.2950\n",
      "Epoch [121/300], Step [83/172], Loss: 47.3036\n",
      "Epoch [121/300], Step [84/172], Loss: 36.0855\n",
      "Epoch [121/300], Step [85/172], Loss: 40.3608\n",
      "Epoch [121/300], Step [86/172], Loss: 33.3795\n",
      "Epoch [121/300], Step [87/172], Loss: 27.7029\n",
      "Epoch [121/300], Step [88/172], Loss: 29.1608\n",
      "Epoch [121/300], Step [89/172], Loss: 25.7076\n",
      "Epoch [121/300], Step [90/172], Loss: 24.9309\n",
      "Epoch [121/300], Step [91/172], Loss: 27.9748\n",
      "Epoch [121/300], Step [92/172], Loss: 21.2344\n",
      "Epoch [121/300], Step [93/172], Loss: 21.1210\n",
      "Epoch [121/300], Step [94/172], Loss: 29.5598\n",
      "Epoch [121/300], Step [95/172], Loss: 22.5340\n",
      "Epoch [121/300], Step [96/172], Loss: 19.0485\n",
      "Epoch [121/300], Step [97/172], Loss: 26.1497\n",
      "Epoch [121/300], Step [98/172], Loss: 19.0171\n",
      "Epoch [121/300], Step [99/172], Loss: 18.1187\n",
      "Epoch [121/300], Step [100/172], Loss: 15.3068\n",
      "Epoch [121/300], Step [101/172], Loss: 16.7883\n",
      "Epoch [121/300], Step [102/172], Loss: 16.2347\n",
      "Epoch [121/300], Step [103/172], Loss: 13.1169\n",
      "Epoch [121/300], Step [104/172], Loss: 15.9829\n",
      "Epoch [121/300], Step [105/172], Loss: 17.8293\n",
      "Epoch [121/300], Step [106/172], Loss: 16.2939\n",
      "Epoch [121/300], Step [107/172], Loss: 14.8774\n",
      "Epoch [121/300], Step [108/172], Loss: 16.0708\n",
      "Epoch [121/300], Step [109/172], Loss: 16.5145\n",
      "Epoch [121/300], Step [110/172], Loss: 15.6284\n",
      "Epoch [121/300], Step [111/172], Loss: 13.8156\n",
      "Epoch [121/300], Step [112/172], Loss: 18.1656\n",
      "Epoch [121/300], Step [113/172], Loss: 13.6284\n",
      "Epoch [121/300], Step [114/172], Loss: 13.6199\n",
      "Epoch [121/300], Step [115/172], Loss: 20.5402\n",
      "Epoch [121/300], Step [116/172], Loss: 14.3544\n",
      "Epoch [121/300], Step [117/172], Loss: 11.1134\n",
      "Epoch [121/300], Step [118/172], Loss: 14.5705\n",
      "Epoch [121/300], Step [119/172], Loss: 15.1740\n",
      "Epoch [121/300], Step [120/172], Loss: 10.2529\n",
      "Epoch [121/300], Step [121/172], Loss: 10.3088\n",
      "Epoch [121/300], Step [122/172], Loss: 10.3532\n",
      "Epoch [121/300], Step [123/172], Loss: 10.2011\n",
      "Epoch [121/300], Step [124/172], Loss: 8.2116\n",
      "Epoch [121/300], Step [125/172], Loss: 12.6768\n",
      "Epoch [121/300], Step [126/172], Loss: 10.3867\n",
      "Epoch [121/300], Step [127/172], Loss: 11.4357\n",
      "Epoch [121/300], Step [128/172], Loss: 11.6800\n",
      "Epoch [121/300], Step [129/172], Loss: 8.5228\n",
      "Epoch [121/300], Step [130/172], Loss: 11.5080\n",
      "Epoch [121/300], Step [131/172], Loss: 8.2744\n",
      "Epoch [121/300], Step [132/172], Loss: 8.6380\n",
      "Epoch [121/300], Step [133/172], Loss: 9.5019\n",
      "Epoch [121/300], Step [134/172], Loss: 11.7504\n",
      "Epoch [121/300], Step [135/172], Loss: 8.7771\n",
      "Epoch [121/300], Step [136/172], Loss: 8.3743\n",
      "Epoch [121/300], Step [137/172], Loss: 9.6311\n",
      "Epoch [121/300], Step [138/172], Loss: 7.7465\n",
      "Epoch [121/300], Step [139/172], Loss: 9.5131\n",
      "Epoch [121/300], Step [140/172], Loss: 9.3499\n",
      "Epoch [121/300], Step [141/172], Loss: 10.8631\n",
      "Epoch [121/300], Step [142/172], Loss: 13.4882\n",
      "Epoch [121/300], Step [143/172], Loss: 9.7274\n",
      "Epoch [121/300], Step [144/172], Loss: 8.8098\n",
      "Epoch [121/300], Step [145/172], Loss: 9.6486\n",
      "Epoch [121/300], Step [146/172], Loss: 9.5373\n",
      "Epoch [121/300], Step [147/172], Loss: 5.2969\n",
      "Epoch [121/300], Step [148/172], Loss: 6.3974\n",
      "Epoch [121/300], Step [149/172], Loss: 7.4241\n",
      "Epoch [121/300], Step [150/172], Loss: 7.4159\n",
      "Epoch [121/300], Step [151/172], Loss: 6.6310\n",
      "Epoch [121/300], Step [152/172], Loss: 7.6209\n",
      "Epoch [121/300], Step [153/172], Loss: 6.8875\n",
      "Epoch [121/300], Step [154/172], Loss: 7.7090\n",
      "Epoch [121/300], Step [155/172], Loss: 6.6750\n",
      "Epoch [121/300], Step [156/172], Loss: 12.2857\n",
      "Epoch [121/300], Step [157/172], Loss: 9.8445\n",
      "Epoch [121/300], Step [158/172], Loss: 7.3834\n",
      "Epoch [121/300], Step [159/172], Loss: 9.4751\n",
      "Epoch [121/300], Step [160/172], Loss: 9.9240\n",
      "Epoch [121/300], Step [161/172], Loss: 7.0834\n",
      "Epoch [121/300], Step [162/172], Loss: 6.7311\n",
      "Epoch [121/300], Step [163/172], Loss: 6.4181\n",
      "Epoch [121/300], Step [164/172], Loss: 9.0864\n",
      "Epoch [121/300], Step [165/172], Loss: 6.1879\n",
      "Epoch [121/300], Step [166/172], Loss: 5.8261\n",
      "Epoch [121/300], Step [167/172], Loss: 9.4707\n",
      "Epoch [121/300], Step [168/172], Loss: 7.1255\n",
      "Epoch [121/300], Step [169/172], Loss: 6.8688\n",
      "Epoch [121/300], Step [170/172], Loss: 5.4436\n",
      "Epoch [121/300], Step [171/172], Loss: 6.7761\n",
      "Epoch [121/300], Step [172/172], Loss: 5.2545\n",
      "Epoch [122/300], Step [1/172], Loss: 69.1617\n",
      "Epoch [122/300], Step [2/172], Loss: 69.5631\n",
      "Epoch [122/300], Step [3/172], Loss: 64.3092\n",
      "Epoch [122/300], Step [4/172], Loss: 39.2388\n",
      "Epoch [122/300], Step [5/172], Loss: 61.0868\n",
      "Epoch [122/300], Step [6/172], Loss: 19.6085\n",
      "Epoch [122/300], Step [7/172], Loss: 29.9003\n",
      "Epoch [122/300], Step [8/172], Loss: 6.2344\n",
      "Epoch [122/300], Step [9/172], Loss: 39.0879\n",
      "Epoch [122/300], Step [10/172], Loss: 45.3045\n",
      "Epoch [122/300], Step [11/172], Loss: 74.5664\n",
      "Epoch [122/300], Step [12/172], Loss: 81.3004\n",
      "Epoch [122/300], Step [13/172], Loss: 40.9322\n",
      "Epoch [122/300], Step [14/172], Loss: 79.0621\n",
      "Epoch [122/300], Step [15/172], Loss: 68.5968\n",
      "Epoch [122/300], Step [16/172], Loss: 14.7099\n",
      "Epoch [122/300], Step [17/172], Loss: 54.7504\n",
      "Epoch [122/300], Step [18/172], Loss: 62.7537\n",
      "Epoch [122/300], Step [19/172], Loss: 84.9558\n",
      "Epoch [122/300], Step [20/172], Loss: 57.3154\n",
      "Epoch [122/300], Step [21/172], Loss: 93.2816\n",
      "Epoch [122/300], Step [22/172], Loss: 71.9888\n",
      "Epoch [122/300], Step [23/172], Loss: 2.6045\n",
      "Epoch [122/300], Step [24/172], Loss: 66.6812\n",
      "Epoch [122/300], Step [25/172], Loss: 44.6969\n",
      "Epoch [122/300], Step [26/172], Loss: 56.1244\n",
      "Epoch [122/300], Step [27/172], Loss: 70.5507\n",
      "Epoch [122/300], Step [28/172], Loss: 31.0269\n",
      "Epoch [122/300], Step [29/172], Loss: 21.8308\n",
      "Epoch [122/300], Step [30/172], Loss: 79.4291\n",
      "Epoch [122/300], Step [31/172], Loss: 43.0556\n",
      "Epoch [122/300], Step [32/172], Loss: 41.9276\n",
      "Epoch [122/300], Step [33/172], Loss: 71.5506\n",
      "Epoch [122/300], Step [34/172], Loss: 4.3301\n",
      "Epoch [122/300], Step [35/172], Loss: 14.2530\n",
      "Epoch [122/300], Step [36/172], Loss: 20.4333\n",
      "Epoch [122/300], Step [37/172], Loss: 17.4755\n",
      "Epoch [122/300], Step [38/172], Loss: 27.6152\n",
      "Epoch [122/300], Step [39/172], Loss: 40.1218\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [122/300], Step [40/172], Loss: 19.9156\n",
      "Epoch [122/300], Step [41/172], Loss: 36.8908\n",
      "Epoch [122/300], Step [42/172], Loss: 40.3049\n",
      "Epoch [122/300], Step [43/172], Loss: 26.3011\n",
      "Epoch [122/300], Step [44/172], Loss: 20.0091\n",
      "Epoch [122/300], Step [45/172], Loss: 22.6687\n",
      "Epoch [122/300], Step [46/172], Loss: 20.3288\n",
      "Epoch [122/300], Step [47/172], Loss: 46.9529\n",
      "Epoch [122/300], Step [48/172], Loss: 53.6996\n",
      "Epoch [122/300], Step [49/172], Loss: 19.6811\n",
      "Epoch [122/300], Step [50/172], Loss: 49.1670\n",
      "Epoch [122/300], Step [51/172], Loss: 7.5017\n",
      "Epoch [122/300], Step [52/172], Loss: 17.2136\n",
      "Epoch [122/300], Step [53/172], Loss: 22.1725\n",
      "Epoch [122/300], Step [54/172], Loss: 11.6978\n",
      "Epoch [122/300], Step [55/172], Loss: 12.1053\n",
      "Epoch [122/300], Step [56/172], Loss: 12.2644\n",
      "Epoch [122/300], Step [57/172], Loss: 17.9899\n",
      "Epoch [122/300], Step [58/172], Loss: 16.3109\n",
      "Epoch [122/300], Step [59/172], Loss: 29.5897\n",
      "Epoch [122/300], Step [60/172], Loss: 40.3927\n",
      "Epoch [122/300], Step [61/172], Loss: 7.5346\n",
      "Epoch [122/300], Step [62/172], Loss: 22.2826\n",
      "Epoch [122/300], Step [63/172], Loss: 9.0930\n",
      "Epoch [122/300], Step [64/172], Loss: 8.6646\n",
      "Epoch [122/300], Step [65/172], Loss: 20.1100\n",
      "Epoch [122/300], Step [66/172], Loss: 5.6586\n",
      "Epoch [122/300], Step [67/172], Loss: 26.4337\n",
      "Epoch [122/300], Step [68/172], Loss: 5.5994\n",
      "Epoch [122/300], Step [69/172], Loss: 54.1197\n",
      "Epoch [122/300], Step [70/172], Loss: 52.2886\n",
      "Epoch [122/300], Step [71/172], Loss: 49.4717\n",
      "Epoch [122/300], Step [72/172], Loss: 53.5779\n",
      "Epoch [122/300], Step [73/172], Loss: 58.7787\n",
      "Epoch [122/300], Step [74/172], Loss: 32.4482\n",
      "Epoch [122/300], Step [75/172], Loss: 31.7278\n",
      "Epoch [122/300], Step [76/172], Loss: 35.0738\n",
      "Epoch [122/300], Step [77/172], Loss: 60.3588\n",
      "Epoch [122/300], Step [78/172], Loss: 46.1837\n",
      "Epoch [122/300], Step [79/172], Loss: 45.3948\n",
      "Epoch [122/300], Step [80/172], Loss: 56.9965\n",
      "Epoch [122/300], Step [81/172], Loss: 40.1409\n",
      "Epoch [122/300], Step [82/172], Loss: 39.3251\n",
      "Epoch [122/300], Step [83/172], Loss: 47.1069\n",
      "Epoch [122/300], Step [84/172], Loss: 35.9139\n",
      "Epoch [122/300], Step [85/172], Loss: 40.2130\n",
      "Epoch [122/300], Step [86/172], Loss: 33.3109\n",
      "Epoch [122/300], Step [87/172], Loss: 27.5933\n",
      "Epoch [122/300], Step [88/172], Loss: 29.0809\n",
      "Epoch [122/300], Step [89/172], Loss: 25.6212\n",
      "Epoch [122/300], Step [90/172], Loss: 24.9174\n",
      "Epoch [122/300], Step [91/172], Loss: 27.8455\n",
      "Epoch [122/300], Step [92/172], Loss: 21.1864\n",
      "Epoch [122/300], Step [93/172], Loss: 21.0925\n",
      "Epoch [122/300], Step [94/172], Loss: 29.5782\n",
      "Epoch [122/300], Step [95/172], Loss: 22.5074\n",
      "Epoch [122/300], Step [96/172], Loss: 19.0024\n",
      "Epoch [122/300], Step [97/172], Loss: 26.1296\n",
      "Epoch [122/300], Step [98/172], Loss: 18.9994\n",
      "Epoch [122/300], Step [99/172], Loss: 18.1228\n",
      "Epoch [122/300], Step [100/172], Loss: 15.2731\n",
      "Epoch [122/300], Step [101/172], Loss: 16.7811\n",
      "Epoch [122/300], Step [102/172], Loss: 16.3769\n",
      "Epoch [122/300], Step [103/172], Loss: 13.0797\n",
      "Epoch [122/300], Step [104/172], Loss: 15.9942\n",
      "Epoch [122/300], Step [105/172], Loss: 17.9469\n",
      "Epoch [122/300], Step [106/172], Loss: 16.2704\n",
      "Epoch [122/300], Step [107/172], Loss: 14.8194\n",
      "Epoch [122/300], Step [108/172], Loss: 16.0427\n",
      "Epoch [122/300], Step [109/172], Loss: 16.4675\n",
      "Epoch [122/300], Step [110/172], Loss: 15.6074\n",
      "Epoch [122/300], Step [111/172], Loss: 13.7603\n",
      "Epoch [122/300], Step [112/172], Loss: 18.1831\n",
      "Epoch [122/300], Step [113/172], Loss: 13.5453\n",
      "Epoch [122/300], Step [114/172], Loss: 13.5883\n",
      "Epoch [122/300], Step [115/172], Loss: 20.4694\n",
      "Epoch [122/300], Step [116/172], Loss: 14.3622\n",
      "Epoch [122/300], Step [117/172], Loss: 11.1063\n",
      "Epoch [122/300], Step [118/172], Loss: 14.5384\n",
      "Epoch [122/300], Step [119/172], Loss: 15.1146\n",
      "Epoch [122/300], Step [120/172], Loss: 10.1773\n",
      "Epoch [122/300], Step [121/172], Loss: 10.2598\n",
      "Epoch [122/300], Step [122/172], Loss: 10.3048\n",
      "Epoch [122/300], Step [123/172], Loss: 10.1958\n",
      "Epoch [122/300], Step [124/172], Loss: 8.1954\n",
      "Epoch [122/300], Step [125/172], Loss: 12.6363\n",
      "Epoch [122/300], Step [126/172], Loss: 10.3673\n",
      "Epoch [122/300], Step [127/172], Loss: 11.3708\n",
      "Epoch [122/300], Step [128/172], Loss: 11.6029\n",
      "Epoch [122/300], Step [129/172], Loss: 8.4889\n",
      "Epoch [122/300], Step [130/172], Loss: 11.4824\n",
      "Epoch [122/300], Step [131/172], Loss: 8.2067\n",
      "Epoch [122/300], Step [132/172], Loss: 8.6077\n",
      "Epoch [122/300], Step [133/172], Loss: 9.4378\n",
      "Epoch [122/300], Step [134/172], Loss: 11.6478\n",
      "Epoch [122/300], Step [135/172], Loss: 8.7021\n",
      "Epoch [122/300], Step [136/172], Loss: 8.3456\n",
      "Epoch [122/300], Step [137/172], Loss: 9.6008\n",
      "Epoch [122/300], Step [138/172], Loss: 7.7032\n",
      "Epoch [122/300], Step [139/172], Loss: 9.4362\n",
      "Epoch [122/300], Step [140/172], Loss: 9.3237\n",
      "Epoch [122/300], Step [141/172], Loss: 10.7767\n",
      "Epoch [122/300], Step [142/172], Loss: 13.4406\n",
      "Epoch [122/300], Step [143/172], Loss: 9.7303\n",
      "Epoch [122/300], Step [144/172], Loss: 8.7624\n",
      "Epoch [122/300], Step [145/172], Loss: 9.6163\n",
      "Epoch [122/300], Step [146/172], Loss: 9.4782\n",
      "Epoch [122/300], Step [147/172], Loss: 5.2659\n",
      "Epoch [122/300], Step [148/172], Loss: 6.3673\n",
      "Epoch [122/300], Step [149/172], Loss: 7.3688\n",
      "Epoch [122/300], Step [150/172], Loss: 7.3442\n",
      "Epoch [122/300], Step [151/172], Loss: 6.5585\n",
      "Epoch [122/300], Step [152/172], Loss: 7.5537\n",
      "Epoch [122/300], Step [153/172], Loss: 6.8423\n",
      "Epoch [122/300], Step [154/172], Loss: 7.6752\n",
      "Epoch [122/300], Step [155/172], Loss: 6.6120\n",
      "Epoch [122/300], Step [156/172], Loss: 12.2520\n",
      "Epoch [122/300], Step [157/172], Loss: 9.7788\n",
      "Epoch [122/300], Step [158/172], Loss: 7.3457\n",
      "Epoch [122/300], Step [159/172], Loss: 9.4415\n",
      "Epoch [122/300], Step [160/172], Loss: 9.8773\n",
      "Epoch [122/300], Step [161/172], Loss: 7.0458\n",
      "Epoch [122/300], Step [162/172], Loss: 6.6445\n",
      "Epoch [122/300], Step [163/172], Loss: 6.3864\n",
      "Epoch [122/300], Step [164/172], Loss: 9.0954\n",
      "Epoch [122/300], Step [165/172], Loss: 6.1655\n",
      "Epoch [122/300], Step [166/172], Loss: 5.7614\n",
      "Epoch [122/300], Step [167/172], Loss: 9.4380\n",
      "Epoch [122/300], Step [168/172], Loss: 7.0448\n",
      "Epoch [122/300], Step [169/172], Loss: 6.8045\n",
      "Epoch [122/300], Step [170/172], Loss: 5.3847\n",
      "Epoch [122/300], Step [171/172], Loss: 6.7431\n",
      "Epoch [122/300], Step [172/172], Loss: 5.1828\n",
      "Epoch [123/300], Step [1/172], Loss: 68.4427\n",
      "Epoch [123/300], Step [2/172], Loss: 69.0061\n",
      "Epoch [123/300], Step [3/172], Loss: 63.9273\n",
      "Epoch [123/300], Step [4/172], Loss: 39.0232\n",
      "Epoch [123/300], Step [5/172], Loss: 60.7670\n",
      "Epoch [123/300], Step [6/172], Loss: 19.5502\n",
      "Epoch [123/300], Step [7/172], Loss: 29.6103\n",
      "Epoch [123/300], Step [8/172], Loss: 5.7172\n",
      "Epoch [123/300], Step [9/172], Loss: 38.8292\n",
      "Epoch [123/300], Step [10/172], Loss: 45.4297\n",
      "Epoch [123/300], Step [11/172], Loss: 74.3031\n",
      "Epoch [123/300], Step [12/172], Loss: 81.1635\n",
      "Epoch [123/300], Step [13/172], Loss: 40.5946\n",
      "Epoch [123/300], Step [14/172], Loss: 78.2912\n",
      "Epoch [123/300], Step [15/172], Loss: 68.4240\n",
      "Epoch [123/300], Step [16/172], Loss: 15.4695\n",
      "Epoch [123/300], Step [17/172], Loss: 54.4331\n",
      "Epoch [123/300], Step [18/172], Loss: 62.7239\n",
      "Epoch [123/300], Step [19/172], Loss: 84.9964\n",
      "Epoch [123/300], Step [20/172], Loss: 56.7923\n",
      "Epoch [123/300], Step [21/172], Loss: 93.1209\n",
      "Epoch [123/300], Step [22/172], Loss: 71.8422\n",
      "Epoch [123/300], Step [23/172], Loss: 2.5776\n",
      "Epoch [123/300], Step [24/172], Loss: 66.4799\n",
      "Epoch [123/300], Step [25/172], Loss: 44.7306\n",
      "Epoch [123/300], Step [26/172], Loss: 55.9792\n",
      "Epoch [123/300], Step [27/172], Loss: 70.6064\n",
      "Epoch [123/300], Step [28/172], Loss: 30.6045\n",
      "Epoch [123/300], Step [29/172], Loss: 21.6194\n",
      "Epoch [123/300], Step [30/172], Loss: 79.5075\n",
      "Epoch [123/300], Step [31/172], Loss: 43.0529\n",
      "Epoch [123/300], Step [32/172], Loss: 41.8152\n",
      "Epoch [123/300], Step [33/172], Loss: 71.3417\n",
      "Epoch [123/300], Step [34/172], Loss: 4.1951\n",
      "Epoch [123/300], Step [35/172], Loss: 14.1393\n",
      "Epoch [123/300], Step [36/172], Loss: 20.0256\n",
      "Epoch [123/300], Step [37/172], Loss: 17.3422\n",
      "Epoch [123/300], Step [38/172], Loss: 27.5707\n",
      "Epoch [123/300], Step [39/172], Loss: 39.8253\n",
      "Epoch [123/300], Step [40/172], Loss: 19.8351\n",
      "Epoch [123/300], Step [41/172], Loss: 36.9334\n",
      "Epoch [123/300], Step [42/172], Loss: 40.2878\n",
      "Epoch [123/300], Step [43/172], Loss: 26.2582\n",
      "Epoch [123/300], Step [44/172], Loss: 20.0547\n",
      "Epoch [123/300], Step [45/172], Loss: 22.5903\n",
      "Epoch [123/300], Step [46/172], Loss: 20.2277\n",
      "Epoch [123/300], Step [47/172], Loss: 46.8339\n",
      "Epoch [123/300], Step [48/172], Loss: 53.7059\n",
      "Epoch [123/300], Step [49/172], Loss: 19.6430\n",
      "Epoch [123/300], Step [50/172], Loss: 48.8803\n",
      "Epoch [123/300], Step [51/172], Loss: 7.4899\n",
      "Epoch [123/300], Step [52/172], Loss: 17.1184\n",
      "Epoch [123/300], Step [53/172], Loss: 22.0008\n",
      "Epoch [123/300], Step [54/172], Loss: 11.7497\n",
      "Epoch [123/300], Step [55/172], Loss: 12.1028\n",
      "Epoch [123/300], Step [56/172], Loss: 12.3444\n",
      "Epoch [123/300], Step [57/172], Loss: 17.7995\n",
      "Epoch [123/300], Step [58/172], Loss: 16.2551\n",
      "Epoch [123/300], Step [59/172], Loss: 29.4826\n",
      "Epoch [123/300], Step [60/172], Loss: 40.4555\n",
      "Epoch [123/300], Step [61/172], Loss: 7.3537\n",
      "Epoch [123/300], Step [62/172], Loss: 22.2708\n",
      "Epoch [123/300], Step [63/172], Loss: 8.9473\n",
      "Epoch [123/300], Step [64/172], Loss: 8.5948\n",
      "Epoch [123/300], Step [65/172], Loss: 20.0245\n",
      "Epoch [123/300], Step [66/172], Loss: 5.5521\n",
      "Epoch [123/300], Step [67/172], Loss: 26.3009\n",
      "Epoch [123/300], Step [68/172], Loss: 5.6106\n",
      "Epoch [123/300], Step [69/172], Loss: 53.5047\n",
      "Epoch [123/300], Step [70/172], Loss: 52.2240\n",
      "Epoch [123/300], Step [71/172], Loss: 49.5777\n",
      "Epoch [123/300], Step [72/172], Loss: 53.5633\n",
      "Epoch [123/300], Step [73/172], Loss: 58.8989\n",
      "Epoch [123/300], Step [74/172], Loss: 32.4924\n",
      "Epoch [123/300], Step [75/172], Loss: 31.9726\n",
      "Epoch [123/300], Step [76/172], Loss: 35.0602\n",
      "Epoch [123/300], Step [77/172], Loss: 60.4466\n",
      "Epoch [123/300], Step [78/172], Loss: 46.1840\n",
      "Epoch [123/300], Step [79/172], Loss: 45.4600\n",
      "Epoch [123/300], Step [80/172], Loss: 57.1986\n",
      "Epoch [123/300], Step [81/172], Loss: 40.1717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [123/300], Step [82/172], Loss: 39.2289\n",
      "Epoch [123/300], Step [83/172], Loss: 47.0800\n",
      "Epoch [123/300], Step [84/172], Loss: 35.9748\n",
      "Epoch [123/300], Step [85/172], Loss: 40.2840\n",
      "Epoch [123/300], Step [86/172], Loss: 33.3862\n",
      "Epoch [123/300], Step [87/172], Loss: 27.6303\n",
      "Epoch [123/300], Step [88/172], Loss: 29.0379\n",
      "Epoch [123/300], Step [89/172], Loss: 25.6609\n",
      "Epoch [123/300], Step [90/172], Loss: 24.8689\n",
      "Epoch [123/300], Step [91/172], Loss: 27.8780\n",
      "Epoch [123/300], Step [92/172], Loss: 21.1681\n",
      "Epoch [123/300], Step [93/172], Loss: 21.0935\n",
      "Epoch [123/300], Step [94/172], Loss: 29.5262\n",
      "Epoch [123/300], Step [95/172], Loss: 22.4286\n",
      "Epoch [123/300], Step [96/172], Loss: 19.0598\n",
      "Epoch [123/300], Step [97/172], Loss: 26.1671\n",
      "Epoch [123/300], Step [98/172], Loss: 18.9554\n",
      "Epoch [123/300], Step [99/172], Loss: 18.0818\n",
      "Epoch [123/300], Step [100/172], Loss: 15.2272\n",
      "Epoch [123/300], Step [101/172], Loss: 16.7223\n",
      "Epoch [123/300], Step [102/172], Loss: 16.3348\n",
      "Epoch [123/300], Step [103/172], Loss: 13.0103\n",
      "Epoch [123/300], Step [104/172], Loss: 15.9643\n",
      "Epoch [123/300], Step [105/172], Loss: 17.8770\n",
      "Epoch [123/300], Step [106/172], Loss: 16.1722\n",
      "Epoch [123/300], Step [107/172], Loss: 14.8065\n",
      "Epoch [123/300], Step [108/172], Loss: 15.9788\n",
      "Epoch [123/300], Step [109/172], Loss: 16.3435\n",
      "Epoch [123/300], Step [110/172], Loss: 15.5077\n",
      "Epoch [123/300], Step [111/172], Loss: 13.7310\n",
      "Epoch [123/300], Step [112/172], Loss: 18.1234\n",
      "Epoch [123/300], Step [113/172], Loss: 13.4522\n",
      "Epoch [123/300], Step [114/172], Loss: 13.5556\n",
      "Epoch [123/300], Step [115/172], Loss: 20.3115\n",
      "Epoch [123/300], Step [116/172], Loss: 14.2614\n",
      "Epoch [123/300], Step [117/172], Loss: 11.0350\n",
      "Epoch [123/300], Step [118/172], Loss: 14.5027\n",
      "Epoch [123/300], Step [119/172], Loss: 15.1080\n",
      "Epoch [123/300], Step [120/172], Loss: 10.1102\n",
      "Epoch [123/300], Step [121/172], Loss: 10.1923\n",
      "Epoch [123/300], Step [122/172], Loss: 10.2760\n",
      "Epoch [123/300], Step [123/172], Loss: 10.1739\n",
      "Epoch [123/300], Step [124/172], Loss: 8.1573\n",
      "Epoch [123/300], Step [125/172], Loss: 12.6030\n",
      "Epoch [123/300], Step [126/172], Loss: 10.3272\n",
      "Epoch [123/300], Step [127/172], Loss: 11.3053\n",
      "Epoch [123/300], Step [128/172], Loss: 11.5414\n",
      "Epoch [123/300], Step [129/172], Loss: 8.4288\n",
      "Epoch [123/300], Step [130/172], Loss: 11.4250\n",
      "Epoch [123/300], Step [131/172], Loss: 8.1479\n",
      "Epoch [123/300], Step [132/172], Loss: 8.5505\n",
      "Epoch [123/300], Step [133/172], Loss: 9.4115\n",
      "Epoch [123/300], Step [134/172], Loss: 11.6356\n",
      "Epoch [123/300], Step [135/172], Loss: 8.6789\n",
      "Epoch [123/300], Step [136/172], Loss: 8.3290\n",
      "Epoch [123/300], Step [137/172], Loss: 9.5287\n",
      "Epoch [123/300], Step [138/172], Loss: 7.6445\n",
      "Epoch [123/300], Step [139/172], Loss: 9.4153\n",
      "Epoch [123/300], Step [140/172], Loss: 9.2614\n",
      "Epoch [123/300], Step [141/172], Loss: 10.7244\n",
      "Epoch [123/300], Step [142/172], Loss: 13.4161\n",
      "Epoch [123/300], Step [143/172], Loss: 9.6896\n",
      "Epoch [123/300], Step [144/172], Loss: 8.7411\n",
      "Epoch [123/300], Step [145/172], Loss: 9.5805\n",
      "Epoch [123/300], Step [146/172], Loss: 9.4292\n",
      "Epoch [123/300], Step [147/172], Loss: 5.2381\n",
      "Epoch [123/300], Step [148/172], Loss: 6.3205\n",
      "Epoch [123/300], Step [149/172], Loss: 7.3187\n",
      "Epoch [123/300], Step [150/172], Loss: 7.3025\n",
      "Epoch [123/300], Step [151/172], Loss: 6.4739\n",
      "Epoch [123/300], Step [152/172], Loss: 7.5164\n",
      "Epoch [123/300], Step [153/172], Loss: 6.8099\n",
      "Epoch [123/300], Step [154/172], Loss: 7.6130\n",
      "Epoch [123/300], Step [155/172], Loss: 6.6019\n",
      "Epoch [123/300], Step [156/172], Loss: 12.2222\n",
      "Epoch [123/300], Step [157/172], Loss: 9.7334\n",
      "Epoch [123/300], Step [158/172], Loss: 7.2978\n",
      "Epoch [123/300], Step [159/172], Loss: 9.3796\n",
      "Epoch [123/300], Step [160/172], Loss: 9.8550\n",
      "Epoch [123/300], Step [161/172], Loss: 6.9845\n",
      "Epoch [123/300], Step [162/172], Loss: 6.5865\n",
      "Epoch [123/300], Step [163/172], Loss: 6.3255\n",
      "Epoch [123/300], Step [164/172], Loss: 8.9883\n",
      "Epoch [123/300], Step [165/172], Loss: 6.1041\n",
      "Epoch [123/300], Step [166/172], Loss: 5.7088\n",
      "Epoch [123/300], Step [167/172], Loss: 9.4358\n",
      "Epoch [123/300], Step [168/172], Loss: 7.0400\n",
      "Epoch [123/300], Step [169/172], Loss: 6.7620\n",
      "Epoch [123/300], Step [170/172], Loss: 5.3334\n",
      "Epoch [123/300], Step [171/172], Loss: 6.7314\n",
      "Epoch [123/300], Step [172/172], Loss: 5.1512\n",
      "Epoch [124/300], Step [1/172], Loss: 68.2916\n",
      "Epoch [124/300], Step [2/172], Loss: 69.1971\n",
      "Epoch [124/300], Step [3/172], Loss: 63.7592\n",
      "Epoch [124/300], Step [4/172], Loss: 38.6330\n",
      "Epoch [124/300], Step [5/172], Loss: 60.9271\n",
      "Epoch [124/300], Step [6/172], Loss: 19.7429\n",
      "Epoch [124/300], Step [7/172], Loss: 29.8214\n",
      "Epoch [124/300], Step [8/172], Loss: 5.7488\n",
      "Epoch [124/300], Step [9/172], Loss: 38.6427\n",
      "Epoch [124/300], Step [10/172], Loss: 45.2625\n",
      "Epoch [124/300], Step [11/172], Loss: 73.9901\n",
      "Epoch [124/300], Step [12/172], Loss: 81.0325\n",
      "Epoch [124/300], Step [13/172], Loss: 40.6824\n",
      "Epoch [124/300], Step [14/172], Loss: 77.9450\n",
      "Epoch [124/300], Step [15/172], Loss: 68.0774\n",
      "Epoch [124/300], Step [16/172], Loss: 14.6741\n",
      "Epoch [124/300], Step [17/172], Loss: 54.2351\n",
      "Epoch [124/300], Step [18/172], Loss: 62.7241\n",
      "Epoch [124/300], Step [19/172], Loss: 84.8135\n",
      "Epoch [124/300], Step [20/172], Loss: 56.4746\n",
      "Epoch [124/300], Step [21/172], Loss: 92.7631\n",
      "Epoch [124/300], Step [22/172], Loss: 71.5774\n",
      "Epoch [124/300], Step [23/172], Loss: 2.4457\n",
      "Epoch [124/300], Step [24/172], Loss: 66.3046\n",
      "Epoch [124/300], Step [25/172], Loss: 44.4359\n",
      "Epoch [124/300], Step [26/172], Loss: 55.7992\n",
      "Epoch [124/300], Step [27/172], Loss: 70.0761\n",
      "Epoch [124/300], Step [28/172], Loss: 30.5292\n",
      "Epoch [124/300], Step [29/172], Loss: 21.6646\n",
      "Epoch [124/300], Step [30/172], Loss: 78.9555\n",
      "Epoch [124/300], Step [31/172], Loss: 42.7464\n",
      "Epoch [124/300], Step [32/172], Loss: 41.8339\n",
      "Epoch [124/300], Step [33/172], Loss: 71.3285\n",
      "Epoch [124/300], Step [34/172], Loss: 4.2725\n",
      "Epoch [124/300], Step [35/172], Loss: 14.2115\n",
      "Epoch [124/300], Step [36/172], Loss: 20.0636\n",
      "Epoch [124/300], Step [37/172], Loss: 17.4562\n",
      "Epoch [124/300], Step [38/172], Loss: 27.6516\n",
      "Epoch [124/300], Step [39/172], Loss: 39.8513\n",
      "Epoch [124/300], Step [40/172], Loss: 19.8418\n",
      "Epoch [124/300], Step [41/172], Loss: 36.8310\n",
      "Epoch [124/300], Step [42/172], Loss: 40.1191\n",
      "Epoch [124/300], Step [43/172], Loss: 26.3608\n",
      "Epoch [124/300], Step [44/172], Loss: 19.9658\n",
      "Epoch [124/300], Step [45/172], Loss: 22.7735\n",
      "Epoch [124/300], Step [46/172], Loss: 20.1737\n",
      "Epoch [124/300], Step [47/172], Loss: 46.8161\n",
      "Epoch [124/300], Step [48/172], Loss: 54.0946\n",
      "Epoch [124/300], Step [49/172], Loss: 19.7136\n",
      "Epoch [124/300], Step [50/172], Loss: 49.0058\n",
      "Epoch [124/300], Step [51/172], Loss: 7.5583\n",
      "Epoch [124/300], Step [52/172], Loss: 17.2235\n",
      "Epoch [124/300], Step [53/172], Loss: 22.1132\n",
      "Epoch [124/300], Step [54/172], Loss: 11.7643\n",
      "Epoch [124/300], Step [55/172], Loss: 12.0726\n",
      "Epoch [124/300], Step [56/172], Loss: 12.5254\n",
      "Epoch [124/300], Step [57/172], Loss: 17.7438\n",
      "Epoch [124/300], Step [58/172], Loss: 16.1431\n",
      "Epoch [124/300], Step [59/172], Loss: 29.4693\n",
      "Epoch [124/300], Step [60/172], Loss: 40.2410\n",
      "Epoch [124/300], Step [61/172], Loss: 7.4269\n",
      "Epoch [124/300], Step [62/172], Loss: 22.2436\n",
      "Epoch [124/300], Step [63/172], Loss: 9.0531\n",
      "Epoch [124/300], Step [64/172], Loss: 8.6626\n",
      "Epoch [124/300], Step [65/172], Loss: 20.0528\n",
      "Epoch [124/300], Step [66/172], Loss: 5.5695\n",
      "Epoch [124/300], Step [67/172], Loss: 26.2667\n",
      "Epoch [124/300], Step [68/172], Loss: 5.5005\n",
      "Epoch [124/300], Step [69/172], Loss: 53.0647\n",
      "Epoch [124/300], Step [70/172], Loss: 52.1916\n",
      "Epoch [124/300], Step [71/172], Loss: 49.4355\n",
      "Epoch [124/300], Step [72/172], Loss: 53.4578\n",
      "Epoch [124/300], Step [73/172], Loss: 58.8121\n",
      "Epoch [124/300], Step [74/172], Loss: 32.3828\n",
      "Epoch [124/300], Step [75/172], Loss: 32.0742\n",
      "Epoch [124/300], Step [76/172], Loss: 35.0288\n",
      "Epoch [124/300], Step [77/172], Loss: 60.4167\n",
      "Epoch [124/300], Step [78/172], Loss: 46.1142\n",
      "Epoch [124/300], Step [79/172], Loss: 45.3046\n",
      "Epoch [124/300], Step [80/172], Loss: 57.0626\n",
      "Epoch [124/300], Step [81/172], Loss: 40.0546\n",
      "Epoch [124/300], Step [82/172], Loss: 39.1944\n",
      "Epoch [124/300], Step [83/172], Loss: 46.9466\n",
      "Epoch [124/300], Step [84/172], Loss: 35.8443\n",
      "Epoch [124/300], Step [85/172], Loss: 40.2592\n",
      "Epoch [124/300], Step [86/172], Loss: 33.3590\n",
      "Epoch [124/300], Step [87/172], Loss: 27.6288\n",
      "Epoch [124/300], Step [88/172], Loss: 28.9910\n",
      "Epoch [124/300], Step [89/172], Loss: 25.6026\n",
      "Epoch [124/300], Step [90/172], Loss: 24.8486\n",
      "Epoch [124/300], Step [91/172], Loss: 27.8273\n",
      "Epoch [124/300], Step [92/172], Loss: 21.1699\n",
      "Epoch [124/300], Step [93/172], Loss: 21.1474\n",
      "Epoch [124/300], Step [94/172], Loss: 29.5231\n",
      "Epoch [124/300], Step [95/172], Loss: 22.3760\n",
      "Epoch [124/300], Step [96/172], Loss: 19.0925\n",
      "Epoch [124/300], Step [97/172], Loss: 26.2302\n",
      "Epoch [124/300], Step [98/172], Loss: 18.9186\n",
      "Epoch [124/300], Step [99/172], Loss: 18.0933\n",
      "Epoch [124/300], Step [100/172], Loss: 15.2014\n",
      "Epoch [124/300], Step [101/172], Loss: 16.7106\n",
      "Epoch [124/300], Step [102/172], Loss: 16.2917\n",
      "Epoch [124/300], Step [103/172], Loss: 12.9786\n",
      "Epoch [124/300], Step [104/172], Loss: 15.9992\n",
      "Epoch [124/300], Step [105/172], Loss: 17.8077\n",
      "Epoch [124/300], Step [106/172], Loss: 16.1572\n",
      "Epoch [124/300], Step [107/172], Loss: 14.8064\n",
      "Epoch [124/300], Step [108/172], Loss: 15.9481\n",
      "Epoch [124/300], Step [109/172], Loss: 16.2491\n",
      "Epoch [124/300], Step [110/172], Loss: 15.5215\n",
      "Epoch [124/300], Step [111/172], Loss: 13.7374\n",
      "Epoch [124/300], Step [112/172], Loss: 18.1333\n",
      "Epoch [124/300], Step [113/172], Loss: 13.3695\n",
      "Epoch [124/300], Step [114/172], Loss: 13.5339\n",
      "Epoch [124/300], Step [115/172], Loss: 20.2553\n",
      "Epoch [124/300], Step [116/172], Loss: 14.2594\n",
      "Epoch [124/300], Step [117/172], Loss: 10.9964\n",
      "Epoch [124/300], Step [118/172], Loss: 14.5277\n",
      "Epoch [124/300], Step [119/172], Loss: 15.0824\n",
      "Epoch [124/300], Step [120/172], Loss: 10.0356\n",
      "Epoch [124/300], Step [121/172], Loss: 10.1560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124/300], Step [122/172], Loss: 10.2814\n",
      "Epoch [124/300], Step [123/172], Loss: 10.1616\n",
      "Epoch [124/300], Step [124/172], Loss: 8.1594\n",
      "Epoch [124/300], Step [125/172], Loss: 12.6059\n",
      "Epoch [124/300], Step [126/172], Loss: 10.3009\n",
      "Epoch [124/300], Step [127/172], Loss: 11.2851\n",
      "Epoch [124/300], Step [128/172], Loss: 11.5072\n",
      "Epoch [124/300], Step [129/172], Loss: 8.4034\n",
      "Epoch [124/300], Step [130/172], Loss: 11.4305\n",
      "Epoch [124/300], Step [131/172], Loss: 8.1050\n",
      "Epoch [124/300], Step [132/172], Loss: 8.5306\n",
      "Epoch [124/300], Step [133/172], Loss: 9.3813\n",
      "Epoch [124/300], Step [134/172], Loss: 11.5879\n",
      "Epoch [124/300], Step [135/172], Loss: 8.6403\n",
      "Epoch [124/300], Step [136/172], Loss: 8.3232\n",
      "Epoch [124/300], Step [137/172], Loss: 9.5154\n",
      "Epoch [124/300], Step [138/172], Loss: 7.5909\n",
      "Epoch [124/300], Step [139/172], Loss: 9.3679\n",
      "Epoch [124/300], Step [140/172], Loss: 9.2354\n",
      "Epoch [124/300], Step [141/172], Loss: 10.6771\n",
      "Epoch [124/300], Step [142/172], Loss: 13.4010\n",
      "Epoch [124/300], Step [143/172], Loss: 9.6954\n",
      "Epoch [124/300], Step [144/172], Loss: 8.7309\n",
      "Epoch [124/300], Step [145/172], Loss: 9.5580\n",
      "Epoch [124/300], Step [146/172], Loss: 9.3929\n",
      "Epoch [124/300], Step [147/172], Loss: 5.2089\n",
      "Epoch [124/300], Step [148/172], Loss: 6.2838\n",
      "Epoch [124/300], Step [149/172], Loss: 7.2521\n",
      "Epoch [124/300], Step [150/172], Loss: 7.2490\n",
      "Epoch [124/300], Step [151/172], Loss: 6.4036\n",
      "Epoch [124/300], Step [152/172], Loss: 7.4658\n",
      "Epoch [124/300], Step [153/172], Loss: 6.7657\n",
      "Epoch [124/300], Step [154/172], Loss: 7.6108\n",
      "Epoch [124/300], Step [155/172], Loss: 6.5523\n",
      "Epoch [124/300], Step [156/172], Loss: 12.2460\n",
      "Epoch [124/300], Step [157/172], Loss: 9.7313\n",
      "Epoch [124/300], Step [158/172], Loss: 7.2772\n",
      "Epoch [124/300], Step [159/172], Loss: 9.3767\n",
      "Epoch [124/300], Step [160/172], Loss: 9.8585\n",
      "Epoch [124/300], Step [161/172], Loss: 6.9410\n",
      "Epoch [124/300], Step [162/172], Loss: 6.5412\n",
      "Epoch [124/300], Step [163/172], Loss: 6.3041\n",
      "Epoch [124/300], Step [164/172], Loss: 8.8829\n",
      "Epoch [124/300], Step [165/172], Loss: 6.0772\n",
      "Epoch [124/300], Step [166/172], Loss: 5.6647\n",
      "Epoch [124/300], Step [167/172], Loss: 9.4317\n",
      "Epoch [124/300], Step [168/172], Loss: 6.9904\n",
      "Epoch [124/300], Step [169/172], Loss: 6.7403\n",
      "Epoch [124/300], Step [170/172], Loss: 5.2809\n",
      "Epoch [124/300], Step [171/172], Loss: 6.7027\n",
      "Epoch [124/300], Step [172/172], Loss: 5.1017\n",
      "Epoch [125/300], Step [1/172], Loss: 67.6143\n",
      "Epoch [125/300], Step [2/172], Loss: 68.2591\n",
      "Epoch [125/300], Step [3/172], Loss: 63.5517\n",
      "Epoch [125/300], Step [4/172], Loss: 38.3147\n",
      "Epoch [125/300], Step [5/172], Loss: 60.6062\n",
      "Epoch [125/300], Step [6/172], Loss: 19.4033\n",
      "Epoch [125/300], Step [7/172], Loss: 29.4963\n",
      "Epoch [125/300], Step [8/172], Loss: 5.9697\n",
      "Epoch [125/300], Step [9/172], Loss: 38.4482\n",
      "Epoch [125/300], Step [10/172], Loss: 45.2043\n",
      "Epoch [125/300], Step [11/172], Loss: 73.7478\n",
      "Epoch [125/300], Step [12/172], Loss: 81.0237\n",
      "Epoch [125/300], Step [13/172], Loss: 40.7720\n",
      "Epoch [125/300], Step [14/172], Loss: 77.9451\n",
      "Epoch [125/300], Step [15/172], Loss: 68.0234\n",
      "Epoch [125/300], Step [16/172], Loss: 14.4043\n",
      "Epoch [125/300], Step [17/172], Loss: 54.1942\n",
      "Epoch [125/300], Step [18/172], Loss: 62.8103\n",
      "Epoch [125/300], Step [19/172], Loss: 84.9463\n",
      "Epoch [125/300], Step [20/172], Loss: 56.2213\n",
      "Epoch [125/300], Step [21/172], Loss: 92.6436\n",
      "Epoch [125/300], Step [22/172], Loss: 71.3292\n",
      "Epoch [125/300], Step [23/172], Loss: 2.4357\n",
      "Epoch [125/300], Step [24/172], Loss: 66.1042\n",
      "Epoch [125/300], Step [25/172], Loss: 44.3079\n",
      "Epoch [125/300], Step [26/172], Loss: 55.6138\n",
      "Epoch [125/300], Step [27/172], Loss: 69.8133\n",
      "Epoch [125/300], Step [28/172], Loss: 30.1614\n",
      "Epoch [125/300], Step [29/172], Loss: 21.5535\n",
      "Epoch [125/300], Step [30/172], Loss: 78.7804\n",
      "Epoch [125/300], Step [31/172], Loss: 42.5349\n",
      "Epoch [125/300], Step [32/172], Loss: 41.7481\n",
      "Epoch [125/300], Step [33/172], Loss: 71.2557\n",
      "Epoch [125/300], Step [34/172], Loss: 4.2419\n",
      "Epoch [125/300], Step [35/172], Loss: 14.1609\n",
      "Epoch [125/300], Step [36/172], Loss: 19.9831\n",
      "Epoch [125/300], Step [37/172], Loss: 17.4155\n",
      "Epoch [125/300], Step [38/172], Loss: 27.6224\n",
      "Epoch [125/300], Step [39/172], Loss: 39.7009\n",
      "Epoch [125/300], Step [40/172], Loss: 19.7866\n",
      "Epoch [125/300], Step [41/172], Loss: 36.6231\n",
      "Epoch [125/300], Step [42/172], Loss: 40.0625\n",
      "Epoch [125/300], Step [43/172], Loss: 26.2786\n",
      "Epoch [125/300], Step [44/172], Loss: 19.9887\n",
      "Epoch [125/300], Step [45/172], Loss: 22.7317\n",
      "Epoch [125/300], Step [46/172], Loss: 20.2437\n",
      "Epoch [125/300], Step [47/172], Loss: 46.7748\n",
      "Epoch [125/300], Step [48/172], Loss: 54.4201\n",
      "Epoch [125/300], Step [49/172], Loss: 19.5708\n",
      "Epoch [125/300], Step [50/172], Loss: 48.9363\n",
      "Epoch [125/300], Step [51/172], Loss: 7.5675\n",
      "Epoch [125/300], Step [52/172], Loss: 17.1967\n",
      "Epoch [125/300], Step [53/172], Loss: 22.1690\n",
      "Epoch [125/300], Step [54/172], Loss: 11.7458\n",
      "Epoch [125/300], Step [55/172], Loss: 12.1256\n",
      "Epoch [125/300], Step [56/172], Loss: 12.7739\n",
      "Epoch [125/300], Step [57/172], Loss: 17.6982\n",
      "Epoch [125/300], Step [58/172], Loss: 16.2093\n",
      "Epoch [125/300], Step [59/172], Loss: 29.3785\n",
      "Epoch [125/300], Step [60/172], Loss: 39.9210\n",
      "Epoch [125/300], Step [61/172], Loss: 7.3111\n",
      "Epoch [125/300], Step [62/172], Loss: 22.3490\n",
      "Epoch [125/300], Step [63/172], Loss: 9.1167\n",
      "Epoch [125/300], Step [64/172], Loss: 8.7201\n",
      "Epoch [125/300], Step [65/172], Loss: 20.0254\n",
      "Epoch [125/300], Step [66/172], Loss: 5.5654\n",
      "Epoch [125/300], Step [67/172], Loss: 26.1116\n",
      "Epoch [125/300], Step [68/172], Loss: 5.5024\n",
      "Epoch [125/300], Step [69/172], Loss: 52.5779\n",
      "Epoch [125/300], Step [70/172], Loss: 52.1398\n",
      "Epoch [125/300], Step [71/172], Loss: 49.3583\n",
      "Epoch [125/300], Step [72/172], Loss: 53.3443\n",
      "Epoch [125/300], Step [73/172], Loss: 58.8158\n",
      "Epoch [125/300], Step [74/172], Loss: 32.3500\n",
      "Epoch [125/300], Step [75/172], Loss: 32.2339\n",
      "Epoch [125/300], Step [76/172], Loss: 35.0120\n",
      "Epoch [125/300], Step [77/172], Loss: 60.5674\n",
      "Epoch [125/300], Step [78/172], Loss: 46.1601\n",
      "Epoch [125/300], Step [79/172], Loss: 45.4960\n",
      "Epoch [125/300], Step [80/172], Loss: 57.3023\n",
      "Epoch [125/300], Step [81/172], Loss: 40.1473\n",
      "Epoch [125/300], Step [82/172], Loss: 39.5188\n",
      "Epoch [125/300], Step [83/172], Loss: 46.9882\n",
      "Epoch [125/300], Step [84/172], Loss: 35.8497\n",
      "Epoch [125/300], Step [85/172], Loss: 40.3173\n",
      "Epoch [125/300], Step [86/172], Loss: 33.4072\n",
      "Epoch [125/300], Step [87/172], Loss: 27.6980\n",
      "Epoch [125/300], Step [88/172], Loss: 29.1049\n",
      "Epoch [125/300], Step [89/172], Loss: 25.7538\n",
      "Epoch [125/300], Step [90/172], Loss: 24.9078\n",
      "Epoch [125/300], Step [91/172], Loss: 27.8730\n",
      "Epoch [125/300], Step [92/172], Loss: 21.2685\n",
      "Epoch [125/300], Step [93/172], Loss: 21.2607\n",
      "Epoch [125/300], Step [94/172], Loss: 29.5847\n",
      "Epoch [125/300], Step [95/172], Loss: 22.3385\n",
      "Epoch [125/300], Step [96/172], Loss: 19.1817\n",
      "Epoch [125/300], Step [97/172], Loss: 26.3673\n",
      "Epoch [125/300], Step [98/172], Loss: 19.0437\n",
      "Epoch [125/300], Step [99/172], Loss: 18.1732\n",
      "Epoch [125/300], Step [100/172], Loss: 15.2776\n",
      "Epoch [125/300], Step [101/172], Loss: 16.7614\n",
      "Epoch [125/300], Step [102/172], Loss: 16.4843\n",
      "Epoch [125/300], Step [103/172], Loss: 13.0080\n",
      "Epoch [125/300], Step [104/172], Loss: 16.0992\n",
      "Epoch [125/300], Step [105/172], Loss: 17.9708\n",
      "Epoch [125/300], Step [106/172], Loss: 16.1514\n",
      "Epoch [125/300], Step [107/172], Loss: 14.8132\n",
      "Epoch [125/300], Step [108/172], Loss: 15.9813\n",
      "Epoch [125/300], Step [109/172], Loss: 16.2230\n",
      "Epoch [125/300], Step [110/172], Loss: 15.5719\n",
      "Epoch [125/300], Step [111/172], Loss: 13.7448\n",
      "Epoch [125/300], Step [112/172], Loss: 18.1264\n",
      "Epoch [125/300], Step [113/172], Loss: 13.3328\n",
      "Epoch [125/300], Step [114/172], Loss: 13.5592\n",
      "Epoch [125/300], Step [115/172], Loss: 20.1808\n",
      "Epoch [125/300], Step [116/172], Loss: 14.2533\n",
      "Epoch [125/300], Step [117/172], Loss: 10.9746\n",
      "Epoch [125/300], Step [118/172], Loss: 14.5387\n",
      "Epoch [125/300], Step [119/172], Loss: 15.0349\n",
      "Epoch [125/300], Step [120/172], Loss: 10.0144\n",
      "Epoch [125/300], Step [121/172], Loss: 10.1623\n",
      "Epoch [125/300], Step [122/172], Loss: 10.2655\n",
      "Epoch [125/300], Step [123/172], Loss: 10.1457\n",
      "Epoch [125/300], Step [124/172], Loss: 8.1375\n",
      "Epoch [125/300], Step [125/172], Loss: 12.5587\n",
      "Epoch [125/300], Step [126/172], Loss: 10.2669\n",
      "Epoch [125/300], Step [127/172], Loss: 11.2733\n",
      "Epoch [125/300], Step [128/172], Loss: 11.4594\n",
      "Epoch [125/300], Step [129/172], Loss: 8.3663\n",
      "Epoch [125/300], Step [130/172], Loss: 11.4342\n",
      "Epoch [125/300], Step [131/172], Loss: 8.0682\n",
      "Epoch [125/300], Step [132/172], Loss: 8.5019\n",
      "Epoch [125/300], Step [133/172], Loss: 9.3759\n",
      "Epoch [125/300], Step [134/172], Loss: 11.5216\n",
      "Epoch [125/300], Step [135/172], Loss: 8.6124\n",
      "Epoch [125/300], Step [136/172], Loss: 8.3016\n",
      "Epoch [125/300], Step [137/172], Loss: 9.4830\n",
      "Epoch [125/300], Step [138/172], Loss: 7.5519\n",
      "Epoch [125/300], Step [139/172], Loss: 9.3178\n",
      "Epoch [125/300], Step [140/172], Loss: 9.2205\n",
      "Epoch [125/300], Step [141/172], Loss: 10.6499\n",
      "Epoch [125/300], Step [142/172], Loss: 13.3948\n",
      "Epoch [125/300], Step [143/172], Loss: 9.6964\n",
      "Epoch [125/300], Step [144/172], Loss: 8.7012\n",
      "Epoch [125/300], Step [145/172], Loss: 9.5473\n",
      "Epoch [125/300], Step [146/172], Loss: 9.4080\n",
      "Epoch [125/300], Step [147/172], Loss: 5.1962\n",
      "Epoch [125/300], Step [148/172], Loss: 6.2650\n",
      "Epoch [125/300], Step [149/172], Loss: 7.2075\n",
      "Epoch [125/300], Step [150/172], Loss: 7.2276\n",
      "Epoch [125/300], Step [151/172], Loss: 6.3595\n",
      "Epoch [125/300], Step [152/172], Loss: 7.4235\n",
      "Epoch [125/300], Step [153/172], Loss: 6.7712\n",
      "Epoch [125/300], Step [154/172], Loss: 7.6134\n",
      "Epoch [125/300], Step [155/172], Loss: 6.5547\n",
      "Epoch [125/300], Step [156/172], Loss: 12.2207\n",
      "Epoch [125/300], Step [157/172], Loss: 9.7034\n",
      "Epoch [125/300], Step [158/172], Loss: 7.2608\n",
      "Epoch [125/300], Step [159/172], Loss: 9.3975\n",
      "Epoch [125/300], Step [160/172], Loss: 9.8453\n",
      "Epoch [125/300], Step [161/172], Loss: 6.9530\n",
      "Epoch [125/300], Step [162/172], Loss: 6.4732\n",
      "Epoch [125/300], Step [163/172], Loss: 6.3061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [125/300], Step [164/172], Loss: 8.9064\n",
      "Epoch [125/300], Step [165/172], Loss: 6.0728\n",
      "Epoch [125/300], Step [166/172], Loss: 5.6578\n",
      "Epoch [125/300], Step [167/172], Loss: 9.4382\n",
      "Epoch [125/300], Step [168/172], Loss: 6.9873\n",
      "Epoch [125/300], Step [169/172], Loss: 6.7201\n",
      "Epoch [125/300], Step [170/172], Loss: 5.2928\n",
      "Epoch [125/300], Step [171/172], Loss: 6.7232\n",
      "Epoch [125/300], Step [172/172], Loss: 5.1456\n",
      "Epoch [126/300], Step [1/172], Loss: 67.1217\n",
      "Epoch [126/300], Step [2/172], Loss: 68.0198\n",
      "Epoch [126/300], Step [3/172], Loss: 63.1435\n",
      "Epoch [126/300], Step [4/172], Loss: 38.0554\n",
      "Epoch [126/300], Step [5/172], Loss: 60.1981\n",
      "Epoch [126/300], Step [6/172], Loss: 19.6708\n",
      "Epoch [126/300], Step [7/172], Loss: 29.7922\n",
      "Epoch [126/300], Step [8/172], Loss: 5.5719\n",
      "Epoch [126/300], Step [9/172], Loss: 38.3714\n",
      "Epoch [126/300], Step [10/172], Loss: 45.3040\n",
      "Epoch [126/300], Step [11/172], Loss: 73.3103\n",
      "Epoch [126/300], Step [12/172], Loss: 80.9183\n",
      "Epoch [126/300], Step [13/172], Loss: 40.6522\n",
      "Epoch [126/300], Step [14/172], Loss: 77.2180\n",
      "Epoch [126/300], Step [15/172], Loss: 67.8970\n",
      "Epoch [126/300], Step [16/172], Loss: 15.1748\n",
      "Epoch [126/300], Step [17/172], Loss: 53.9922\n",
      "Epoch [126/300], Step [18/172], Loss: 62.8569\n",
      "Epoch [126/300], Step [19/172], Loss: 85.1212\n",
      "Epoch [126/300], Step [20/172], Loss: 55.9828\n",
      "Epoch [126/300], Step [21/172], Loss: 92.6778\n",
      "Epoch [126/300], Step [22/172], Loss: 71.3771\n",
      "Epoch [126/300], Step [23/172], Loss: 2.4146\n",
      "Epoch [126/300], Step [24/172], Loss: 66.0360\n",
      "Epoch [126/300], Step [25/172], Loss: 44.3349\n",
      "Epoch [126/300], Step [26/172], Loss: 55.4840\n",
      "Epoch [126/300], Step [27/172], Loss: 70.1424\n",
      "Epoch [126/300], Step [28/172], Loss: 29.9754\n",
      "Epoch [126/300], Step [29/172], Loss: 21.2888\n",
      "Epoch [126/300], Step [30/172], Loss: 78.6528\n",
      "Epoch [126/300], Step [31/172], Loss: 42.6242\n",
      "Epoch [126/300], Step [32/172], Loss: 41.8767\n",
      "Epoch [126/300], Step [33/172], Loss: 71.4067\n",
      "Epoch [126/300], Step [34/172], Loss: 4.0917\n",
      "Epoch [126/300], Step [35/172], Loss: 13.9146\n",
      "Epoch [126/300], Step [36/172], Loss: 19.6776\n",
      "Epoch [126/300], Step [37/172], Loss: 17.3455\n",
      "Epoch [126/300], Step [38/172], Loss: 27.6180\n",
      "Epoch [126/300], Step [39/172], Loss: 39.4981\n",
      "Epoch [126/300], Step [40/172], Loss: 19.7050\n",
      "Epoch [126/300], Step [41/172], Loss: 36.5954\n",
      "Epoch [126/300], Step [42/172], Loss: 39.8791\n",
      "Epoch [126/300], Step [43/172], Loss: 26.1550\n",
      "Epoch [126/300], Step [44/172], Loss: 19.9391\n",
      "Epoch [126/300], Step [45/172], Loss: 22.6648\n",
      "Epoch [126/300], Step [46/172], Loss: 19.9159\n",
      "Epoch [126/300], Step [47/172], Loss: 46.6363\n",
      "Epoch [126/300], Step [48/172], Loss: 54.1802\n",
      "Epoch [126/300], Step [49/172], Loss: 19.4937\n",
      "Epoch [126/300], Step [50/172], Loss: 49.0604\n",
      "Epoch [126/300], Step [51/172], Loss: 7.5327\n",
      "Epoch [126/300], Step [52/172], Loss: 17.0921\n",
      "Epoch [126/300], Step [53/172], Loss: 22.1194\n",
      "Epoch [126/300], Step [54/172], Loss: 11.7276\n",
      "Epoch [126/300], Step [55/172], Loss: 12.0932\n",
      "Epoch [126/300], Step [56/172], Loss: 12.8583\n",
      "Epoch [126/300], Step [57/172], Loss: 17.7473\n",
      "Epoch [126/300], Step [58/172], Loss: 16.1860\n",
      "Epoch [126/300], Step [59/172], Loss: 29.3423\n",
      "Epoch [126/300], Step [60/172], Loss: 39.4958\n",
      "Epoch [126/300], Step [61/172], Loss: 7.3228\n",
      "Epoch [126/300], Step [62/172], Loss: 22.2358\n",
      "Epoch [126/300], Step [63/172], Loss: 9.1111\n",
      "Epoch [126/300], Step [64/172], Loss: 8.7563\n",
      "Epoch [126/300], Step [65/172], Loss: 19.9696\n",
      "Epoch [126/300], Step [66/172], Loss: 5.5860\n",
      "Epoch [126/300], Step [67/172], Loss: 26.1823\n",
      "Epoch [126/300], Step [68/172], Loss: 5.4877\n",
      "Epoch [126/300], Step [69/172], Loss: 52.1161\n",
      "Epoch [126/300], Step [70/172], Loss: 51.7341\n",
      "Epoch [126/300], Step [71/172], Loss: 49.1496\n",
      "Epoch [126/300], Step [72/172], Loss: 53.1300\n",
      "Epoch [126/300], Step [73/172], Loss: 58.5704\n",
      "Epoch [126/300], Step [74/172], Loss: 32.2988\n",
      "Epoch [126/300], Step [75/172], Loss: 32.1037\n",
      "Epoch [126/300], Step [76/172], Loss: 34.9354\n",
      "Epoch [126/300], Step [77/172], Loss: 60.5022\n",
      "Epoch [126/300], Step [78/172], Loss: 46.0900\n",
      "Epoch [126/300], Step [79/172], Loss: 45.3466\n",
      "Epoch [126/300], Step [80/172], Loss: 57.3425\n",
      "Epoch [126/300], Step [81/172], Loss: 40.1026\n",
      "Epoch [126/300], Step [82/172], Loss: 39.5514\n",
      "Epoch [126/300], Step [83/172], Loss: 46.9356\n",
      "Epoch [126/300], Step [84/172], Loss: 35.8762\n",
      "Epoch [126/300], Step [85/172], Loss: 40.3650\n",
      "Epoch [126/300], Step [86/172], Loss: 33.3969\n",
      "Epoch [126/300], Step [87/172], Loss: 27.6772\n",
      "Epoch [126/300], Step [88/172], Loss: 29.0570\n",
      "Epoch [126/300], Step [89/172], Loss: 25.7234\n",
      "Epoch [126/300], Step [90/172], Loss: 24.8836\n",
      "Epoch [126/300], Step [91/172], Loss: 27.8798\n",
      "Epoch [126/300], Step [92/172], Loss: 21.2712\n",
      "Epoch [126/300], Step [93/172], Loss: 21.2240\n",
      "Epoch [126/300], Step [94/172], Loss: 29.5019\n",
      "Epoch [126/300], Step [95/172], Loss: 22.2388\n",
      "Epoch [126/300], Step [96/172], Loss: 19.1853\n",
      "Epoch [126/300], Step [97/172], Loss: 26.3700\n",
      "Epoch [126/300], Step [98/172], Loss: 19.0042\n",
      "Epoch [126/300], Step [99/172], Loss: 18.1830\n",
      "Epoch [126/300], Step [100/172], Loss: 15.2242\n",
      "Epoch [126/300], Step [101/172], Loss: 16.7763\n",
      "Epoch [126/300], Step [102/172], Loss: 16.4996\n",
      "Epoch [126/300], Step [103/172], Loss: 12.9800\n",
      "Epoch [126/300], Step [104/172], Loss: 16.1073\n",
      "Epoch [126/300], Step [105/172], Loss: 17.9464\n",
      "Epoch [126/300], Step [106/172], Loss: 16.1123\n",
      "Epoch [126/300], Step [107/172], Loss: 14.8787\n",
      "Epoch [126/300], Step [108/172], Loss: 15.9862\n",
      "Epoch [126/300], Step [109/172], Loss: 16.2137\n",
      "Epoch [126/300], Step [110/172], Loss: 15.5738\n",
      "Epoch [126/300], Step [111/172], Loss: 13.7223\n",
      "Epoch [126/300], Step [112/172], Loss: 18.0196\n",
      "Epoch [126/300], Step [113/172], Loss: 13.2970\n",
      "Epoch [126/300], Step [114/172], Loss: 13.5377\n",
      "Epoch [126/300], Step [115/172], Loss: 20.1564\n",
      "Epoch [126/300], Step [116/172], Loss: 14.2542\n",
      "Epoch [126/300], Step [117/172], Loss: 10.9358\n",
      "Epoch [126/300], Step [118/172], Loss: 14.4819\n",
      "Epoch [126/300], Step [119/172], Loss: 15.1212\n",
      "Epoch [126/300], Step [120/172], Loss: 10.0026\n",
      "Epoch [126/300], Step [121/172], Loss: 10.1477\n",
      "Epoch [126/300], Step [122/172], Loss: 10.2683\n",
      "Epoch [126/300], Step [123/172], Loss: 10.1444\n",
      "Epoch [126/300], Step [124/172], Loss: 8.1232\n",
      "Epoch [126/300], Step [125/172], Loss: 12.5497\n",
      "Epoch [126/300], Step [126/172], Loss: 10.2633\n",
      "Epoch [126/300], Step [127/172], Loss: 11.2430\n",
      "Epoch [126/300], Step [128/172], Loss: 11.4548\n",
      "Epoch [126/300], Step [129/172], Loss: 8.3629\n",
      "Epoch [126/300], Step [130/172], Loss: 11.4264\n",
      "Epoch [126/300], Step [131/172], Loss: 8.0504\n",
      "Epoch [126/300], Step [132/172], Loss: 8.4797\n",
      "Epoch [126/300], Step [133/172], Loss: 9.3714\n",
      "Epoch [126/300], Step [134/172], Loss: 11.5928\n",
      "Epoch [126/300], Step [135/172], Loss: 8.6455\n",
      "Epoch [126/300], Step [136/172], Loss: 8.2856\n",
      "Epoch [126/300], Step [137/172], Loss: 9.4724\n",
      "Epoch [126/300], Step [138/172], Loss: 7.5534\n",
      "Epoch [126/300], Step [139/172], Loss: 9.3526\n",
      "Epoch [126/300], Step [140/172], Loss: 9.2227\n",
      "Epoch [126/300], Step [141/172], Loss: 10.6471\n",
      "Epoch [126/300], Step [142/172], Loss: 13.4254\n",
      "Epoch [126/300], Step [143/172], Loss: 9.6938\n",
      "Epoch [126/300], Step [144/172], Loss: 8.7210\n",
      "Epoch [126/300], Step [145/172], Loss: 9.5415\n",
      "Epoch [126/300], Step [146/172], Loss: 9.4414\n",
      "Epoch [126/300], Step [147/172], Loss: 5.1808\n",
      "Epoch [126/300], Step [148/172], Loss: 6.2498\n",
      "Epoch [126/300], Step [149/172], Loss: 7.1875\n",
      "Epoch [126/300], Step [150/172], Loss: 7.1946\n",
      "Epoch [126/300], Step [151/172], Loss: 6.3679\n",
      "Epoch [126/300], Step [152/172], Loss: 7.4722\n",
      "Epoch [126/300], Step [153/172], Loss: 6.7721\n",
      "Epoch [126/300], Step [154/172], Loss: 7.6072\n",
      "Epoch [126/300], Step [155/172], Loss: 6.5415\n",
      "Epoch [126/300], Step [156/172], Loss: 12.2408\n",
      "Epoch [126/300], Step [157/172], Loss: 9.6958\n",
      "Epoch [126/300], Step [158/172], Loss: 7.2582\n",
      "Epoch [126/300], Step [159/172], Loss: 9.3877\n",
      "Epoch [126/300], Step [160/172], Loss: 9.8724\n",
      "Epoch [126/300], Step [161/172], Loss: 6.9891\n",
      "Epoch [126/300], Step [162/172], Loss: 6.4768\n",
      "Epoch [126/300], Step [163/172], Loss: 6.3189\n",
      "Epoch [126/300], Step [164/172], Loss: 8.9631\n",
      "Epoch [126/300], Step [165/172], Loss: 6.0854\n",
      "Epoch [126/300], Step [166/172], Loss: 5.6743\n",
      "Epoch [126/300], Step [167/172], Loss: 9.4634\n",
      "Epoch [126/300], Step [168/172], Loss: 6.9924\n",
      "Epoch [126/300], Step [169/172], Loss: 6.7184\n",
      "Epoch [126/300], Step [170/172], Loss: 5.3007\n",
      "Epoch [126/300], Step [171/172], Loss: 6.7529\n",
      "Epoch [126/300], Step [172/172], Loss: 5.1425\n",
      "Epoch [127/300], Step [1/172], Loss: 66.6179\n",
      "Epoch [127/300], Step [2/172], Loss: 67.7192\n",
      "Epoch [127/300], Step [3/172], Loss: 62.8703\n",
      "Epoch [127/300], Step [4/172], Loss: 37.6517\n",
      "Epoch [127/300], Step [5/172], Loss: 60.2459\n",
      "Epoch [127/300], Step [6/172], Loss: 19.3112\n",
      "Epoch [127/300], Step [7/172], Loss: 29.3900\n",
      "Epoch [127/300], Step [8/172], Loss: 5.4511\n",
      "Epoch [127/300], Step [9/172], Loss: 38.2328\n",
      "Epoch [127/300], Step [10/172], Loss: 45.2060\n",
      "Epoch [127/300], Step [11/172], Loss: 72.9671\n",
      "Epoch [127/300], Step [12/172], Loss: 80.6559\n",
      "Epoch [127/300], Step [13/172], Loss: 40.6404\n",
      "Epoch [127/300], Step [14/172], Loss: 76.7640\n",
      "Epoch [127/300], Step [15/172], Loss: 67.5129\n",
      "Epoch [127/300], Step [16/172], Loss: 14.6631\n",
      "Epoch [127/300], Step [17/172], Loss: 53.7521\n",
      "Epoch [127/300], Step [18/172], Loss: 62.8254\n",
      "Epoch [127/300], Step [19/172], Loss: 85.1305\n",
      "Epoch [127/300], Step [20/172], Loss: 55.3840\n",
      "Epoch [127/300], Step [21/172], Loss: 92.4451\n",
      "Epoch [127/300], Step [22/172], Loss: 70.9945\n",
      "Epoch [127/300], Step [23/172], Loss: 2.3947\n",
      "Epoch [127/300], Step [24/172], Loss: 65.7736\n",
      "Epoch [127/300], Step [25/172], Loss: 44.2863\n",
      "Epoch [127/300], Step [26/172], Loss: 55.3362\n",
      "Epoch [127/300], Step [27/172], Loss: 69.6212\n",
      "Epoch [127/300], Step [28/172], Loss: 29.6806\n",
      "Epoch [127/300], Step [29/172], Loss: 21.2606\n",
      "Epoch [127/300], Step [30/172], Loss: 78.1800\n",
      "Epoch [127/300], Step [31/172], Loss: 42.4700\n",
      "Epoch [127/300], Step [32/172], Loss: 41.7810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [127/300], Step [33/172], Loss: 71.3616\n",
      "Epoch [127/300], Step [34/172], Loss: 4.0551\n",
      "Epoch [127/300], Step [35/172], Loss: 13.9145\n",
      "Epoch [127/300], Step [36/172], Loss: 19.8043\n",
      "Epoch [127/300], Step [37/172], Loss: 17.3318\n",
      "Epoch [127/300], Step [38/172], Loss: 27.5291\n",
      "Epoch [127/300], Step [39/172], Loss: 39.3332\n",
      "Epoch [127/300], Step [40/172], Loss: 19.6695\n",
      "Epoch [127/300], Step [41/172], Loss: 36.5231\n",
      "Epoch [127/300], Step [42/172], Loss: 39.6582\n",
      "Epoch [127/300], Step [43/172], Loss: 26.1829\n",
      "Epoch [127/300], Step [44/172], Loss: 19.8663\n",
      "Epoch [127/300], Step [45/172], Loss: 22.6632\n",
      "Epoch [127/300], Step [46/172], Loss: 19.7892\n",
      "Epoch [127/300], Step [47/172], Loss: 46.6140\n",
      "Epoch [127/300], Step [48/172], Loss: 54.7040\n",
      "Epoch [127/300], Step [49/172], Loss: 19.5606\n",
      "Epoch [127/300], Step [50/172], Loss: 49.1577\n",
      "Epoch [127/300], Step [51/172], Loss: 7.5711\n",
      "Epoch [127/300], Step [52/172], Loss: 17.1262\n",
      "Epoch [127/300], Step [53/172], Loss: 22.1269\n",
      "Epoch [127/300], Step [54/172], Loss: 11.6953\n",
      "Epoch [127/300], Step [55/172], Loss: 12.0453\n",
      "Epoch [127/300], Step [56/172], Loss: 12.9851\n",
      "Epoch [127/300], Step [57/172], Loss: 17.8740\n",
      "Epoch [127/300], Step [58/172], Loss: 16.1297\n",
      "Epoch [127/300], Step [59/172], Loss: 29.5970\n",
      "Epoch [127/300], Step [60/172], Loss: 39.7893\n",
      "Epoch [127/300], Step [61/172], Loss: 7.2797\n",
      "Epoch [127/300], Step [62/172], Loss: 22.4300\n",
      "Epoch [127/300], Step [63/172], Loss: 9.1463\n",
      "Epoch [127/300], Step [64/172], Loss: 8.8027\n",
      "Epoch [127/300], Step [65/172], Loss: 20.0312\n",
      "Epoch [127/300], Step [66/172], Loss: 5.6208\n",
      "Epoch [127/300], Step [67/172], Loss: 26.2139\n",
      "Epoch [127/300], Step [68/172], Loss: 5.4245\n",
      "Epoch [127/300], Step [69/172], Loss: 51.5492\n",
      "Epoch [127/300], Step [70/172], Loss: 51.5404\n",
      "Epoch [127/300], Step [71/172], Loss: 48.9085\n",
      "Epoch [127/300], Step [72/172], Loss: 52.7909\n",
      "Epoch [127/300], Step [73/172], Loss: 58.3545\n",
      "Epoch [127/300], Step [74/172], Loss: 32.0439\n",
      "Epoch [127/300], Step [75/172], Loss: 32.0251\n",
      "Epoch [127/300], Step [76/172], Loss: 34.7765\n",
      "Epoch [127/300], Step [77/172], Loss: 60.2846\n",
      "Epoch [127/300], Step [78/172], Loss: 45.7647\n",
      "Epoch [127/300], Step [79/172], Loss: 44.9860\n",
      "Epoch [127/300], Step [80/172], Loss: 57.0849\n",
      "Epoch [127/300], Step [81/172], Loss: 39.8347\n",
      "Epoch [127/300], Step [82/172], Loss: 39.1063\n",
      "Epoch [127/300], Step [83/172], Loss: 46.5745\n",
      "Epoch [127/300], Step [84/172], Loss: 35.6496\n",
      "Epoch [127/300], Step [85/172], Loss: 40.1724\n",
      "Epoch [127/300], Step [86/172], Loss: 33.1980\n",
      "Epoch [127/300], Step [87/172], Loss: 27.4940\n",
      "Epoch [127/300], Step [88/172], Loss: 28.8044\n",
      "Epoch [127/300], Step [89/172], Loss: 25.4636\n",
      "Epoch [127/300], Step [90/172], Loss: 24.7123\n",
      "Epoch [127/300], Step [91/172], Loss: 27.7408\n",
      "Epoch [127/300], Step [92/172], Loss: 21.1047\n",
      "Epoch [127/300], Step [93/172], Loss: 21.1034\n",
      "Epoch [127/300], Step [94/172], Loss: 29.3405\n",
      "Epoch [127/300], Step [95/172], Loss: 22.0755\n",
      "Epoch [127/300], Step [96/172], Loss: 19.0301\n",
      "Epoch [127/300], Step [97/172], Loss: 26.2609\n",
      "Epoch [127/300], Step [98/172], Loss: 18.8290\n",
      "Epoch [127/300], Step [99/172], Loss: 18.0475\n",
      "Epoch [127/300], Step [100/172], Loss: 15.0628\n",
      "Epoch [127/300], Step [101/172], Loss: 16.6410\n",
      "Epoch [127/300], Step [102/172], Loss: 16.3333\n",
      "Epoch [127/300], Step [103/172], Loss: 12.8095\n",
      "Epoch [127/300], Step [104/172], Loss: 15.9924\n",
      "Epoch [127/300], Step [105/172], Loss: 17.7388\n",
      "Epoch [127/300], Step [106/172], Loss: 15.9646\n",
      "Epoch [127/300], Step [107/172], Loss: 14.8339\n",
      "Epoch [127/300], Step [108/172], Loss: 15.8700\n",
      "Epoch [127/300], Step [109/172], Loss: 16.0677\n",
      "Epoch [127/300], Step [110/172], Loss: 15.4139\n",
      "Epoch [127/300], Step [111/172], Loss: 13.6247\n",
      "Epoch [127/300], Step [112/172], Loss: 17.8877\n",
      "Epoch [127/300], Step [113/172], Loss: 13.1482\n",
      "Epoch [127/300], Step [114/172], Loss: 13.3956\n",
      "Epoch [127/300], Step [115/172], Loss: 20.0784\n",
      "Epoch [127/300], Step [116/172], Loss: 14.0662\n",
      "Epoch [127/300], Step [117/172], Loss: 10.7976\n",
      "Epoch [127/300], Step [118/172], Loss: 14.4458\n",
      "Epoch [127/300], Step [119/172], Loss: 15.0444\n",
      "Epoch [127/300], Step [120/172], Loss: 9.8915\n",
      "Epoch [127/300], Step [121/172], Loss: 10.0019\n",
      "Epoch [127/300], Step [122/172], Loss: 10.2982\n",
      "Epoch [127/300], Step [123/172], Loss: 10.0518\n",
      "Epoch [127/300], Step [124/172], Loss: 8.0252\n",
      "Epoch [127/300], Step [125/172], Loss: 12.4400\n",
      "Epoch [127/300], Step [126/172], Loss: 10.1585\n",
      "Epoch [127/300], Step [127/172], Loss: 11.0936\n",
      "Epoch [127/300], Step [128/172], Loss: 11.2432\n",
      "Epoch [127/300], Step [129/172], Loss: 8.2591\n",
      "Epoch [127/300], Step [130/172], Loss: 11.3155\n",
      "Epoch [127/300], Step [131/172], Loss: 7.9636\n",
      "Epoch [127/300], Step [132/172], Loss: 8.3497\n",
      "Epoch [127/300], Step [133/172], Loss: 9.3302\n",
      "Epoch [127/300], Step [134/172], Loss: 11.5573\n",
      "Epoch [127/300], Step [135/172], Loss: 8.5874\n",
      "Epoch [127/300], Step [136/172], Loss: 8.2040\n",
      "Epoch [127/300], Step [137/172], Loss: 9.3415\n",
      "Epoch [127/300], Step [138/172], Loss: 7.4447\n",
      "Epoch [127/300], Step [139/172], Loss: 9.2845\n",
      "Epoch [127/300], Step [140/172], Loss: 9.1225\n",
      "Epoch [127/300], Step [141/172], Loss: 10.5320\n",
      "Epoch [127/300], Step [142/172], Loss: 13.3288\n",
      "Epoch [127/300], Step [143/172], Loss: 9.6245\n",
      "Epoch [127/300], Step [144/172], Loss: 8.6793\n",
      "Epoch [127/300], Step [145/172], Loss: 9.4459\n",
      "Epoch [127/300], Step [146/172], Loss: 9.3608\n",
      "Epoch [127/300], Step [147/172], Loss: 5.1258\n",
      "Epoch [127/300], Step [148/172], Loss: 6.1761\n",
      "Epoch [127/300], Step [149/172], Loss: 7.1233\n",
      "Epoch [127/300], Step [150/172], Loss: 7.0946\n",
      "Epoch [127/300], Step [151/172], Loss: 6.2797\n",
      "Epoch [127/300], Step [152/172], Loss: 7.4289\n",
      "Epoch [127/300], Step [153/172], Loss: 6.7044\n",
      "Epoch [127/300], Step [154/172], Loss: 7.5475\n",
      "Epoch [127/300], Step [155/172], Loss: 6.4973\n",
      "Epoch [127/300], Step [156/172], Loss: 12.2319\n",
      "Epoch [127/300], Step [157/172], Loss: 9.6614\n",
      "Epoch [127/300], Step [158/172], Loss: 7.1991\n",
      "Epoch [127/300], Step [159/172], Loss: 9.3113\n",
      "Epoch [127/300], Step [160/172], Loss: 9.8664\n",
      "Epoch [127/300], Step [161/172], Loss: 6.9601\n",
      "Epoch [127/300], Step [162/172], Loss: 6.4361\n",
      "Epoch [127/300], Step [163/172], Loss: 6.2778\n",
      "Epoch [127/300], Step [164/172], Loss: 8.8321\n",
      "Epoch [127/300], Step [165/172], Loss: 6.0337\n",
      "Epoch [127/300], Step [166/172], Loss: 5.6136\n",
      "Epoch [127/300], Step [167/172], Loss: 9.4352\n",
      "Epoch [127/300], Step [168/172], Loss: 6.9630\n",
      "Epoch [127/300], Step [169/172], Loss: 6.6875\n",
      "Epoch [127/300], Step [170/172], Loss: 5.2538\n",
      "Epoch [127/300], Step [171/172], Loss: 6.7356\n",
      "Epoch [127/300], Step [172/172], Loss: 5.1409\n",
      "Epoch [128/300], Step [1/172], Loss: 66.2938\n",
      "Epoch [128/300], Step [2/172], Loss: 67.4193\n",
      "Epoch [128/300], Step [3/172], Loss: 62.6617\n",
      "Epoch [128/300], Step [4/172], Loss: 37.3135\n",
      "Epoch [128/300], Step [5/172], Loss: 59.9777\n",
      "Epoch [128/300], Step [6/172], Loss: 19.3203\n",
      "Epoch [128/300], Step [7/172], Loss: 29.6832\n",
      "Epoch [128/300], Step [8/172], Loss: 5.9357\n",
      "Epoch [128/300], Step [9/172], Loss: 38.1682\n",
      "Epoch [128/300], Step [10/172], Loss: 45.0302\n",
      "Epoch [128/300], Step [11/172], Loss: 72.6484\n",
      "Epoch [128/300], Step [12/172], Loss: 80.4046\n",
      "Epoch [128/300], Step [13/172], Loss: 40.8345\n",
      "Epoch [128/300], Step [14/172], Loss: 76.6124\n",
      "Epoch [128/300], Step [15/172], Loss: 67.3266\n",
      "Epoch [128/300], Step [16/172], Loss: 14.0413\n",
      "Epoch [128/300], Step [17/172], Loss: 53.6696\n",
      "Epoch [128/300], Step [18/172], Loss: 62.7036\n",
      "Epoch [128/300], Step [19/172], Loss: 84.8776\n",
      "Epoch [128/300], Step [20/172], Loss: 55.4257\n",
      "Epoch [128/300], Step [21/172], Loss: 91.7923\n",
      "Epoch [128/300], Step [22/172], Loss: 70.5652\n",
      "Epoch [128/300], Step [23/172], Loss: 2.3637\n",
      "Epoch [128/300], Step [24/172], Loss: 65.3342\n",
      "Epoch [128/300], Step [25/172], Loss: 44.1823\n",
      "Epoch [128/300], Step [26/172], Loss: 55.1096\n",
      "Epoch [128/300], Step [27/172], Loss: 68.8803\n",
      "Epoch [128/300], Step [28/172], Loss: 29.4708\n",
      "Epoch [128/300], Step [29/172], Loss: 20.9872\n",
      "Epoch [128/300], Step [30/172], Loss: 77.5582\n",
      "Epoch [128/300], Step [31/172], Loss: 41.9894\n",
      "Epoch [128/300], Step [32/172], Loss: 41.7257\n",
      "Epoch [128/300], Step [33/172], Loss: 71.1656\n",
      "Epoch [128/300], Step [34/172], Loss: 4.3246\n",
      "Epoch [128/300], Step [35/172], Loss: 13.7249\n",
      "Epoch [128/300], Step [36/172], Loss: 20.0079\n",
      "Epoch [128/300], Step [37/172], Loss: 17.3298\n",
      "Epoch [128/300], Step [38/172], Loss: 27.4878\n",
      "Epoch [128/300], Step [39/172], Loss: 39.0291\n",
      "Epoch [128/300], Step [40/172], Loss: 19.6263\n",
      "Epoch [128/300], Step [41/172], Loss: 36.2790\n",
      "Epoch [128/300], Step [42/172], Loss: 39.5919\n",
      "Epoch [128/300], Step [43/172], Loss: 26.1340\n",
      "Epoch [128/300], Step [44/172], Loss: 19.7711\n",
      "Epoch [128/300], Step [45/172], Loss: 22.7393\n",
      "Epoch [128/300], Step [46/172], Loss: 19.6587\n",
      "Epoch [128/300], Step [47/172], Loss: 46.4480\n",
      "Epoch [128/300], Step [48/172], Loss: 54.2899\n",
      "Epoch [128/300], Step [49/172], Loss: 19.5483\n",
      "Epoch [128/300], Step [50/172], Loss: 48.7873\n",
      "Epoch [128/300], Step [51/172], Loss: 7.5943\n",
      "Epoch [128/300], Step [52/172], Loss: 17.1105\n",
      "Epoch [128/300], Step [53/172], Loss: 22.0519\n",
      "Epoch [128/300], Step [54/172], Loss: 11.8118\n",
      "Epoch [128/300], Step [55/172], Loss: 12.1042\n",
      "Epoch [128/300], Step [56/172], Loss: 13.1006\n",
      "Epoch [128/300], Step [57/172], Loss: 17.7221\n",
      "Epoch [128/300], Step [58/172], Loss: 15.9877\n",
      "Epoch [128/300], Step [59/172], Loss: 29.2934\n",
      "Epoch [128/300], Step [60/172], Loss: 39.1970\n",
      "Epoch [128/300], Step [61/172], Loss: 7.2683\n",
      "Epoch [128/300], Step [62/172], Loss: 22.2759\n",
      "Epoch [128/300], Step [63/172], Loss: 9.2389\n",
      "Epoch [128/300], Step [64/172], Loss: 8.8488\n",
      "Epoch [128/300], Step [65/172], Loss: 20.0348\n",
      "Epoch [128/300], Step [66/172], Loss: 5.6509\n",
      "Epoch [128/300], Step [67/172], Loss: 26.1991\n",
      "Epoch [128/300], Step [68/172], Loss: 5.5009\n",
      "Epoch [128/300], Step [69/172], Loss: 51.2095\n",
      "Epoch [128/300], Step [70/172], Loss: 51.5402\n",
      "Epoch [128/300], Step [71/172], Loss: 48.7957\n",
      "Epoch [128/300], Step [72/172], Loss: 52.8029\n",
      "Epoch [128/300], Step [73/172], Loss: 58.4380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [128/300], Step [74/172], Loss: 32.0188\n",
      "Epoch [128/300], Step [75/172], Loss: 32.1036\n",
      "Epoch [128/300], Step [76/172], Loss: 34.8081\n",
      "Epoch [128/300], Step [77/172], Loss: 60.2311\n",
      "Epoch [128/300], Step [78/172], Loss: 45.7710\n",
      "Epoch [128/300], Step [79/172], Loss: 44.9327\n",
      "Epoch [128/300], Step [80/172], Loss: 57.1071\n",
      "Epoch [128/300], Step [81/172], Loss: 39.8610\n",
      "Epoch [128/300], Step [82/172], Loss: 39.4553\n",
      "Epoch [128/300], Step [83/172], Loss: 46.6519\n",
      "Epoch [128/300], Step [84/172], Loss: 35.5786\n",
      "Epoch [128/300], Step [85/172], Loss: 40.2226\n",
      "Epoch [128/300], Step [86/172], Loss: 33.2849\n",
      "Epoch [128/300], Step [87/172], Loss: 27.5719\n",
      "Epoch [128/300], Step [88/172], Loss: 28.9032\n",
      "Epoch [128/300], Step [89/172], Loss: 25.5774\n",
      "Epoch [128/300], Step [90/172], Loss: 24.7666\n",
      "Epoch [128/300], Step [91/172], Loss: 27.7238\n",
      "Epoch [128/300], Step [92/172], Loss: 21.1417\n",
      "Epoch [128/300], Step [93/172], Loss: 21.2107\n",
      "Epoch [128/300], Step [94/172], Loss: 29.5160\n",
      "Epoch [128/300], Step [95/172], Loss: 22.0657\n",
      "Epoch [128/300], Step [96/172], Loss: 19.1360\n",
      "Epoch [128/300], Step [97/172], Loss: 26.3779\n",
      "Epoch [128/300], Step [98/172], Loss: 18.9409\n",
      "Epoch [128/300], Step [99/172], Loss: 18.1594\n",
      "Epoch [128/300], Step [100/172], Loss: 15.1159\n",
      "Epoch [128/300], Step [101/172], Loss: 16.7436\n",
      "Epoch [128/300], Step [102/172], Loss: 16.4942\n",
      "Epoch [128/300], Step [103/172], Loss: 12.8643\n",
      "Epoch [128/300], Step [104/172], Loss: 16.1068\n",
      "Epoch [128/300], Step [105/172], Loss: 17.9078\n",
      "Epoch [128/300], Step [106/172], Loss: 16.0179\n",
      "Epoch [128/300], Step [107/172], Loss: 14.8334\n",
      "Epoch [128/300], Step [108/172], Loss: 15.9148\n",
      "Epoch [128/300], Step [109/172], Loss: 16.0934\n",
      "Epoch [128/300], Step [110/172], Loss: 15.4491\n",
      "Epoch [128/300], Step [111/172], Loss: 13.6777\n",
      "Epoch [128/300], Step [112/172], Loss: 17.9304\n",
      "Epoch [128/300], Step [113/172], Loss: 13.1057\n",
      "Epoch [128/300], Step [114/172], Loss: 13.4305\n",
      "Epoch [128/300], Step [115/172], Loss: 20.1268\n",
      "Epoch [128/300], Step [116/172], Loss: 14.0909\n",
      "Epoch [128/300], Step [117/172], Loss: 10.8353\n",
      "Epoch [128/300], Step [118/172], Loss: 14.4898\n",
      "Epoch [128/300], Step [119/172], Loss: 15.0136\n",
      "Epoch [128/300], Step [120/172], Loss: 9.8873\n",
      "Epoch [128/300], Step [121/172], Loss: 10.0079\n",
      "Epoch [128/300], Step [122/172], Loss: 10.2918\n",
      "Epoch [128/300], Step [123/172], Loss: 10.0728\n",
      "Epoch [128/300], Step [124/172], Loss: 8.0330\n",
      "Epoch [128/300], Step [125/172], Loss: 12.4087\n",
      "Epoch [128/300], Step [126/172], Loss: 10.1496\n",
      "Epoch [128/300], Step [127/172], Loss: 11.0904\n",
      "Epoch [128/300], Step [128/172], Loss: 11.2348\n",
      "Epoch [128/300], Step [129/172], Loss: 8.2651\n",
      "Epoch [128/300], Step [130/172], Loss: 11.3468\n",
      "Epoch [128/300], Step [131/172], Loss: 7.9559\n",
      "Epoch [128/300], Step [132/172], Loss: 8.3612\n",
      "Epoch [128/300], Step [133/172], Loss: 9.3176\n",
      "Epoch [128/300], Step [134/172], Loss: 11.5162\n",
      "Epoch [128/300], Step [135/172], Loss: 8.5692\n",
      "Epoch [128/300], Step [136/172], Loss: 8.1913\n",
      "Epoch [128/300], Step [137/172], Loss: 9.3388\n",
      "Epoch [128/300], Step [138/172], Loss: 7.4413\n",
      "Epoch [128/300], Step [139/172], Loss: 9.2645\n",
      "Epoch [128/300], Step [140/172], Loss: 9.1287\n",
      "Epoch [128/300], Step [141/172], Loss: 10.5653\n",
      "Epoch [128/300], Step [142/172], Loss: 13.3683\n",
      "Epoch [128/300], Step [143/172], Loss: 9.6597\n",
      "Epoch [128/300], Step [144/172], Loss: 8.6764\n",
      "Epoch [128/300], Step [145/172], Loss: 9.4781\n",
      "Epoch [128/300], Step [146/172], Loss: 9.3523\n",
      "Epoch [128/300], Step [147/172], Loss: 5.1174\n",
      "Epoch [128/300], Step [148/172], Loss: 6.1721\n",
      "Epoch [128/300], Step [149/172], Loss: 7.0824\n",
      "Epoch [128/300], Step [150/172], Loss: 7.0850\n",
      "Epoch [128/300], Step [151/172], Loss: 6.2702\n",
      "Epoch [128/300], Step [152/172], Loss: 7.3909\n",
      "Epoch [128/300], Step [153/172], Loss: 6.7090\n",
      "Epoch [128/300], Step [154/172], Loss: 7.5396\n",
      "Epoch [128/300], Step [155/172], Loss: 6.4914\n",
      "Epoch [128/300], Step [156/172], Loss: 12.2442\n",
      "Epoch [128/300], Step [157/172], Loss: 9.6578\n",
      "Epoch [128/300], Step [158/172], Loss: 7.2089\n",
      "Epoch [128/300], Step [159/172], Loss: 9.2893\n",
      "Epoch [128/300], Step [160/172], Loss: 9.8648\n",
      "Epoch [128/300], Step [161/172], Loss: 6.9445\n",
      "Epoch [128/300], Step [162/172], Loss: 6.3758\n",
      "Epoch [128/300], Step [163/172], Loss: 6.2779\n",
      "Epoch [128/300], Step [164/172], Loss: 8.8705\n",
      "Epoch [128/300], Step [165/172], Loss: 6.0335\n",
      "Epoch [128/300], Step [166/172], Loss: 5.6119\n",
      "Epoch [128/300], Step [167/172], Loss: 9.4426\n",
      "Epoch [128/300], Step [168/172], Loss: 6.9415\n",
      "Epoch [128/300], Step [169/172], Loss: 6.6972\n",
      "Epoch [128/300], Step [170/172], Loss: 5.2382\n",
      "Epoch [128/300], Step [171/172], Loss: 6.7400\n",
      "Epoch [128/300], Step [172/172], Loss: 5.1426\n",
      "Epoch [129/300], Step [1/172], Loss: 65.7687\n",
      "Epoch [129/300], Step [2/172], Loss: 67.1187\n",
      "Epoch [129/300], Step [3/172], Loss: 62.4128\n",
      "Epoch [129/300], Step [4/172], Loss: 37.0591\n",
      "Epoch [129/300], Step [5/172], Loss: 60.0622\n",
      "Epoch [129/300], Step [6/172], Loss: 19.2166\n",
      "Epoch [129/300], Step [7/172], Loss: 29.1448\n",
      "Epoch [129/300], Step [8/172], Loss: 5.4112\n",
      "Epoch [129/300], Step [9/172], Loss: 37.9390\n",
      "Epoch [129/300], Step [10/172], Loss: 45.0565\n",
      "Epoch [129/300], Step [11/172], Loss: 72.2540\n",
      "Epoch [129/300], Step [12/172], Loss: 80.2444\n",
      "Epoch [129/300], Step [13/172], Loss: 40.6141\n",
      "Epoch [129/300], Step [14/172], Loss: 76.0073\n",
      "Epoch [129/300], Step [15/172], Loss: 67.1925\n",
      "Epoch [129/300], Step [16/172], Loss: 14.7104\n",
      "Epoch [129/300], Step [17/172], Loss: 53.4495\n",
      "Epoch [129/300], Step [18/172], Loss: 62.7648\n",
      "Epoch [129/300], Step [19/172], Loss: 84.9818\n",
      "Epoch [129/300], Step [20/172], Loss: 54.9901\n",
      "Epoch [129/300], Step [21/172], Loss: 91.8661\n",
      "Epoch [129/300], Step [22/172], Loss: 70.5108\n",
      "Epoch [129/300], Step [23/172], Loss: 2.2972\n",
      "Epoch [129/300], Step [24/172], Loss: 65.4251\n",
      "Epoch [129/300], Step [25/172], Loss: 44.1862\n",
      "Epoch [129/300], Step [26/172], Loss: 55.0788\n",
      "Epoch [129/300], Step [27/172], Loss: 69.1795\n",
      "Epoch [129/300], Step [28/172], Loss: 29.3126\n",
      "Epoch [129/300], Step [29/172], Loss: 20.9554\n",
      "Epoch [129/300], Step [30/172], Loss: 77.5039\n",
      "Epoch [129/300], Step [31/172], Loss: 42.0703\n",
      "Epoch [129/300], Step [32/172], Loss: 41.8584\n",
      "Epoch [129/300], Step [33/172], Loss: 71.4508\n",
      "Epoch [129/300], Step [34/172], Loss: 4.0232\n",
      "Epoch [129/300], Step [35/172], Loss: 13.7159\n",
      "Epoch [129/300], Step [36/172], Loss: 19.6596\n",
      "Epoch [129/300], Step [37/172], Loss: 17.2541\n",
      "Epoch [129/300], Step [38/172], Loss: 27.4858\n",
      "Epoch [129/300], Step [39/172], Loss: 38.9000\n",
      "Epoch [129/300], Step [40/172], Loss: 19.5247\n",
      "Epoch [129/300], Step [41/172], Loss: 36.2472\n",
      "Epoch [129/300], Step [42/172], Loss: 39.4014\n",
      "Epoch [129/300], Step [43/172], Loss: 26.0310\n",
      "Epoch [129/300], Step [44/172], Loss: 19.7670\n",
      "Epoch [129/300], Step [45/172], Loss: 22.6145\n",
      "Epoch [129/300], Step [46/172], Loss: 19.5472\n",
      "Epoch [129/300], Step [47/172], Loss: 46.2919\n",
      "Epoch [129/300], Step [48/172], Loss: 54.7298\n",
      "Epoch [129/300], Step [49/172], Loss: 19.4382\n",
      "Epoch [129/300], Step [50/172], Loss: 48.8751\n",
      "Epoch [129/300], Step [51/172], Loss: 7.5570\n",
      "Epoch [129/300], Step [52/172], Loss: 17.1311\n",
      "Epoch [129/300], Step [53/172], Loss: 22.0671\n",
      "Epoch [129/300], Step [54/172], Loss: 11.7485\n",
      "Epoch [129/300], Step [55/172], Loss: 12.0874\n",
      "Epoch [129/300], Step [56/172], Loss: 13.3211\n",
      "Epoch [129/300], Step [57/172], Loss: 17.7986\n",
      "Epoch [129/300], Step [58/172], Loss: 16.0613\n",
      "Epoch [129/300], Step [59/172], Loss: 29.4555\n",
      "Epoch [129/300], Step [60/172], Loss: 38.9989\n",
      "Epoch [129/300], Step [61/172], Loss: 7.1584\n",
      "Epoch [129/300], Step [62/172], Loss: 22.3784\n",
      "Epoch [129/300], Step [63/172], Loss: 9.2694\n",
      "Epoch [129/300], Step [64/172], Loss: 8.8967\n",
      "Epoch [129/300], Step [65/172], Loss: 19.9642\n",
      "Epoch [129/300], Step [66/172], Loss: 5.6758\n",
      "Epoch [129/300], Step [67/172], Loss: 26.2096\n",
      "Epoch [129/300], Step [68/172], Loss: 5.4420\n",
      "Epoch [129/300], Step [69/172], Loss: 50.7033\n",
      "Epoch [129/300], Step [70/172], Loss: 51.2093\n",
      "Epoch [129/300], Step [71/172], Loss: 48.5921\n",
      "Epoch [129/300], Step [72/172], Loss: 52.4230\n",
      "Epoch [129/300], Step [73/172], Loss: 58.1378\n",
      "Epoch [129/300], Step [74/172], Loss: 31.9084\n",
      "Epoch [129/300], Step [75/172], Loss: 32.0534\n",
      "Epoch [129/300], Step [76/172], Loss: 34.6034\n",
      "Epoch [129/300], Step [77/172], Loss: 60.1664\n",
      "Epoch [129/300], Step [78/172], Loss: 45.6945\n",
      "Epoch [129/300], Step [79/172], Loss: 44.7571\n",
      "Epoch [129/300], Step [80/172], Loss: 57.1272\n",
      "Epoch [129/300], Step [81/172], Loss: 39.8090\n",
      "Epoch [129/300], Step [82/172], Loss: 39.3469\n",
      "Epoch [129/300], Step [83/172], Loss: 46.5737\n",
      "Epoch [129/300], Step [84/172], Loss: 35.6085\n",
      "Epoch [129/300], Step [85/172], Loss: 40.2852\n",
      "Epoch [129/300], Step [86/172], Loss: 33.2979\n",
      "Epoch [129/300], Step [87/172], Loss: 27.5318\n",
      "Epoch [129/300], Step [88/172], Loss: 28.8307\n",
      "Epoch [129/300], Step [89/172], Loss: 25.5598\n",
      "Epoch [129/300], Step [90/172], Loss: 24.7183\n",
      "Epoch [129/300], Step [91/172], Loss: 27.7081\n",
      "Epoch [129/300], Step [92/172], Loss: 21.1443\n",
      "Epoch [129/300], Step [93/172], Loss: 21.1941\n",
      "Epoch [129/300], Step [94/172], Loss: 29.4674\n",
      "Epoch [129/300], Step [95/172], Loss: 21.9464\n",
      "Epoch [129/300], Step [96/172], Loss: 19.1302\n",
      "Epoch [129/300], Step [97/172], Loss: 26.4545\n",
      "Epoch [129/300], Step [98/172], Loss: 18.8982\n",
      "Epoch [129/300], Step [99/172], Loss: 18.1198\n",
      "Epoch [129/300], Step [100/172], Loss: 15.0720\n",
      "Epoch [129/300], Step [101/172], Loss: 16.6967\n",
      "Epoch [129/300], Step [102/172], Loss: 16.4522\n",
      "Epoch [129/300], Step [103/172], Loss: 12.7761\n",
      "Epoch [129/300], Step [104/172], Loss: 16.0957\n",
      "Epoch [129/300], Step [105/172], Loss: 17.8287\n",
      "Epoch [129/300], Step [106/172], Loss: 15.9543\n",
      "Epoch [129/300], Step [107/172], Loss: 14.8580\n",
      "Epoch [129/300], Step [108/172], Loss: 15.8839\n",
      "Epoch [129/300], Step [109/172], Loss: 16.0216\n",
      "Epoch [129/300], Step [110/172], Loss: 15.3987\n",
      "Epoch [129/300], Step [111/172], Loss: 13.6521\n",
      "Epoch [129/300], Step [112/172], Loss: 17.8261\n",
      "Epoch [129/300], Step [113/172], Loss: 13.0232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [129/300], Step [114/172], Loss: 13.3718\n",
      "Epoch [129/300], Step [115/172], Loss: 20.0666\n",
      "Epoch [129/300], Step [116/172], Loss: 14.0029\n",
      "Epoch [129/300], Step [117/172], Loss: 10.7624\n",
      "Epoch [129/300], Step [118/172], Loss: 14.4777\n",
      "Epoch [129/300], Step [119/172], Loss: 15.0402\n",
      "Epoch [129/300], Step [120/172], Loss: 9.8576\n",
      "Epoch [129/300], Step [121/172], Loss: 9.9150\n",
      "Epoch [129/300], Step [122/172], Loss: 10.3126\n",
      "Epoch [129/300], Step [123/172], Loss: 10.0126\n",
      "Epoch [129/300], Step [124/172], Loss: 7.9709\n",
      "Epoch [129/300], Step [125/172], Loss: 12.3305\n",
      "Epoch [129/300], Step [126/172], Loss: 10.0799\n",
      "Epoch [129/300], Step [127/172], Loss: 11.0179\n",
      "Epoch [129/300], Step [128/172], Loss: 11.1219\n",
      "Epoch [129/300], Step [129/172], Loss: 8.1831\n",
      "Epoch [129/300], Step [130/172], Loss: 11.2906\n",
      "Epoch [129/300], Step [131/172], Loss: 7.8994\n",
      "Epoch [129/300], Step [132/172], Loss: 8.2756\n",
      "Epoch [129/300], Step [133/172], Loss: 9.3074\n",
      "Epoch [129/300], Step [134/172], Loss: 11.5591\n",
      "Epoch [129/300], Step [135/172], Loss: 8.5515\n",
      "Epoch [129/300], Step [136/172], Loss: 8.1237\n",
      "Epoch [129/300], Step [137/172], Loss: 9.2497\n",
      "Epoch [129/300], Step [138/172], Loss: 7.3424\n",
      "Epoch [129/300], Step [139/172], Loss: 9.2356\n",
      "Epoch [129/300], Step [140/172], Loss: 9.0601\n",
      "Epoch [129/300], Step [141/172], Loss: 10.4536\n",
      "Epoch [129/300], Step [142/172], Loss: 13.3286\n",
      "Epoch [129/300], Step [143/172], Loss: 9.5998\n",
      "Epoch [129/300], Step [144/172], Loss: 8.6226\n",
      "Epoch [129/300], Step [145/172], Loss: 9.3990\n",
      "Epoch [129/300], Step [146/172], Loss: 9.3071\n",
      "Epoch [129/300], Step [147/172], Loss: 5.0721\n",
      "Epoch [129/300], Step [148/172], Loss: 6.1145\n",
      "Epoch [129/300], Step [149/172], Loss: 7.0143\n",
      "Epoch [129/300], Step [150/172], Loss: 6.9962\n",
      "Epoch [129/300], Step [151/172], Loss: 6.2071\n",
      "Epoch [129/300], Step [152/172], Loss: 7.3780\n",
      "Epoch [129/300], Step [153/172], Loss: 6.6504\n",
      "Epoch [129/300], Step [154/172], Loss: 7.4874\n",
      "Epoch [129/300], Step [155/172], Loss: 6.4467\n",
      "Epoch [129/300], Step [156/172], Loss: 12.2497\n",
      "Epoch [129/300], Step [157/172], Loss: 9.6137\n",
      "Epoch [129/300], Step [158/172], Loss: 7.1603\n",
      "Epoch [129/300], Step [159/172], Loss: 9.2533\n",
      "Epoch [129/300], Step [160/172], Loss: 9.8512\n",
      "Epoch [129/300], Step [161/172], Loss: 6.9017\n",
      "Epoch [129/300], Step [162/172], Loss: 6.3250\n",
      "Epoch [129/300], Step [163/172], Loss: 6.2389\n",
      "Epoch [129/300], Step [164/172], Loss: 8.8162\n",
      "Epoch [129/300], Step [165/172], Loss: 5.9775\n",
      "Epoch [129/300], Step [166/172], Loss: 5.5717\n",
      "Epoch [129/300], Step [167/172], Loss: 9.4328\n",
      "Epoch [129/300], Step [168/172], Loss: 6.9123\n",
      "Epoch [129/300], Step [169/172], Loss: 6.6526\n",
      "Epoch [129/300], Step [170/172], Loss: 5.1960\n",
      "Epoch [129/300], Step [171/172], Loss: 6.7232\n",
      "Epoch [129/300], Step [172/172], Loss: 5.1271\n",
      "Epoch [130/300], Step [1/172], Loss: 65.4896\n",
      "Epoch [130/300], Step [2/172], Loss: 66.7786\n",
      "Epoch [130/300], Step [3/172], Loss: 62.3134\n",
      "Epoch [130/300], Step [4/172], Loss: 36.7722\n",
      "Epoch [130/300], Step [5/172], Loss: 59.6606\n",
      "Epoch [130/300], Step [6/172], Loss: 19.3502\n",
      "Epoch [130/300], Step [7/172], Loss: 29.5825\n",
      "Epoch [130/300], Step [8/172], Loss: 5.5932\n",
      "Epoch [130/300], Step [9/172], Loss: 37.8841\n",
      "Epoch [130/300], Step [10/172], Loss: 45.0116\n",
      "Epoch [130/300], Step [11/172], Loss: 71.9860\n",
      "Epoch [130/300], Step [12/172], Loss: 79.9490\n",
      "Epoch [130/300], Step [13/172], Loss: 40.8175\n",
      "Epoch [130/300], Step [14/172], Loss: 75.8402\n",
      "Epoch [130/300], Step [15/172], Loss: 66.8763\n",
      "Epoch [130/300], Step [16/172], Loss: 14.0016\n",
      "Epoch [130/300], Step [17/172], Loss: 53.2899\n",
      "Epoch [130/300], Step [18/172], Loss: 62.6660\n",
      "Epoch [130/300], Step [19/172], Loss: 84.8619\n",
      "Epoch [130/300], Step [20/172], Loss: 54.7493\n",
      "Epoch [130/300], Step [21/172], Loss: 91.4420\n",
      "Epoch [130/300], Step [22/172], Loss: 70.3030\n",
      "Epoch [130/300], Step [23/172], Loss: 2.2664\n",
      "Epoch [130/300], Step [24/172], Loss: 65.1302\n",
      "Epoch [130/300], Step [25/172], Loss: 44.2622\n",
      "Epoch [130/300], Step [26/172], Loss: 54.9927\n",
      "Epoch [130/300], Step [27/172], Loss: 68.6930\n",
      "Epoch [130/300], Step [28/172], Loss: 29.3103\n",
      "Epoch [130/300], Step [29/172], Loss: 21.0361\n",
      "Epoch [130/300], Step [30/172], Loss: 77.1019\n",
      "Epoch [130/300], Step [31/172], Loss: 41.8643\n",
      "Epoch [130/300], Step [32/172], Loss: 41.9197\n",
      "Epoch [130/300], Step [33/172], Loss: 71.4221\n",
      "Epoch [130/300], Step [34/172], Loss: 4.2701\n",
      "Epoch [130/300], Step [35/172], Loss: 13.5895\n",
      "Epoch [130/300], Step [36/172], Loss: 20.0963\n",
      "Epoch [130/300], Step [37/172], Loss: 17.3213\n",
      "Epoch [130/300], Step [38/172], Loss: 27.5661\n",
      "Epoch [130/300], Step [39/172], Loss: 38.7057\n",
      "Epoch [130/300], Step [40/172], Loss: 19.6234\n",
      "Epoch [130/300], Step [41/172], Loss: 36.1251\n",
      "Epoch [130/300], Step [42/172], Loss: 39.2576\n",
      "Epoch [130/300], Step [43/172], Loss: 26.1075\n",
      "Epoch [130/300], Step [44/172], Loss: 19.6145\n",
      "Epoch [130/300], Step [45/172], Loss: 22.8053\n",
      "Epoch [130/300], Step [46/172], Loss: 19.4422\n",
      "Epoch [130/300], Step [47/172], Loss: 46.1962\n",
      "Epoch [130/300], Step [48/172], Loss: 54.5856\n",
      "Epoch [130/300], Step [49/172], Loss: 19.5506\n",
      "Epoch [130/300], Step [50/172], Loss: 48.3891\n",
      "Epoch [130/300], Step [51/172], Loss: 7.6269\n",
      "Epoch [130/300], Step [52/172], Loss: 17.1303\n",
      "Epoch [130/300], Step [53/172], Loss: 22.0127\n",
      "Epoch [130/300], Step [54/172], Loss: 11.8916\n",
      "Epoch [130/300], Step [55/172], Loss: 12.1452\n",
      "Epoch [130/300], Step [56/172], Loss: 13.4793\n",
      "Epoch [130/300], Step [57/172], Loss: 17.6799\n",
      "Epoch [130/300], Step [58/172], Loss: 15.9255\n",
      "Epoch [130/300], Step [59/172], Loss: 29.2423\n",
      "Epoch [130/300], Step [60/172], Loss: 38.6746\n",
      "Epoch [130/300], Step [61/172], Loss: 7.1094\n",
      "Epoch [130/300], Step [62/172], Loss: 22.2095\n",
      "Epoch [130/300], Step [63/172], Loss: 9.3105\n",
      "Epoch [130/300], Step [64/172], Loss: 8.9286\n",
      "Epoch [130/300], Step [65/172], Loss: 19.9024\n",
      "Epoch [130/300], Step [66/172], Loss: 5.6799\n",
      "Epoch [130/300], Step [67/172], Loss: 26.0505\n",
      "Epoch [130/300], Step [68/172], Loss: 5.3946\n",
      "Epoch [130/300], Step [69/172], Loss: 50.0944\n",
      "Epoch [130/300], Step [70/172], Loss: 51.0382\n",
      "Epoch [130/300], Step [71/172], Loss: 48.3734\n",
      "Epoch [130/300], Step [72/172], Loss: 52.2588\n",
      "Epoch [130/300], Step [73/172], Loss: 58.0488\n",
      "Epoch [130/300], Step [74/172], Loss: 31.7133\n",
      "Epoch [130/300], Step [75/172], Loss: 31.8315\n",
      "Epoch [130/300], Step [76/172], Loss: 34.4856\n",
      "Epoch [130/300], Step [77/172], Loss: 60.0595\n",
      "Epoch [130/300], Step [78/172], Loss: 45.5799\n",
      "Epoch [130/300], Step [79/172], Loss: 44.7034\n",
      "Epoch [130/300], Step [80/172], Loss: 56.9751\n",
      "Epoch [130/300], Step [81/172], Loss: 39.7607\n",
      "Epoch [130/300], Step [82/172], Loss: 39.3420\n",
      "Epoch [130/300], Step [83/172], Loss: 46.5698\n",
      "Epoch [130/300], Step [84/172], Loss: 35.4805\n",
      "Epoch [130/300], Step [85/172], Loss: 40.2232\n",
      "Epoch [130/300], Step [86/172], Loss: 33.1859\n",
      "Epoch [130/300], Step [87/172], Loss: 27.4975\n",
      "Epoch [130/300], Step [88/172], Loss: 28.8074\n",
      "Epoch [130/300], Step [89/172], Loss: 25.5003\n",
      "Epoch [130/300], Step [90/172], Loss: 24.6344\n",
      "Epoch [130/300], Step [91/172], Loss: 27.5801\n",
      "Epoch [130/300], Step [92/172], Loss: 21.0597\n",
      "Epoch [130/300], Step [93/172], Loss: 21.1602\n",
      "Epoch [130/300], Step [94/172], Loss: 29.4369\n",
      "Epoch [130/300], Step [95/172], Loss: 21.8347\n",
      "Epoch [130/300], Step [96/172], Loss: 19.0904\n",
      "Epoch [130/300], Step [97/172], Loss: 26.4278\n",
      "Epoch [130/300], Step [98/172], Loss: 18.8848\n",
      "Epoch [130/300], Step [99/172], Loss: 18.1450\n",
      "Epoch [130/300], Step [100/172], Loss: 15.0035\n",
      "Epoch [130/300], Step [101/172], Loss: 16.7226\n",
      "Epoch [130/300], Step [102/172], Loss: 16.4589\n",
      "Epoch [130/300], Step [103/172], Loss: 12.7261\n",
      "Epoch [130/300], Step [104/172], Loss: 16.1050\n",
      "Epoch [130/300], Step [105/172], Loss: 17.8164\n",
      "Epoch [130/300], Step [106/172], Loss: 15.9149\n",
      "Epoch [130/300], Step [107/172], Loss: 14.8000\n",
      "Epoch [130/300], Step [108/172], Loss: 15.8425\n",
      "Epoch [130/300], Step [109/172], Loss: 15.9830\n",
      "Epoch [130/300], Step [110/172], Loss: 15.3417\n",
      "Epoch [130/300], Step [111/172], Loss: 13.6164\n",
      "Epoch [130/300], Step [112/172], Loss: 17.7499\n",
      "Epoch [130/300], Step [113/172], Loss: 12.9579\n",
      "Epoch [130/300], Step [114/172], Loss: 13.3319\n",
      "Epoch [130/300], Step [115/172], Loss: 20.0851\n",
      "Epoch [130/300], Step [116/172], Loss: 13.9892\n",
      "Epoch [130/300], Step [117/172], Loss: 10.7438\n",
      "Epoch [130/300], Step [118/172], Loss: 14.4302\n",
      "Epoch [130/300], Step [119/172], Loss: 14.9603\n",
      "Epoch [130/300], Step [120/172], Loss: 9.8121\n",
      "Epoch [130/300], Step [121/172], Loss: 9.8573\n",
      "Epoch [130/300], Step [122/172], Loss: 10.2563\n",
      "Epoch [130/300], Step [123/172], Loss: 9.9798\n",
      "Epoch [130/300], Step [124/172], Loss: 7.9545\n",
      "Epoch [130/300], Step [125/172], Loss: 12.2686\n",
      "Epoch [130/300], Step [126/172], Loss: 10.0584\n",
      "Epoch [130/300], Step [127/172], Loss: 10.9548\n",
      "Epoch [130/300], Step [128/172], Loss: 11.0446\n",
      "Epoch [130/300], Step [129/172], Loss: 8.1615\n",
      "Epoch [130/300], Step [130/172], Loss: 11.2726\n",
      "Epoch [130/300], Step [131/172], Loss: 7.8485\n",
      "Epoch [130/300], Step [132/172], Loss: 8.2467\n",
      "Epoch [130/300], Step [133/172], Loss: 9.2583\n",
      "Epoch [130/300], Step [134/172], Loss: 11.4681\n",
      "Epoch [130/300], Step [135/172], Loss: 8.4911\n",
      "Epoch [130/300], Step [136/172], Loss: 8.0965\n",
      "Epoch [130/300], Step [137/172], Loss: 9.1998\n",
      "Epoch [130/300], Step [138/172], Loss: 7.3030\n",
      "Epoch [130/300], Step [139/172], Loss: 9.1821\n",
      "Epoch [130/300], Step [140/172], Loss: 9.0461\n",
      "Epoch [130/300], Step [141/172], Loss: 10.4139\n",
      "Epoch [130/300], Step [142/172], Loss: 13.2852\n",
      "Epoch [130/300], Step [143/172], Loss: 9.6033\n",
      "Epoch [130/300], Step [144/172], Loss: 8.6060\n",
      "Epoch [130/300], Step [145/172], Loss: 9.3861\n",
      "Epoch [130/300], Step [146/172], Loss: 9.2722\n",
      "Epoch [130/300], Step [147/172], Loss: 5.0538\n",
      "Epoch [130/300], Step [148/172], Loss: 6.0962\n",
      "Epoch [130/300], Step [149/172], Loss: 6.9695\n",
      "Epoch [130/300], Step [150/172], Loss: 6.9421\n",
      "Epoch [130/300], Step [151/172], Loss: 6.1830\n",
      "Epoch [130/300], Step [152/172], Loss: 7.3175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/300], Step [153/172], Loss: 6.6223\n",
      "Epoch [130/300], Step [154/172], Loss: 7.4678\n",
      "Epoch [130/300], Step [155/172], Loss: 6.3946\n",
      "Epoch [130/300], Step [156/172], Loss: 12.2402\n",
      "Epoch [130/300], Step [157/172], Loss: 9.5961\n",
      "Epoch [130/300], Step [158/172], Loss: 7.1424\n",
      "Epoch [130/300], Step [159/172], Loss: 9.2149\n",
      "Epoch [130/300], Step [160/172], Loss: 9.8486\n",
      "Epoch [130/300], Step [161/172], Loss: 6.8761\n",
      "Epoch [130/300], Step [162/172], Loss: 6.2850\n",
      "Epoch [130/300], Step [163/172], Loss: 6.2156\n",
      "Epoch [130/300], Step [164/172], Loss: 8.7867\n",
      "Epoch [130/300], Step [165/172], Loss: 5.9652\n",
      "Epoch [130/300], Step [166/172], Loss: 5.5442\n",
      "Epoch [130/300], Step [167/172], Loss: 9.3764\n",
      "Epoch [130/300], Step [168/172], Loss: 6.8771\n",
      "Epoch [130/300], Step [169/172], Loss: 6.6314\n",
      "Epoch [130/300], Step [170/172], Loss: 5.1598\n",
      "Epoch [130/300], Step [171/172], Loss: 6.7209\n",
      "Epoch [130/300], Step [172/172], Loss: 5.0899\n",
      "Epoch [131/300], Step [1/172], Loss: 64.9559\n",
      "Epoch [131/300], Step [2/172], Loss: 66.4109\n",
      "Epoch [131/300], Step [3/172], Loss: 61.8764\n",
      "Epoch [131/300], Step [4/172], Loss: 36.5692\n",
      "Epoch [131/300], Step [5/172], Loss: 59.6710\n",
      "Epoch [131/300], Step [6/172], Loss: 19.1481\n",
      "Epoch [131/300], Step [7/172], Loss: 29.3416\n",
      "Epoch [131/300], Step [8/172], Loss: 5.5367\n",
      "Epoch [131/300], Step [9/172], Loss: 37.8646\n",
      "Epoch [131/300], Step [10/172], Loss: 45.0886\n",
      "Epoch [131/300], Step [11/172], Loss: 71.9029\n",
      "Epoch [131/300], Step [12/172], Loss: 80.0271\n",
      "Epoch [131/300], Step [13/172], Loss: 40.8928\n",
      "Epoch [131/300], Step [14/172], Loss: 75.8541\n",
      "Epoch [131/300], Step [15/172], Loss: 66.9813\n",
      "Epoch [131/300], Step [16/172], Loss: 14.2232\n",
      "Epoch [131/300], Step [17/172], Loss: 53.5071\n",
      "Epoch [131/300], Step [18/172], Loss: 62.9198\n",
      "Epoch [131/300], Step [19/172], Loss: 85.3904\n",
      "Epoch [131/300], Step [20/172], Loss: 54.5471\n",
      "Epoch [131/300], Step [21/172], Loss: 91.8657\n",
      "Epoch [131/300], Step [22/172], Loss: 70.3936\n",
      "Epoch [131/300], Step [23/172], Loss: 2.3064\n",
      "Epoch [131/300], Step [24/172], Loss: 65.3923\n",
      "Epoch [131/300], Step [25/172], Loss: 44.4352\n",
      "Epoch [131/300], Step [26/172], Loss: 55.0085\n",
      "Epoch [131/300], Step [27/172], Loss: 69.0113\n",
      "Epoch [131/300], Step [28/172], Loss: 29.1862\n",
      "Epoch [131/300], Step [29/172], Loss: 20.9157\n",
      "Epoch [131/300], Step [30/172], Loss: 76.7982\n",
      "Epoch [131/300], Step [31/172], Loss: 41.7942\n",
      "Epoch [131/300], Step [32/172], Loss: 42.1032\n",
      "Epoch [131/300], Step [33/172], Loss: 71.6992\n",
      "Epoch [131/300], Step [34/172], Loss: 3.9790\n",
      "Epoch [131/300], Step [35/172], Loss: 13.6310\n",
      "Epoch [131/300], Step [36/172], Loss: 19.6386\n",
      "Epoch [131/300], Step [37/172], Loss: 17.3366\n",
      "Epoch [131/300], Step [38/172], Loss: 27.7700\n",
      "Epoch [131/300], Step [39/172], Loss: 38.6482\n",
      "Epoch [131/300], Step [40/172], Loss: 19.5330\n",
      "Epoch [131/300], Step [41/172], Loss: 36.1009\n",
      "Epoch [131/300], Step [42/172], Loss: 39.3413\n",
      "Epoch [131/300], Step [43/172], Loss: 26.0860\n",
      "Epoch [131/300], Step [44/172], Loss: 19.6332\n",
      "Epoch [131/300], Step [45/172], Loss: 22.7284\n",
      "Epoch [131/300], Step [46/172], Loss: 19.4531\n",
      "Epoch [131/300], Step [47/172], Loss: 46.1196\n",
      "Epoch [131/300], Step [48/172], Loss: 54.7708\n",
      "Epoch [131/300], Step [49/172], Loss: 19.3872\n",
      "Epoch [131/300], Step [50/172], Loss: 48.3876\n",
      "Epoch [131/300], Step [51/172], Loss: 7.5869\n",
      "Epoch [131/300], Step [52/172], Loss: 17.1306\n",
      "Epoch [131/300], Step [53/172], Loss: 21.9578\n",
      "Epoch [131/300], Step [54/172], Loss: 11.7850\n",
      "Epoch [131/300], Step [55/172], Loss: 12.0617\n",
      "Epoch [131/300], Step [56/172], Loss: 13.6234\n",
      "Epoch [131/300], Step [57/172], Loss: 17.4955\n",
      "Epoch [131/300], Step [58/172], Loss: 15.8328\n",
      "Epoch [131/300], Step [59/172], Loss: 29.3323\n",
      "Epoch [131/300], Step [60/172], Loss: 38.2123\n",
      "Epoch [131/300], Step [61/172], Loss: 7.0026\n",
      "Epoch [131/300], Step [62/172], Loss: 22.2289\n",
      "Epoch [131/300], Step [63/172], Loss: 9.3485\n",
      "Epoch [131/300], Step [64/172], Loss: 8.9774\n",
      "Epoch [131/300], Step [65/172], Loss: 19.8378\n",
      "Epoch [131/300], Step [66/172], Loss: 5.6852\n",
      "Epoch [131/300], Step [67/172], Loss: 25.8758\n",
      "Epoch [131/300], Step [68/172], Loss: 5.3204\n",
      "Epoch [131/300], Step [69/172], Loss: 49.4765\n",
      "Epoch [131/300], Step [70/172], Loss: 50.6405\n",
      "Epoch [131/300], Step [71/172], Loss: 48.0751\n",
      "Epoch [131/300], Step [72/172], Loss: 51.9049\n",
      "Epoch [131/300], Step [73/172], Loss: 57.7447\n",
      "Epoch [131/300], Step [74/172], Loss: 31.5925\n",
      "Epoch [131/300], Step [75/172], Loss: 31.6693\n",
      "Epoch [131/300], Step [76/172], Loss: 34.2376\n",
      "Epoch [131/300], Step [77/172], Loss: 59.8684\n",
      "Epoch [131/300], Step [78/172], Loss: 45.3761\n",
      "Epoch [131/300], Step [79/172], Loss: 44.4374\n",
      "Epoch [131/300], Step [80/172], Loss: 56.9429\n",
      "Epoch [131/300], Step [81/172], Loss: 39.5694\n",
      "Epoch [131/300], Step [82/172], Loss: 39.3612\n",
      "Epoch [131/300], Step [83/172], Loss: 46.3749\n",
      "Epoch [131/300], Step [84/172], Loss: 35.4579\n",
      "Epoch [131/300], Step [85/172], Loss: 40.0439\n",
      "Epoch [131/300], Step [86/172], Loss: 33.1488\n",
      "Epoch [131/300], Step [87/172], Loss: 27.4317\n",
      "Epoch [131/300], Step [88/172], Loss: 28.6861\n",
      "Epoch [131/300], Step [89/172], Loss: 25.4868\n",
      "Epoch [131/300], Step [90/172], Loss: 24.5533\n",
      "Epoch [131/300], Step [91/172], Loss: 27.5834\n",
      "Epoch [131/300], Step [92/172], Loss: 21.0371\n",
      "Epoch [131/300], Step [93/172], Loss: 21.1839\n",
      "Epoch [131/300], Step [94/172], Loss: 29.4094\n",
      "Epoch [131/300], Step [95/172], Loss: 21.7458\n",
      "Epoch [131/300], Step [96/172], Loss: 19.1240\n",
      "Epoch [131/300], Step [97/172], Loss: 26.4475\n",
      "Epoch [131/300], Step [98/172], Loss: 18.8404\n",
      "Epoch [131/300], Step [99/172], Loss: 18.1136\n",
      "Epoch [131/300], Step [100/172], Loss: 14.9621\n",
      "Epoch [131/300], Step [101/172], Loss: 16.6919\n",
      "Epoch [131/300], Step [102/172], Loss: 16.5302\n",
      "Epoch [131/300], Step [103/172], Loss: 12.6922\n",
      "Epoch [131/300], Step [104/172], Loss: 16.1172\n",
      "Epoch [131/300], Step [105/172], Loss: 17.8748\n",
      "Epoch [131/300], Step [106/172], Loss: 15.8879\n",
      "Epoch [131/300], Step [107/172], Loss: 14.8371\n",
      "Epoch [131/300], Step [108/172], Loss: 15.8372\n",
      "Epoch [131/300], Step [109/172], Loss: 15.9459\n",
      "Epoch [131/300], Step [110/172], Loss: 15.3530\n",
      "Epoch [131/300], Step [111/172], Loss: 13.6423\n",
      "Epoch [131/300], Step [112/172], Loss: 17.7147\n",
      "Epoch [131/300], Step [113/172], Loss: 12.9464\n",
      "Epoch [131/300], Step [114/172], Loss: 13.3146\n",
      "Epoch [131/300], Step [115/172], Loss: 20.0655\n",
      "Epoch [131/300], Step [116/172], Loss: 14.0228\n",
      "Epoch [131/300], Step [117/172], Loss: 10.7471\n",
      "Epoch [131/300], Step [118/172], Loss: 14.4631\n",
      "Epoch [131/300], Step [119/172], Loss: 15.0402\n",
      "Epoch [131/300], Step [120/172], Loss: 9.8176\n",
      "Epoch [131/300], Step [121/172], Loss: 9.8307\n",
      "Epoch [131/300], Step [122/172], Loss: 10.2499\n",
      "Epoch [131/300], Step [123/172], Loss: 9.9893\n",
      "Epoch [131/300], Step [124/172], Loss: 7.9414\n",
      "Epoch [131/300], Step [125/172], Loss: 12.2524\n",
      "Epoch [131/300], Step [126/172], Loss: 10.0680\n",
      "Epoch [131/300], Step [127/172], Loss: 10.9800\n",
      "Epoch [131/300], Step [128/172], Loss: 11.0697\n",
      "Epoch [131/300], Step [129/172], Loss: 8.1458\n",
      "Epoch [131/300], Step [130/172], Loss: 11.3158\n",
      "Epoch [131/300], Step [131/172], Loss: 7.8286\n",
      "Epoch [131/300], Step [132/172], Loss: 8.2212\n",
      "Epoch [131/300], Step [133/172], Loss: 9.2464\n",
      "Epoch [131/300], Step [134/172], Loss: 11.5420\n",
      "Epoch [131/300], Step [135/172], Loss: 8.5195\n",
      "Epoch [131/300], Step [136/172], Loss: 8.1234\n",
      "Epoch [131/300], Step [137/172], Loss: 9.2385\n",
      "Epoch [131/300], Step [138/172], Loss: 7.3092\n",
      "Epoch [131/300], Step [139/172], Loss: 9.2198\n",
      "Epoch [131/300], Step [140/172], Loss: 9.0597\n",
      "Epoch [131/300], Step [141/172], Loss: 10.4302\n",
      "Epoch [131/300], Step [142/172], Loss: 13.3386\n",
      "Epoch [131/300], Step [143/172], Loss: 9.6156\n",
      "Epoch [131/300], Step [144/172], Loss: 8.5961\n",
      "Epoch [131/300], Step [145/172], Loss: 9.3807\n",
      "Epoch [131/300], Step [146/172], Loss: 9.2955\n",
      "Epoch [131/300], Step [147/172], Loss: 5.0435\n",
      "Epoch [131/300], Step [148/172], Loss: 6.0767\n",
      "Epoch [131/300], Step [149/172], Loss: 6.9348\n",
      "Epoch [131/300], Step [150/172], Loss: 6.9210\n",
      "Epoch [131/300], Step [151/172], Loss: 6.1775\n",
      "Epoch [131/300], Step [152/172], Loss: 7.3379\n",
      "Epoch [131/300], Step [153/172], Loss: 6.6048\n",
      "Epoch [131/300], Step [154/172], Loss: 7.4929\n",
      "Epoch [131/300], Step [155/172], Loss: 6.3721\n",
      "Epoch [131/300], Step [156/172], Loss: 12.2585\n",
      "Epoch [131/300], Step [157/172], Loss: 9.5950\n",
      "Epoch [131/300], Step [158/172], Loss: 7.1298\n",
      "Epoch [131/300], Step [159/172], Loss: 9.2362\n",
      "Epoch [131/300], Step [160/172], Loss: 9.8543\n",
      "Epoch [131/300], Step [161/172], Loss: 6.8761\n",
      "Epoch [131/300], Step [162/172], Loss: 6.2596\n",
      "Epoch [131/300], Step [163/172], Loss: 6.2280\n",
      "Epoch [131/300], Step [164/172], Loss: 8.8289\n",
      "Epoch [131/300], Step [165/172], Loss: 5.9425\n",
      "Epoch [131/300], Step [166/172], Loss: 5.5534\n",
      "Epoch [131/300], Step [167/172], Loss: 9.3975\n",
      "Epoch [131/300], Step [168/172], Loss: 6.8460\n",
      "Epoch [131/300], Step [169/172], Loss: 6.6230\n",
      "Epoch [131/300], Step [170/172], Loss: 5.1262\n",
      "Epoch [131/300], Step [171/172], Loss: 6.7170\n",
      "Epoch [131/300], Step [172/172], Loss: 5.0573\n",
      "Epoch [132/300], Step [1/172], Loss: 64.8035\n",
      "Epoch [132/300], Step [2/172], Loss: 65.9381\n",
      "Epoch [132/300], Step [3/172], Loss: 61.9427\n",
      "Epoch [132/300], Step [4/172], Loss: 36.2433\n",
      "Epoch [132/300], Step [5/172], Loss: 59.5096\n",
      "Epoch [132/300], Step [6/172], Loss: 18.9209\n",
      "Epoch [132/300], Step [7/172], Loss: 28.9567\n",
      "Epoch [132/300], Step [8/172], Loss: 5.2451\n",
      "Epoch [132/300], Step [9/172], Loss: 37.6481\n",
      "Epoch [132/300], Step [10/172], Loss: 44.9718\n",
      "Epoch [132/300], Step [11/172], Loss: 71.3835\n",
      "Epoch [132/300], Step [12/172], Loss: 79.5484\n",
      "Epoch [132/300], Step [13/172], Loss: 40.6769\n",
      "Epoch [132/300], Step [14/172], Loss: 74.9245\n",
      "Epoch [132/300], Step [15/172], Loss: 66.4535\n",
      "Epoch [132/300], Step [16/172], Loss: 14.5213\n",
      "Epoch [132/300], Step [17/172], Loss: 52.9137\n",
      "Epoch [132/300], Step [18/172], Loss: 62.5966\n",
      "Epoch [132/300], Step [19/172], Loss: 84.9574\n",
      "Epoch [132/300], Step [20/172], Loss: 54.0738\n",
      "Epoch [132/300], Step [21/172], Loss: 91.1585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/300], Step [22/172], Loss: 69.8360\n",
      "Epoch [132/300], Step [23/172], Loss: 2.3176\n",
      "Epoch [132/300], Step [24/172], Loss: 64.9744\n",
      "Epoch [132/300], Step [25/172], Loss: 44.4702\n",
      "Epoch [132/300], Step [26/172], Loss: 54.8586\n",
      "Epoch [132/300], Step [27/172], Loss: 68.5896\n",
      "Epoch [132/300], Step [28/172], Loss: 29.0746\n",
      "Epoch [132/300], Step [29/172], Loss: 20.8619\n",
      "Epoch [132/300], Step [30/172], Loss: 76.4861\n",
      "Epoch [132/300], Step [31/172], Loss: 41.9754\n",
      "Epoch [132/300], Step [32/172], Loss: 42.1764\n",
      "Epoch [132/300], Step [33/172], Loss: 71.7472\n",
      "Epoch [132/300], Step [34/172], Loss: 3.9952\n",
      "Epoch [132/300], Step [35/172], Loss: 13.4732\n",
      "Epoch [132/300], Step [36/172], Loss: 19.6340\n",
      "Epoch [132/300], Step [37/172], Loss: 17.4137\n",
      "Epoch [132/300], Step [38/172], Loss: 27.9306\n",
      "Epoch [132/300], Step [39/172], Loss: 38.5579\n",
      "Epoch [132/300], Step [40/172], Loss: 19.7348\n",
      "Epoch [132/300], Step [41/172], Loss: 36.2209\n",
      "Epoch [132/300], Step [42/172], Loss: 39.5608\n",
      "Epoch [132/300], Step [43/172], Loss: 26.2121\n",
      "Epoch [132/300], Step [44/172], Loss: 19.7326\n",
      "Epoch [132/300], Step [45/172], Loss: 22.9598\n",
      "Epoch [132/300], Step [46/172], Loss: 19.1470\n",
      "Epoch [132/300], Step [47/172], Loss: 46.1167\n",
      "Epoch [132/300], Step [48/172], Loss: 54.9347\n",
      "Epoch [132/300], Step [49/172], Loss: 19.4585\n",
      "Epoch [132/300], Step [50/172], Loss: 48.0647\n",
      "Epoch [132/300], Step [51/172], Loss: 7.5923\n",
      "Epoch [132/300], Step [52/172], Loss: 17.1976\n",
      "Epoch [132/300], Step [53/172], Loss: 21.9410\n",
      "Epoch [132/300], Step [54/172], Loss: 11.8704\n",
      "Epoch [132/300], Step [55/172], Loss: 12.1525\n",
      "Epoch [132/300], Step [56/172], Loss: 13.7330\n",
      "Epoch [132/300], Step [57/172], Loss: 17.4040\n",
      "Epoch [132/300], Step [58/172], Loss: 15.7177\n",
      "Epoch [132/300], Step [59/172], Loss: 29.2302\n",
      "Epoch [132/300], Step [60/172], Loss: 38.1031\n",
      "Epoch [132/300], Step [61/172], Loss: 6.9404\n",
      "Epoch [132/300], Step [62/172], Loss: 22.1051\n",
      "Epoch [132/300], Step [63/172], Loss: 9.2206\n",
      "Epoch [132/300], Step [64/172], Loss: 8.9357\n",
      "Epoch [132/300], Step [65/172], Loss: 19.7582\n",
      "Epoch [132/300], Step [66/172], Loss: 5.6295\n",
      "Epoch [132/300], Step [67/172], Loss: 25.6260\n",
      "Epoch [132/300], Step [68/172], Loss: 5.2448\n",
      "Epoch [132/300], Step [69/172], Loss: 49.0439\n",
      "Epoch [132/300], Step [70/172], Loss: 50.6220\n",
      "Epoch [132/300], Step [71/172], Loss: 48.1320\n",
      "Epoch [132/300], Step [72/172], Loss: 51.8188\n",
      "Epoch [132/300], Step [73/172], Loss: 57.7731\n",
      "Epoch [132/300], Step [74/172], Loss: 31.4947\n",
      "Epoch [132/300], Step [75/172], Loss: 31.6904\n",
      "Epoch [132/300], Step [76/172], Loss: 34.1981\n",
      "Epoch [132/300], Step [77/172], Loss: 59.8275\n",
      "Epoch [132/300], Step [78/172], Loss: 45.2493\n",
      "Epoch [132/300], Step [79/172], Loss: 44.4333\n",
      "Epoch [132/300], Step [80/172], Loss: 56.8283\n",
      "Epoch [132/300], Step [81/172], Loss: 39.5542\n",
      "Epoch [132/300], Step [82/172], Loss: 39.0575\n",
      "Epoch [132/300], Step [83/172], Loss: 46.4788\n",
      "Epoch [132/300], Step [84/172], Loss: 35.5172\n",
      "Epoch [132/300], Step [85/172], Loss: 40.2737\n",
      "Epoch [132/300], Step [86/172], Loss: 33.2634\n",
      "Epoch [132/300], Step [87/172], Loss: 27.5067\n",
      "Epoch [132/300], Step [88/172], Loss: 28.6966\n",
      "Epoch [132/300], Step [89/172], Loss: 25.5653\n",
      "Epoch [132/300], Step [90/172], Loss: 24.5113\n",
      "Epoch [132/300], Step [91/172], Loss: 27.6404\n",
      "Epoch [132/300], Step [92/172], Loss: 21.1051\n",
      "Epoch [132/300], Step [93/172], Loss: 21.2198\n",
      "Epoch [132/300], Step [94/172], Loss: 29.3883\n",
      "Epoch [132/300], Step [95/172], Loss: 21.7652\n",
      "Epoch [132/300], Step [96/172], Loss: 19.1745\n",
      "Epoch [132/300], Step [97/172], Loss: 26.5451\n",
      "Epoch [132/300], Step [98/172], Loss: 18.9121\n",
      "Epoch [132/300], Step [99/172], Loss: 18.1851\n",
      "Epoch [132/300], Step [100/172], Loss: 15.0276\n",
      "Epoch [132/300], Step [101/172], Loss: 16.7790\n",
      "Epoch [132/300], Step [102/172], Loss: 16.4757\n",
      "Epoch [132/300], Step [103/172], Loss: 12.7149\n",
      "Epoch [132/300], Step [104/172], Loss: 16.2387\n",
      "Epoch [132/300], Step [105/172], Loss: 17.8255\n",
      "Epoch [132/300], Step [106/172], Loss: 15.8949\n",
      "Epoch [132/300], Step [107/172], Loss: 14.9250\n",
      "Epoch [132/300], Step [108/172], Loss: 15.8547\n",
      "Epoch [132/300], Step [109/172], Loss: 15.9085\n",
      "Epoch [132/300], Step [110/172], Loss: 15.3554\n",
      "Epoch [132/300], Step [111/172], Loss: 13.6974\n",
      "Epoch [132/300], Step [112/172], Loss: 17.6919\n",
      "Epoch [132/300], Step [113/172], Loss: 12.9107\n",
      "Epoch [132/300], Step [114/172], Loss: 13.3278\n",
      "Epoch [132/300], Step [115/172], Loss: 20.0685\n",
      "Epoch [132/300], Step [116/172], Loss: 13.9840\n",
      "Epoch [132/300], Step [117/172], Loss: 10.7525\n",
      "Epoch [132/300], Step [118/172], Loss: 14.4651\n",
      "Epoch [132/300], Step [119/172], Loss: 15.1016\n",
      "Epoch [132/300], Step [120/172], Loss: 9.8214\n",
      "Epoch [132/300], Step [121/172], Loss: 9.8274\n",
      "Epoch [132/300], Step [122/172], Loss: 10.2350\n",
      "Epoch [132/300], Step [123/172], Loss: 10.0178\n",
      "Epoch [132/300], Step [124/172], Loss: 7.9273\n",
      "Epoch [132/300], Step [125/172], Loss: 12.2755\n",
      "Epoch [132/300], Step [126/172], Loss: 10.0525\n",
      "Epoch [132/300], Step [127/172], Loss: 10.8885\n",
      "Epoch [132/300], Step [128/172], Loss: 10.9773\n",
      "Epoch [132/300], Step [129/172], Loss: 8.1089\n",
      "Epoch [132/300], Step [130/172], Loss: 11.2663\n",
      "Epoch [132/300], Step [131/172], Loss: 7.7888\n",
      "Epoch [132/300], Step [132/172], Loss: 8.1966\n",
      "Epoch [132/300], Step [133/172], Loss: 9.2189\n",
      "Epoch [132/300], Step [134/172], Loss: 11.6088\n",
      "Epoch [132/300], Step [135/172], Loss: 8.5494\n",
      "Epoch [132/300], Step [136/172], Loss: 8.0992\n",
      "Epoch [132/300], Step [137/172], Loss: 9.1955\n",
      "Epoch [132/300], Step [138/172], Loss: 7.2707\n",
      "Epoch [132/300], Step [139/172], Loss: 9.2259\n",
      "Epoch [132/300], Step [140/172], Loss: 9.0304\n",
      "Epoch [132/300], Step [141/172], Loss: 10.3837\n",
      "Epoch [132/300], Step [142/172], Loss: 13.2916\n",
      "Epoch [132/300], Step [143/172], Loss: 9.5934\n",
      "Epoch [132/300], Step [144/172], Loss: 8.6204\n",
      "Epoch [132/300], Step [145/172], Loss: 9.3719\n",
      "Epoch [132/300], Step [146/172], Loss: 9.2570\n",
      "Epoch [132/300], Step [147/172], Loss: 5.0271\n",
      "Epoch [132/300], Step [148/172], Loss: 6.0521\n",
      "Epoch [132/300], Step [149/172], Loss: 6.9212\n",
      "Epoch [132/300], Step [150/172], Loss: 6.8878\n",
      "Epoch [132/300], Step [151/172], Loss: 6.1333\n",
      "Epoch [132/300], Step [152/172], Loss: 7.3634\n",
      "Epoch [132/300], Step [153/172], Loss: 6.5866\n",
      "Epoch [132/300], Step [154/172], Loss: 7.4829\n",
      "Epoch [132/300], Step [155/172], Loss: 6.3733\n",
      "Epoch [132/300], Step [156/172], Loss: 12.2567\n",
      "Epoch [132/300], Step [157/172], Loss: 9.5631\n",
      "Epoch [132/300], Step [158/172], Loss: 7.0975\n",
      "Epoch [132/300], Step [159/172], Loss: 9.2375\n",
      "Epoch [132/300], Step [160/172], Loss: 9.8441\n",
      "Epoch [132/300], Step [161/172], Loss: 6.8400\n",
      "Epoch [132/300], Step [162/172], Loss: 6.2477\n",
      "Epoch [132/300], Step [163/172], Loss: 6.1997\n",
      "Epoch [132/300], Step [164/172], Loss: 8.7695\n",
      "Epoch [132/300], Step [165/172], Loss: 5.9296\n",
      "Epoch [132/300], Step [166/172], Loss: 5.5296\n",
      "Epoch [132/300], Step [167/172], Loss: 9.4213\n",
      "Epoch [132/300], Step [168/172], Loss: 6.8433\n",
      "Epoch [132/300], Step [169/172], Loss: 6.6264\n",
      "Epoch [132/300], Step [170/172], Loss: 5.1126\n",
      "Epoch [132/300], Step [171/172], Loss: 6.7267\n",
      "Epoch [132/300], Step [172/172], Loss: 5.0912\n",
      "Epoch [133/300], Step [1/172], Loss: 64.4645\n",
      "Epoch [133/300], Step [2/172], Loss: 65.5016\n",
      "Epoch [133/300], Step [3/172], Loss: 61.3754\n",
      "Epoch [133/300], Step [4/172], Loss: 35.9941\n",
      "Epoch [133/300], Step [5/172], Loss: 59.0375\n",
      "Epoch [133/300], Step [6/172], Loss: 19.1131\n",
      "Epoch [133/300], Step [7/172], Loss: 29.7333\n",
      "Epoch [133/300], Step [8/172], Loss: 5.5011\n",
      "Epoch [133/300], Step [9/172], Loss: 37.6127\n",
      "Epoch [133/300], Step [10/172], Loss: 45.0112\n",
      "Epoch [133/300], Step [11/172], Loss: 71.3513\n",
      "Epoch [133/300], Step [12/172], Loss: 79.6638\n",
      "Epoch [133/300], Step [13/172], Loss: 41.1252\n",
      "Epoch [133/300], Step [14/172], Loss: 75.3047\n",
      "Epoch [133/300], Step [15/172], Loss: 66.5199\n",
      "Epoch [133/300], Step [16/172], Loss: 13.6967\n",
      "Epoch [133/300], Step [17/172], Loss: 53.0836\n",
      "Epoch [133/300], Step [18/172], Loss: 62.7753\n",
      "Epoch [133/300], Step [19/172], Loss: 85.1432\n",
      "Epoch [133/300], Step [20/172], Loss: 53.7462\n",
      "Epoch [133/300], Step [21/172], Loss: 91.0827\n",
      "Epoch [133/300], Step [22/172], Loss: 69.5511\n",
      "Epoch [133/300], Step [23/172], Loss: 2.2608\n",
      "Epoch [133/300], Step [24/172], Loss: 64.6547\n",
      "Epoch [133/300], Step [25/172], Loss: 44.2170\n",
      "Epoch [133/300], Step [26/172], Loss: 54.5889\n",
      "Epoch [133/300], Step [27/172], Loss: 67.8333\n",
      "Epoch [133/300], Step [28/172], Loss: 28.8376\n",
      "Epoch [133/300], Step [29/172], Loss: 20.7419\n",
      "Epoch [133/300], Step [30/172], Loss: 75.3219\n",
      "Epoch [133/300], Step [31/172], Loss: 41.3699\n",
      "Epoch [133/300], Step [32/172], Loss: 41.9244\n",
      "Epoch [133/300], Step [33/172], Loss: 71.4572\n",
      "Epoch [133/300], Step [34/172], Loss: 4.2117\n",
      "Epoch [133/300], Step [35/172], Loss: 13.4567\n",
      "Epoch [133/300], Step [36/172], Loss: 19.8722\n",
      "Epoch [133/300], Step [37/172], Loss: 17.2970\n",
      "Epoch [133/300], Step [38/172], Loss: 27.6844\n",
      "Epoch [133/300], Step [39/172], Loss: 38.2774\n",
      "Epoch [133/300], Step [40/172], Loss: 19.5765\n",
      "Epoch [133/300], Step [41/172], Loss: 35.8633\n",
      "Epoch [133/300], Step [42/172], Loss: 39.1072\n",
      "Epoch [133/300], Step [43/172], Loss: 26.0473\n",
      "Epoch [133/300], Step [44/172], Loss: 19.5896\n",
      "Epoch [133/300], Step [45/172], Loss: 22.9232\n",
      "Epoch [133/300], Step [46/172], Loss: 19.1214\n",
      "Epoch [133/300], Step [47/172], Loss: 46.0443\n",
      "Epoch [133/300], Step [48/172], Loss: 54.9736\n",
      "Epoch [133/300], Step [49/172], Loss: 19.5809\n",
      "Epoch [133/300], Step [50/172], Loss: 48.0708\n",
      "Epoch [133/300], Step [51/172], Loss: 7.6601\n",
      "Epoch [133/300], Step [52/172], Loss: 17.2211\n",
      "Epoch [133/300], Step [53/172], Loss: 21.9032\n",
      "Epoch [133/300], Step [54/172], Loss: 11.9535\n",
      "Epoch [133/300], Step [55/172], Loss: 12.1807\n",
      "Epoch [133/300], Step [56/172], Loss: 13.8911\n",
      "Epoch [133/300], Step [57/172], Loss: 17.5088\n",
      "Epoch [133/300], Step [58/172], Loss: 15.6842\n",
      "Epoch [133/300], Step [59/172], Loss: 29.0867\n",
      "Epoch [133/300], Step [60/172], Loss: 38.1158\n",
      "Epoch [133/300], Step [61/172], Loss: 6.9378\n",
      "Epoch [133/300], Step [62/172], Loss: 22.0736\n",
      "Epoch [133/300], Step [63/172], Loss: 9.3456\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [133/300], Step [64/172], Loss: 9.0234\n",
      "Epoch [133/300], Step [65/172], Loss: 19.8343\n",
      "Epoch [133/300], Step [66/172], Loss: 5.7153\n",
      "Epoch [133/300], Step [67/172], Loss: 25.7268\n",
      "Epoch [133/300], Step [68/172], Loss: 5.2193\n",
      "Epoch [133/300], Step [69/172], Loss: 48.4741\n",
      "Epoch [133/300], Step [70/172], Loss: 50.5251\n",
      "Epoch [133/300], Step [71/172], Loss: 47.8928\n",
      "Epoch [133/300], Step [72/172], Loss: 51.6238\n",
      "Epoch [133/300], Step [73/172], Loss: 57.6142\n",
      "Epoch [133/300], Step [74/172], Loss: 31.3376\n",
      "Epoch [133/300], Step [75/172], Loss: 31.5761\n",
      "Epoch [133/300], Step [76/172], Loss: 34.0819\n",
      "Epoch [133/300], Step [77/172], Loss: 59.5776\n",
      "Epoch [133/300], Step [78/172], Loss: 45.1015\n",
      "Epoch [133/300], Step [79/172], Loss: 44.3233\n",
      "Epoch [133/300], Step [80/172], Loss: 56.5930\n",
      "Epoch [133/300], Step [81/172], Loss: 39.4857\n",
      "Epoch [133/300], Step [82/172], Loss: 39.0312\n",
      "Epoch [133/300], Step [83/172], Loss: 46.4260\n",
      "Epoch [133/300], Step [84/172], Loss: 35.4255\n",
      "Epoch [133/300], Step [85/172], Loss: 40.2712\n",
      "Epoch [133/300], Step [86/172], Loss: 33.1919\n",
      "Epoch [133/300], Step [87/172], Loss: 27.5052\n",
      "Epoch [133/300], Step [88/172], Loss: 28.7106\n",
      "Epoch [133/300], Step [89/172], Loss: 25.5834\n",
      "Epoch [133/300], Step [90/172], Loss: 24.3931\n",
      "Epoch [133/300], Step [91/172], Loss: 27.5513\n",
      "Epoch [133/300], Step [92/172], Loss: 21.0130\n",
      "Epoch [133/300], Step [93/172], Loss: 21.2045\n",
      "Epoch [133/300], Step [94/172], Loss: 29.4808\n",
      "Epoch [133/300], Step [95/172], Loss: 21.7071\n",
      "Epoch [133/300], Step [96/172], Loss: 19.2227\n",
      "Epoch [133/300], Step [97/172], Loss: 26.5806\n",
      "Epoch [133/300], Step [98/172], Loss: 18.9577\n",
      "Epoch [133/300], Step [99/172], Loss: 18.2201\n",
      "Epoch [133/300], Step [100/172], Loss: 15.0630\n",
      "Epoch [133/300], Step [101/172], Loss: 16.8429\n",
      "Epoch [133/300], Step [102/172], Loss: 16.5346\n",
      "Epoch [133/300], Step [103/172], Loss: 12.6881\n",
      "Epoch [133/300], Step [104/172], Loss: 16.2884\n",
      "Epoch [133/300], Step [105/172], Loss: 17.8700\n",
      "Epoch [133/300], Step [106/172], Loss: 15.8870\n",
      "Epoch [133/300], Step [107/172], Loss: 14.8477\n",
      "Epoch [133/300], Step [108/172], Loss: 15.8136\n",
      "Epoch [133/300], Step [109/172], Loss: 15.8743\n",
      "Epoch [133/300], Step [110/172], Loss: 15.3128\n",
      "Epoch [133/300], Step [111/172], Loss: 13.6776\n",
      "Epoch [133/300], Step [112/172], Loss: 17.6622\n",
      "Epoch [133/300], Step [113/172], Loss: 12.8099\n",
      "Epoch [133/300], Step [114/172], Loss: 13.2913\n",
      "Epoch [133/300], Step [115/172], Loss: 20.0528\n",
      "Epoch [133/300], Step [116/172], Loss: 13.9264\n",
      "Epoch [133/300], Step [117/172], Loss: 10.7289\n",
      "Epoch [133/300], Step [118/172], Loss: 14.4947\n",
      "Epoch [133/300], Step [119/172], Loss: 14.9842\n",
      "Epoch [133/300], Step [120/172], Loss: 9.7913\n",
      "Epoch [133/300], Step [121/172], Loss: 9.8003\n",
      "Epoch [133/300], Step [122/172], Loss: 10.2039\n",
      "Epoch [133/300], Step [123/172], Loss: 9.9841\n",
      "Epoch [133/300], Step [124/172], Loss: 7.8797\n",
      "Epoch [133/300], Step [125/172], Loss: 12.2071\n",
      "Epoch [133/300], Step [126/172], Loss: 10.0195\n",
      "Epoch [133/300], Step [127/172], Loss: 10.8067\n",
      "Epoch [133/300], Step [128/172], Loss: 10.8399\n",
      "Epoch [133/300], Step [129/172], Loss: 8.0562\n",
      "Epoch [133/300], Step [130/172], Loss: 11.2423\n",
      "Epoch [133/300], Step [131/172], Loss: 7.7222\n",
      "Epoch [133/300], Step [132/172], Loss: 8.1731\n",
      "Epoch [133/300], Step [133/172], Loss: 9.1446\n",
      "Epoch [133/300], Step [134/172], Loss: 11.4770\n",
      "Epoch [133/300], Step [135/172], Loss: 8.4574\n",
      "Epoch [133/300], Step [136/172], Loss: 8.0421\n",
      "Epoch [133/300], Step [137/172], Loss: 9.1478\n",
      "Epoch [133/300], Step [138/172], Loss: 7.1984\n",
      "Epoch [133/300], Step [139/172], Loss: 9.1358\n",
      "Epoch [133/300], Step [140/172], Loss: 8.9767\n",
      "Epoch [133/300], Step [141/172], Loss: 10.3085\n",
      "Epoch [133/300], Step [142/172], Loss: 13.2432\n",
      "Epoch [133/300], Step [143/172], Loss: 9.5956\n",
      "Epoch [133/300], Step [144/172], Loss: 8.5380\n",
      "Epoch [133/300], Step [145/172], Loss: 9.3525\n",
      "Epoch [133/300], Step [146/172], Loss: 9.1544\n",
      "Epoch [133/300], Step [147/172], Loss: 4.9935\n",
      "Epoch [133/300], Step [148/172], Loss: 6.0191\n",
      "Epoch [133/300], Step [149/172], Loss: 6.8471\n",
      "Epoch [133/300], Step [150/172], Loss: 6.8192\n",
      "Epoch [133/300], Step [151/172], Loss: 6.0747\n",
      "Epoch [133/300], Step [152/172], Loss: 7.2590\n",
      "Epoch [133/300], Step [153/172], Loss: 6.5387\n",
      "Epoch [133/300], Step [154/172], Loss: 7.4320\n",
      "Epoch [133/300], Step [155/172], Loss: 6.3180\n",
      "Epoch [133/300], Step [156/172], Loss: 12.2113\n",
      "Epoch [133/300], Step [157/172], Loss: 9.4966\n",
      "Epoch [133/300], Step [158/172], Loss: 7.0550\n",
      "Epoch [133/300], Step [159/172], Loss: 9.1247\n",
      "Epoch [133/300], Step [160/172], Loss: 9.7883\n",
      "Epoch [133/300], Step [161/172], Loss: 6.7423\n",
      "Epoch [133/300], Step [162/172], Loss: 6.1542\n",
      "Epoch [133/300], Step [163/172], Loss: 6.1444\n",
      "Epoch [133/300], Step [164/172], Loss: 8.7210\n",
      "Epoch [133/300], Step [165/172], Loss: 5.8822\n",
      "Epoch [133/300], Step [166/172], Loss: 5.4626\n",
      "Epoch [133/300], Step [167/172], Loss: 9.3743\n",
      "Epoch [133/300], Step [168/172], Loss: 6.7728\n",
      "Epoch [133/300], Step [169/172], Loss: 6.5986\n",
      "Epoch [133/300], Step [170/172], Loss: 5.0398\n",
      "Epoch [133/300], Step [171/172], Loss: 6.6746\n",
      "Epoch [133/300], Step [172/172], Loss: 5.0159\n",
      "Epoch [134/300], Step [1/172], Loss: 64.2554\n",
      "Epoch [134/300], Step [2/172], Loss: 65.2382\n",
      "Epoch [134/300], Step [3/172], Loss: 61.2908\n",
      "Epoch [134/300], Step [4/172], Loss: 35.7561\n",
      "Epoch [134/300], Step [5/172], Loss: 59.2879\n",
      "Epoch [134/300], Step [6/172], Loss: 19.2172\n",
      "Epoch [134/300], Step [7/172], Loss: 29.5111\n",
      "Epoch [134/300], Step [8/172], Loss: 5.2878\n",
      "Epoch [134/300], Step [9/172], Loss: 37.3650\n",
      "Epoch [134/300], Step [10/172], Loss: 44.9908\n",
      "Epoch [134/300], Step [11/172], Loss: 71.0358\n",
      "Epoch [134/300], Step [12/172], Loss: 79.4144\n",
      "Epoch [134/300], Step [13/172], Loss: 40.8581\n",
      "Epoch [134/300], Step [14/172], Loss: 74.9610\n",
      "Epoch [134/300], Step [15/172], Loss: 66.3271\n",
      "Epoch [134/300], Step [16/172], Loss: 14.0469\n",
      "Epoch [134/300], Step [17/172], Loss: 53.0546\n",
      "Epoch [134/300], Step [18/172], Loss: 62.8096\n",
      "Epoch [134/300], Step [19/172], Loss: 85.3781\n",
      "Epoch [134/300], Step [20/172], Loss: 53.4786\n",
      "Epoch [134/300], Step [21/172], Loss: 91.3497\n",
      "Epoch [134/300], Step [22/172], Loss: 69.7160\n",
      "Epoch [134/300], Step [23/172], Loss: 2.2110\n",
      "Epoch [134/300], Step [24/172], Loss: 64.7862\n",
      "Epoch [134/300], Step [25/172], Loss: 44.2756\n",
      "Epoch [134/300], Step [26/172], Loss: 54.5648\n",
      "Epoch [134/300], Step [27/172], Loss: 68.3028\n",
      "Epoch [134/300], Step [28/172], Loss: 28.9752\n",
      "Epoch [134/300], Step [29/172], Loss: 20.6831\n",
      "Epoch [134/300], Step [30/172], Loss: 75.8930\n",
      "Epoch [134/300], Step [31/172], Loss: 41.8482\n",
      "Epoch [134/300], Step [32/172], Loss: 42.2757\n",
      "Epoch [134/300], Step [33/172], Loss: 71.8264\n",
      "Epoch [134/300], Step [34/172], Loss: 3.8674\n",
      "Epoch [134/300], Step [35/172], Loss: 13.3867\n",
      "Epoch [134/300], Step [36/172], Loss: 19.5000\n",
      "Epoch [134/300], Step [37/172], Loss: 17.3447\n",
      "Epoch [134/300], Step [38/172], Loss: 27.8575\n",
      "Epoch [134/300], Step [39/172], Loss: 38.2385\n",
      "Epoch [134/300], Step [40/172], Loss: 19.6154\n",
      "Epoch [134/300], Step [41/172], Loss: 35.9444\n",
      "Epoch [134/300], Step [42/172], Loss: 39.2515\n",
      "Epoch [134/300], Step [43/172], Loss: 26.0854\n",
      "Epoch [134/300], Step [44/172], Loss: 19.5775\n",
      "Epoch [134/300], Step [45/172], Loss: 22.9063\n",
      "Epoch [134/300], Step [46/172], Loss: 19.0675\n",
      "Epoch [134/300], Step [47/172], Loss: 46.0375\n",
      "Epoch [134/300], Step [48/172], Loss: 55.1205\n",
      "Epoch [134/300], Step [49/172], Loss: 19.4571\n",
      "Epoch [134/300], Step [50/172], Loss: 47.6692\n",
      "Epoch [134/300], Step [51/172], Loss: 7.6510\n",
      "Epoch [134/300], Step [52/172], Loss: 17.2159\n",
      "Epoch [134/300], Step [53/172], Loss: 21.8562\n",
      "Epoch [134/300], Step [54/172], Loss: 11.9745\n",
      "Epoch [134/300], Step [55/172], Loss: 12.2064\n",
      "Epoch [134/300], Step [56/172], Loss: 14.0677\n",
      "Epoch [134/300], Step [57/172], Loss: 17.2510\n",
      "Epoch [134/300], Step [58/172], Loss: 15.5514\n",
      "Epoch [134/300], Step [59/172], Loss: 29.0547\n",
      "Epoch [134/300], Step [60/172], Loss: 37.4245\n",
      "Epoch [134/300], Step [61/172], Loss: 6.8262\n",
      "Epoch [134/300], Step [62/172], Loss: 22.0755\n",
      "Epoch [134/300], Step [63/172], Loss: 9.3245\n",
      "Epoch [134/300], Step [64/172], Loss: 9.0525\n",
      "Epoch [134/300], Step [65/172], Loss: 19.7537\n",
      "Epoch [134/300], Step [66/172], Loss: 5.6892\n",
      "Epoch [134/300], Step [67/172], Loss: 25.4308\n",
      "Epoch [134/300], Step [68/172], Loss: 5.2201\n",
      "Epoch [134/300], Step [69/172], Loss: 47.9166\n",
      "Epoch [134/300], Step [70/172], Loss: 50.0492\n",
      "Epoch [134/300], Step [71/172], Loss: 47.6303\n",
      "Epoch [134/300], Step [72/172], Loss: 51.2228\n",
      "Epoch [134/300], Step [73/172], Loss: 57.3041\n",
      "Epoch [134/300], Step [74/172], Loss: 31.1951\n",
      "Epoch [134/300], Step [75/172], Loss: 31.4889\n",
      "Epoch [134/300], Step [76/172], Loss: 33.8478\n",
      "Epoch [134/300], Step [77/172], Loss: 59.3551\n",
      "Epoch [134/300], Step [78/172], Loss: 44.9318\n",
      "Epoch [134/300], Step [79/172], Loss: 44.1428\n",
      "Epoch [134/300], Step [80/172], Loss: 56.5435\n",
      "Epoch [134/300], Step [81/172], Loss: 39.3302\n",
      "Epoch [134/300], Step [82/172], Loss: 38.9219\n",
      "Epoch [134/300], Step [83/172], Loss: 46.3554\n",
      "Epoch [134/300], Step [84/172], Loss: 35.4178\n",
      "Epoch [134/300], Step [85/172], Loss: 40.2654\n",
      "Epoch [134/300], Step [86/172], Loss: 33.2176\n",
      "Epoch [134/300], Step [87/172], Loss: 27.5266\n",
      "Epoch [134/300], Step [88/172], Loss: 28.6737\n",
      "Epoch [134/300], Step [89/172], Loss: 25.6827\n",
      "Epoch [134/300], Step [90/172], Loss: 24.3718\n",
      "Epoch [134/300], Step [91/172], Loss: 27.5782\n",
      "Epoch [134/300], Step [92/172], Loss: 21.0753\n",
      "Epoch [134/300], Step [93/172], Loss: 21.3020\n",
      "Epoch [134/300], Step [94/172], Loss: 29.4535\n",
      "Epoch [134/300], Step [95/172], Loss: 21.6982\n",
      "Epoch [134/300], Step [96/172], Loss: 19.3034\n",
      "Epoch [134/300], Step [97/172], Loss: 26.6405\n",
      "Epoch [134/300], Step [98/172], Loss: 19.0273\n",
      "Epoch [134/300], Step [99/172], Loss: 18.2964\n",
      "Epoch [134/300], Step [100/172], Loss: 15.1330\n",
      "Epoch [134/300], Step [101/172], Loss: 16.9447\n",
      "Epoch [134/300], Step [102/172], Loss: 16.5987\n",
      "Epoch [134/300], Step [103/172], Loss: 12.7306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [134/300], Step [104/172], Loss: 16.3917\n",
      "Epoch [134/300], Step [105/172], Loss: 17.9324\n",
      "Epoch [134/300], Step [106/172], Loss: 15.9264\n",
      "Epoch [134/300], Step [107/172], Loss: 14.9150\n",
      "Epoch [134/300], Step [108/172], Loss: 15.8639\n",
      "Epoch [134/300], Step [109/172], Loss: 15.8449\n",
      "Epoch [134/300], Step [110/172], Loss: 15.3885\n",
      "Epoch [134/300], Step [111/172], Loss: 13.7545\n",
      "Epoch [134/300], Step [112/172], Loss: 17.6721\n",
      "Epoch [134/300], Step [113/172], Loss: 12.8955\n",
      "Epoch [134/300], Step [114/172], Loss: 13.3029\n",
      "Epoch [134/300], Step [115/172], Loss: 20.0199\n",
      "Epoch [134/300], Step [116/172], Loss: 14.0101\n",
      "Epoch [134/300], Step [117/172], Loss: 10.7932\n",
      "Epoch [134/300], Step [118/172], Loss: 14.4912\n",
      "Epoch [134/300], Step [119/172], Loss: 15.0699\n",
      "Epoch [134/300], Step [120/172], Loss: 9.8267\n",
      "Epoch [134/300], Step [121/172], Loss: 9.7972\n",
      "Epoch [134/300], Step [122/172], Loss: 10.2098\n",
      "Epoch [134/300], Step [123/172], Loss: 10.0857\n",
      "Epoch [134/300], Step [124/172], Loss: 7.8921\n",
      "Epoch [134/300], Step [125/172], Loss: 12.2430\n",
      "Epoch [134/300], Step [126/172], Loss: 10.0663\n",
      "Epoch [134/300], Step [127/172], Loss: 10.8465\n",
      "Epoch [134/300], Step [128/172], Loss: 10.8812\n",
      "Epoch [134/300], Step [129/172], Loss: 8.0536\n",
      "Epoch [134/300], Step [130/172], Loss: 11.2787\n",
      "Epoch [134/300], Step [131/172], Loss: 7.7241\n",
      "Epoch [134/300], Step [132/172], Loss: 8.2031\n",
      "Epoch [134/300], Step [133/172], Loss: 9.1373\n",
      "Epoch [134/300], Step [134/172], Loss: 11.5318\n",
      "Epoch [134/300], Step [135/172], Loss: 8.5063\n",
      "Epoch [134/300], Step [136/172], Loss: 8.0920\n",
      "Epoch [134/300], Step [137/172], Loss: 9.1916\n",
      "Epoch [134/300], Step [138/172], Loss: 7.2200\n",
      "Epoch [134/300], Step [139/172], Loss: 9.1649\n",
      "Epoch [134/300], Step [140/172], Loss: 9.0158\n",
      "Epoch [134/300], Step [141/172], Loss: 10.3527\n",
      "Epoch [134/300], Step [142/172], Loss: 13.3163\n",
      "Epoch [134/300], Step [143/172], Loss: 9.6251\n",
      "Epoch [134/300], Step [144/172], Loss: 8.5554\n",
      "Epoch [134/300], Step [145/172], Loss: 9.3965\n",
      "Epoch [134/300], Step [146/172], Loss: 9.1691\n",
      "Epoch [134/300], Step [147/172], Loss: 5.0056\n",
      "Epoch [134/300], Step [148/172], Loss: 6.0238\n",
      "Epoch [134/300], Step [149/172], Loss: 6.8597\n",
      "Epoch [134/300], Step [150/172], Loss: 6.8253\n",
      "Epoch [134/300], Step [151/172], Loss: 6.0638\n",
      "Epoch [134/300], Step [152/172], Loss: 7.2766\n",
      "Epoch [134/300], Step [153/172], Loss: 6.5430\n",
      "Epoch [134/300], Step [154/172], Loss: 7.4552\n",
      "Epoch [134/300], Step [155/172], Loss: 6.3255\n",
      "Epoch [134/300], Step [156/172], Loss: 12.2401\n",
      "Epoch [134/300], Step [157/172], Loss: 9.5073\n",
      "Epoch [134/300], Step [158/172], Loss: 7.0624\n",
      "Epoch [134/300], Step [159/172], Loss: 9.1689\n",
      "Epoch [134/300], Step [160/172], Loss: 9.8057\n",
      "Epoch [134/300], Step [161/172], Loss: 6.7380\n",
      "Epoch [134/300], Step [162/172], Loss: 6.1377\n",
      "Epoch [134/300], Step [163/172], Loss: 6.1640\n",
      "Epoch [134/300], Step [164/172], Loss: 8.7004\n",
      "Epoch [134/300], Step [165/172], Loss: 5.8630\n",
      "Epoch [134/300], Step [166/172], Loss: 5.4455\n",
      "Epoch [134/300], Step [167/172], Loss: 9.4038\n",
      "Epoch [134/300], Step [168/172], Loss: 6.7604\n",
      "Epoch [134/300], Step [169/172], Loss: 6.6315\n",
      "Epoch [134/300], Step [170/172], Loss: 5.0156\n",
      "Epoch [134/300], Step [171/172], Loss: 6.7168\n",
      "Epoch [134/300], Step [172/172], Loss: 5.0173\n",
      "Epoch [135/300], Step [1/172], Loss: 64.0315\n",
      "Epoch [135/300], Step [2/172], Loss: 64.8257\n",
      "Epoch [135/300], Step [3/172], Loss: 60.7457\n",
      "Epoch [135/300], Step [4/172], Loss: 35.5659\n",
      "Epoch [135/300], Step [5/172], Loss: 59.2061\n",
      "Epoch [135/300], Step [6/172], Loss: 19.1109\n",
      "Epoch [135/300], Step [7/172], Loss: 29.5040\n",
      "Epoch [135/300], Step [8/172], Loss: 5.3760\n",
      "Epoch [135/300], Step [9/172], Loss: 37.2544\n",
      "Epoch [135/300], Step [10/172], Loss: 45.0962\n",
      "Epoch [135/300], Step [11/172], Loss: 70.7875\n",
      "Epoch [135/300], Step [12/172], Loss: 79.3543\n",
      "Epoch [135/300], Step [13/172], Loss: 41.0137\n",
      "Epoch [135/300], Step [14/172], Loss: 74.7700\n",
      "Epoch [135/300], Step [15/172], Loss: 66.1418\n",
      "Epoch [135/300], Step [16/172], Loss: 13.9354\n",
      "Epoch [135/300], Step [17/172], Loss: 53.0446\n",
      "Epoch [135/300], Step [18/172], Loss: 62.8476\n",
      "Epoch [135/300], Step [19/172], Loss: 85.4985\n",
      "Epoch [135/300], Step [20/172], Loss: 52.9854\n",
      "Epoch [135/300], Step [21/172], Loss: 91.2133\n",
      "Epoch [135/300], Step [22/172], Loss: 69.4413\n",
      "Epoch [135/300], Step [23/172], Loss: 2.2225\n",
      "Epoch [135/300], Step [24/172], Loss: 64.6986\n",
      "Epoch [135/300], Step [25/172], Loss: 44.5069\n",
      "Epoch [135/300], Step [26/172], Loss: 54.5096\n",
      "Epoch [135/300], Step [27/172], Loss: 68.0670\n",
      "Epoch [135/300], Step [28/172], Loss: 28.8122\n",
      "Epoch [135/300], Step [29/172], Loss: 20.6644\n",
      "Epoch [135/300], Step [30/172], Loss: 75.1945\n",
      "Epoch [135/300], Step [31/172], Loss: 41.6918\n",
      "Epoch [135/300], Step [32/172], Loss: 42.4160\n",
      "Epoch [135/300], Step [33/172], Loss: 72.1696\n",
      "Epoch [135/300], Step [34/172], Loss: 3.8547\n",
      "Epoch [135/300], Step [35/172], Loss: 13.4094\n",
      "Epoch [135/300], Step [36/172], Loss: 19.5407\n",
      "Epoch [135/300], Step [37/172], Loss: 17.4687\n",
      "Epoch [135/300], Step [38/172], Loss: 28.1486\n",
      "Epoch [135/300], Step [39/172], Loss: 38.1787\n",
      "Epoch [135/300], Step [40/172], Loss: 19.7633\n",
      "Epoch [135/300], Step [41/172], Loss: 36.0364\n",
      "Epoch [135/300], Step [42/172], Loss: 39.4846\n",
      "Epoch [135/300], Step [43/172], Loss: 26.1831\n",
      "Epoch [135/300], Step [44/172], Loss: 19.7109\n",
      "Epoch [135/300], Step [45/172], Loss: 23.1014\n",
      "Epoch [135/300], Step [46/172], Loss: 19.0586\n",
      "Epoch [135/300], Step [47/172], Loss: 46.0475\n",
      "Epoch [135/300], Step [48/172], Loss: 55.2181\n",
      "Epoch [135/300], Step [49/172], Loss: 19.6189\n",
      "Epoch [135/300], Step [50/172], Loss: 47.5340\n",
      "Epoch [135/300], Step [51/172], Loss: 7.7384\n",
      "Epoch [135/300], Step [52/172], Loss: 17.2721\n",
      "Epoch [135/300], Step [53/172], Loss: 21.8119\n",
      "Epoch [135/300], Step [54/172], Loss: 12.0575\n",
      "Epoch [135/300], Step [55/172], Loss: 12.3402\n",
      "Epoch [135/300], Step [56/172], Loss: 14.3669\n",
      "Epoch [135/300], Step [57/172], Loss: 17.3161\n",
      "Epoch [135/300], Step [58/172], Loss: 15.4706\n",
      "Epoch [135/300], Step [59/172], Loss: 28.9276\n",
      "Epoch [135/300], Step [60/172], Loss: 37.5624\n",
      "Epoch [135/300], Step [61/172], Loss: 6.7473\n",
      "Epoch [135/300], Step [62/172], Loss: 21.9245\n",
      "Epoch [135/300], Step [63/172], Loss: 9.3509\n",
      "Epoch [135/300], Step [64/172], Loss: 9.1150\n",
      "Epoch [135/300], Step [65/172], Loss: 19.7546\n",
      "Epoch [135/300], Step [66/172], Loss: 5.7049\n",
      "Epoch [135/300], Step [67/172], Loss: 25.3753\n",
      "Epoch [135/300], Step [68/172], Loss: 5.2629\n",
      "Epoch [135/300], Step [69/172], Loss: 47.2849\n",
      "Epoch [135/300], Step [70/172], Loss: 49.7091\n",
      "Epoch [135/300], Step [71/172], Loss: 47.3119\n",
      "Epoch [135/300], Step [72/172], Loss: 50.8461\n",
      "Epoch [135/300], Step [73/172], Loss: 56.9909\n",
      "Epoch [135/300], Step [74/172], Loss: 30.9322\n",
      "Epoch [135/300], Step [75/172], Loss: 31.1435\n",
      "Epoch [135/300], Step [76/172], Loss: 33.5588\n",
      "Epoch [135/300], Step [77/172], Loss: 59.0322\n",
      "Epoch [135/300], Step [78/172], Loss: 44.7093\n",
      "Epoch [135/300], Step [79/172], Loss: 43.8688\n",
      "Epoch [135/300], Step [80/172], Loss: 56.3037\n",
      "Epoch [135/300], Step [81/172], Loss: 39.1448\n",
      "Epoch [135/300], Step [82/172], Loss: 38.8000\n",
      "Epoch [135/300], Step [83/172], Loss: 46.1073\n",
      "Epoch [135/300], Step [84/172], Loss: 35.3022\n",
      "Epoch [135/300], Step [85/172], Loss: 40.1975\n",
      "Epoch [135/300], Step [86/172], Loss: 33.2179\n",
      "Epoch [135/300], Step [87/172], Loss: 27.4435\n",
      "Epoch [135/300], Step [88/172], Loss: 28.5927\n",
      "Epoch [135/300], Step [89/172], Loss: 25.7674\n",
      "Epoch [135/300], Step [90/172], Loss: 24.2404\n",
      "Epoch [135/300], Step [91/172], Loss: 27.5079\n",
      "Epoch [135/300], Step [92/172], Loss: 21.0324\n",
      "Epoch [135/300], Step [93/172], Loss: 21.2734\n",
      "Epoch [135/300], Step [94/172], Loss: 29.3602\n",
      "Epoch [135/300], Step [95/172], Loss: 21.6487\n",
      "Epoch [135/300], Step [96/172], Loss: 19.3242\n",
      "Epoch [135/300], Step [97/172], Loss: 26.6462\n",
      "Epoch [135/300], Step [98/172], Loss: 19.0856\n",
      "Epoch [135/300], Step [99/172], Loss: 18.3042\n",
      "Epoch [135/300], Step [100/172], Loss: 15.1599\n",
      "Epoch [135/300], Step [101/172], Loss: 16.9837\n",
      "Epoch [135/300], Step [102/172], Loss: 16.6342\n",
      "Epoch [135/300], Step [103/172], Loss: 12.7439\n",
      "Epoch [135/300], Step [104/172], Loss: 16.4565\n",
      "Epoch [135/300], Step [105/172], Loss: 18.0061\n",
      "Epoch [135/300], Step [106/172], Loss: 15.8980\n",
      "Epoch [135/300], Step [107/172], Loss: 14.9271\n",
      "Epoch [135/300], Step [108/172], Loss: 15.8487\n",
      "Epoch [135/300], Step [109/172], Loss: 15.7695\n",
      "Epoch [135/300], Step [110/172], Loss: 15.3864\n",
      "Epoch [135/300], Step [111/172], Loss: 13.7907\n",
      "Epoch [135/300], Step [112/172], Loss: 17.7155\n",
      "Epoch [135/300], Step [113/172], Loss: 12.8619\n",
      "Epoch [135/300], Step [114/172], Loss: 13.3117\n",
      "Epoch [135/300], Step [115/172], Loss: 19.9877\n",
      "Epoch [135/300], Step [116/172], Loss: 14.0048\n",
      "Epoch [135/300], Step [117/172], Loss: 10.7852\n",
      "Epoch [135/300], Step [118/172], Loss: 14.4714\n",
      "Epoch [135/300], Step [119/172], Loss: 15.0916\n",
      "Epoch [135/300], Step [120/172], Loss: 9.8213\n",
      "Epoch [135/300], Step [121/172], Loss: 9.8110\n",
      "Epoch [135/300], Step [122/172], Loss: 10.2053\n",
      "Epoch [135/300], Step [123/172], Loss: 10.1007\n",
      "Epoch [135/300], Step [124/172], Loss: 7.8695\n",
      "Epoch [135/300], Step [125/172], Loss: 12.2046\n",
      "Epoch [135/300], Step [126/172], Loss: 10.0675\n",
      "Epoch [135/300], Step [127/172], Loss: 10.8226\n",
      "Epoch [135/300], Step [128/172], Loss: 10.8623\n",
      "Epoch [135/300], Step [129/172], Loss: 8.0483\n",
      "Epoch [135/300], Step [130/172], Loss: 11.2739\n",
      "Epoch [135/300], Step [131/172], Loss: 7.7139\n",
      "Epoch [135/300], Step [132/172], Loss: 8.1934\n",
      "Epoch [135/300], Step [133/172], Loss: 9.1236\n",
      "Epoch [135/300], Step [134/172], Loss: 11.5475\n",
      "Epoch [135/300], Step [135/172], Loss: 8.5179\n",
      "Epoch [135/300], Step [136/172], Loss: 8.0832\n",
      "Epoch [135/300], Step [137/172], Loss: 9.2138\n",
      "Epoch [135/300], Step [138/172], Loss: 7.2236\n",
      "Epoch [135/300], Step [139/172], Loss: 9.1714\n",
      "Epoch [135/300], Step [140/172], Loss: 9.0218\n",
      "Epoch [135/300], Step [141/172], Loss: 10.3398\n",
      "Epoch [135/300], Step [142/172], Loss: 13.3174\n",
      "Epoch [135/300], Step [143/172], Loss: 9.6315\n",
      "Epoch [135/300], Step [144/172], Loss: 8.5738\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [135/300], Step [145/172], Loss: 9.3777\n",
      "Epoch [135/300], Step [146/172], Loss: 9.1906\n",
      "Epoch [135/300], Step [147/172], Loss: 5.0067\n",
      "Epoch [135/300], Step [148/172], Loss: 6.0205\n",
      "Epoch [135/300], Step [149/172], Loss: 6.8573\n",
      "Epoch [135/300], Step [150/172], Loss: 6.8133\n",
      "Epoch [135/300], Step [151/172], Loss: 6.0754\n",
      "Epoch [135/300], Step [152/172], Loss: 7.2927\n",
      "Epoch [135/300], Step [153/172], Loss: 6.5603\n",
      "Epoch [135/300], Step [154/172], Loss: 7.4870\n",
      "Epoch [135/300], Step [155/172], Loss: 6.3450\n",
      "Epoch [135/300], Step [156/172], Loss: 12.2559\n",
      "Epoch [135/300], Step [157/172], Loss: 9.5096\n",
      "Epoch [135/300], Step [158/172], Loss: 7.0748\n",
      "Epoch [135/300], Step [159/172], Loss: 9.1523\n",
      "Epoch [135/300], Step [160/172], Loss: 9.8375\n",
      "Epoch [135/300], Step [161/172], Loss: 6.7900\n",
      "Epoch [135/300], Step [162/172], Loss: 6.1314\n",
      "Epoch [135/300], Step [163/172], Loss: 6.1860\n",
      "Epoch [135/300], Step [164/172], Loss: 8.7598\n",
      "Epoch [135/300], Step [165/172], Loss: 5.8769\n",
      "Epoch [135/300], Step [166/172], Loss: 5.4743\n",
      "Epoch [135/300], Step [167/172], Loss: 9.4397\n",
      "Epoch [135/300], Step [168/172], Loss: 6.7969\n",
      "Epoch [135/300], Step [169/172], Loss: 6.6562\n",
      "Epoch [135/300], Step [170/172], Loss: 5.0503\n",
      "Epoch [135/300], Step [171/172], Loss: 6.7741\n",
      "Epoch [135/300], Step [172/172], Loss: 5.0871\n",
      "Epoch [136/300], Step [1/172], Loss: 63.7313\n",
      "Epoch [136/300], Step [2/172], Loss: 64.6112\n",
      "Epoch [136/300], Step [3/172], Loss: 60.6214\n",
      "Epoch [136/300], Step [4/172], Loss: 35.2543\n",
      "Epoch [136/300], Step [5/172], Loss: 59.3757\n",
      "Epoch [136/300], Step [6/172], Loss: 19.1424\n",
      "Epoch [136/300], Step [7/172], Loss: 29.6275\n",
      "Epoch [136/300], Step [8/172], Loss: 5.1816\n",
      "Epoch [136/300], Step [9/172], Loss: 37.0519\n",
      "Epoch [136/300], Step [10/172], Loss: 44.9914\n",
      "Epoch [136/300], Step [11/172], Loss: 70.2735\n",
      "Epoch [136/300], Step [12/172], Loss: 78.8812\n",
      "Epoch [136/300], Step [13/172], Loss: 40.7699\n",
      "Epoch [136/300], Step [14/172], Loss: 74.0867\n",
      "Epoch [136/300], Step [15/172], Loss: 65.8279\n",
      "Epoch [136/300], Step [16/172], Loss: 14.1286\n",
      "Epoch [136/300], Step [17/172], Loss: 52.7060\n",
      "Epoch [136/300], Step [18/172], Loss: 62.7596\n",
      "Epoch [136/300], Step [19/172], Loss: 85.3607\n",
      "Epoch [136/300], Step [20/172], Loss: 52.7806\n",
      "Epoch [136/300], Step [21/172], Loss: 91.1188\n",
      "Epoch [136/300], Step [22/172], Loss: 69.3191\n",
      "Epoch [136/300], Step [23/172], Loss: 2.2069\n",
      "Epoch [136/300], Step [24/172], Loss: 64.6644\n",
      "Epoch [136/300], Step [25/172], Loss: 44.3652\n",
      "Epoch [136/300], Step [26/172], Loss: 54.3329\n",
      "Epoch [136/300], Step [27/172], Loss: 68.1931\n",
      "Epoch [136/300], Step [28/172], Loss: 28.8888\n",
      "Epoch [136/300], Step [29/172], Loss: 20.6244\n",
      "Epoch [136/300], Step [30/172], Loss: 74.9246\n",
      "Epoch [136/300], Step [31/172], Loss: 41.8263\n",
      "Epoch [136/300], Step [32/172], Loss: 42.5893\n",
      "Epoch [136/300], Step [33/172], Loss: 72.4158\n",
      "Epoch [136/300], Step [34/172], Loss: 3.8314\n",
      "Epoch [136/300], Step [35/172], Loss: 13.3942\n",
      "Epoch [136/300], Step [36/172], Loss: 19.3392\n",
      "Epoch [136/300], Step [37/172], Loss: 17.4796\n",
      "Epoch [136/300], Step [38/172], Loss: 28.1788\n",
      "Epoch [136/300], Step [39/172], Loss: 38.2042\n",
      "Epoch [136/300], Step [40/172], Loss: 19.8394\n",
      "Epoch [136/300], Step [41/172], Loss: 36.1382\n",
      "Epoch [136/300], Step [42/172], Loss: 39.5015\n",
      "Epoch [136/300], Step [43/172], Loss: 26.2679\n",
      "Epoch [136/300], Step [44/172], Loss: 19.7025\n",
      "Epoch [136/300], Step [45/172], Loss: 23.1794\n",
      "Epoch [136/300], Step [46/172], Loss: 18.9013\n",
      "Epoch [136/300], Step [47/172], Loss: 46.0523\n",
      "Epoch [136/300], Step [48/172], Loss: 55.3553\n",
      "Epoch [136/300], Step [49/172], Loss: 19.5823\n",
      "Epoch [136/300], Step [50/172], Loss: 47.3883\n",
      "Epoch [136/300], Step [51/172], Loss: 7.7382\n",
      "Epoch [136/300], Step [52/172], Loss: 17.2810\n",
      "Epoch [136/300], Step [53/172], Loss: 21.8354\n",
      "Epoch [136/300], Step [54/172], Loss: 12.0617\n",
      "Epoch [136/300], Step [55/172], Loss: 12.3728\n",
      "Epoch [136/300], Step [56/172], Loss: 14.4982\n",
      "Epoch [136/300], Step [57/172], Loss: 17.2715\n",
      "Epoch [136/300], Step [58/172], Loss: 15.2515\n",
      "Epoch [136/300], Step [59/172], Loss: 28.7872\n",
      "Epoch [136/300], Step [60/172], Loss: 36.9820\n",
      "Epoch [136/300], Step [61/172], Loss: 6.7201\n",
      "Epoch [136/300], Step [62/172], Loss: 21.7966\n",
      "Epoch [136/300], Step [63/172], Loss: 9.3628\n",
      "Epoch [136/300], Step [64/172], Loss: 9.1403\n",
      "Epoch [136/300], Step [65/172], Loss: 19.7446\n",
      "Epoch [136/300], Step [66/172], Loss: 5.7489\n",
      "Epoch [136/300], Step [67/172], Loss: 25.2903\n",
      "Epoch [136/300], Step [68/172], Loss: 5.2449\n",
      "Epoch [136/300], Step [69/172], Loss: 46.7989\n",
      "Epoch [136/300], Step [70/172], Loss: 49.3632\n",
      "Epoch [136/300], Step [71/172], Loss: 47.0916\n",
      "Epoch [136/300], Step [72/172], Loss: 50.5166\n",
      "Epoch [136/300], Step [73/172], Loss: 56.6154\n",
      "Epoch [136/300], Step [74/172], Loss: 30.8107\n",
      "Epoch [136/300], Step [75/172], Loss: 30.9861\n",
      "Epoch [136/300], Step [76/172], Loss: 33.3168\n",
      "Epoch [136/300], Step [77/172], Loss: 58.6282\n",
      "Epoch [136/300], Step [78/172], Loss: 44.4502\n",
      "Epoch [136/300], Step [79/172], Loss: 43.6456\n",
      "Epoch [136/300], Step [80/172], Loss: 56.0231\n",
      "Epoch [136/300], Step [81/172], Loss: 38.9810\n",
      "Epoch [136/300], Step [82/172], Loss: 38.5575\n",
      "Epoch [136/300], Step [83/172], Loss: 46.0158\n",
      "Epoch [136/300], Step [84/172], Loss: 35.3227\n",
      "Epoch [136/300], Step [85/172], Loss: 40.2288\n",
      "Epoch [136/300], Step [86/172], Loss: 33.2454\n",
      "Epoch [136/300], Step [87/172], Loss: 27.4479\n",
      "Epoch [136/300], Step [88/172], Loss: 28.5467\n",
      "Epoch [136/300], Step [89/172], Loss: 25.8352\n",
      "Epoch [136/300], Step [90/172], Loss: 24.2497\n",
      "Epoch [136/300], Step [91/172], Loss: 27.5157\n",
      "Epoch [136/300], Step [92/172], Loss: 21.1092\n",
      "Epoch [136/300], Step [93/172], Loss: 21.3680\n",
      "Epoch [136/300], Step [94/172], Loss: 29.3970\n",
      "Epoch [136/300], Step [95/172], Loss: 21.6171\n",
      "Epoch [136/300], Step [96/172], Loss: 19.3882\n",
      "Epoch [136/300], Step [97/172], Loss: 26.7022\n",
      "Epoch [136/300], Step [98/172], Loss: 19.1448\n",
      "Epoch [136/300], Step [99/172], Loss: 18.3882\n",
      "Epoch [136/300], Step [100/172], Loss: 15.2230\n",
      "Epoch [136/300], Step [101/172], Loss: 17.0880\n",
      "Epoch [136/300], Step [102/172], Loss: 16.6219\n",
      "Epoch [136/300], Step [103/172], Loss: 12.7794\n",
      "Epoch [136/300], Step [104/172], Loss: 16.5790\n",
      "Epoch [136/300], Step [105/172], Loss: 17.9859\n",
      "Epoch [136/300], Step [106/172], Loss: 15.9735\n",
      "Epoch [136/300], Step [107/172], Loss: 15.0352\n",
      "Epoch [136/300], Step [108/172], Loss: 15.8821\n",
      "Epoch [136/300], Step [109/172], Loss: 15.7726\n",
      "Epoch [136/300], Step [110/172], Loss: 15.4659\n",
      "Epoch [136/300], Step [111/172], Loss: 13.8579\n",
      "Epoch [136/300], Step [112/172], Loss: 17.6827\n",
      "Epoch [136/300], Step [113/172], Loss: 12.8531\n",
      "Epoch [136/300], Step [114/172], Loss: 13.3313\n",
      "Epoch [136/300], Step [115/172], Loss: 19.9756\n",
      "Epoch [136/300], Step [116/172], Loss: 14.0734\n",
      "Epoch [136/300], Step [117/172], Loss: 10.8303\n",
      "Epoch [136/300], Step [118/172], Loss: 14.4363\n",
      "Epoch [136/300], Step [119/172], Loss: 15.1979\n",
      "Epoch [136/300], Step [120/172], Loss: 9.8254\n",
      "Epoch [136/300], Step [121/172], Loss: 9.8423\n",
      "Epoch [136/300], Step [122/172], Loss: 10.2270\n",
      "Epoch [136/300], Step [123/172], Loss: 10.1022\n",
      "Epoch [136/300], Step [124/172], Loss: 7.8976\n",
      "Epoch [136/300], Step [125/172], Loss: 12.2809\n",
      "Epoch [136/300], Step [126/172], Loss: 10.1143\n",
      "Epoch [136/300], Step [127/172], Loss: 10.8269\n",
      "Epoch [136/300], Step [128/172], Loss: 10.8583\n",
      "Epoch [136/300], Step [129/172], Loss: 8.0653\n",
      "Epoch [136/300], Step [130/172], Loss: 11.3070\n",
      "Epoch [136/300], Step [131/172], Loss: 7.7034\n",
      "Epoch [136/300], Step [132/172], Loss: 8.2120\n",
      "Epoch [136/300], Step [133/172], Loss: 9.1327\n",
      "Epoch [136/300], Step [134/172], Loss: 11.6218\n",
      "Epoch [136/300], Step [135/172], Loss: 8.5596\n",
      "Epoch [136/300], Step [136/172], Loss: 8.1004\n",
      "Epoch [136/300], Step [137/172], Loss: 9.2655\n",
      "Epoch [136/300], Step [138/172], Loss: 7.2515\n",
      "Epoch [136/300], Step [139/172], Loss: 9.2280\n",
      "Epoch [136/300], Step [140/172], Loss: 9.0545\n",
      "Epoch [136/300], Step [141/172], Loss: 10.3506\n",
      "Epoch [136/300], Step [142/172], Loss: 13.3452\n",
      "Epoch [136/300], Step [143/172], Loss: 9.6609\n",
      "Epoch [136/300], Step [144/172], Loss: 8.5968\n",
      "Epoch [136/300], Step [145/172], Loss: 9.3894\n",
      "Epoch [136/300], Step [146/172], Loss: 9.1983\n",
      "Epoch [136/300], Step [147/172], Loss: 5.0216\n",
      "Epoch [136/300], Step [148/172], Loss: 6.0308\n",
      "Epoch [136/300], Step [149/172], Loss: 6.8696\n",
      "Epoch [136/300], Step [150/172], Loss: 6.8035\n",
      "Epoch [136/300], Step [151/172], Loss: 6.0555\n",
      "Epoch [136/300], Step [152/172], Loss: 7.3252\n",
      "Epoch [136/300], Step [153/172], Loss: 6.5674\n",
      "Epoch [136/300], Step [154/172], Loss: 7.5108\n",
      "Epoch [136/300], Step [155/172], Loss: 6.3372\n",
      "Epoch [136/300], Step [156/172], Loss: 12.2973\n",
      "Epoch [136/300], Step [157/172], Loss: 9.5144\n",
      "Epoch [136/300], Step [158/172], Loss: 7.0726\n",
      "Epoch [136/300], Step [159/172], Loss: 9.2046\n",
      "Epoch [136/300], Step [160/172], Loss: 9.8513\n",
      "Epoch [136/300], Step [161/172], Loss: 6.8127\n",
      "Epoch [136/300], Step [162/172], Loss: 6.1182\n",
      "Epoch [136/300], Step [163/172], Loss: 6.1931\n",
      "Epoch [136/300], Step [164/172], Loss: 8.6966\n",
      "Epoch [136/300], Step [165/172], Loss: 5.8753\n",
      "Epoch [136/300], Step [166/172], Loss: 5.4907\n",
      "Epoch [136/300], Step [167/172], Loss: 9.4791\n",
      "Epoch [136/300], Step [168/172], Loss: 6.7979\n",
      "Epoch [136/300], Step [169/172], Loss: 6.6778\n",
      "Epoch [136/300], Step [170/172], Loss: 5.0375\n",
      "Epoch [136/300], Step [171/172], Loss: 6.7988\n",
      "Epoch [136/300], Step [172/172], Loss: 5.0817\n",
      "Epoch [137/300], Step [1/172], Loss: 63.3749\n",
      "Epoch [137/300], Step [2/172], Loss: 64.1808\n",
      "Epoch [137/300], Step [3/172], Loss: 59.8797\n",
      "Epoch [137/300], Step [4/172], Loss: 35.1746\n",
      "Epoch [137/300], Step [5/172], Loss: 59.0339\n",
      "Epoch [137/300], Step [6/172], Loss: 19.0777\n",
      "Epoch [137/300], Step [7/172], Loss: 29.4536\n",
      "Epoch [137/300], Step [8/172], Loss: 5.4892\n",
      "Epoch [137/300], Step [9/172], Loss: 37.0454\n",
      "Epoch [137/300], Step [10/172], Loss: 45.0247\n",
      "Epoch [137/300], Step [11/172], Loss: 70.1450\n",
      "Epoch [137/300], Step [12/172], Loss: 78.7844\n",
      "Epoch [137/300], Step [13/172], Loss: 41.1218\n",
      "Epoch [137/300], Step [14/172], Loss: 73.9760\n",
      "Epoch [137/300], Step [15/172], Loss: 65.6603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [137/300], Step [16/172], Loss: 13.6333\n",
      "Epoch [137/300], Step [17/172], Loss: 52.6895\n",
      "Epoch [137/300], Step [18/172], Loss: 62.7990\n",
      "Epoch [137/300], Step [19/172], Loss: 85.3948\n",
      "Epoch [137/300], Step [20/172], Loss: 52.2124\n",
      "Epoch [137/300], Step [21/172], Loss: 90.9311\n",
      "Epoch [137/300], Step [22/172], Loss: 68.9024\n",
      "Epoch [137/300], Step [23/172], Loss: 2.1905\n",
      "Epoch [137/300], Step [24/172], Loss: 64.2615\n",
      "Epoch [137/300], Step [25/172], Loss: 44.5345\n",
      "Epoch [137/300], Step [26/172], Loss: 54.2426\n",
      "Epoch [137/300], Step [27/172], Loss: 67.6382\n",
      "Epoch [137/300], Step [28/172], Loss: 28.5946\n",
      "Epoch [137/300], Step [29/172], Loss: 20.4920\n",
      "Epoch [137/300], Step [30/172], Loss: 74.0146\n",
      "Epoch [137/300], Step [31/172], Loss: 41.4152\n",
      "Epoch [137/300], Step [32/172], Loss: 42.5201\n",
      "Epoch [137/300], Step [33/172], Loss: 72.3502\n",
      "Epoch [137/300], Step [34/172], Loss: 4.2472\n",
      "Epoch [137/300], Step [35/172], Loss: 13.4213\n",
      "Epoch [137/300], Step [36/172], Loss: 19.4890\n",
      "Epoch [137/300], Step [37/172], Loss: 17.5091\n",
      "Epoch [137/300], Step [38/172], Loss: 28.3226\n",
      "Epoch [137/300], Step [39/172], Loss: 38.1113\n",
      "Epoch [137/300], Step [40/172], Loss: 19.8915\n",
      "Epoch [137/300], Step [41/172], Loss: 36.1975\n",
      "Epoch [137/300], Step [42/172], Loss: 39.6358\n",
      "Epoch [137/300], Step [43/172], Loss: 26.4149\n",
      "Epoch [137/300], Step [44/172], Loss: 19.8542\n",
      "Epoch [137/300], Step [45/172], Loss: 23.3585\n",
      "Epoch [137/300], Step [46/172], Loss: 18.9747\n",
      "Epoch [137/300], Step [47/172], Loss: 46.0648\n",
      "Epoch [137/300], Step [48/172], Loss: 55.6661\n",
      "Epoch [137/300], Step [49/172], Loss: 19.9150\n",
      "Epoch [137/300], Step [50/172], Loss: 47.2791\n",
      "Epoch [137/300], Step [51/172], Loss: 7.8638\n",
      "Epoch [137/300], Step [52/172], Loss: 17.4104\n",
      "Epoch [137/300], Step [53/172], Loss: 21.8774\n",
      "Epoch [137/300], Step [54/172], Loss: 12.3306\n",
      "Epoch [137/300], Step [55/172], Loss: 12.5229\n",
      "Epoch [137/300], Step [56/172], Loss: 14.4158\n",
      "Epoch [137/300], Step [57/172], Loss: 17.2917\n",
      "Epoch [137/300], Step [58/172], Loss: 15.3128\n",
      "Epoch [137/300], Step [59/172], Loss: 28.6368\n",
      "Epoch [137/300], Step [60/172], Loss: 36.8062\n",
      "Epoch [137/300], Step [61/172], Loss: 6.6545\n",
      "Epoch [137/300], Step [62/172], Loss: 21.6827\n",
      "Epoch [137/300], Step [63/172], Loss: 9.4210\n",
      "Epoch [137/300], Step [64/172], Loss: 9.2063\n",
      "Epoch [137/300], Step [65/172], Loss: 19.7087\n",
      "Epoch [137/300], Step [66/172], Loss: 5.7266\n",
      "Epoch [137/300], Step [67/172], Loss: 25.2360\n",
      "Epoch [137/300], Step [68/172], Loss: 5.3367\n",
      "Epoch [137/300], Step [69/172], Loss: 46.2196\n",
      "Epoch [137/300], Step [70/172], Loss: 49.2692\n",
      "Epoch [137/300], Step [71/172], Loss: 46.9035\n",
      "Epoch [137/300], Step [72/172], Loss: 50.3836\n",
      "Epoch [137/300], Step [73/172], Loss: 56.6059\n",
      "Epoch [137/300], Step [74/172], Loss: 30.6822\n",
      "Epoch [137/300], Step [75/172], Loss: 30.7083\n",
      "Epoch [137/300], Step [76/172], Loss: 33.2355\n",
      "Epoch [137/300], Step [77/172], Loss: 58.4769\n",
      "Epoch [137/300], Step [78/172], Loss: 44.3723\n",
      "Epoch [137/300], Step [79/172], Loss: 43.5402\n",
      "Epoch [137/300], Step [80/172], Loss: 55.9683\n",
      "Epoch [137/300], Step [81/172], Loss: 38.9593\n",
      "Epoch [137/300], Step [82/172], Loss: 38.6353\n",
      "Epoch [137/300], Step [83/172], Loss: 45.9843\n",
      "Epoch [137/300], Step [84/172], Loss: 35.1676\n",
      "Epoch [137/300], Step [85/172], Loss: 40.1374\n",
      "Epoch [137/300], Step [86/172], Loss: 33.1732\n",
      "Epoch [137/300], Step [87/172], Loss: 27.3597\n",
      "Epoch [137/300], Step [88/172], Loss: 28.4547\n",
      "Epoch [137/300], Step [89/172], Loss: 25.8178\n",
      "Epoch [137/300], Step [90/172], Loss: 24.0829\n",
      "Epoch [137/300], Step [91/172], Loss: 27.3953\n",
      "Epoch [137/300], Step [92/172], Loss: 20.9804\n",
      "Epoch [137/300], Step [93/172], Loss: 21.3238\n",
      "Epoch [137/300], Step [94/172], Loss: 29.3949\n",
      "Epoch [137/300], Step [95/172], Loss: 21.5582\n",
      "Epoch [137/300], Step [96/172], Loss: 19.3846\n",
      "Epoch [137/300], Step [97/172], Loss: 26.6592\n",
      "Epoch [137/300], Step [98/172], Loss: 19.1448\n",
      "Epoch [137/300], Step [99/172], Loss: 18.3430\n",
      "Epoch [137/300], Step [100/172], Loss: 15.1713\n",
      "Epoch [137/300], Step [101/172], Loss: 17.0715\n",
      "Epoch [137/300], Step [102/172], Loss: 16.6745\n",
      "Epoch [137/300], Step [103/172], Loss: 12.7214\n",
      "Epoch [137/300], Step [104/172], Loss: 16.5664\n",
      "Epoch [137/300], Step [105/172], Loss: 18.0538\n",
      "Epoch [137/300], Step [106/172], Loss: 15.9155\n",
      "Epoch [137/300], Step [107/172], Loss: 14.9364\n",
      "Epoch [137/300], Step [108/172], Loss: 15.8237\n",
      "Epoch [137/300], Step [109/172], Loss: 15.7464\n",
      "Epoch [137/300], Step [110/172], Loss: 15.4009\n",
      "Epoch [137/300], Step [111/172], Loss: 13.8368\n",
      "Epoch [137/300], Step [112/172], Loss: 17.7218\n",
      "Epoch [137/300], Step [113/172], Loss: 12.7884\n",
      "Epoch [137/300], Step [114/172], Loss: 13.3225\n",
      "Epoch [137/300], Step [115/172], Loss: 19.9844\n",
      "Epoch [137/300], Step [116/172], Loss: 14.0412\n",
      "Epoch [137/300], Step [117/172], Loss: 10.8295\n",
      "Epoch [137/300], Step [118/172], Loss: 14.4175\n",
      "Epoch [137/300], Step [119/172], Loss: 15.1062\n",
      "Epoch [137/300], Step [120/172], Loss: 9.8270\n",
      "Epoch [137/300], Step [121/172], Loss: 9.7966\n",
      "Epoch [137/300], Step [122/172], Loss: 10.2533\n",
      "Epoch [137/300], Step [123/172], Loss: 10.0698\n",
      "Epoch [137/300], Step [124/172], Loss: 7.8524\n",
      "Epoch [137/300], Step [125/172], Loss: 12.1842\n",
      "Epoch [137/300], Step [126/172], Loss: 10.0995\n",
      "Epoch [137/300], Step [127/172], Loss: 10.8180\n",
      "Epoch [137/300], Step [128/172], Loss: 10.8065\n",
      "Epoch [137/300], Step [129/172], Loss: 8.0314\n",
      "Epoch [137/300], Step [130/172], Loss: 11.2952\n",
      "Epoch [137/300], Step [131/172], Loss: 7.6764\n",
      "Epoch [137/300], Step [132/172], Loss: 8.1776\n",
      "Epoch [137/300], Step [133/172], Loss: 9.1425\n",
      "Epoch [137/300], Step [134/172], Loss: 11.5731\n",
      "Epoch [137/300], Step [135/172], Loss: 8.5138\n",
      "Epoch [137/300], Step [136/172], Loss: 8.0582\n",
      "Epoch [137/300], Step [137/172], Loss: 9.2594\n",
      "Epoch [137/300], Step [138/172], Loss: 7.2202\n",
      "Epoch [137/300], Step [139/172], Loss: 9.2005\n",
      "Epoch [137/300], Step [140/172], Loss: 9.0292\n",
      "Epoch [137/300], Step [141/172], Loss: 10.2980\n",
      "Epoch [137/300], Step [142/172], Loss: 13.3635\n",
      "Epoch [137/300], Step [143/172], Loss: 9.6552\n",
      "Epoch [137/300], Step [144/172], Loss: 8.5844\n",
      "Epoch [137/300], Step [145/172], Loss: 9.3611\n",
      "Epoch [137/300], Step [146/172], Loss: 9.1919\n",
      "Epoch [137/300], Step [147/172], Loss: 5.0124\n",
      "Epoch [137/300], Step [148/172], Loss: 6.0087\n",
      "Epoch [137/300], Step [149/172], Loss: 6.8293\n",
      "Epoch [137/300], Step [150/172], Loss: 6.7586\n",
      "Epoch [137/300], Step [151/172], Loss: 6.0508\n",
      "Epoch [137/300], Step [152/172], Loss: 7.2710\n",
      "Epoch [137/300], Step [153/172], Loss: 6.5541\n",
      "Epoch [137/300], Step [154/172], Loss: 7.4869\n",
      "Epoch [137/300], Step [155/172], Loss: 6.3273\n",
      "Epoch [137/300], Step [156/172], Loss: 12.3340\n",
      "Epoch [137/300], Step [157/172], Loss: 9.5279\n",
      "Epoch [137/300], Step [158/172], Loss: 7.0843\n",
      "Epoch [137/300], Step [159/172], Loss: 9.1298\n",
      "Epoch [137/300], Step [160/172], Loss: 9.9120\n",
      "Epoch [137/300], Step [161/172], Loss: 6.8044\n",
      "Epoch [137/300], Step [162/172], Loss: 6.0770\n",
      "Epoch [137/300], Step [163/172], Loss: 6.1901\n",
      "Epoch [137/300], Step [164/172], Loss: 8.7624\n",
      "Epoch [137/300], Step [165/172], Loss: 5.8610\n",
      "Epoch [137/300], Step [166/172], Loss: 5.5050\n",
      "Epoch [137/300], Step [167/172], Loss: 9.4846\n",
      "Epoch [137/300], Step [168/172], Loss: 6.7930\n",
      "Epoch [137/300], Step [169/172], Loss: 6.6765\n",
      "Epoch [137/300], Step [170/172], Loss: 5.0243\n",
      "Epoch [137/300], Step [171/172], Loss: 6.7944\n",
      "Epoch [137/300], Step [172/172], Loss: 5.0797\n",
      "Epoch [138/300], Step [1/172], Loss: 63.1515\n",
      "Epoch [138/300], Step [2/172], Loss: 63.9308\n",
      "Epoch [138/300], Step [3/172], Loss: 59.6173\n",
      "Epoch [138/300], Step [4/172], Loss: 34.9149\n",
      "Epoch [138/300], Step [5/172], Loss: 59.0540\n",
      "Epoch [138/300], Step [6/172], Loss: 18.9991\n",
      "Epoch [138/300], Step [7/172], Loss: 29.1204\n",
      "Epoch [138/300], Step [8/172], Loss: 5.1183\n",
      "Epoch [138/300], Step [9/172], Loss: 36.8831\n",
      "Epoch [138/300], Step [10/172], Loss: 45.0078\n",
      "Epoch [138/300], Step [11/172], Loss: 69.7421\n",
      "Epoch [138/300], Step [12/172], Loss: 78.4041\n",
      "Epoch [138/300], Step [13/172], Loss: 40.6923\n",
      "Epoch [138/300], Step [14/172], Loss: 72.9940\n",
      "Epoch [138/300], Step [15/172], Loss: 65.2859\n",
      "Epoch [138/300], Step [16/172], Loss: 14.2012\n",
      "Epoch [138/300], Step [17/172], Loss: 52.2884\n",
      "Epoch [138/300], Step [18/172], Loss: 62.7156\n",
      "Epoch [138/300], Step [19/172], Loss: 85.4159\n",
      "Epoch [138/300], Step [20/172], Loss: 51.9094\n",
      "Epoch [138/300], Step [21/172], Loss: 90.8148\n",
      "Epoch [138/300], Step [22/172], Loss: 68.7477\n",
      "Epoch [138/300], Step [23/172], Loss: 2.1210\n",
      "Epoch [138/300], Step [24/172], Loss: 64.2870\n",
      "Epoch [138/300], Step [25/172], Loss: 44.4736\n",
      "Epoch [138/300], Step [26/172], Loss: 54.1905\n",
      "Epoch [138/300], Step [27/172], Loss: 67.9734\n",
      "Epoch [138/300], Step [28/172], Loss: 28.6227\n",
      "Epoch [138/300], Step [29/172], Loss: 20.4759\n",
      "Epoch [138/300], Step [30/172], Loss: 73.6278\n",
      "Epoch [138/300], Step [31/172], Loss: 41.5159\n",
      "Epoch [138/300], Step [32/172], Loss: 42.7573\n",
      "Epoch [138/300], Step [33/172], Loss: 72.6621\n",
      "Epoch [138/300], Step [34/172], Loss: 3.8521\n",
      "Epoch [138/300], Step [35/172], Loss: 13.4381\n",
      "Epoch [138/300], Step [36/172], Loss: 19.3301\n",
      "Epoch [138/300], Step [37/172], Loss: 17.5483\n",
      "Epoch [138/300], Step [38/172], Loss: 28.4925\n",
      "Epoch [138/300], Step [39/172], Loss: 38.1129\n",
      "Epoch [138/300], Step [40/172], Loss: 19.9128\n",
      "Epoch [138/300], Step [41/172], Loss: 36.1677\n",
      "Epoch [138/300], Step [42/172], Loss: 39.7322\n",
      "Epoch [138/300], Step [43/172], Loss: 26.4396\n",
      "Epoch [138/300], Step [44/172], Loss: 19.8366\n",
      "Epoch [138/300], Step [45/172], Loss: 23.3706\n",
      "Epoch [138/300], Step [46/172], Loss: 18.7712\n",
      "Epoch [138/300], Step [47/172], Loss: 46.0465\n",
      "Epoch [138/300], Step [48/172], Loss: 55.7435\n",
      "Epoch [138/300], Step [49/172], Loss: 19.7625\n",
      "Epoch [138/300], Step [50/172], Loss: 47.1722\n",
      "Epoch [138/300], Step [51/172], Loss: 7.8155\n",
      "Epoch [138/300], Step [52/172], Loss: 17.3737\n",
      "Epoch [138/300], Step [53/172], Loss: 21.8588\n",
      "Epoch [138/300], Step [54/172], Loss: 12.3192\n",
      "Epoch [138/300], Step [55/172], Loss: 12.5951\n",
      "Epoch [138/300], Step [56/172], Loss: 14.8601\n",
      "Epoch [138/300], Step [57/172], Loss: 17.1913\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [138/300], Step [58/172], Loss: 15.2520\n",
      "Epoch [138/300], Step [59/172], Loss: 28.5561\n",
      "Epoch [138/300], Step [60/172], Loss: 36.3759\n",
      "Epoch [138/300], Step [61/172], Loss: 6.6133\n",
      "Epoch [138/300], Step [62/172], Loss: 21.6536\n",
      "Epoch [138/300], Step [63/172], Loss: 9.4082\n",
      "Epoch [138/300], Step [64/172], Loss: 9.2954\n",
      "Epoch [138/300], Step [65/172], Loss: 19.6919\n",
      "Epoch [138/300], Step [66/172], Loss: 5.7443\n",
      "Epoch [138/300], Step [67/172], Loss: 25.1302\n",
      "Epoch [138/300], Step [68/172], Loss: 5.6105\n",
      "Epoch [138/300], Step [69/172], Loss: 45.8055\n",
      "Epoch [138/300], Step [70/172], Loss: 48.7574\n",
      "Epoch [138/300], Step [71/172], Loss: 46.6429\n",
      "Epoch [138/300], Step [72/172], Loss: 49.8882\n",
      "Epoch [138/300], Step [73/172], Loss: 56.2883\n",
      "Epoch [138/300], Step [74/172], Loss: 30.4754\n",
      "Epoch [138/300], Step [75/172], Loss: 30.6036\n",
      "Epoch [138/300], Step [76/172], Loss: 33.0173\n",
      "Epoch [138/300], Step [77/172], Loss: 58.2195\n",
      "Epoch [138/300], Step [78/172], Loss: 44.2070\n",
      "Epoch [138/300], Step [79/172], Loss: 43.3949\n",
      "Epoch [138/300], Step [80/172], Loss: 55.7557\n",
      "Epoch [138/300], Step [81/172], Loss: 38.8529\n",
      "Epoch [138/300], Step [82/172], Loss: 38.3673\n",
      "Epoch [138/300], Step [83/172], Loss: 45.9319\n",
      "Epoch [138/300], Step [84/172], Loss: 35.2122\n",
      "Epoch [138/300], Step [85/172], Loss: 40.1622\n",
      "Epoch [138/300], Step [86/172], Loss: 33.2274\n",
      "Epoch [138/300], Step [87/172], Loss: 27.3867\n",
      "Epoch [138/300], Step [88/172], Loss: 28.4002\n",
      "Epoch [138/300], Step [89/172], Loss: 25.8895\n",
      "Epoch [138/300], Step [90/172], Loss: 24.0640\n",
      "Epoch [138/300], Step [91/172], Loss: 27.4184\n",
      "Epoch [138/300], Step [92/172], Loss: 21.0150\n",
      "Epoch [138/300], Step [93/172], Loss: 21.3769\n",
      "Epoch [138/300], Step [94/172], Loss: 29.3584\n",
      "Epoch [138/300], Step [95/172], Loss: 21.5138\n",
      "Epoch [138/300], Step [96/172], Loss: 19.4122\n",
      "Epoch [138/300], Step [97/172], Loss: 26.6914\n",
      "Epoch [138/300], Step [98/172], Loss: 19.1579\n",
      "Epoch [138/300], Step [99/172], Loss: 18.3752\n",
      "Epoch [138/300], Step [100/172], Loss: 15.1898\n",
      "Epoch [138/300], Step [101/172], Loss: 17.1483\n",
      "Epoch [138/300], Step [102/172], Loss: 16.6103\n",
      "Epoch [138/300], Step [103/172], Loss: 12.7397\n",
      "Epoch [138/300], Step [104/172], Loss: 16.6143\n",
      "Epoch [138/300], Step [105/172], Loss: 18.0050\n",
      "Epoch [138/300], Step [106/172], Loss: 15.9277\n",
      "Epoch [138/300], Step [107/172], Loss: 14.9922\n",
      "Epoch [138/300], Step [108/172], Loss: 15.7974\n",
      "Epoch [138/300], Step [109/172], Loss: 15.6693\n",
      "Epoch [138/300], Step [110/172], Loss: 15.4052\n",
      "Epoch [138/300], Step [111/172], Loss: 13.8807\n",
      "Epoch [138/300], Step [112/172], Loss: 17.6670\n",
      "Epoch [138/300], Step [113/172], Loss: 12.7827\n",
      "Epoch [138/300], Step [114/172], Loss: 13.3317\n",
      "Epoch [138/300], Step [115/172], Loss: 19.9359\n",
      "Epoch [138/300], Step [116/172], Loss: 14.0910\n",
      "Epoch [138/300], Step [117/172], Loss: 10.8700\n",
      "Epoch [138/300], Step [118/172], Loss: 14.3627\n",
      "Epoch [138/300], Step [119/172], Loss: 15.1740\n",
      "Epoch [138/300], Step [120/172], Loss: 9.8000\n",
      "Epoch [138/300], Step [121/172], Loss: 9.8232\n",
      "Epoch [138/300], Step [122/172], Loss: 10.1592\n",
      "Epoch [138/300], Step [123/172], Loss: 10.0773\n",
      "Epoch [138/300], Step [124/172], Loss: 7.8799\n",
      "Epoch [138/300], Step [125/172], Loss: 12.2489\n",
      "Epoch [138/300], Step [126/172], Loss: 10.1511\n",
      "Epoch [138/300], Step [127/172], Loss: 10.8098\n",
      "Epoch [138/300], Step [128/172], Loss: 10.8342\n",
      "Epoch [138/300], Step [129/172], Loss: 8.0601\n",
      "Epoch [138/300], Step [130/172], Loss: 11.3549\n",
      "Epoch [138/300], Step [131/172], Loss: 7.6561\n",
      "Epoch [138/300], Step [132/172], Loss: 8.2036\n",
      "Epoch [138/300], Step [133/172], Loss: 9.0768\n",
      "Epoch [138/300], Step [134/172], Loss: 11.5890\n",
      "Epoch [138/300], Step [135/172], Loss: 8.5437\n",
      "Epoch [138/300], Step [136/172], Loss: 8.1053\n",
      "Epoch [138/300], Step [137/172], Loss: 9.2772\n",
      "Epoch [138/300], Step [138/172], Loss: 7.2495\n",
      "Epoch [138/300], Step [139/172], Loss: 9.2335\n",
      "Epoch [138/300], Step [140/172], Loss: 9.0638\n",
      "Epoch [138/300], Step [141/172], Loss: 10.3041\n",
      "Epoch [138/300], Step [142/172], Loss: 13.3673\n",
      "Epoch [138/300], Step [143/172], Loss: 9.6949\n",
      "Epoch [138/300], Step [144/172], Loss: 8.6000\n",
      "Epoch [138/300], Step [145/172], Loss: 9.4043\n",
      "Epoch [138/300], Step [146/172], Loss: 9.1970\n",
      "Epoch [138/300], Step [147/172], Loss: 5.0320\n",
      "Epoch [138/300], Step [148/172], Loss: 6.0183\n",
      "Epoch [138/300], Step [149/172], Loss: 6.8626\n",
      "Epoch [138/300], Step [150/172], Loss: 6.7687\n",
      "Epoch [138/300], Step [151/172], Loss: 6.0059\n",
      "Epoch [138/300], Step [152/172], Loss: 7.2994\n",
      "Epoch [138/300], Step [153/172], Loss: 6.5734\n",
      "Epoch [138/300], Step [154/172], Loss: 7.4830\n",
      "Epoch [138/300], Step [155/172], Loss: 6.3323\n",
      "Epoch [138/300], Step [156/172], Loss: 12.2974\n",
      "Epoch [138/300], Step [157/172], Loss: 9.4659\n",
      "Epoch [138/300], Step [158/172], Loss: 7.0619\n",
      "Epoch [138/300], Step [159/172], Loss: 9.1357\n",
      "Epoch [138/300], Step [160/172], Loss: 9.8772\n",
      "Epoch [138/300], Step [161/172], Loss: 6.8658\n",
      "Epoch [138/300], Step [162/172], Loss: 6.0737\n",
      "Epoch [138/300], Step [163/172], Loss: 6.2147\n",
      "Epoch [138/300], Step [164/172], Loss: 8.6655\n",
      "Epoch [138/300], Step [165/172], Loss: 5.8679\n",
      "Epoch [138/300], Step [166/172], Loss: 5.4882\n",
      "Epoch [138/300], Step [167/172], Loss: 9.5097\n",
      "Epoch [138/300], Step [168/172], Loss: 6.7955\n",
      "Epoch [138/300], Step [169/172], Loss: 6.6720\n",
      "Epoch [138/300], Step [170/172], Loss: 5.0227\n",
      "Epoch [138/300], Step [171/172], Loss: 6.8477\n",
      "Epoch [138/300], Step [172/172], Loss: 5.0940\n",
      "Epoch [139/300], Step [1/172], Loss: 62.8547\n",
      "Epoch [139/300], Step [2/172], Loss: 63.7004\n",
      "Epoch [139/300], Step [3/172], Loss: 59.0787\n",
      "Epoch [139/300], Step [4/172], Loss: 34.5777\n",
      "Epoch [139/300], Step [5/172], Loss: 58.9963\n",
      "Epoch [139/300], Step [6/172], Loss: 19.2960\n",
      "Epoch [139/300], Step [7/172], Loss: 30.0313\n",
      "Epoch [139/300], Step [8/172], Loss: 5.4818\n",
      "Epoch [139/300], Step [9/172], Loss: 36.8268\n",
      "Epoch [139/300], Step [10/172], Loss: 44.8816\n",
      "Epoch [139/300], Step [11/172], Loss: 69.5219\n",
      "Epoch [139/300], Step [12/172], Loss: 78.0716\n",
      "Epoch [139/300], Step [13/172], Loss: 40.7921\n",
      "Epoch [139/300], Step [14/172], Loss: 72.9680\n",
      "Epoch [139/300], Step [15/172], Loss: 65.0575\n",
      "Epoch [139/300], Step [16/172], Loss: 13.4338\n",
      "Epoch [139/300], Step [17/172], Loss: 52.2485\n",
      "Epoch [139/300], Step [18/172], Loss: 62.7493\n",
      "Epoch [139/300], Step [19/172], Loss: 85.3542\n",
      "Epoch [139/300], Step [20/172], Loss: 51.5747\n",
      "Epoch [139/300], Step [21/172], Loss: 90.6022\n",
      "Epoch [139/300], Step [22/172], Loss: 68.5081\n",
      "Epoch [139/300], Step [23/172], Loss: 2.1570\n",
      "Epoch [139/300], Step [24/172], Loss: 63.8650\n",
      "Epoch [139/300], Step [25/172], Loss: 44.2435\n",
      "Epoch [139/300], Step [26/172], Loss: 53.8716\n",
      "Epoch [139/300], Step [27/172], Loss: 67.5645\n",
      "Epoch [139/300], Step [28/172], Loss: 28.4212\n",
      "Epoch [139/300], Step [29/172], Loss: 20.2809\n",
      "Epoch [139/300], Step [30/172], Loss: 72.6950\n",
      "Epoch [139/300], Step [31/172], Loss: 41.2365\n",
      "Epoch [139/300], Step [32/172], Loss: 42.5909\n",
      "Epoch [139/300], Step [33/172], Loss: 72.3346\n",
      "Epoch [139/300], Step [34/172], Loss: 4.0783\n",
      "Epoch [139/300], Step [35/172], Loss: 13.3522\n",
      "Epoch [139/300], Step [36/172], Loss: 19.2944\n",
      "Epoch [139/300], Step [37/172], Loss: 17.5362\n",
      "Epoch [139/300], Step [38/172], Loss: 28.4265\n",
      "Epoch [139/300], Step [39/172], Loss: 38.0557\n",
      "Epoch [139/300], Step [40/172], Loss: 20.0248\n",
      "Epoch [139/300], Step [41/172], Loss: 36.2720\n",
      "Epoch [139/300], Step [42/172], Loss: 39.7325\n",
      "Epoch [139/300], Step [43/172], Loss: 26.5881\n",
      "Epoch [139/300], Step [44/172], Loss: 19.8701\n",
      "Epoch [139/300], Step [45/172], Loss: 23.5836\n",
      "Epoch [139/300], Step [46/172], Loss: 18.6959\n",
      "Epoch [139/300], Step [47/172], Loss: 46.1122\n",
      "Epoch [139/300], Step [48/172], Loss: 55.6590\n",
      "Epoch [139/300], Step [49/172], Loss: 19.9902\n",
      "Epoch [139/300], Step [50/172], Loss: 46.9822\n",
      "Epoch [139/300], Step [51/172], Loss: 7.8923\n",
      "Epoch [139/300], Step [52/172], Loss: 17.4434\n",
      "Epoch [139/300], Step [53/172], Loss: 21.9372\n",
      "Epoch [139/300], Step [54/172], Loss: 12.4841\n",
      "Epoch [139/300], Step [55/172], Loss: 12.6166\n",
      "Epoch [139/300], Step [56/172], Loss: 14.4763\n",
      "Epoch [139/300], Step [57/172], Loss: 17.2594\n",
      "Epoch [139/300], Step [58/172], Loss: 15.1622\n",
      "Epoch [139/300], Step [59/172], Loss: 28.4435\n",
      "Epoch [139/300], Step [60/172], Loss: 36.3926\n",
      "Epoch [139/300], Step [61/172], Loss: 6.6572\n",
      "Epoch [139/300], Step [62/172], Loss: 21.4807\n",
      "Epoch [139/300], Step [63/172], Loss: 9.4029\n",
      "Epoch [139/300], Step [64/172], Loss: 9.2479\n",
      "Epoch [139/300], Step [65/172], Loss: 19.6622\n",
      "Epoch [139/300], Step [66/172], Loss: 5.7351\n",
      "Epoch [139/300], Step [67/172], Loss: 25.0382\n",
      "Epoch [139/300], Step [68/172], Loss: 5.2326\n",
      "Epoch [139/300], Step [69/172], Loss: 45.5572\n",
      "Epoch [139/300], Step [70/172], Loss: 48.9125\n",
      "Epoch [139/300], Step [71/172], Loss: 46.5956\n",
      "Epoch [139/300], Step [72/172], Loss: 49.8824\n",
      "Epoch [139/300], Step [73/172], Loss: 56.3257\n",
      "Epoch [139/300], Step [74/172], Loss: 30.3537\n",
      "Epoch [139/300], Step [75/172], Loss: 30.5519\n",
      "Epoch [139/300], Step [76/172], Loss: 32.9787\n",
      "Epoch [139/300], Step [77/172], Loss: 58.0051\n",
      "Epoch [139/300], Step [78/172], Loss: 44.1890\n",
      "Epoch [139/300], Step [79/172], Loss: 43.2754\n",
      "Epoch [139/300], Step [80/172], Loss: 55.7055\n",
      "Epoch [139/300], Step [81/172], Loss: 38.8427\n",
      "Epoch [139/300], Step [82/172], Loss: 38.6047\n",
      "Epoch [139/300], Step [83/172], Loss: 46.0327\n",
      "Epoch [139/300], Step [84/172], Loss: 35.2386\n",
      "Epoch [139/300], Step [85/172], Loss: 40.2240\n",
      "Epoch [139/300], Step [86/172], Loss: 33.2664\n",
      "Epoch [139/300], Step [87/172], Loss: 27.4345\n",
      "Epoch [139/300], Step [88/172], Loss: 28.4648\n",
      "Epoch [139/300], Step [89/172], Loss: 25.9676\n",
      "Epoch [139/300], Step [90/172], Loss: 23.9898\n",
      "Epoch [139/300], Step [91/172], Loss: 27.4186\n",
      "Epoch [139/300], Step [92/172], Loss: 20.9955\n",
      "Epoch [139/300], Step [93/172], Loss: 21.3759\n",
      "Epoch [139/300], Step [94/172], Loss: 29.3323\n",
      "Epoch [139/300], Step [95/172], Loss: 21.5083\n",
      "Epoch [139/300], Step [96/172], Loss: 19.4686\n",
      "Epoch [139/300], Step [97/172], Loss: 26.8028\n",
      "Epoch [139/300], Step [98/172], Loss: 19.2393\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [139/300], Step [99/172], Loss: 18.4287\n",
      "Epoch [139/300], Step [100/172], Loss: 15.2236\n",
      "Epoch [139/300], Step [101/172], Loss: 17.2070\n",
      "Epoch [139/300], Step [102/172], Loss: 16.7638\n",
      "Epoch [139/300], Step [103/172], Loss: 12.7430\n",
      "Epoch [139/300], Step [104/172], Loss: 16.7010\n",
      "Epoch [139/300], Step [105/172], Loss: 18.1415\n",
      "Epoch [139/300], Step [106/172], Loss: 15.8843\n",
      "Epoch [139/300], Step [107/172], Loss: 14.9573\n",
      "Epoch [139/300], Step [108/172], Loss: 15.7672\n",
      "Epoch [139/300], Step [109/172], Loss: 15.6816\n",
      "Epoch [139/300], Step [110/172], Loss: 15.3800\n",
      "Epoch [139/300], Step [111/172], Loss: 13.8810\n",
      "Epoch [139/300], Step [112/172], Loss: 17.6706\n",
      "Epoch [139/300], Step [113/172], Loss: 12.6874\n",
      "Epoch [139/300], Step [114/172], Loss: 13.3488\n",
      "Epoch [139/300], Step [115/172], Loss: 19.9007\n",
      "Epoch [139/300], Step [116/172], Loss: 14.0841\n",
      "Epoch [139/300], Step [117/172], Loss: 10.8583\n",
      "Epoch [139/300], Step [118/172], Loss: 14.3021\n",
      "Epoch [139/300], Step [119/172], Loss: 15.1230\n",
      "Epoch [139/300], Step [120/172], Loss: 9.7979\n",
      "Epoch [139/300], Step [121/172], Loss: 9.8167\n",
      "Epoch [139/300], Step [122/172], Loss: 10.1471\n",
      "Epoch [139/300], Step [123/172], Loss: 10.0373\n",
      "Epoch [139/300], Step [124/172], Loss: 7.8804\n",
      "Epoch [139/300], Step [125/172], Loss: 12.2209\n",
      "Epoch [139/300], Step [126/172], Loss: 10.1598\n",
      "Epoch [139/300], Step [127/172], Loss: 10.8052\n",
      "Epoch [139/300], Step [128/172], Loss: 10.8061\n",
      "Epoch [139/300], Step [129/172], Loss: 8.0494\n",
      "Epoch [139/300], Step [130/172], Loss: 11.3433\n",
      "Epoch [139/300], Step [131/172], Loss: 7.6436\n",
      "Epoch [139/300], Step [132/172], Loss: 8.2106\n",
      "Epoch [139/300], Step [133/172], Loss: 9.0798\n",
      "Epoch [139/300], Step [134/172], Loss: 11.5239\n",
      "Epoch [139/300], Step [135/172], Loss: 8.4915\n",
      "Epoch [139/300], Step [136/172], Loss: 8.0966\n",
      "Epoch [139/300], Step [137/172], Loss: 9.2572\n",
      "Epoch [139/300], Step [138/172], Loss: 7.2187\n",
      "Epoch [139/300], Step [139/172], Loss: 9.2048\n",
      "Epoch [139/300], Step [140/172], Loss: 9.0520\n",
      "Epoch [139/300], Step [141/172], Loss: 10.2496\n",
      "Epoch [139/300], Step [142/172], Loss: 13.3899\n",
      "Epoch [139/300], Step [143/172], Loss: 9.6918\n",
      "Epoch [139/300], Step [144/172], Loss: 8.5845\n",
      "Epoch [139/300], Step [145/172], Loss: 9.4022\n",
      "Epoch [139/300], Step [146/172], Loss: 9.1712\n",
      "Epoch [139/300], Step [147/172], Loss: 5.0226\n",
      "Epoch [139/300], Step [148/172], Loss: 6.0074\n",
      "Epoch [139/300], Step [149/172], Loss: 6.8274\n",
      "Epoch [139/300], Step [150/172], Loss: 6.7482\n",
      "Epoch [139/300], Step [151/172], Loss: 5.9877\n",
      "Epoch [139/300], Step [152/172], Loss: 7.2520\n",
      "Epoch [139/300], Step [153/172], Loss: 6.5565\n",
      "Epoch [139/300], Step [154/172], Loss: 7.4567\n",
      "Epoch [139/300], Step [155/172], Loss: 6.3125\n",
      "Epoch [139/300], Step [156/172], Loss: 12.3307\n",
      "Epoch [139/300], Step [157/172], Loss: 9.4500\n",
      "Epoch [139/300], Step [158/172], Loss: 7.0714\n",
      "Epoch [139/300], Step [159/172], Loss: 9.0873\n",
      "Epoch [139/300], Step [160/172], Loss: 9.9144\n",
      "Epoch [139/300], Step [161/172], Loss: 6.8333\n",
      "Epoch [139/300], Step [162/172], Loss: 6.0206\n",
      "Epoch [139/300], Step [163/172], Loss: 6.2000\n",
      "Epoch [139/300], Step [164/172], Loss: 8.7195\n",
      "Epoch [139/300], Step [165/172], Loss: 5.8541\n",
      "Epoch [139/300], Step [166/172], Loss: 5.4726\n",
      "Epoch [139/300], Step [167/172], Loss: 9.5024\n",
      "Epoch [139/300], Step [168/172], Loss: 6.7445\n",
      "Epoch [139/300], Step [169/172], Loss: 6.6561\n",
      "Epoch [139/300], Step [170/172], Loss: 4.9856\n",
      "Epoch [139/300], Step [171/172], Loss: 6.8615\n",
      "Epoch [139/300], Step [172/172], Loss: 5.0455\n",
      "Epoch [140/300], Step [1/172], Loss: 62.5432\n",
      "Epoch [140/300], Step [2/172], Loss: 63.5341\n",
      "Epoch [140/300], Step [3/172], Loss: 58.8167\n",
      "Epoch [140/300], Step [4/172], Loss: 34.3266\n",
      "Epoch [140/300], Step [5/172], Loss: 58.8636\n",
      "Epoch [140/300], Step [6/172], Loss: 19.1867\n",
      "Epoch [140/300], Step [7/172], Loss: 29.5206\n",
      "Epoch [140/300], Step [8/172], Loss: 5.0563\n",
      "Epoch [140/300], Step [9/172], Loss: 36.5971\n",
      "Epoch [140/300], Step [10/172], Loss: 44.8273\n",
      "Epoch [140/300], Step [11/172], Loss: 69.0581\n",
      "Epoch [140/300], Step [12/172], Loss: 77.6352\n",
      "Epoch [140/300], Step [13/172], Loss: 40.3213\n",
      "Epoch [140/300], Step [14/172], Loss: 72.1119\n",
      "Epoch [140/300], Step [15/172], Loss: 64.7195\n",
      "Epoch [140/300], Step [16/172], Loss: 14.1294\n",
      "Epoch [140/300], Step [17/172], Loss: 51.7812\n",
      "Epoch [140/300], Step [18/172], Loss: 62.6082\n",
      "Epoch [140/300], Step [19/172], Loss: 85.1207\n",
      "Epoch [140/300], Step [20/172], Loss: 51.1267\n",
      "Epoch [140/300], Step [21/172], Loss: 90.4081\n",
      "Epoch [140/300], Step [22/172], Loss: 68.3612\n",
      "Epoch [140/300], Step [23/172], Loss: 2.1000\n",
      "Epoch [140/300], Step [24/172], Loss: 63.9869\n",
      "Epoch [140/300], Step [25/172], Loss: 44.1152\n",
      "Epoch [140/300], Step [26/172], Loss: 53.7975\n",
      "Epoch [140/300], Step [27/172], Loss: 67.9514\n",
      "Epoch [140/300], Step [28/172], Loss: 28.4611\n",
      "Epoch [140/300], Step [29/172], Loss: 20.2394\n",
      "Epoch [140/300], Step [30/172], Loss: 72.7965\n",
      "Epoch [140/300], Step [31/172], Loss: 41.3665\n",
      "Epoch [140/300], Step [32/172], Loss: 42.8596\n",
      "Epoch [140/300], Step [33/172], Loss: 72.7913\n",
      "Epoch [140/300], Step [34/172], Loss: 3.7915\n",
      "Epoch [140/300], Step [35/172], Loss: 13.3690\n",
      "Epoch [140/300], Step [36/172], Loss: 19.0761\n",
      "Epoch [140/300], Step [37/172], Loss: 17.6174\n",
      "Epoch [140/300], Step [38/172], Loss: 28.6054\n",
      "Epoch [140/300], Step [39/172], Loss: 38.1229\n",
      "Epoch [140/300], Step [40/172], Loss: 20.0661\n",
      "Epoch [140/300], Step [41/172], Loss: 36.2195\n",
      "Epoch [140/300], Step [42/172], Loss: 39.6040\n",
      "Epoch [140/300], Step [43/172], Loss: 26.5354\n",
      "Epoch [140/300], Step [44/172], Loss: 19.8508\n",
      "Epoch [140/300], Step [45/172], Loss: 23.5820\n",
      "Epoch [140/300], Step [46/172], Loss: 18.6465\n",
      "Epoch [140/300], Step [47/172], Loss: 46.1219\n",
      "Epoch [140/300], Step [48/172], Loss: 56.1702\n",
      "Epoch [140/300], Step [49/172], Loss: 19.7617\n",
      "Epoch [140/300], Step [50/172], Loss: 46.9944\n",
      "Epoch [140/300], Step [51/172], Loss: 7.8494\n",
      "Epoch [140/300], Step [52/172], Loss: 17.4205\n",
      "Epoch [140/300], Step [53/172], Loss: 21.8944\n",
      "Epoch [140/300], Step [54/172], Loss: 12.4500\n",
      "Epoch [140/300], Step [55/172], Loss: 12.6962\n",
      "Epoch [140/300], Step [56/172], Loss: 14.9304\n",
      "Epoch [140/300], Step [57/172], Loss: 17.0787\n",
      "Epoch [140/300], Step [58/172], Loss: 15.0934\n",
      "Epoch [140/300], Step [59/172], Loss: 28.4302\n",
      "Epoch [140/300], Step [60/172], Loss: 35.7647\n",
      "Epoch [140/300], Step [61/172], Loss: 6.6312\n",
      "Epoch [140/300], Step [62/172], Loss: 21.5099\n",
      "Epoch [140/300], Step [63/172], Loss: 9.3848\n",
      "Epoch [140/300], Step [64/172], Loss: 9.3403\n",
      "Epoch [140/300], Step [65/172], Loss: 19.6456\n",
      "Epoch [140/300], Step [66/172], Loss: 5.7040\n",
      "Epoch [140/300], Step [67/172], Loss: 24.8556\n",
      "Epoch [140/300], Step [68/172], Loss: 5.4177\n",
      "Epoch [140/300], Step [69/172], Loss: 45.0767\n",
      "Epoch [140/300], Step [70/172], Loss: 48.5096\n",
      "Epoch [140/300], Step [71/172], Loss: 46.4462\n",
      "Epoch [140/300], Step [72/172], Loss: 49.5229\n",
      "Epoch [140/300], Step [73/172], Loss: 56.1523\n",
      "Epoch [140/300], Step [74/172], Loss: 30.2289\n",
      "Epoch [140/300], Step [75/172], Loss: 30.4724\n",
      "Epoch [140/300], Step [76/172], Loss: 32.8460\n",
      "Epoch [140/300], Step [77/172], Loss: 57.7761\n",
      "Epoch [140/300], Step [78/172], Loss: 44.0203\n",
      "Epoch [140/300], Step [79/172], Loss: 43.2110\n",
      "Epoch [140/300], Step [80/172], Loss: 55.5760\n",
      "Epoch [140/300], Step [81/172], Loss: 38.8063\n",
      "Epoch [140/300], Step [82/172], Loss: 38.3586\n",
      "Epoch [140/300], Step [83/172], Loss: 45.9513\n",
      "Epoch [140/300], Step [84/172], Loss: 35.2738\n",
      "Epoch [140/300], Step [85/172], Loss: 40.3179\n",
      "Epoch [140/300], Step [86/172], Loss: 33.3596\n",
      "Epoch [140/300], Step [87/172], Loss: 27.4733\n",
      "Epoch [140/300], Step [88/172], Loss: 28.4193\n",
      "Epoch [140/300], Step [89/172], Loss: 26.1042\n",
      "Epoch [140/300], Step [90/172], Loss: 24.0603\n",
      "Epoch [140/300], Step [91/172], Loss: 27.4257\n",
      "Epoch [140/300], Step [92/172], Loss: 21.0540\n",
      "Epoch [140/300], Step [93/172], Loss: 21.4699\n",
      "Epoch [140/300], Step [94/172], Loss: 29.3213\n",
      "Epoch [140/300], Step [95/172], Loss: 21.5318\n",
      "Epoch [140/300], Step [96/172], Loss: 19.5412\n",
      "Epoch [140/300], Step [97/172], Loss: 26.9127\n",
      "Epoch [140/300], Step [98/172], Loss: 19.3043\n",
      "Epoch [140/300], Step [99/172], Loss: 18.5110\n",
      "Epoch [140/300], Step [100/172], Loss: 15.2726\n",
      "Epoch [140/300], Step [101/172], Loss: 17.3492\n",
      "Epoch [140/300], Step [102/172], Loss: 16.7090\n",
      "Epoch [140/300], Step [103/172], Loss: 12.7864\n",
      "Epoch [140/300], Step [104/172], Loss: 16.8183\n",
      "Epoch [140/300], Step [105/172], Loss: 18.1329\n",
      "Epoch [140/300], Step [106/172], Loss: 15.9343\n",
      "Epoch [140/300], Step [107/172], Loss: 15.0351\n",
      "Epoch [140/300], Step [108/172], Loss: 15.7566\n",
      "Epoch [140/300], Step [109/172], Loss: 15.6501\n",
      "Epoch [140/300], Step [110/172], Loss: 15.4751\n",
      "Epoch [140/300], Step [111/172], Loss: 13.9646\n",
      "Epoch [140/300], Step [112/172], Loss: 17.7151\n",
      "Epoch [140/300], Step [113/172], Loss: 12.7218\n",
      "Epoch [140/300], Step [114/172], Loss: 13.3655\n",
      "Epoch [140/300], Step [115/172], Loss: 19.8981\n",
      "Epoch [140/300], Step [116/172], Loss: 14.1588\n",
      "Epoch [140/300], Step [117/172], Loss: 10.9101\n",
      "Epoch [140/300], Step [118/172], Loss: 14.3323\n",
      "Epoch [140/300], Step [119/172], Loss: 15.1957\n",
      "Epoch [140/300], Step [120/172], Loss: 9.7923\n",
      "Epoch [140/300], Step [121/172], Loss: 9.8260\n",
      "Epoch [140/300], Step [122/172], Loss: 10.1328\n",
      "Epoch [140/300], Step [123/172], Loss: 10.0709\n",
      "Epoch [140/300], Step [124/172], Loss: 7.8793\n",
      "Epoch [140/300], Step [125/172], Loss: 12.2686\n",
      "Epoch [140/300], Step [126/172], Loss: 10.1959\n",
      "Epoch [140/300], Step [127/172], Loss: 10.8060\n",
      "Epoch [140/300], Step [128/172], Loss: 10.8136\n",
      "Epoch [140/300], Step [129/172], Loss: 8.0580\n",
      "Epoch [140/300], Step [130/172], Loss: 11.3864\n",
      "Epoch [140/300], Step [131/172], Loss: 7.6346\n",
      "Epoch [140/300], Step [132/172], Loss: 8.2302\n",
      "Epoch [140/300], Step [133/172], Loss: 9.0631\n",
      "Epoch [140/300], Step [134/172], Loss: 11.5410\n",
      "Epoch [140/300], Step [135/172], Loss: 8.5100\n",
      "Epoch [140/300], Step [136/172], Loss: 8.0978\n",
      "Epoch [140/300], Step [137/172], Loss: 9.2696\n",
      "Epoch [140/300], Step [138/172], Loss: 7.2130\n",
      "Epoch [140/300], Step [139/172], Loss: 9.2093\n",
      "Epoch [140/300], Step [140/172], Loss: 9.0833\n",
      "Epoch [140/300], Step [141/172], Loss: 10.2597\n",
      "Epoch [140/300], Step [142/172], Loss: 13.3816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/300], Step [143/172], Loss: 9.7153\n",
      "Epoch [140/300], Step [144/172], Loss: 8.5790\n",
      "Epoch [140/300], Step [145/172], Loss: 9.4135\n",
      "Epoch [140/300], Step [146/172], Loss: 9.1840\n",
      "Epoch [140/300], Step [147/172], Loss: 5.0278\n",
      "Epoch [140/300], Step [148/172], Loss: 6.0117\n",
      "Epoch [140/300], Step [149/172], Loss: 6.8530\n",
      "Epoch [140/300], Step [150/172], Loss: 6.7286\n",
      "Epoch [140/300], Step [151/172], Loss: 5.9825\n",
      "Epoch [140/300], Step [152/172], Loss: 7.2669\n",
      "Epoch [140/300], Step [153/172], Loss: 6.5552\n",
      "Epoch [140/300], Step [154/172], Loss: 7.4914\n",
      "Epoch [140/300], Step [155/172], Loss: 6.2874\n",
      "Epoch [140/300], Step [156/172], Loss: 12.3460\n",
      "Epoch [140/300], Step [157/172], Loss: 9.4255\n",
      "Epoch [140/300], Step [158/172], Loss: 7.0616\n",
      "Epoch [140/300], Step [159/172], Loss: 9.0922\n",
      "Epoch [140/300], Step [160/172], Loss: 9.9171\n",
      "Epoch [140/300], Step [161/172], Loss: 6.8839\n",
      "Epoch [140/300], Step [162/172], Loss: 6.0163\n",
      "Epoch [140/300], Step [163/172], Loss: 6.2343\n",
      "Epoch [140/300], Step [164/172], Loss: 8.6508\n",
      "Epoch [140/300], Step [165/172], Loss: 5.8454\n",
      "Epoch [140/300], Step [166/172], Loss: 5.4697\n",
      "Epoch [140/300], Step [167/172], Loss: 9.4822\n",
      "Epoch [140/300], Step [168/172], Loss: 6.7146\n",
      "Epoch [140/300], Step [169/172], Loss: 6.6698\n",
      "Epoch [140/300], Step [170/172], Loss: 4.9775\n",
      "Epoch [140/300], Step [171/172], Loss: 6.8729\n",
      "Epoch [140/300], Step [172/172], Loss: 5.0253\n",
      "Epoch [141/300], Step [1/172], Loss: 62.2053\n",
      "Epoch [141/300], Step [2/172], Loss: 63.2421\n",
      "Epoch [141/300], Step [3/172], Loss: 58.9065\n",
      "Epoch [141/300], Step [4/172], Loss: 34.2021\n",
      "Epoch [141/300], Step [5/172], Loss: 58.6847\n",
      "Epoch [141/300], Step [6/172], Loss: 19.1494\n",
      "Epoch [141/300], Step [7/172], Loss: 29.6129\n",
      "Epoch [141/300], Step [8/172], Loss: 5.5097\n",
      "Epoch [141/300], Step [9/172], Loss: 36.5516\n",
      "Epoch [141/300], Step [10/172], Loss: 44.7635\n",
      "Epoch [141/300], Step [11/172], Loss: 68.9871\n",
      "Epoch [141/300], Step [12/172], Loss: 77.5939\n",
      "Epoch [141/300], Step [13/172], Loss: 40.6836\n",
      "Epoch [141/300], Step [14/172], Loss: 72.2644\n",
      "Epoch [141/300], Step [15/172], Loss: 64.5784\n",
      "Epoch [141/300], Step [16/172], Loss: 13.3936\n",
      "Epoch [141/300], Step [17/172], Loss: 52.0001\n",
      "Epoch [141/300], Step [18/172], Loss: 62.6780\n",
      "Epoch [141/300], Step [19/172], Loss: 85.3154\n",
      "Epoch [141/300], Step [20/172], Loss: 51.0579\n",
      "Epoch [141/300], Step [21/172], Loss: 90.2695\n",
      "Epoch [141/300], Step [22/172], Loss: 68.0178\n",
      "Epoch [141/300], Step [23/172], Loss: 2.1680\n",
      "Epoch [141/300], Step [24/172], Loss: 63.7312\n",
      "Epoch [141/300], Step [25/172], Loss: 44.1945\n",
      "Epoch [141/300], Step [26/172], Loss: 53.6257\n",
      "Epoch [141/300], Step [27/172], Loss: 67.6499\n",
      "Epoch [141/300], Step [28/172], Loss: 28.2194\n",
      "Epoch [141/300], Step [29/172], Loss: 20.2252\n",
      "Epoch [141/300], Step [30/172], Loss: 72.0107\n",
      "Epoch [141/300], Step [31/172], Loss: 41.0130\n",
      "Epoch [141/300], Step [32/172], Loss: 42.7255\n",
      "Epoch [141/300], Step [33/172], Loss: 72.5453\n",
      "Epoch [141/300], Step [34/172], Loss: 3.7785\n",
      "Epoch [141/300], Step [35/172], Loss: 13.3171\n",
      "Epoch [141/300], Step [36/172], Loss: 19.0200\n",
      "Epoch [141/300], Step [37/172], Loss: 17.5821\n",
      "Epoch [141/300], Step [38/172], Loss: 28.5638\n",
      "Epoch [141/300], Step [39/172], Loss: 37.9068\n",
      "Epoch [141/300], Step [40/172], Loss: 20.0001\n",
      "Epoch [141/300], Step [41/172], Loss: 36.1007\n",
      "Epoch [141/300], Step [42/172], Loss: 39.3936\n",
      "Epoch [141/300], Step [43/172], Loss: 26.5270\n",
      "Epoch [141/300], Step [44/172], Loss: 19.8966\n",
      "Epoch [141/300], Step [45/172], Loss: 23.6771\n",
      "Epoch [141/300], Step [46/172], Loss: 18.7933\n",
      "Epoch [141/300], Step [47/172], Loss: 46.2171\n",
      "Epoch [141/300], Step [48/172], Loss: 56.2452\n",
      "Epoch [141/300], Step [49/172], Loss: 19.8130\n",
      "Epoch [141/300], Step [50/172], Loss: 47.0194\n",
      "Epoch [141/300], Step [51/172], Loss: 7.9609\n",
      "Epoch [141/300], Step [52/172], Loss: 17.5504\n",
      "Epoch [141/300], Step [53/172], Loss: 22.0453\n",
      "Epoch [141/300], Step [54/172], Loss: 12.5082\n",
      "Epoch [141/300], Step [55/172], Loss: 12.7479\n",
      "Epoch [141/300], Step [56/172], Loss: 15.1360\n",
      "Epoch [141/300], Step [57/172], Loss: 16.9524\n",
      "Epoch [141/300], Step [58/172], Loss: 15.0055\n",
      "Epoch [141/300], Step [59/172], Loss: 28.4544\n",
      "Epoch [141/300], Step [60/172], Loss: 35.5385\n",
      "Epoch [141/300], Step [61/172], Loss: 6.6166\n",
      "Epoch [141/300], Step [62/172], Loss: 21.5633\n",
      "Epoch [141/300], Step [63/172], Loss: 9.6198\n",
      "Epoch [141/300], Step [64/172], Loss: 9.4515\n",
      "Epoch [141/300], Step [65/172], Loss: 19.6876\n",
      "Epoch [141/300], Step [66/172], Loss: 5.7619\n",
      "Epoch [141/300], Step [67/172], Loss: 24.7705\n",
      "Epoch [141/300], Step [68/172], Loss: 5.3033\n",
      "Epoch [141/300], Step [69/172], Loss: 44.6355\n",
      "Epoch [141/300], Step [70/172], Loss: 48.1776\n",
      "Epoch [141/300], Step [71/172], Loss: 46.0305\n",
      "Epoch [141/300], Step [72/172], Loss: 49.1527\n",
      "Epoch [141/300], Step [73/172], Loss: 55.8236\n",
      "Epoch [141/300], Step [74/172], Loss: 30.0234\n",
      "Epoch [141/300], Step [75/172], Loss: 30.2123\n",
      "Epoch [141/300], Step [76/172], Loss: 32.5764\n",
      "Epoch [141/300], Step [77/172], Loss: 57.3878\n",
      "Epoch [141/300], Step [78/172], Loss: 43.7914\n",
      "Epoch [141/300], Step [79/172], Loss: 42.9503\n",
      "Epoch [141/300], Step [80/172], Loss: 55.4717\n",
      "Epoch [141/300], Step [81/172], Loss: 38.6829\n",
      "Epoch [141/300], Step [82/172], Loss: 38.4951\n",
      "Epoch [141/300], Step [83/172], Loss: 45.7726\n",
      "Epoch [141/300], Step [84/172], Loss: 35.1776\n",
      "Epoch [141/300], Step [85/172], Loss: 40.1941\n",
      "Epoch [141/300], Step [86/172], Loss: 33.2885\n",
      "Epoch [141/300], Step [87/172], Loss: 27.3999\n",
      "Epoch [141/300], Step [88/172], Loss: 28.2996\n",
      "Epoch [141/300], Step [89/172], Loss: 26.0850\n",
      "Epoch [141/300], Step [90/172], Loss: 23.9206\n",
      "Epoch [141/300], Step [91/172], Loss: 27.3447\n",
      "Epoch [141/300], Step [92/172], Loss: 20.9949\n",
      "Epoch [141/300], Step [93/172], Loss: 21.4548\n",
      "Epoch [141/300], Step [94/172], Loss: 29.3227\n",
      "Epoch [141/300], Step [95/172], Loss: 21.5407\n",
      "Epoch [141/300], Step [96/172], Loss: 19.5390\n",
      "Epoch [141/300], Step [97/172], Loss: 26.9305\n",
      "Epoch [141/300], Step [98/172], Loss: 19.2880\n",
      "Epoch [141/300], Step [99/172], Loss: 18.4976\n",
      "Epoch [141/300], Step [100/172], Loss: 15.2542\n",
      "Epoch [141/300], Step [101/172], Loss: 17.3730\n",
      "Epoch [141/300], Step [102/172], Loss: 16.7873\n",
      "Epoch [141/300], Step [103/172], Loss: 12.7654\n",
      "Epoch [141/300], Step [104/172], Loss: 16.8426\n",
      "Epoch [141/300], Step [105/172], Loss: 18.1739\n",
      "Epoch [141/300], Step [106/172], Loss: 15.9138\n",
      "Epoch [141/300], Step [107/172], Loss: 15.0329\n",
      "Epoch [141/300], Step [108/172], Loss: 15.7013\n",
      "Epoch [141/300], Step [109/172], Loss: 15.6061\n",
      "Epoch [141/300], Step [110/172], Loss: 15.5055\n",
      "Epoch [141/300], Step [111/172], Loss: 14.0090\n",
      "Epoch [141/300], Step [112/172], Loss: 17.6870\n",
      "Epoch [141/300], Step [113/172], Loss: 12.7097\n",
      "Epoch [141/300], Step [114/172], Loss: 13.3716\n",
      "Epoch [141/300], Step [115/172], Loss: 19.8546\n",
      "Epoch [141/300], Step [116/172], Loss: 14.1955\n",
      "Epoch [141/300], Step [117/172], Loss: 10.9041\n",
      "Epoch [141/300], Step [118/172], Loss: 14.3814\n",
      "Epoch [141/300], Step [119/172], Loss: 15.1527\n",
      "Epoch [141/300], Step [120/172], Loss: 9.7992\n",
      "Epoch [141/300], Step [121/172], Loss: 9.7872\n",
      "Epoch [141/300], Step [122/172], Loss: 10.1377\n",
      "Epoch [141/300], Step [123/172], Loss: 10.0522\n",
      "Epoch [141/300], Step [124/172], Loss: 7.8613\n",
      "Epoch [141/300], Step [125/172], Loss: 12.2167\n",
      "Epoch [141/300], Step [126/172], Loss: 10.2090\n",
      "Epoch [141/300], Step [127/172], Loss: 10.8093\n",
      "Epoch [141/300], Step [128/172], Loss: 10.8132\n",
      "Epoch [141/300], Step [129/172], Loss: 8.0303\n",
      "Epoch [141/300], Step [130/172], Loss: 11.3889\n",
      "Epoch [141/300], Step [131/172], Loss: 7.6050\n",
      "Epoch [141/300], Step [132/172], Loss: 8.2166\n",
      "Epoch [141/300], Step [133/172], Loss: 9.0404\n",
      "Epoch [141/300], Step [134/172], Loss: 11.4860\n",
      "Epoch [141/300], Step [135/172], Loss: 8.4849\n",
      "Epoch [141/300], Step [136/172], Loss: 8.0960\n",
      "Epoch [141/300], Step [137/172], Loss: 9.2616\n",
      "Epoch [141/300], Step [138/172], Loss: 7.2004\n",
      "Epoch [141/300], Step [139/172], Loss: 9.1878\n",
      "Epoch [141/300], Step [140/172], Loss: 9.1016\n",
      "Epoch [141/300], Step [141/172], Loss: 10.2395\n",
      "Epoch [141/300], Step [142/172], Loss: 13.4106\n",
      "Epoch [141/300], Step [143/172], Loss: 9.7487\n",
      "Epoch [141/300], Step [144/172], Loss: 8.5801\n",
      "Epoch [141/300], Step [145/172], Loss: 9.4006\n",
      "Epoch [141/300], Step [146/172], Loss: 9.1875\n",
      "Epoch [141/300], Step [147/172], Loss: 5.0186\n",
      "Epoch [141/300], Step [148/172], Loss: 5.9995\n",
      "Epoch [141/300], Step [149/172], Loss: 6.8291\n",
      "Epoch [141/300], Step [150/172], Loss: 6.6891\n",
      "Epoch [141/300], Step [151/172], Loss: 5.9473\n",
      "Epoch [141/300], Step [152/172], Loss: 7.2210\n",
      "Epoch [141/300], Step [153/172], Loss: 6.5326\n",
      "Epoch [141/300], Step [154/172], Loss: 7.4749\n",
      "Epoch [141/300], Step [155/172], Loss: 6.2604\n",
      "Epoch [141/300], Step [156/172], Loss: 12.3552\n",
      "Epoch [141/300], Step [157/172], Loss: 9.4023\n",
      "Epoch [141/300], Step [158/172], Loss: 7.0444\n",
      "Epoch [141/300], Step [159/172], Loss: 9.0791\n",
      "Epoch [141/300], Step [160/172], Loss: 9.9246\n",
      "Epoch [141/300], Step [161/172], Loss: 6.8776\n",
      "Epoch [141/300], Step [162/172], Loss: 5.9748\n",
      "Epoch [141/300], Step [163/172], Loss: 6.2155\n",
      "Epoch [141/300], Step [164/172], Loss: 8.6502\n",
      "Epoch [141/300], Step [165/172], Loss: 5.8290\n",
      "Epoch [141/300], Step [166/172], Loss: 5.4541\n",
      "Epoch [141/300], Step [167/172], Loss: 9.4720\n",
      "Epoch [141/300], Step [168/172], Loss: 6.6837\n",
      "Epoch [141/300], Step [169/172], Loss: 6.6552\n",
      "Epoch [141/300], Step [170/172], Loss: 4.9312\n",
      "Epoch [141/300], Step [171/172], Loss: 6.8663\n",
      "Epoch [141/300], Step [172/172], Loss: 4.9961\n",
      "Epoch [142/300], Step [1/172], Loss: 61.9037\n",
      "Epoch [142/300], Step [2/172], Loss: 63.0275\n",
      "Epoch [142/300], Step [3/172], Loss: 58.4931\n",
      "Epoch [142/300], Step [4/172], Loss: 33.9967\n",
      "Epoch [142/300], Step [5/172], Loss: 58.5968\n",
      "Epoch [142/300], Step [6/172], Loss: 19.0610\n",
      "Epoch [142/300], Step [7/172], Loss: 29.4790\n",
      "Epoch [142/300], Step [8/172], Loss: 5.3103\n",
      "Epoch [142/300], Step [9/172], Loss: 36.3119\n",
      "Epoch [142/300], Step [10/172], Loss: 44.6263\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [142/300], Step [11/172], Loss: 68.6776\n",
      "Epoch [142/300], Step [12/172], Loss: 77.2761\n",
      "Epoch [142/300], Step [13/172], Loss: 40.4455\n",
      "Epoch [142/300], Step [14/172], Loss: 71.9477\n",
      "Epoch [142/300], Step [15/172], Loss: 64.2874\n",
      "Epoch [142/300], Step [16/172], Loss: 13.4959\n",
      "Epoch [142/300], Step [17/172], Loss: 51.8203\n",
      "Epoch [142/300], Step [18/172], Loss: 62.6307\n",
      "Epoch [142/300], Step [19/172], Loss: 85.2862\n",
      "Epoch [142/300], Step [20/172], Loss: 50.3808\n",
      "Epoch [142/300], Step [21/172], Loss: 90.1781\n",
      "Epoch [142/300], Step [22/172], Loss: 67.7933\n",
      "Epoch [142/300], Step [23/172], Loss: 2.1399\n",
      "Epoch [142/300], Step [24/172], Loss: 63.6182\n",
      "Epoch [142/300], Step [25/172], Loss: 44.1299\n",
      "Epoch [142/300], Step [26/172], Loss: 53.5863\n",
      "Epoch [142/300], Step [27/172], Loss: 67.7970\n",
      "Epoch [142/300], Step [28/172], Loss: 28.1331\n",
      "Epoch [142/300], Step [29/172], Loss: 20.1464\n",
      "Epoch [142/300], Step [30/172], Loss: 71.8901\n",
      "Epoch [142/300], Step [31/172], Loss: 41.0956\n",
      "Epoch [142/300], Step [32/172], Loss: 42.8401\n",
      "Epoch [142/300], Step [33/172], Loss: 72.6830\n",
      "Epoch [142/300], Step [34/172], Loss: 3.7698\n",
      "Epoch [142/300], Step [35/172], Loss: 13.2859\n",
      "Epoch [142/300], Step [36/172], Loss: 18.8327\n",
      "Epoch [142/300], Step [37/172], Loss: 17.4634\n",
      "Epoch [142/300], Step [38/172], Loss: 28.5544\n",
      "Epoch [142/300], Step [39/172], Loss: 37.9225\n",
      "Epoch [142/300], Step [40/172], Loss: 20.0046\n",
      "Epoch [142/300], Step [41/172], Loss: 36.0483\n",
      "Epoch [142/300], Step [42/172], Loss: 39.3096\n",
      "Epoch [142/300], Step [43/172], Loss: 26.5062\n",
      "Epoch [142/300], Step [44/172], Loss: 19.9375\n",
      "Epoch [142/300], Step [45/172], Loss: 23.6779\n",
      "Epoch [142/300], Step [46/172], Loss: 18.5610\n",
      "Epoch [142/300], Step [47/172], Loss: 46.1356\n",
      "Epoch [142/300], Step [48/172], Loss: 56.5574\n",
      "Epoch [142/300], Step [49/172], Loss: 19.7619\n",
      "Epoch [142/300], Step [50/172], Loss: 46.9232\n",
      "Epoch [142/300], Step [51/172], Loss: 7.8920\n",
      "Epoch [142/300], Step [52/172], Loss: 17.5504\n",
      "Epoch [142/300], Step [53/172], Loss: 22.0123\n",
      "Epoch [142/300], Step [54/172], Loss: 12.6062\n",
      "Epoch [142/300], Step [55/172], Loss: 12.8246\n",
      "Epoch [142/300], Step [56/172], Loss: 15.3046\n",
      "Epoch [142/300], Step [57/172], Loss: 16.9959\n",
      "Epoch [142/300], Step [58/172], Loss: 15.0474\n",
      "Epoch [142/300], Step [59/172], Loss: 28.5622\n",
      "Epoch [142/300], Step [60/172], Loss: 35.7212\n",
      "Epoch [142/300], Step [61/172], Loss: 6.6076\n",
      "Epoch [142/300], Step [62/172], Loss: 21.4925\n",
      "Epoch [142/300], Step [63/172], Loss: 9.5890\n",
      "Epoch [142/300], Step [64/172], Loss: 9.4953\n",
      "Epoch [142/300], Step [65/172], Loss: 19.7202\n",
      "Epoch [142/300], Step [66/172], Loss: 5.7451\n",
      "Epoch [142/300], Step [67/172], Loss: 24.7891\n",
      "Epoch [142/300], Step [68/172], Loss: 5.2776\n",
      "Epoch [142/300], Step [69/172], Loss: 44.4335\n",
      "Epoch [142/300], Step [70/172], Loss: 47.9322\n",
      "Epoch [142/300], Step [71/172], Loss: 45.9761\n",
      "Epoch [142/300], Step [72/172], Loss: 48.8389\n",
      "Epoch [142/300], Step [73/172], Loss: 55.5160\n",
      "Epoch [142/300], Step [74/172], Loss: 29.8969\n",
      "Epoch [142/300], Step [75/172], Loss: 30.2606\n",
      "Epoch [142/300], Step [76/172], Loss: 32.4798\n",
      "Epoch [142/300], Step [77/172], Loss: 57.1397\n",
      "Epoch [142/300], Step [78/172], Loss: 43.6087\n",
      "Epoch [142/300], Step [79/172], Loss: 42.7044\n",
      "Epoch [142/300], Step [80/172], Loss: 55.2254\n",
      "Epoch [142/300], Step [81/172], Loss: 38.4931\n",
      "Epoch [142/300], Step [82/172], Loss: 38.4046\n",
      "Epoch [142/300], Step [83/172], Loss: 45.6453\n",
      "Epoch [142/300], Step [84/172], Loss: 35.1114\n",
      "Epoch [142/300], Step [85/172], Loss: 40.1993\n",
      "Epoch [142/300], Step [86/172], Loss: 33.3457\n",
      "Epoch [142/300], Step [87/172], Loss: 27.3733\n",
      "Epoch [142/300], Step [88/172], Loss: 28.2353\n",
      "Epoch [142/300], Step [89/172], Loss: 26.0683\n",
      "Epoch [142/300], Step [90/172], Loss: 23.9524\n",
      "Epoch [142/300], Step [91/172], Loss: 27.3442\n",
      "Epoch [142/300], Step [92/172], Loss: 20.9737\n",
      "Epoch [142/300], Step [93/172], Loss: 21.4487\n",
      "Epoch [142/300], Step [94/172], Loss: 29.2615\n",
      "Epoch [142/300], Step [95/172], Loss: 21.5703\n",
      "Epoch [142/300], Step [96/172], Loss: 19.5363\n",
      "Epoch [142/300], Step [97/172], Loss: 26.9839\n",
      "Epoch [142/300], Step [98/172], Loss: 19.3111\n",
      "Epoch [142/300], Step [99/172], Loss: 18.5173\n",
      "Epoch [142/300], Step [100/172], Loss: 15.2663\n",
      "Epoch [142/300], Step [101/172], Loss: 17.4331\n",
      "Epoch [142/300], Step [102/172], Loss: 16.8764\n",
      "Epoch [142/300], Step [103/172], Loss: 12.7632\n",
      "Epoch [142/300], Step [104/172], Loss: 16.8920\n",
      "Epoch [142/300], Step [105/172], Loss: 18.2803\n",
      "Epoch [142/300], Step [106/172], Loss: 15.9218\n",
      "Epoch [142/300], Step [107/172], Loss: 15.0782\n",
      "Epoch [142/300], Step [108/172], Loss: 15.6588\n",
      "Epoch [142/300], Step [109/172], Loss: 15.5964\n",
      "Epoch [142/300], Step [110/172], Loss: 15.5070\n",
      "Epoch [142/300], Step [111/172], Loss: 14.0034\n",
      "Epoch [142/300], Step [112/172], Loss: 17.6933\n",
      "Epoch [142/300], Step [113/172], Loss: 12.6717\n",
      "Epoch [142/300], Step [114/172], Loss: 13.3488\n",
      "Epoch [142/300], Step [115/172], Loss: 19.8243\n",
      "Epoch [142/300], Step [116/172], Loss: 14.2087\n",
      "Epoch [142/300], Step [117/172], Loss: 10.8935\n",
      "Epoch [142/300], Step [118/172], Loss: 14.3398\n",
      "Epoch [142/300], Step [119/172], Loss: 15.1970\n",
      "Epoch [142/300], Step [120/172], Loss: 9.8168\n",
      "Epoch [142/300], Step [121/172], Loss: 9.7892\n",
      "Epoch [142/300], Step [122/172], Loss: 10.1216\n",
      "Epoch [142/300], Step [123/172], Loss: 10.0618\n",
      "Epoch [142/300], Step [124/172], Loss: 7.8695\n",
      "Epoch [142/300], Step [125/172], Loss: 12.2149\n",
      "Epoch [142/300], Step [126/172], Loss: 10.2290\n",
      "Epoch [142/300], Step [127/172], Loss: 10.7882\n",
      "Epoch [142/300], Step [128/172], Loss: 10.8058\n",
      "Epoch [142/300], Step [129/172], Loss: 8.0205\n",
      "Epoch [142/300], Step [130/172], Loss: 11.4357\n",
      "Epoch [142/300], Step [131/172], Loss: 7.5950\n",
      "Epoch [142/300], Step [132/172], Loss: 8.2222\n",
      "Epoch [142/300], Step [133/172], Loss: 9.0172\n",
      "Epoch [142/300], Step [134/172], Loss: 11.5258\n",
      "Epoch [142/300], Step [135/172], Loss: 8.5212\n",
      "Epoch [142/300], Step [136/172], Loss: 8.1072\n",
      "Epoch [142/300], Step [137/172], Loss: 9.2841\n",
      "Epoch [142/300], Step [138/172], Loss: 7.1837\n",
      "Epoch [142/300], Step [139/172], Loss: 9.2124\n",
      "Epoch [142/300], Step [140/172], Loss: 9.1073\n",
      "Epoch [142/300], Step [141/172], Loss: 10.2337\n",
      "Epoch [142/300], Step [142/172], Loss: 13.3924\n",
      "Epoch [142/300], Step [143/172], Loss: 9.7733\n",
      "Epoch [142/300], Step [144/172], Loss: 8.6004\n",
      "Epoch [142/300], Step [145/172], Loss: 9.3968\n",
      "Epoch [142/300], Step [146/172], Loss: 9.1941\n",
      "Epoch [142/300], Step [147/172], Loss: 5.0280\n",
      "Epoch [142/300], Step [148/172], Loss: 5.9997\n",
      "Epoch [142/300], Step [149/172], Loss: 6.8544\n",
      "Epoch [142/300], Step [150/172], Loss: 6.6930\n",
      "Epoch [142/300], Step [151/172], Loss: 5.9533\n",
      "Epoch [142/300], Step [152/172], Loss: 7.2501\n",
      "Epoch [142/300], Step [153/172], Loss: 6.5578\n",
      "Epoch [142/300], Step [154/172], Loss: 7.5223\n",
      "Epoch [142/300], Step [155/172], Loss: 6.2967\n",
      "Epoch [142/300], Step [156/172], Loss: 12.3623\n",
      "Epoch [142/300], Step [157/172], Loss: 9.3878\n",
      "Epoch [142/300], Step [158/172], Loss: 7.0351\n",
      "Epoch [142/300], Step [159/172], Loss: 9.0435\n",
      "Epoch [142/300], Step [160/172], Loss: 9.9365\n",
      "Epoch [142/300], Step [161/172], Loss: 6.9542\n",
      "Epoch [142/300], Step [162/172], Loss: 5.9661\n",
      "Epoch [142/300], Step [163/172], Loss: 6.2612\n",
      "Epoch [142/300], Step [164/172], Loss: 8.7366\n",
      "Epoch [142/300], Step [165/172], Loss: 5.8422\n",
      "Epoch [142/300], Step [166/172], Loss: 5.4506\n",
      "Epoch [142/300], Step [167/172], Loss: 9.5050\n",
      "Epoch [142/300], Step [168/172], Loss: 6.6756\n",
      "Epoch [142/300], Step [169/172], Loss: 6.6606\n",
      "Epoch [142/300], Step [170/172], Loss: 4.9366\n",
      "Epoch [142/300], Step [171/172], Loss: 6.9069\n",
      "Epoch [142/300], Step [172/172], Loss: 5.0474\n",
      "Epoch [143/300], Step [1/172], Loss: 61.6541\n",
      "Epoch [143/300], Step [2/172], Loss: 62.8142\n",
      "Epoch [143/300], Step [3/172], Loss: 58.3104\n",
      "Epoch [143/300], Step [4/172], Loss: 33.7447\n",
      "Epoch [143/300], Step [5/172], Loss: 58.6598\n",
      "Epoch [143/300], Step [6/172], Loss: 19.1893\n",
      "Epoch [143/300], Step [7/172], Loss: 30.0372\n",
      "Epoch [143/300], Step [8/172], Loss: 4.9660\n",
      "Epoch [143/300], Step [9/172], Loss: 36.1456\n",
      "Epoch [143/300], Step [10/172], Loss: 44.5796\n",
      "Epoch [143/300], Step [11/172], Loss: 68.3025\n",
      "Epoch [143/300], Step [12/172], Loss: 76.9013\n",
      "Epoch [143/300], Step [13/172], Loss: 40.3130\n",
      "Epoch [143/300], Step [14/172], Loss: 71.5010\n",
      "Epoch [143/300], Step [15/172], Loss: 64.1333\n",
      "Epoch [143/300], Step [16/172], Loss: 13.8583\n",
      "Epoch [143/300], Step [17/172], Loss: 51.4521\n",
      "Epoch [143/300], Step [18/172], Loss: 62.4646\n",
      "Epoch [143/300], Step [19/172], Loss: 85.0580\n",
      "Epoch [143/300], Step [20/172], Loss: 49.8246\n",
      "Epoch [143/300], Step [21/172], Loss: 89.6498\n",
      "Epoch [143/300], Step [22/172], Loss: 67.3994\n",
      "Epoch [143/300], Step [23/172], Loss: 2.2116\n",
      "Epoch [143/300], Step [24/172], Loss: 63.2664\n",
      "Epoch [143/300], Step [25/172], Loss: 43.9620\n",
      "Epoch [143/300], Step [26/172], Loss: 53.2009\n",
      "Epoch [143/300], Step [27/172], Loss: 67.6061\n",
      "Epoch [143/300], Step [28/172], Loss: 27.9173\n",
      "Epoch [143/300], Step [29/172], Loss: 19.9327\n",
      "Epoch [143/300], Step [30/172], Loss: 71.4236\n",
      "Epoch [143/300], Step [31/172], Loss: 41.0115\n",
      "Epoch [143/300], Step [32/172], Loss: 42.6862\n",
      "Epoch [143/300], Step [33/172], Loss: 72.6177\n",
      "Epoch [143/300], Step [34/172], Loss: 3.6548\n",
      "Epoch [143/300], Step [35/172], Loss: 13.2152\n",
      "Epoch [143/300], Step [36/172], Loss: 18.7449\n",
      "Epoch [143/300], Step [37/172], Loss: 17.5367\n",
      "Epoch [143/300], Step [38/172], Loss: 28.6079\n",
      "Epoch [143/300], Step [39/172], Loss: 37.8535\n",
      "Epoch [143/300], Step [40/172], Loss: 20.0835\n",
      "Epoch [143/300], Step [41/172], Loss: 35.9605\n",
      "Epoch [143/300], Step [42/172], Loss: 39.2670\n",
      "Epoch [143/300], Step [43/172], Loss: 26.4080\n",
      "Epoch [143/300], Step [44/172], Loss: 19.9186\n",
      "Epoch [143/300], Step [45/172], Loss: 23.7616\n",
      "Epoch [143/300], Step [46/172], Loss: 18.5133\n",
      "Epoch [143/300], Step [47/172], Loss: 45.9294\n",
      "Epoch [143/300], Step [48/172], Loss: 56.5571\n",
      "Epoch [143/300], Step [49/172], Loss: 19.8137\n",
      "Epoch [143/300], Step [50/172], Loss: 47.0157\n",
      "Epoch [143/300], Step [51/172], Loss: 7.8567\n",
      "Epoch [143/300], Step [52/172], Loss: 17.5190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [143/300], Step [53/172], Loss: 22.0321\n",
      "Epoch [143/300], Step [54/172], Loss: 12.5817\n",
      "Epoch [143/300], Step [55/172], Loss: 12.7984\n",
      "Epoch [143/300], Step [56/172], Loss: 15.1734\n",
      "Epoch [143/300], Step [57/172], Loss: 16.9655\n",
      "Epoch [143/300], Step [58/172], Loss: 14.8640\n",
      "Epoch [143/300], Step [59/172], Loss: 28.4734\n",
      "Epoch [143/300], Step [60/172], Loss: 35.1974\n",
      "Epoch [143/300], Step [61/172], Loss: 6.5584\n",
      "Epoch [143/300], Step [62/172], Loss: 21.5521\n",
      "Epoch [143/300], Step [63/172], Loss: 9.4439\n",
      "Epoch [143/300], Step [64/172], Loss: 9.3631\n",
      "Epoch [143/300], Step [65/172], Loss: 19.6821\n",
      "Epoch [143/300], Step [66/172], Loss: 5.7782\n",
      "Epoch [143/300], Step [67/172], Loss: 24.5980\n",
      "Epoch [143/300], Step [68/172], Loss: 5.2200\n",
      "Epoch [143/300], Step [69/172], Loss: 44.0503\n",
      "Epoch [143/300], Step [70/172], Loss: 47.9062\n",
      "Epoch [143/300], Step [71/172], Loss: 45.9689\n",
      "Epoch [143/300], Step [72/172], Loss: 48.7541\n",
      "Epoch [143/300], Step [73/172], Loss: 55.6236\n",
      "Epoch [143/300], Step [74/172], Loss: 29.8579\n",
      "Epoch [143/300], Step [75/172], Loss: 30.2111\n",
      "Epoch [143/300], Step [76/172], Loss: 32.4952\n",
      "Epoch [143/300], Step [77/172], Loss: 56.8972\n",
      "Epoch [143/300], Step [78/172], Loss: 43.5460\n",
      "Epoch [143/300], Step [79/172], Loss: 42.6198\n",
      "Epoch [143/300], Step [80/172], Loss: 55.1681\n",
      "Epoch [143/300], Step [81/172], Loss: 38.4628\n",
      "Epoch [143/300], Step [82/172], Loss: 38.2290\n",
      "Epoch [143/300], Step [83/172], Loss: 45.7128\n",
      "Epoch [143/300], Step [84/172], Loss: 35.1376\n",
      "Epoch [143/300], Step [85/172], Loss: 40.2841\n",
      "Epoch [143/300], Step [86/172], Loss: 33.3526\n",
      "Epoch [143/300], Step [87/172], Loss: 27.3761\n",
      "Epoch [143/300], Step [88/172], Loss: 28.1505\n",
      "Epoch [143/300], Step [89/172], Loss: 26.0738\n",
      "Epoch [143/300], Step [90/172], Loss: 23.8396\n",
      "Epoch [143/300], Step [91/172], Loss: 27.3552\n",
      "Epoch [143/300], Step [92/172], Loss: 20.9707\n",
      "Epoch [143/300], Step [93/172], Loss: 21.4164\n",
      "Epoch [143/300], Step [94/172], Loss: 29.2699\n",
      "Epoch [143/300], Step [95/172], Loss: 21.5329\n",
      "Epoch [143/300], Step [96/172], Loss: 19.5870\n",
      "Epoch [143/300], Step [97/172], Loss: 27.0507\n",
      "Epoch [143/300], Step [98/172], Loss: 19.3326\n",
      "Epoch [143/300], Step [99/172], Loss: 18.5500\n",
      "Epoch [143/300], Step [100/172], Loss: 15.2852\n",
      "Epoch [143/300], Step [101/172], Loss: 17.4699\n",
      "Epoch [143/300], Step [102/172], Loss: 16.7785\n",
      "Epoch [143/300], Step [103/172], Loss: 12.7531\n",
      "Epoch [143/300], Step [104/172], Loss: 16.9489\n",
      "Epoch [143/300], Step [105/172], Loss: 18.2056\n",
      "Epoch [143/300], Step [106/172], Loss: 15.9236\n",
      "Epoch [143/300], Step [107/172], Loss: 15.1101\n",
      "Epoch [143/300], Step [108/172], Loss: 15.6011\n",
      "Epoch [143/300], Step [109/172], Loss: 15.4944\n",
      "Epoch [143/300], Step [110/172], Loss: 15.4979\n",
      "Epoch [143/300], Step [111/172], Loss: 14.0087\n",
      "Epoch [143/300], Step [112/172], Loss: 17.6464\n",
      "Epoch [143/300], Step [113/172], Loss: 12.5826\n",
      "Epoch [143/300], Step [114/172], Loss: 13.3218\n",
      "Epoch [143/300], Step [115/172], Loss: 19.6965\n",
      "Epoch [143/300], Step [116/172], Loss: 14.1880\n",
      "Epoch [143/300], Step [117/172], Loss: 10.8433\n",
      "Epoch [143/300], Step [118/172], Loss: 14.3423\n",
      "Epoch [143/300], Step [119/172], Loss: 15.2339\n",
      "Epoch [143/300], Step [120/172], Loss: 9.7879\n",
      "Epoch [143/300], Step [121/172], Loss: 9.7668\n",
      "Epoch [143/300], Step [122/172], Loss: 10.0973\n",
      "Epoch [143/300], Step [123/172], Loss: 9.9980\n",
      "Epoch [143/300], Step [124/172], Loss: 7.8372\n",
      "Epoch [143/300], Step [125/172], Loss: 12.2139\n",
      "Epoch [143/300], Step [126/172], Loss: 10.2223\n",
      "Epoch [143/300], Step [127/172], Loss: 10.7526\n",
      "Epoch [143/300], Step [128/172], Loss: 10.7469\n",
      "Epoch [143/300], Step [129/172], Loss: 7.9914\n",
      "Epoch [143/300], Step [130/172], Loss: 11.4143\n",
      "Epoch [143/300], Step [131/172], Loss: 7.5501\n",
      "Epoch [143/300], Step [132/172], Loss: 8.2209\n",
      "Epoch [143/300], Step [133/172], Loss: 8.9671\n",
      "Epoch [143/300], Step [134/172], Loss: 11.4881\n",
      "Epoch [143/300], Step [135/172], Loss: 8.4933\n",
      "Epoch [143/300], Step [136/172], Loss: 8.0700\n",
      "Epoch [143/300], Step [137/172], Loss: 9.2344\n",
      "Epoch [143/300], Step [138/172], Loss: 7.1295\n",
      "Epoch [143/300], Step [139/172], Loss: 9.1970\n",
      "Epoch [143/300], Step [140/172], Loss: 9.1014\n",
      "Epoch [143/300], Step [141/172], Loss: 10.1684\n",
      "Epoch [143/300], Step [142/172], Loss: 13.3568\n",
      "Epoch [143/300], Step [143/172], Loss: 9.7782\n",
      "Epoch [143/300], Step [144/172], Loss: 8.5830\n",
      "Epoch [143/300], Step [145/172], Loss: 9.3717\n",
      "Epoch [143/300], Step [146/172], Loss: 9.1517\n",
      "Epoch [143/300], Step [147/172], Loss: 5.0121\n",
      "Epoch [143/300], Step [148/172], Loss: 5.9744\n",
      "Epoch [143/300], Step [149/172], Loss: 6.8380\n",
      "Epoch [143/300], Step [150/172], Loss: 6.6472\n",
      "Epoch [143/300], Step [151/172], Loss: 5.9013\n",
      "Epoch [143/300], Step [152/172], Loss: 7.2308\n",
      "Epoch [143/300], Step [153/172], Loss: 6.5311\n",
      "Epoch [143/300], Step [154/172], Loss: 7.4775\n",
      "Epoch [143/300], Step [155/172], Loss: 6.2582\n",
      "Epoch [143/300], Step [156/172], Loss: 12.3563\n",
      "Epoch [143/300], Step [157/172], Loss: 9.3530\n",
      "Epoch [143/300], Step [158/172], Loss: 7.0025\n",
      "Epoch [143/300], Step [159/172], Loss: 9.0233\n",
      "Epoch [143/300], Step [160/172], Loss: 9.9150\n",
      "Epoch [143/300], Step [161/172], Loss: 6.9318\n",
      "Epoch [143/300], Step [162/172], Loss: 5.9317\n",
      "Epoch [143/300], Step [163/172], Loss: 6.2336\n",
      "Epoch [143/300], Step [164/172], Loss: 8.6186\n",
      "Epoch [143/300], Step [165/172], Loss: 5.8243\n",
      "Epoch [143/300], Step [166/172], Loss: 5.4107\n",
      "Epoch [143/300], Step [167/172], Loss: 9.4777\n",
      "Epoch [143/300], Step [168/172], Loss: 6.6557\n",
      "Epoch [143/300], Step [169/172], Loss: 6.6243\n",
      "Epoch [143/300], Step [170/172], Loss: 4.9022\n",
      "Epoch [143/300], Step [171/172], Loss: 6.8798\n",
      "Epoch [143/300], Step [172/172], Loss: 5.0096\n",
      "Epoch [144/300], Step [1/172], Loss: 61.3635\n",
      "Epoch [144/300], Step [2/172], Loss: 62.5130\n",
      "Epoch [144/300], Step [3/172], Loss: 58.0582\n",
      "Epoch [144/300], Step [4/172], Loss: 33.5942\n",
      "Epoch [144/300], Step [5/172], Loss: 58.4354\n",
      "Epoch [144/300], Step [6/172], Loss: 19.3316\n",
      "Epoch [144/300], Step [7/172], Loss: 30.4402\n",
      "Epoch [144/300], Step [8/172], Loss: 5.4551\n",
      "Epoch [144/300], Step [9/172], Loss: 36.2095\n",
      "Epoch [144/300], Step [10/172], Loss: 44.5526\n",
      "Epoch [144/300], Step [11/172], Loss: 68.2390\n",
      "Epoch [144/300], Step [12/172], Loss: 76.9058\n",
      "Epoch [144/300], Step [13/172], Loss: 40.6452\n",
      "Epoch [144/300], Step [14/172], Loss: 71.6853\n",
      "Epoch [144/300], Step [15/172], Loss: 64.0854\n",
      "Epoch [144/300], Step [16/172], Loss: 13.1232\n",
      "Epoch [144/300], Step [17/172], Loss: 51.5836\n",
      "Epoch [144/300], Step [18/172], Loss: 62.4860\n",
      "Epoch [144/300], Step [19/172], Loss: 85.0870\n",
      "Epoch [144/300], Step [20/172], Loss: 49.4206\n",
      "Epoch [144/300], Step [21/172], Loss: 89.3502\n",
      "Epoch [144/300], Step [22/172], Loss: 67.0418\n",
      "Epoch [144/300], Step [23/172], Loss: 2.1478\n",
      "Epoch [144/300], Step [24/172], Loss: 62.9287\n",
      "Epoch [144/300], Step [25/172], Loss: 43.9098\n",
      "Epoch [144/300], Step [26/172], Loss: 53.0212\n",
      "Epoch [144/300], Step [27/172], Loss: 67.0382\n",
      "Epoch [144/300], Step [28/172], Loss: 27.7934\n",
      "Epoch [144/300], Step [29/172], Loss: 19.8515\n",
      "Epoch [144/300], Step [30/172], Loss: 70.5338\n",
      "Epoch [144/300], Step [31/172], Loss: 40.6249\n",
      "Epoch [144/300], Step [32/172], Loss: 42.5609\n",
      "Epoch [144/300], Step [33/172], Loss: 72.3434\n",
      "Epoch [144/300], Step [34/172], Loss: 3.8747\n",
      "Epoch [144/300], Step [35/172], Loss: 13.1672\n",
      "Epoch [144/300], Step [36/172], Loss: 18.8158\n",
      "Epoch [144/300], Step [37/172], Loss: 17.5057\n",
      "Epoch [144/300], Step [38/172], Loss: 28.5714\n",
      "Epoch [144/300], Step [39/172], Loss: 37.7431\n",
      "Epoch [144/300], Step [40/172], Loss: 20.1284\n",
      "Epoch [144/300], Step [41/172], Loss: 36.0393\n",
      "Epoch [144/300], Step [42/172], Loss: 39.4136\n",
      "Epoch [144/300], Step [43/172], Loss: 26.5207\n",
      "Epoch [144/300], Step [44/172], Loss: 20.0589\n",
      "Epoch [144/300], Step [45/172], Loss: 24.0136\n",
      "Epoch [144/300], Step [46/172], Loss: 18.5849\n",
      "Epoch [144/300], Step [47/172], Loss: 45.9889\n",
      "Epoch [144/300], Step [48/172], Loss: 56.9110\n",
      "Epoch [144/300], Step [49/172], Loss: 20.1510\n",
      "Epoch [144/300], Step [50/172], Loss: 47.0502\n",
      "Epoch [144/300], Step [51/172], Loss: 7.9668\n",
      "Epoch [144/300], Step [52/172], Loss: 17.6372\n",
      "Epoch [144/300], Step [53/172], Loss: 22.1318\n",
      "Epoch [144/300], Step [54/172], Loss: 12.8241\n",
      "Epoch [144/300], Step [55/172], Loss: 12.9090\n",
      "Epoch [144/300], Step [56/172], Loss: 14.7844\n",
      "Epoch [144/300], Step [57/172], Loss: 17.1237\n",
      "Epoch [144/300], Step [58/172], Loss: 14.9205\n",
      "Epoch [144/300], Step [59/172], Loss: 28.4003\n",
      "Epoch [144/300], Step [60/172], Loss: 34.9688\n",
      "Epoch [144/300], Step [61/172], Loss: 6.6511\n",
      "Epoch [144/300], Step [62/172], Loss: 21.4985\n",
      "Epoch [144/300], Step [63/172], Loss: 9.5385\n",
      "Epoch [144/300], Step [64/172], Loss: 9.3566\n",
      "Epoch [144/300], Step [65/172], Loss: 19.7343\n",
      "Epoch [144/300], Step [66/172], Loss: 5.8209\n",
      "Epoch [144/300], Step [67/172], Loss: 24.6285\n",
      "Epoch [144/300], Step [68/172], Loss: 5.1971\n",
      "Epoch [144/300], Step [69/172], Loss: 43.8702\n",
      "Epoch [144/300], Step [70/172], Loss: 47.8927\n",
      "Epoch [144/300], Step [71/172], Loss: 45.8286\n",
      "Epoch [144/300], Step [72/172], Loss: 48.7100\n",
      "Epoch [144/300], Step [73/172], Loss: 55.5535\n",
      "Epoch [144/300], Step [74/172], Loss: 29.7726\n",
      "Epoch [144/300], Step [75/172], Loss: 30.1084\n",
      "Epoch [144/300], Step [76/172], Loss: 32.5183\n",
      "Epoch [144/300], Step [77/172], Loss: 56.6630\n",
      "Epoch [144/300], Step [78/172], Loss: 43.4914\n",
      "Epoch [144/300], Step [79/172], Loss: 42.4994\n",
      "Epoch [144/300], Step [80/172], Loss: 55.0533\n",
      "Epoch [144/300], Step [81/172], Loss: 38.4424\n",
      "Epoch [144/300], Step [82/172], Loss: 38.4421\n",
      "Epoch [144/300], Step [83/172], Loss: 45.7424\n",
      "Epoch [144/300], Step [84/172], Loss: 35.1029\n",
      "Epoch [144/300], Step [85/172], Loss: 40.2135\n",
      "Epoch [144/300], Step [86/172], Loss: 33.2941\n",
      "Epoch [144/300], Step [87/172], Loss: 27.3699\n",
      "Epoch [144/300], Step [88/172], Loss: 28.1519\n",
      "Epoch [144/300], Step [89/172], Loss: 26.0589\n",
      "Epoch [144/300], Step [90/172], Loss: 23.7652\n",
      "Epoch [144/300], Step [91/172], Loss: 27.2855\n",
      "Epoch [144/300], Step [92/172], Loss: 20.9070\n",
      "Epoch [144/300], Step [93/172], Loss: 21.4036\n",
      "Epoch [144/300], Step [94/172], Loss: 29.3911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [144/300], Step [95/172], Loss: 21.5730\n",
      "Epoch [144/300], Step [96/172], Loss: 19.6341\n",
      "Epoch [144/300], Step [97/172], Loss: 27.0618\n",
      "Epoch [144/300], Step [98/172], Loss: 19.4072\n",
      "Epoch [144/300], Step [99/172], Loss: 18.6231\n",
      "Epoch [144/300], Step [100/172], Loss: 15.3042\n",
      "Epoch [144/300], Step [101/172], Loss: 17.5545\n",
      "Epoch [144/300], Step [102/172], Loss: 16.8965\n",
      "Epoch [144/300], Step [103/172], Loss: 12.7736\n",
      "Epoch [144/300], Step [104/172], Loss: 17.0353\n",
      "Epoch [144/300], Step [105/172], Loss: 18.3371\n",
      "Epoch [144/300], Step [106/172], Loss: 15.9537\n",
      "Epoch [144/300], Step [107/172], Loss: 15.0629\n",
      "Epoch [144/300], Step [108/172], Loss: 15.5616\n",
      "Epoch [144/300], Step [109/172], Loss: 15.5080\n",
      "Epoch [144/300], Step [110/172], Loss: 15.4981\n",
      "Epoch [144/300], Step [111/172], Loss: 14.0572\n",
      "Epoch [144/300], Step [112/172], Loss: 17.6472\n",
      "Epoch [144/300], Step [113/172], Loss: 12.4992\n",
      "Epoch [144/300], Step [114/172], Loss: 13.3679\n",
      "Epoch [144/300], Step [115/172], Loss: 19.7055\n",
      "Epoch [144/300], Step [116/172], Loss: 14.2038\n",
      "Epoch [144/300], Step [117/172], Loss: 10.8661\n",
      "Epoch [144/300], Step [118/172], Loss: 14.3242\n",
      "Epoch [144/300], Step [119/172], Loss: 15.1435\n",
      "Epoch [144/300], Step [120/172], Loss: 9.8136\n",
      "Epoch [144/300], Step [121/172], Loss: 9.7558\n",
      "Epoch [144/300], Step [122/172], Loss: 10.1437\n",
      "Epoch [144/300], Step [123/172], Loss: 9.9824\n",
      "Epoch [144/300], Step [124/172], Loss: 7.8218\n",
      "Epoch [144/300], Step [125/172], Loss: 12.1869\n",
      "Epoch [144/300], Step [126/172], Loss: 10.2423\n",
      "Epoch [144/300], Step [127/172], Loss: 10.7658\n",
      "Epoch [144/300], Step [128/172], Loss: 10.7122\n",
      "Epoch [144/300], Step [129/172], Loss: 7.9981\n",
      "Epoch [144/300], Step [130/172], Loss: 11.4264\n",
      "Epoch [144/300], Step [131/172], Loss: 7.5460\n",
      "Epoch [144/300], Step [132/172], Loss: 8.2242\n",
      "Epoch [144/300], Step [133/172], Loss: 8.9806\n",
      "Epoch [144/300], Step [134/172], Loss: 11.4109\n",
      "Epoch [144/300], Step [135/172], Loss: 8.4492\n",
      "Epoch [144/300], Step [136/172], Loss: 8.0515\n",
      "Epoch [144/300], Step [137/172], Loss: 9.2590\n",
      "Epoch [144/300], Step [138/172], Loss: 7.1234\n",
      "Epoch [144/300], Step [139/172], Loss: 9.1650\n",
      "Epoch [144/300], Step [140/172], Loss: 9.1097\n",
      "Epoch [144/300], Step [141/172], Loss: 10.1531\n",
      "Epoch [144/300], Step [142/172], Loss: 13.3727\n",
      "Epoch [144/300], Step [143/172], Loss: 9.8040\n",
      "Epoch [144/300], Step [144/172], Loss: 8.5730\n",
      "Epoch [144/300], Step [145/172], Loss: 9.3544\n",
      "Epoch [144/300], Step [146/172], Loss: 9.1402\n",
      "Epoch [144/300], Step [147/172], Loss: 5.0135\n",
      "Epoch [144/300], Step [148/172], Loss: 5.9755\n",
      "Epoch [144/300], Step [149/172], Loss: 6.8119\n",
      "Epoch [144/300], Step [150/172], Loss: 6.6096\n",
      "Epoch [144/300], Step [151/172], Loss: 5.8912\n",
      "Epoch [144/300], Step [152/172], Loss: 7.1707\n",
      "Epoch [144/300], Step [153/172], Loss: 6.5096\n",
      "Epoch [144/300], Step [154/172], Loss: 7.4769\n",
      "Epoch [144/300], Step [155/172], Loss: 6.2296\n",
      "Epoch [144/300], Step [156/172], Loss: 12.3763\n",
      "Epoch [144/300], Step [157/172], Loss: 9.3390\n",
      "Epoch [144/300], Step [158/172], Loss: 7.0122\n",
      "Epoch [144/300], Step [159/172], Loss: 8.9584\n",
      "Epoch [144/300], Step [160/172], Loss: 9.9374\n",
      "Epoch [144/300], Step [161/172], Loss: 6.9547\n",
      "Epoch [144/300], Step [162/172], Loss: 5.8734\n",
      "Epoch [144/300], Step [163/172], Loss: 6.2139\n",
      "Epoch [144/300], Step [164/172], Loss: 8.6949\n",
      "Epoch [144/300], Step [165/172], Loss: 5.8202\n",
      "Epoch [144/300], Step [166/172], Loss: 5.4102\n",
      "Epoch [144/300], Step [167/172], Loss: 9.4590\n",
      "Epoch [144/300], Step [168/172], Loss: 6.6249\n",
      "Epoch [144/300], Step [169/172], Loss: 6.6172\n",
      "Epoch [144/300], Step [170/172], Loss: 4.8744\n",
      "Epoch [144/300], Step [171/172], Loss: 6.8597\n",
      "Epoch [144/300], Step [172/172], Loss: 4.9617\n",
      "Epoch [145/300], Step [1/172], Loss: 60.9957\n",
      "Epoch [145/300], Step [2/172], Loss: 62.2081\n",
      "Epoch [145/300], Step [3/172], Loss: 57.7503\n",
      "Epoch [145/300], Step [4/172], Loss: 33.4463\n",
      "Epoch [145/300], Step [5/172], Loss: 58.5066\n",
      "Epoch [145/300], Step [6/172], Loss: 19.3104\n",
      "Epoch [145/300], Step [7/172], Loss: 29.8833\n",
      "Epoch [145/300], Step [8/172], Loss: 5.0295\n",
      "Epoch [145/300], Step [9/172], Loss: 36.0328\n",
      "Epoch [145/300], Step [10/172], Loss: 44.5674\n",
      "Epoch [145/300], Step [11/172], Loss: 67.8958\n",
      "Epoch [145/300], Step [12/172], Loss: 76.3957\n",
      "Epoch [145/300], Step [13/172], Loss: 40.1815\n",
      "Epoch [145/300], Step [14/172], Loss: 70.8311\n",
      "Epoch [145/300], Step [15/172], Loss: 63.8511\n",
      "Epoch [145/300], Step [16/172], Loss: 14.0852\n",
      "Epoch [145/300], Step [17/172], Loss: 51.2469\n",
      "Epoch [145/300], Step [18/172], Loss: 62.3825\n",
      "Epoch [145/300], Step [19/172], Loss: 85.1420\n",
      "Epoch [145/300], Step [20/172], Loss: 49.0221\n",
      "Epoch [145/300], Step [21/172], Loss: 89.7078\n",
      "Epoch [145/300], Step [22/172], Loss: 66.9405\n",
      "Epoch [145/300], Step [23/172], Loss: 2.1189\n",
      "Epoch [145/300], Step [24/172], Loss: 63.0020\n",
      "Epoch [145/300], Step [25/172], Loss: 43.9284\n",
      "Epoch [145/300], Step [26/172], Loss: 52.9471\n",
      "Epoch [145/300], Step [27/172], Loss: 67.2805\n",
      "Epoch [145/300], Step [28/172], Loss: 27.6475\n",
      "Epoch [145/300], Step [29/172], Loss: 19.6819\n",
      "Epoch [145/300], Step [30/172], Loss: 70.9203\n",
      "Epoch [145/300], Step [31/172], Loss: 40.8817\n",
      "Epoch [145/300], Step [32/172], Loss: 42.7300\n",
      "Epoch [145/300], Step [33/172], Loss: 72.4513\n",
      "Epoch [145/300], Step [34/172], Loss: 3.6333\n",
      "Epoch [145/300], Step [35/172], Loss: 13.0697\n",
      "Epoch [145/300], Step [36/172], Loss: 18.4833\n",
      "Epoch [145/300], Step [37/172], Loss: 17.3522\n",
      "Epoch [145/300], Step [38/172], Loss: 28.4640\n",
      "Epoch [145/300], Step [39/172], Loss: 37.3987\n",
      "Epoch [145/300], Step [40/172], Loss: 19.9801\n",
      "Epoch [145/300], Step [41/172], Loss: 35.7784\n",
      "Epoch [145/300], Step [42/172], Loss: 39.1652\n",
      "Epoch [145/300], Step [43/172], Loss: 26.3588\n",
      "Epoch [145/300], Step [44/172], Loss: 19.8963\n",
      "Epoch [145/300], Step [45/172], Loss: 23.7175\n",
      "Epoch [145/300], Step [46/172], Loss: 18.3448\n",
      "Epoch [145/300], Step [47/172], Loss: 45.9565\n",
      "Epoch [145/300], Step [48/172], Loss: 56.7691\n",
      "Epoch [145/300], Step [49/172], Loss: 19.8739\n",
      "Epoch [145/300], Step [50/172], Loss: 46.7574\n",
      "Epoch [145/300], Step [51/172], Loss: 7.9818\n",
      "Epoch [145/300], Step [52/172], Loss: 17.6844\n",
      "Epoch [145/300], Step [53/172], Loss: 22.0941\n",
      "Epoch [145/300], Step [54/172], Loss: 12.7881\n",
      "Epoch [145/300], Step [55/172], Loss: 13.0462\n",
      "Epoch [145/300], Step [56/172], Loss: 15.3440\n",
      "Epoch [145/300], Step [57/172], Loss: 16.9903\n",
      "Epoch [145/300], Step [58/172], Loss: 14.8417\n",
      "Epoch [145/300], Step [59/172], Loss: 28.4651\n",
      "Epoch [145/300], Step [60/172], Loss: 34.4276\n",
      "Epoch [145/300], Step [61/172], Loss: 6.6295\n",
      "Epoch [145/300], Step [62/172], Loss: 21.6390\n",
      "Epoch [145/300], Step [63/172], Loss: 9.7342\n",
      "Epoch [145/300], Step [64/172], Loss: 9.6072\n",
      "Epoch [145/300], Step [65/172], Loss: 19.7963\n",
      "Epoch [145/300], Step [66/172], Loss: 5.8735\n",
      "Epoch [145/300], Step [67/172], Loss: 24.7172\n",
      "Epoch [145/300], Step [68/172], Loss: 5.4891\n",
      "Epoch [145/300], Step [69/172], Loss: 43.4155\n",
      "Epoch [145/300], Step [70/172], Loss: 47.2005\n",
      "Epoch [145/300], Step [71/172], Loss: 45.4586\n",
      "Epoch [145/300], Step [72/172], Loss: 48.1632\n",
      "Epoch [145/300], Step [73/172], Loss: 55.0376\n",
      "Epoch [145/300], Step [74/172], Loss: 29.6223\n",
      "Epoch [145/300], Step [75/172], Loss: 29.8791\n",
      "Epoch [145/300], Step [76/172], Loss: 32.2374\n",
      "Epoch [145/300], Step [77/172], Loss: 56.2029\n",
      "Epoch [145/300], Step [78/172], Loss: 43.2456\n",
      "Epoch [145/300], Step [79/172], Loss: 42.3015\n",
      "Epoch [145/300], Step [80/172], Loss: 55.0154\n",
      "Epoch [145/300], Step [81/172], Loss: 38.3281\n",
      "Epoch [145/300], Step [82/172], Loss: 38.3285\n",
      "Epoch [145/300], Step [83/172], Loss: 45.7224\n",
      "Epoch [145/300], Step [84/172], Loss: 35.1702\n",
      "Epoch [145/300], Step [85/172], Loss: 40.3476\n",
      "Epoch [145/300], Step [86/172], Loss: 33.4592\n",
      "Epoch [145/300], Step [87/172], Loss: 27.3998\n",
      "Epoch [145/300], Step [88/172], Loss: 28.1247\n",
      "Epoch [145/300], Step [89/172], Loss: 26.2433\n",
      "Epoch [145/300], Step [90/172], Loss: 23.8617\n",
      "Epoch [145/300], Step [91/172], Loss: 27.3156\n",
      "Epoch [145/300], Step [92/172], Loss: 20.9812\n",
      "Epoch [145/300], Step [93/172], Loss: 21.4758\n",
      "Epoch [145/300], Step [94/172], Loss: 29.4280\n",
      "Epoch [145/300], Step [95/172], Loss: 21.5961\n",
      "Epoch [145/300], Step [96/172], Loss: 19.7064\n",
      "Epoch [145/300], Step [97/172], Loss: 27.1813\n",
      "Epoch [145/300], Step [98/172], Loss: 19.4643\n",
      "Epoch [145/300], Step [99/172], Loss: 18.6831\n",
      "Epoch [145/300], Step [100/172], Loss: 15.3428\n",
      "Epoch [145/300], Step [101/172], Loss: 17.6339\n",
      "Epoch [145/300], Step [102/172], Loss: 16.9047\n",
      "Epoch [145/300], Step [103/172], Loss: 12.8052\n",
      "Epoch [145/300], Step [104/172], Loss: 17.1217\n",
      "Epoch [145/300], Step [105/172], Loss: 18.3686\n",
      "Epoch [145/300], Step [106/172], Loss: 16.0084\n",
      "Epoch [145/300], Step [107/172], Loss: 15.1494\n",
      "Epoch [145/300], Step [108/172], Loss: 15.5575\n",
      "Epoch [145/300], Step [109/172], Loss: 15.4801\n",
      "Epoch [145/300], Step [110/172], Loss: 15.5507\n",
      "Epoch [145/300], Step [111/172], Loss: 14.0969\n",
      "Epoch [145/300], Step [112/172], Loss: 17.6641\n",
      "Epoch [145/300], Step [113/172], Loss: 12.4446\n",
      "Epoch [145/300], Step [114/172], Loss: 13.3944\n",
      "Epoch [145/300], Step [115/172], Loss: 19.7039\n",
      "Epoch [145/300], Step [116/172], Loss: 14.2398\n",
      "Epoch [145/300], Step [117/172], Loss: 10.8897\n",
      "Epoch [145/300], Step [118/172], Loss: 14.3216\n",
      "Epoch [145/300], Step [119/172], Loss: 15.2332\n",
      "Epoch [145/300], Step [120/172], Loss: 9.8129\n",
      "Epoch [145/300], Step [121/172], Loss: 9.7655\n",
      "Epoch [145/300], Step [122/172], Loss: 10.2049\n",
      "Epoch [145/300], Step [123/172], Loss: 9.9939\n",
      "Epoch [145/300], Step [124/172], Loss: 7.8298\n",
      "Epoch [145/300], Step [125/172], Loss: 12.2220\n",
      "Epoch [145/300], Step [126/172], Loss: 10.2754\n",
      "Epoch [145/300], Step [127/172], Loss: 10.7927\n",
      "Epoch [145/300], Step [128/172], Loss: 10.7424\n",
      "Epoch [145/300], Step [129/172], Loss: 8.0041\n",
      "Epoch [145/300], Step [130/172], Loss: 11.5078\n",
      "Epoch [145/300], Step [131/172], Loss: 7.5439\n",
      "Epoch [145/300], Step [132/172], Loss: 8.2350\n",
      "Epoch [145/300], Step [133/172], Loss: 9.0036\n",
      "Epoch [145/300], Step [134/172], Loss: 11.4965\n",
      "Epoch [145/300], Step [135/172], Loss: 8.4866\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [145/300], Step [136/172], Loss: 8.0434\n",
      "Epoch [145/300], Step [137/172], Loss: 9.2826\n",
      "Epoch [145/300], Step [138/172], Loss: 7.1176\n",
      "Epoch [145/300], Step [139/172], Loss: 9.2155\n",
      "Epoch [145/300], Step [140/172], Loss: 9.1307\n",
      "Epoch [145/300], Step [141/172], Loss: 10.1326\n",
      "Epoch [145/300], Step [142/172], Loss: 13.4094\n",
      "Epoch [145/300], Step [143/172], Loss: 9.8372\n",
      "Epoch [145/300], Step [144/172], Loss: 8.6081\n",
      "Epoch [145/300], Step [145/172], Loss: 9.3583\n",
      "Epoch [145/300], Step [146/172], Loss: 9.1683\n",
      "Epoch [145/300], Step [147/172], Loss: 5.0246\n",
      "Epoch [145/300], Step [148/172], Loss: 5.9795\n",
      "Epoch [145/300], Step [149/172], Loss: 6.8152\n",
      "Epoch [145/300], Step [150/172], Loss: 6.5957\n",
      "Epoch [145/300], Step [151/172], Loss: 5.8740\n",
      "Epoch [145/300], Step [152/172], Loss: 7.2052\n",
      "Epoch [145/300], Step [153/172], Loss: 6.5098\n",
      "Epoch [145/300], Step [154/172], Loss: 7.4949\n",
      "Epoch [145/300], Step [155/172], Loss: 6.2182\n",
      "Epoch [145/300], Step [156/172], Loss: 12.4454\n",
      "Epoch [145/300], Step [157/172], Loss: 9.3689\n",
      "Epoch [145/300], Step [158/172], Loss: 7.0221\n",
      "Epoch [145/300], Step [159/172], Loss: 8.9548\n",
      "Epoch [145/300], Step [160/172], Loss: 10.0020\n",
      "Epoch [145/300], Step [161/172], Loss: 7.0181\n",
      "Epoch [145/300], Step [162/172], Loss: 5.8678\n",
      "Epoch [145/300], Step [163/172], Loss: 6.2648\n",
      "Epoch [145/300], Step [164/172], Loss: 8.6788\n",
      "Epoch [145/300], Step [165/172], Loss: 5.8329\n",
      "Epoch [145/300], Step [166/172], Loss: 5.4209\n",
      "Epoch [145/300], Step [167/172], Loss: 9.5012\n",
      "Epoch [145/300], Step [168/172], Loss: 6.6258\n",
      "Epoch [145/300], Step [169/172], Loss: 6.6296\n",
      "Epoch [145/300], Step [170/172], Loss: 4.8777\n",
      "Epoch [145/300], Step [171/172], Loss: 6.9225\n",
      "Epoch [145/300], Step [172/172], Loss: 4.9622\n",
      "Epoch [146/300], Step [1/172], Loss: 60.7078\n",
      "Epoch [146/300], Step [2/172], Loss: 61.7158\n",
      "Epoch [146/300], Step [3/172], Loss: 57.2752\n",
      "Epoch [146/300], Step [4/172], Loss: 33.1836\n",
      "Epoch [146/300], Step [5/172], Loss: 58.4296\n",
      "Epoch [146/300], Step [6/172], Loss: 19.1779\n",
      "Epoch [146/300], Step [7/172], Loss: 29.6471\n",
      "Epoch [146/300], Step [8/172], Loss: 5.0842\n",
      "Epoch [146/300], Step [9/172], Loss: 35.8983\n",
      "Epoch [146/300], Step [10/172], Loss: 44.4620\n",
      "Epoch [146/300], Step [11/172], Loss: 67.7441\n",
      "Epoch [146/300], Step [12/172], Loss: 76.2401\n",
      "Epoch [146/300], Step [13/172], Loss: 40.3416\n",
      "Epoch [146/300], Step [14/172], Loss: 70.8889\n",
      "Epoch [146/300], Step [15/172], Loss: 63.6167\n",
      "Epoch [146/300], Step [16/172], Loss: 13.3740\n",
      "Epoch [146/300], Step [17/172], Loss: 51.1020\n",
      "Epoch [146/300], Step [18/172], Loss: 62.3554\n",
      "Epoch [146/300], Step [19/172], Loss: 85.0877\n",
      "Epoch [146/300], Step [20/172], Loss: 48.7252\n",
      "Epoch [146/300], Step [21/172], Loss: 89.5079\n",
      "Epoch [146/300], Step [22/172], Loss: 66.7862\n",
      "Epoch [146/300], Step [23/172], Loss: 2.1863\n",
      "Epoch [146/300], Step [24/172], Loss: 62.8880\n",
      "Epoch [146/300], Step [25/172], Loss: 43.9113\n",
      "Epoch [146/300], Step [26/172], Loss: 52.7865\n",
      "Epoch [146/300], Step [27/172], Loss: 67.2401\n",
      "Epoch [146/300], Step [28/172], Loss: 27.7132\n",
      "Epoch [146/300], Step [29/172], Loss: 19.7011\n",
      "Epoch [146/300], Step [30/172], Loss: 70.5353\n",
      "Epoch [146/300], Step [31/172], Loss: 40.8452\n",
      "Epoch [146/300], Step [32/172], Loss: 42.7761\n",
      "Epoch [146/300], Step [33/172], Loss: 72.4786\n",
      "Epoch [146/300], Step [34/172], Loss: 3.6542\n",
      "Epoch [146/300], Step [35/172], Loss: 13.1917\n",
      "Epoch [146/300], Step [36/172], Loss: 18.4972\n",
      "Epoch [146/300], Step [37/172], Loss: 17.4841\n",
      "Epoch [146/300], Step [38/172], Loss: 28.6936\n",
      "Epoch [146/300], Step [39/172], Loss: 37.6264\n",
      "Epoch [146/300], Step [40/172], Loss: 20.1172\n",
      "Epoch [146/300], Step [41/172], Loss: 35.8273\n",
      "Epoch [146/300], Step [42/172], Loss: 39.3895\n",
      "Epoch [146/300], Step [43/172], Loss: 26.4904\n",
      "Epoch [146/300], Step [44/172], Loss: 19.9924\n",
      "Epoch [146/300], Step [45/172], Loss: 23.9706\n",
      "Epoch [146/300], Step [46/172], Loss: 18.5060\n",
      "Epoch [146/300], Step [47/172], Loss: 46.2344\n",
      "Epoch [146/300], Step [48/172], Loss: 57.2681\n",
      "Epoch [146/300], Step [49/172], Loss: 20.0589\n",
      "Epoch [146/300], Step [50/172], Loss: 46.9212\n",
      "Epoch [146/300], Step [51/172], Loss: 8.0277\n",
      "Epoch [146/300], Step [52/172], Loss: 17.7883\n",
      "Epoch [146/300], Step [53/172], Loss: 22.1099\n",
      "Epoch [146/300], Step [54/172], Loss: 12.9018\n",
      "Epoch [146/300], Step [55/172], Loss: 13.0762\n",
      "Epoch [146/300], Step [56/172], Loss: 15.3323\n",
      "Epoch [146/300], Step [57/172], Loss: 16.9597\n",
      "Epoch [146/300], Step [58/172], Loss: 14.5856\n",
      "Epoch [146/300], Step [59/172], Loss: 28.3278\n",
      "Epoch [146/300], Step [60/172], Loss: 34.2136\n",
      "Epoch [146/300], Step [61/172], Loss: 6.6104\n",
      "Epoch [146/300], Step [62/172], Loss: 21.4172\n",
      "Epoch [146/300], Step [63/172], Loss: 9.7203\n",
      "Epoch [146/300], Step [64/172], Loss: 9.5733\n",
      "Epoch [146/300], Step [65/172], Loss: 19.7195\n",
      "Epoch [146/300], Step [66/172], Loss: 5.8328\n",
      "Epoch [146/300], Step [67/172], Loss: 24.3640\n",
      "Epoch [146/300], Step [68/172], Loss: 5.2364\n",
      "Epoch [146/300], Step [69/172], Loss: 43.0599\n",
      "Epoch [146/300], Step [70/172], Loss: 47.1557\n",
      "Epoch [146/300], Step [71/172], Loss: 45.4722\n",
      "Epoch [146/300], Step [72/172], Loss: 48.0575\n",
      "Epoch [146/300], Step [73/172], Loss: 55.0849\n",
      "Epoch [146/300], Step [74/172], Loss: 29.5064\n",
      "Epoch [146/300], Step [75/172], Loss: 29.8708\n",
      "Epoch [146/300], Step [76/172], Loss: 32.2303\n",
      "Epoch [146/300], Step [77/172], Loss: 56.1675\n",
      "Epoch [146/300], Step [78/172], Loss: 43.2384\n",
      "Epoch [146/300], Step [79/172], Loss: 42.3114\n",
      "Epoch [146/300], Step [80/172], Loss: 54.9929\n",
      "Epoch [146/300], Step [81/172], Loss: 38.3156\n",
      "Epoch [146/300], Step [82/172], Loss: 38.3467\n",
      "Epoch [146/300], Step [83/172], Loss: 45.7835\n",
      "Epoch [146/300], Step [84/172], Loss: 35.2183\n",
      "Epoch [146/300], Step [85/172], Loss: 40.3968\n",
      "Epoch [146/300], Step [86/172], Loss: 33.5145\n",
      "Epoch [146/300], Step [87/172], Loss: 27.4170\n",
      "Epoch [146/300], Step [88/172], Loss: 28.1226\n",
      "Epoch [146/300], Step [89/172], Loss: 26.3086\n",
      "Epoch [146/300], Step [90/172], Loss: 23.8730\n",
      "Epoch [146/300], Step [91/172], Loss: 27.3798\n",
      "Epoch [146/300], Step [92/172], Loss: 21.0027\n",
      "Epoch [146/300], Step [93/172], Loss: 21.4549\n",
      "Epoch [146/300], Step [94/172], Loss: 29.4696\n",
      "Epoch [146/300], Step [95/172], Loss: 21.5795\n",
      "Epoch [146/300], Step [96/172], Loss: 19.7469\n",
      "Epoch [146/300], Step [97/172], Loss: 27.3013\n",
      "Epoch [146/300], Step [98/172], Loss: 19.5430\n",
      "Epoch [146/300], Step [99/172], Loss: 18.7785\n",
      "Epoch [146/300], Step [100/172], Loss: 15.4143\n",
      "Epoch [146/300], Step [101/172], Loss: 17.7302\n",
      "Epoch [146/300], Step [102/172], Loss: 17.0051\n",
      "Epoch [146/300], Step [103/172], Loss: 12.8236\n",
      "Epoch [146/300], Step [104/172], Loss: 17.2173\n",
      "Epoch [146/300], Step [105/172], Loss: 18.4517\n",
      "Epoch [146/300], Step [106/172], Loss: 16.0066\n",
      "Epoch [146/300], Step [107/172], Loss: 15.2140\n",
      "Epoch [146/300], Step [108/172], Loss: 15.5413\n",
      "Epoch [146/300], Step [109/172], Loss: 15.4885\n",
      "Epoch [146/300], Step [110/172], Loss: 15.5873\n",
      "Epoch [146/300], Step [111/172], Loss: 14.1340\n",
      "Epoch [146/300], Step [112/172], Loss: 17.6219\n",
      "Epoch [146/300], Step [113/172], Loss: 12.3809\n",
      "Epoch [146/300], Step [114/172], Loss: 13.4050\n",
      "Epoch [146/300], Step [115/172], Loss: 19.6987\n",
      "Epoch [146/300], Step [116/172], Loss: 14.2541\n",
      "Epoch [146/300], Step [117/172], Loss: 10.9032\n",
      "Epoch [146/300], Step [118/172], Loss: 14.3442\n",
      "Epoch [146/300], Step [119/172], Loss: 15.2388\n",
      "Epoch [146/300], Step [120/172], Loss: 9.8305\n",
      "Epoch [146/300], Step [121/172], Loss: 9.7589\n",
      "Epoch [146/300], Step [122/172], Loss: 10.2132\n",
      "Epoch [146/300], Step [123/172], Loss: 9.9687\n",
      "Epoch [146/300], Step [124/172], Loss: 7.8227\n",
      "Epoch [146/300], Step [125/172], Loss: 12.1989\n",
      "Epoch [146/300], Step [126/172], Loss: 10.2894\n",
      "Epoch [146/300], Step [127/172], Loss: 10.7949\n",
      "Epoch [146/300], Step [128/172], Loss: 10.7175\n",
      "Epoch [146/300], Step [129/172], Loss: 7.9934\n",
      "Epoch [146/300], Step [130/172], Loss: 11.5498\n",
      "Epoch [146/300], Step [131/172], Loss: 7.5300\n",
      "Epoch [146/300], Step [132/172], Loss: 8.2520\n",
      "Epoch [146/300], Step [133/172], Loss: 8.9909\n",
      "Epoch [146/300], Step [134/172], Loss: 11.4812\n",
      "Epoch [146/300], Step [135/172], Loss: 8.4928\n",
      "Epoch [146/300], Step [136/172], Loss: 8.0037\n",
      "Epoch [146/300], Step [137/172], Loss: 9.2862\n",
      "Epoch [146/300], Step [138/172], Loss: 7.0896\n",
      "Epoch [146/300], Step [139/172], Loss: 9.2114\n",
      "Epoch [146/300], Step [140/172], Loss: 9.1598\n",
      "Epoch [146/300], Step [141/172], Loss: 10.1117\n",
      "Epoch [146/300], Step [142/172], Loss: 13.4024\n",
      "Epoch [146/300], Step [143/172], Loss: 9.8686\n",
      "Epoch [146/300], Step [144/172], Loss: 8.6172\n",
      "Epoch [146/300], Step [145/172], Loss: 9.3649\n",
      "Epoch [146/300], Step [146/172], Loss: 9.1762\n",
      "Epoch [146/300], Step [147/172], Loss: 5.0354\n",
      "Epoch [146/300], Step [148/172], Loss: 5.9888\n",
      "Epoch [146/300], Step [149/172], Loss: 6.8143\n",
      "Epoch [146/300], Step [150/172], Loss: 6.5958\n",
      "Epoch [146/300], Step [151/172], Loss: 5.8310\n",
      "Epoch [146/300], Step [152/172], Loss: 7.2159\n",
      "Epoch [146/300], Step [153/172], Loss: 6.5220\n",
      "Epoch [146/300], Step [154/172], Loss: 7.4861\n",
      "Epoch [146/300], Step [155/172], Loss: 6.2281\n",
      "Epoch [146/300], Step [156/172], Loss: 12.4317\n",
      "Epoch [146/300], Step [157/172], Loss: 9.3496\n",
      "Epoch [146/300], Step [158/172], Loss: 7.0074\n",
      "Epoch [146/300], Step [159/172], Loss: 8.9564\n",
      "Epoch [146/300], Step [160/172], Loss: 9.9843\n",
      "Epoch [146/300], Step [161/172], Loss: 7.0358\n",
      "Epoch [146/300], Step [162/172], Loss: 5.8529\n",
      "Epoch [146/300], Step [163/172], Loss: 6.2759\n",
      "Epoch [146/300], Step [164/172], Loss: 8.6587\n",
      "Epoch [146/300], Step [165/172], Loss: 5.8526\n",
      "Epoch [146/300], Step [166/172], Loss: 5.4161\n",
      "Epoch [146/300], Step [167/172], Loss: 9.5364\n",
      "Epoch [146/300], Step [168/172], Loss: 6.6329\n",
      "Epoch [146/300], Step [169/172], Loss: 6.6290\n",
      "Epoch [146/300], Step [170/172], Loss: 4.8783\n",
      "Epoch [146/300], Step [171/172], Loss: 6.9603\n",
      "Epoch [146/300], Step [172/172], Loss: 4.9851\n",
      "Epoch [147/300], Step [1/172], Loss: 60.4553\n",
      "Epoch [147/300], Step [2/172], Loss: 61.2593\n",
      "Epoch [147/300], Step [3/172], Loss: 56.8542\n",
      "Epoch [147/300], Step [4/172], Loss: 32.9898\n",
      "Epoch [147/300], Step [5/172], Loss: 57.8070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [147/300], Step [6/172], Loss: 19.3971\n",
      "Epoch [147/300], Step [7/172], Loss: 30.3315\n",
      "Epoch [147/300], Step [8/172], Loss: 5.0413\n",
      "Epoch [147/300], Step [9/172], Loss: 35.7108\n",
      "Epoch [147/300], Step [10/172], Loss: 44.3585\n",
      "Epoch [147/300], Step [11/172], Loss: 67.3780\n",
      "Epoch [147/300], Step [12/172], Loss: 75.6952\n",
      "Epoch [147/300], Step [13/172], Loss: 40.1000\n",
      "Epoch [147/300], Step [14/172], Loss: 70.5771\n",
      "Epoch [147/300], Step [15/172], Loss: 63.2620\n",
      "Epoch [147/300], Step [16/172], Loss: 13.3700\n",
      "Epoch [147/300], Step [17/172], Loss: 50.8837\n",
      "Epoch [147/300], Step [18/172], Loss: 62.2604\n",
      "Epoch [147/300], Step [19/172], Loss: 85.1011\n",
      "Epoch [147/300], Step [20/172], Loss: 48.2078\n",
      "Epoch [147/300], Step [21/172], Loss: 89.6234\n",
      "Epoch [147/300], Step [22/172], Loss: 66.7467\n",
      "Epoch [147/300], Step [23/172], Loss: 2.2288\n",
      "Epoch [147/300], Step [24/172], Loss: 62.6762\n",
      "Epoch [147/300], Step [25/172], Loss: 43.7547\n",
      "Epoch [147/300], Step [26/172], Loss: 52.5510\n",
      "Epoch [147/300], Step [27/172], Loss: 67.2162\n",
      "Epoch [147/300], Step [28/172], Loss: 27.6345\n",
      "Epoch [147/300], Step [29/172], Loss: 19.6682\n",
      "Epoch [147/300], Step [30/172], Loss: 70.5473\n",
      "Epoch [147/300], Step [31/172], Loss: 40.8533\n",
      "Epoch [147/300], Step [32/172], Loss: 42.8151\n",
      "Epoch [147/300], Step [33/172], Loss: 72.4887\n",
      "Epoch [147/300], Step [34/172], Loss: 3.6487\n",
      "Epoch [147/300], Step [35/172], Loss: 13.1615\n",
      "Epoch [147/300], Step [36/172], Loss: 18.4507\n",
      "Epoch [147/300], Step [37/172], Loss: 17.5092\n",
      "Epoch [147/300], Step [38/172], Loss: 28.7934\n",
      "Epoch [147/300], Step [39/172], Loss: 37.4766\n",
      "Epoch [147/300], Step [40/172], Loss: 20.1531\n",
      "Epoch [147/300], Step [41/172], Loss: 35.7230\n",
      "Epoch [147/300], Step [42/172], Loss: 39.2862\n",
      "Epoch [147/300], Step [43/172], Loss: 26.5105\n",
      "Epoch [147/300], Step [44/172], Loss: 20.0112\n",
      "Epoch [147/300], Step [45/172], Loss: 24.0741\n",
      "Epoch [147/300], Step [46/172], Loss: 18.4052\n",
      "Epoch [147/300], Step [47/172], Loss: 46.1562\n",
      "Epoch [147/300], Step [48/172], Loss: 57.1454\n",
      "Epoch [147/300], Step [49/172], Loss: 20.0615\n",
      "Epoch [147/300], Step [50/172], Loss: 46.8377\n",
      "Epoch [147/300], Step [51/172], Loss: 8.0275\n",
      "Epoch [147/300], Step [52/172], Loss: 17.8162\n",
      "Epoch [147/300], Step [53/172], Loss: 22.0782\n",
      "Epoch [147/300], Step [54/172], Loss: 12.9341\n",
      "Epoch [147/300], Step [55/172], Loss: 13.0997\n",
      "Epoch [147/300], Step [56/172], Loss: 15.4477\n",
      "Epoch [147/300], Step [57/172], Loss: 16.9418\n",
      "Epoch [147/300], Step [58/172], Loss: 14.5660\n",
      "Epoch [147/300], Step [59/172], Loss: 28.1539\n",
      "Epoch [147/300], Step [60/172], Loss: 33.9627\n",
      "Epoch [147/300], Step [61/172], Loss: 6.5906\n",
      "Epoch [147/300], Step [62/172], Loss: 21.4220\n",
      "Epoch [147/300], Step [63/172], Loss: 9.6328\n",
      "Epoch [147/300], Step [64/172], Loss: 9.5774\n",
      "Epoch [147/300], Step [65/172], Loss: 19.6895\n",
      "Epoch [147/300], Step [66/172], Loss: 5.8959\n",
      "Epoch [147/300], Step [67/172], Loss: 24.4151\n",
      "Epoch [147/300], Step [68/172], Loss: 5.2326\n",
      "Epoch [147/300], Step [69/172], Loss: 42.8215\n",
      "Epoch [147/300], Step [70/172], Loss: 46.8954\n",
      "Epoch [147/300], Step [71/172], Loss: 45.2309\n",
      "Epoch [147/300], Step [72/172], Loss: 47.7407\n",
      "Epoch [147/300], Step [73/172], Loss: 54.8800\n",
      "Epoch [147/300], Step [74/172], Loss: 29.3378\n",
      "Epoch [147/300], Step [75/172], Loss: 29.6639\n",
      "Epoch [147/300], Step [76/172], Loss: 32.0271\n",
      "Epoch [147/300], Step [77/172], Loss: 55.6463\n",
      "Epoch [147/300], Step [78/172], Loss: 42.8948\n",
      "Epoch [147/300], Step [79/172], Loss: 41.9146\n",
      "Epoch [147/300], Step [80/172], Loss: 54.7059\n",
      "Epoch [147/300], Step [81/172], Loss: 38.0463\n",
      "Epoch [147/300], Step [82/172], Loss: 38.1017\n",
      "Epoch [147/300], Step [83/172], Loss: 45.6071\n",
      "Epoch [147/300], Step [84/172], Loss: 35.0877\n",
      "Epoch [147/300], Step [85/172], Loss: 40.3025\n",
      "Epoch [147/300], Step [86/172], Loss: 33.4633\n",
      "Epoch [147/300], Step [87/172], Loss: 27.3435\n",
      "Epoch [147/300], Step [88/172], Loss: 27.9945\n",
      "Epoch [147/300], Step [89/172], Loss: 26.3343\n",
      "Epoch [147/300], Step [90/172], Loss: 23.7174\n",
      "Epoch [147/300], Step [91/172], Loss: 27.3201\n",
      "Epoch [147/300], Step [92/172], Loss: 20.9662\n",
      "Epoch [147/300], Step [93/172], Loss: 21.4235\n",
      "Epoch [147/300], Step [94/172], Loss: 29.4992\n",
      "Epoch [147/300], Step [95/172], Loss: 21.5823\n",
      "Epoch [147/300], Step [96/172], Loss: 19.7577\n",
      "Epoch [147/300], Step [97/172], Loss: 27.3322\n",
      "Epoch [147/300], Step [98/172], Loss: 19.5785\n",
      "Epoch [147/300], Step [99/172], Loss: 18.7845\n",
      "Epoch [147/300], Step [100/172], Loss: 15.4259\n",
      "Epoch [147/300], Step [101/172], Loss: 17.7817\n",
      "Epoch [147/300], Step [102/172], Loss: 16.9921\n",
      "Epoch [147/300], Step [103/172], Loss: 12.8342\n",
      "Epoch [147/300], Step [104/172], Loss: 17.2884\n",
      "Epoch [147/300], Step [105/172], Loss: 18.4653\n",
      "Epoch [147/300], Step [106/172], Loss: 16.0146\n",
      "Epoch [147/300], Step [107/172], Loss: 15.2253\n",
      "Epoch [147/300], Step [108/172], Loss: 15.4667\n",
      "Epoch [147/300], Step [109/172], Loss: 15.4190\n",
      "Epoch [147/300], Step [110/172], Loss: 15.5980\n",
      "Epoch [147/300], Step [111/172], Loss: 14.1519\n",
      "Epoch [147/300], Step [112/172], Loss: 17.6054\n",
      "Epoch [147/300], Step [113/172], Loss: 12.3050\n",
      "Epoch [147/300], Step [114/172], Loss: 13.3974\n",
      "Epoch [147/300], Step [115/172], Loss: 19.6086\n",
      "Epoch [147/300], Step [116/172], Loss: 14.2590\n",
      "Epoch [147/300], Step [117/172], Loss: 10.9137\n",
      "Epoch [147/300], Step [118/172], Loss: 14.3343\n",
      "Epoch [147/300], Step [119/172], Loss: 15.2206\n",
      "Epoch [147/300], Step [120/172], Loss: 9.8092\n",
      "Epoch [147/300], Step [121/172], Loss: 9.7480\n",
      "Epoch [147/300], Step [122/172], Loss: 10.1884\n",
      "Epoch [147/300], Step [123/172], Loss: 9.9309\n",
      "Epoch [147/300], Step [124/172], Loss: 7.7986\n",
      "Epoch [147/300], Step [125/172], Loss: 12.1886\n",
      "Epoch [147/300], Step [126/172], Loss: 10.2979\n",
      "Epoch [147/300], Step [127/172], Loss: 10.7789\n",
      "Epoch [147/300], Step [128/172], Loss: 10.6802\n",
      "Epoch [147/300], Step [129/172], Loss: 7.9759\n",
      "Epoch [147/300], Step [130/172], Loss: 11.5556\n",
      "Epoch [147/300], Step [131/172], Loss: 7.5060\n",
      "Epoch [147/300], Step [132/172], Loss: 8.2471\n",
      "Epoch [147/300], Step [133/172], Loss: 8.9629\n",
      "Epoch [147/300], Step [134/172], Loss: 11.4430\n",
      "Epoch [147/300], Step [135/172], Loss: 8.4712\n",
      "Epoch [147/300], Step [136/172], Loss: 7.9761\n",
      "Epoch [147/300], Step [137/172], Loss: 9.2427\n",
      "Epoch [147/300], Step [138/172], Loss: 7.0453\n",
      "Epoch [147/300], Step [139/172], Loss: 9.1790\n",
      "Epoch [147/300], Step [140/172], Loss: 9.1502\n",
      "Epoch [147/300], Step [141/172], Loss: 10.0684\n",
      "Epoch [147/300], Step [142/172], Loss: 13.4026\n",
      "Epoch [147/300], Step [143/172], Loss: 9.8830\n",
      "Epoch [147/300], Step [144/172], Loss: 8.5924\n",
      "Epoch [147/300], Step [145/172], Loss: 9.3647\n",
      "Epoch [147/300], Step [146/172], Loss: 9.1422\n",
      "Epoch [147/300], Step [147/172], Loss: 5.0228\n",
      "Epoch [147/300], Step [148/172], Loss: 5.9672\n",
      "Epoch [147/300], Step [149/172], Loss: 6.7839\n",
      "Epoch [147/300], Step [150/172], Loss: 6.5506\n",
      "Epoch [147/300], Step [151/172], Loss: 5.7938\n",
      "Epoch [147/300], Step [152/172], Loss: 7.1766\n",
      "Epoch [147/300], Step [153/172], Loss: 6.5035\n",
      "Epoch [147/300], Step [154/172], Loss: 7.4379\n",
      "Epoch [147/300], Step [155/172], Loss: 6.2139\n",
      "Epoch [147/300], Step [156/172], Loss: 12.4319\n",
      "Epoch [147/300], Step [157/172], Loss: 9.3303\n",
      "Epoch [147/300], Step [158/172], Loss: 6.9839\n",
      "Epoch [147/300], Step [159/172], Loss: 8.9479\n",
      "Epoch [147/300], Step [160/172], Loss: 9.9290\n",
      "Epoch [147/300], Step [161/172], Loss: 7.0429\n",
      "Epoch [147/300], Step [162/172], Loss: 5.8142\n",
      "Epoch [147/300], Step [163/172], Loss: 6.2724\n",
      "Epoch [147/300], Step [164/172], Loss: 8.6229\n",
      "Epoch [147/300], Step [165/172], Loss: 5.8291\n",
      "Epoch [147/300], Step [166/172], Loss: 5.3819\n",
      "Epoch [147/300], Step [167/172], Loss: 9.5386\n",
      "Epoch [147/300], Step [168/172], Loss: 6.6133\n",
      "Epoch [147/300], Step [169/172], Loss: 6.5883\n",
      "Epoch [147/300], Step [170/172], Loss: 4.8512\n",
      "Epoch [147/300], Step [171/172], Loss: 6.9662\n",
      "Epoch [147/300], Step [172/172], Loss: 4.9704\n",
      "Epoch [148/300], Step [1/172], Loss: 60.2645\n",
      "Epoch [148/300], Step [2/172], Loss: 61.0010\n",
      "Epoch [148/300], Step [3/172], Loss: 56.4708\n",
      "Epoch [148/300], Step [4/172], Loss: 32.8507\n",
      "Epoch [148/300], Step [5/172], Loss: 57.6103\n",
      "Epoch [148/300], Step [6/172], Loss: 19.4759\n",
      "Epoch [148/300], Step [7/172], Loss: 30.4600\n",
      "Epoch [148/300], Step [8/172], Loss: 5.1840\n",
      "Epoch [148/300], Step [9/172], Loss: 35.7582\n",
      "Epoch [148/300], Step [10/172], Loss: 44.3901\n",
      "Epoch [148/300], Step [11/172], Loss: 67.1048\n",
      "Epoch [148/300], Step [12/172], Loss: 75.5282\n",
      "Epoch [148/300], Step [13/172], Loss: 40.1297\n",
      "Epoch [148/300], Step [14/172], Loss: 70.4827\n",
      "Epoch [148/300], Step [15/172], Loss: 63.1591\n",
      "Epoch [148/300], Step [16/172], Loss: 13.2367\n",
      "Epoch [148/300], Step [17/172], Loss: 50.8283\n",
      "Epoch [148/300], Step [18/172], Loss: 62.2137\n",
      "Epoch [148/300], Step [19/172], Loss: 85.1830\n",
      "Epoch [148/300], Step [20/172], Loss: 47.8788\n",
      "Epoch [148/300], Step [21/172], Loss: 89.5471\n",
      "Epoch [148/300], Step [22/172], Loss: 66.4869\n",
      "Epoch [148/300], Step [23/172], Loss: 2.1854\n",
      "Epoch [148/300], Step [24/172], Loss: 62.4572\n",
      "Epoch [148/300], Step [25/172], Loss: 43.8526\n",
      "Epoch [148/300], Step [26/172], Loss: 52.4868\n",
      "Epoch [148/300], Step [27/172], Loss: 67.0845\n",
      "Epoch [148/300], Step [28/172], Loss: 27.6424\n",
      "Epoch [148/300], Step [29/172], Loss: 19.6010\n",
      "Epoch [148/300], Step [30/172], Loss: 70.2636\n",
      "Epoch [148/300], Step [31/172], Loss: 40.8789\n",
      "Epoch [148/300], Step [32/172], Loss: 42.8866\n",
      "Epoch [148/300], Step [33/172], Loss: 72.5235\n",
      "Epoch [148/300], Step [34/172], Loss: 3.6274\n",
      "Epoch [148/300], Step [35/172], Loss: 13.2926\n",
      "Epoch [148/300], Step [36/172], Loss: 18.3624\n",
      "Epoch [148/300], Step [37/172], Loss: 17.5345\n",
      "Epoch [148/300], Step [38/172], Loss: 28.8811\n",
      "Epoch [148/300], Step [39/172], Loss: 37.4088\n",
      "Epoch [148/300], Step [40/172], Loss: 20.1947\n",
      "Epoch [148/300], Step [41/172], Loss: 35.7127\n",
      "Epoch [148/300], Step [42/172], Loss: 39.3975\n",
      "Epoch [148/300], Step [43/172], Loss: 26.5873\n",
      "Epoch [148/300], Step [44/172], Loss: 20.0102\n",
      "Epoch [148/300], Step [45/172], Loss: 24.1763\n",
      "Epoch [148/300], Step [46/172], Loss: 18.3906\n",
      "Epoch [148/300], Step [47/172], Loss: 46.1512\n",
      "Epoch [148/300], Step [48/172], Loss: 57.1480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [148/300], Step [49/172], Loss: 20.1203\n",
      "Epoch [148/300], Step [50/172], Loss: 46.5463\n",
      "Epoch [148/300], Step [51/172], Loss: 8.0570\n",
      "Epoch [148/300], Step [52/172], Loss: 17.8139\n",
      "Epoch [148/300], Step [53/172], Loss: 21.9980\n",
      "Epoch [148/300], Step [54/172], Loss: 13.0306\n",
      "Epoch [148/300], Step [55/172], Loss: 13.1639\n",
      "Epoch [148/300], Step [56/172], Loss: 15.6001\n",
      "Epoch [148/300], Step [57/172], Loss: 16.8596\n",
      "Epoch [148/300], Step [58/172], Loss: 14.4567\n",
      "Epoch [148/300], Step [59/172], Loss: 28.0645\n",
      "Epoch [148/300], Step [60/172], Loss: 33.5838\n",
      "Epoch [148/300], Step [61/172], Loss: 6.6075\n",
      "Epoch [148/300], Step [62/172], Loss: 21.3329\n",
      "Epoch [148/300], Step [63/172], Loss: 9.6956\n",
      "Epoch [148/300], Step [64/172], Loss: 9.6112\n",
      "Epoch [148/300], Step [65/172], Loss: 19.7003\n",
      "Epoch [148/300], Step [66/172], Loss: 5.8356\n",
      "Epoch [148/300], Step [67/172], Loss: 24.2556\n",
      "Epoch [148/300], Step [68/172], Loss: 5.2446\n",
      "Epoch [148/300], Step [69/172], Loss: 42.4286\n",
      "Epoch [148/300], Step [70/172], Loss: 46.7036\n",
      "Epoch [148/300], Step [71/172], Loss: 45.1099\n",
      "Epoch [148/300], Step [72/172], Loss: 47.5937\n",
      "Epoch [148/300], Step [73/172], Loss: 54.8046\n",
      "Epoch [148/300], Step [74/172], Loss: 29.1910\n",
      "Epoch [148/300], Step [75/172], Loss: 29.6293\n",
      "Epoch [148/300], Step [76/172], Loss: 31.9894\n",
      "Epoch [148/300], Step [77/172], Loss: 55.4251\n",
      "Epoch [148/300], Step [78/172], Loss: 42.7840\n",
      "Epoch [148/300], Step [79/172], Loss: 41.8833\n",
      "Epoch [148/300], Step [80/172], Loss: 54.7099\n",
      "Epoch [148/300], Step [81/172], Loss: 37.9761\n",
      "Epoch [148/300], Step [82/172], Loss: 38.1820\n",
      "Epoch [148/300], Step [83/172], Loss: 45.5527\n",
      "Epoch [148/300], Step [84/172], Loss: 35.1561\n",
      "Epoch [148/300], Step [85/172], Loss: 40.3335\n",
      "Epoch [148/300], Step [86/172], Loss: 33.5421\n",
      "Epoch [148/300], Step [87/172], Loss: 27.3665\n",
      "Epoch [148/300], Step [88/172], Loss: 27.9788\n",
      "Epoch [148/300], Step [89/172], Loss: 26.4718\n",
      "Epoch [148/300], Step [90/172], Loss: 23.6354\n",
      "Epoch [148/300], Step [91/172], Loss: 27.3284\n",
      "Epoch [148/300], Step [92/172], Loss: 20.9524\n",
      "Epoch [148/300], Step [93/172], Loss: 21.3968\n",
      "Epoch [148/300], Step [94/172], Loss: 29.4624\n",
      "Epoch [148/300], Step [95/172], Loss: 21.5916\n",
      "Epoch [148/300], Step [96/172], Loss: 19.7821\n",
      "Epoch [148/300], Step [97/172], Loss: 27.3459\n",
      "Epoch [148/300], Step [98/172], Loss: 19.6160\n",
      "Epoch [148/300], Step [99/172], Loss: 18.8199\n",
      "Epoch [148/300], Step [100/172], Loss: 15.4142\n",
      "Epoch [148/300], Step [101/172], Loss: 17.8151\n",
      "Epoch [148/300], Step [102/172], Loss: 17.0659\n",
      "Epoch [148/300], Step [103/172], Loss: 12.8255\n",
      "Epoch [148/300], Step [104/172], Loss: 17.3173\n",
      "Epoch [148/300], Step [105/172], Loss: 18.5277\n",
      "Epoch [148/300], Step [106/172], Loss: 15.9787\n",
      "Epoch [148/300], Step [107/172], Loss: 15.2220\n",
      "Epoch [148/300], Step [108/172], Loss: 15.3945\n",
      "Epoch [148/300], Step [109/172], Loss: 15.3583\n",
      "Epoch [148/300], Step [110/172], Loss: 15.6032\n",
      "Epoch [148/300], Step [111/172], Loss: 14.2031\n",
      "Epoch [148/300], Step [112/172], Loss: 17.6151\n",
      "Epoch [148/300], Step [113/172], Loss: 12.2334\n",
      "Epoch [148/300], Step [114/172], Loss: 13.4130\n",
      "Epoch [148/300], Step [115/172], Loss: 19.5843\n",
      "Epoch [148/300], Step [116/172], Loss: 14.2684\n",
      "Epoch [148/300], Step [117/172], Loss: 10.9535\n",
      "Epoch [148/300], Step [118/172], Loss: 14.2950\n",
      "Epoch [148/300], Step [119/172], Loss: 15.2429\n",
      "Epoch [148/300], Step [120/172], Loss: 9.7980\n",
      "Epoch [148/300], Step [121/172], Loss: 9.7230\n",
      "Epoch [148/300], Step [122/172], Loss: 10.2409\n",
      "Epoch [148/300], Step [123/172], Loss: 9.9598\n",
      "Epoch [148/300], Step [124/172], Loss: 7.7918\n",
      "Epoch [148/300], Step [125/172], Loss: 12.1607\n",
      "Epoch [148/300], Step [126/172], Loss: 10.3113\n",
      "Epoch [148/300], Step [127/172], Loss: 10.7880\n",
      "Epoch [148/300], Step [128/172], Loss: 10.6510\n",
      "Epoch [148/300], Step [129/172], Loss: 7.9871\n",
      "Epoch [148/300], Step [130/172], Loss: 11.5805\n",
      "Epoch [148/300], Step [131/172], Loss: 7.5089\n",
      "Epoch [148/300], Step [132/172], Loss: 8.2507\n",
      "Epoch [148/300], Step [133/172], Loss: 8.9676\n",
      "Epoch [148/300], Step [134/172], Loss: 11.4525\n",
      "Epoch [148/300], Step [135/172], Loss: 8.4805\n",
      "Epoch [148/300], Step [136/172], Loss: 7.9924\n",
      "Epoch [148/300], Step [137/172], Loss: 9.2618\n",
      "Epoch [148/300], Step [138/172], Loss: 7.0450\n",
      "Epoch [148/300], Step [139/172], Loss: 9.1818\n",
      "Epoch [148/300], Step [140/172], Loss: 9.1644\n",
      "Epoch [148/300], Step [141/172], Loss: 10.0364\n",
      "Epoch [148/300], Step [142/172], Loss: 13.4460\n",
      "Epoch [148/300], Step [143/172], Loss: 9.9072\n",
      "Epoch [148/300], Step [144/172], Loss: 8.6052\n",
      "Epoch [148/300], Step [145/172], Loss: 9.3731\n",
      "Epoch [148/300], Step [146/172], Loss: 9.1391\n",
      "Epoch [148/300], Step [147/172], Loss: 5.0184\n",
      "Epoch [148/300], Step [148/172], Loss: 5.9608\n",
      "Epoch [148/300], Step [149/172], Loss: 6.7670\n",
      "Epoch [148/300], Step [150/172], Loss: 6.5274\n",
      "Epoch [148/300], Step [151/172], Loss: 5.7758\n",
      "Epoch [148/300], Step [152/172], Loss: 7.1791\n",
      "Epoch [148/300], Step [153/172], Loss: 6.4905\n",
      "Epoch [148/300], Step [154/172], Loss: 7.4621\n",
      "Epoch [148/300], Step [155/172], Loss: 6.2040\n",
      "Epoch [148/300], Step [156/172], Loss: 12.4970\n",
      "Epoch [148/300], Step [157/172], Loss: 9.3586\n",
      "Epoch [148/300], Step [158/172], Loss: 7.0006\n",
      "Epoch [148/300], Step [159/172], Loss: 8.9148\n",
      "Epoch [148/300], Step [160/172], Loss: 9.9363\n",
      "Epoch [148/300], Step [161/172], Loss: 7.1035\n",
      "Epoch [148/300], Step [162/172], Loss: 5.7911\n",
      "Epoch [148/300], Step [163/172], Loss: 6.3093\n",
      "Epoch [148/300], Step [164/172], Loss: 8.6334\n",
      "Epoch [148/300], Step [165/172], Loss: 5.8208\n",
      "Epoch [148/300], Step [166/172], Loss: 5.3750\n",
      "Epoch [148/300], Step [167/172], Loss: 9.5614\n",
      "Epoch [148/300], Step [168/172], Loss: 6.5984\n",
      "Epoch [148/300], Step [169/172], Loss: 6.6047\n",
      "Epoch [148/300], Step [170/172], Loss: 4.8572\n",
      "Epoch [148/300], Step [171/172], Loss: 7.0009\n",
      "Epoch [148/300], Step [172/172], Loss: 4.9884\n",
      "Epoch [149/300], Step [1/172], Loss: 60.0344\n",
      "Epoch [149/300], Step [2/172], Loss: 60.7205\n",
      "Epoch [149/300], Step [3/172], Loss: 56.1336\n",
      "Epoch [149/300], Step [4/172], Loss: 32.7504\n",
      "Epoch [149/300], Step [5/172], Loss: 57.4946\n",
      "Epoch [149/300], Step [6/172], Loss: 19.3704\n",
      "Epoch [149/300], Step [7/172], Loss: 30.0357\n",
      "Epoch [149/300], Step [8/172], Loss: 5.2436\n",
      "Epoch [149/300], Step [9/172], Loss: 35.7451\n",
      "Epoch [149/300], Step [10/172], Loss: 44.4036\n",
      "Epoch [149/300], Step [11/172], Loss: 66.8445\n",
      "Epoch [149/300], Step [12/172], Loss: 75.1289\n",
      "Epoch [149/300], Step [13/172], Loss: 39.9080\n",
      "Epoch [149/300], Step [14/172], Loss: 70.0784\n",
      "Epoch [149/300], Step [15/172], Loss: 62.8990\n",
      "Epoch [149/300], Step [16/172], Loss: 13.3256\n",
      "Epoch [149/300], Step [17/172], Loss: 50.5484\n",
      "Epoch [149/300], Step [18/172], Loss: 62.1303\n",
      "Epoch [149/300], Step [19/172], Loss: 85.2057\n",
      "Epoch [149/300], Step [20/172], Loss: 47.5071\n",
      "Epoch [149/300], Step [21/172], Loss: 89.6919\n",
      "Epoch [149/300], Step [22/172], Loss: 66.4362\n",
      "Epoch [149/300], Step [23/172], Loss: 2.2199\n",
      "Epoch [149/300], Step [24/172], Loss: 62.3580\n",
      "Epoch [149/300], Step [25/172], Loss: 43.8411\n",
      "Epoch [149/300], Step [26/172], Loss: 52.3368\n",
      "Epoch [149/300], Step [27/172], Loss: 67.0338\n",
      "Epoch [149/300], Step [28/172], Loss: 27.5118\n",
      "Epoch [149/300], Step [29/172], Loss: 19.5508\n",
      "Epoch [149/300], Step [30/172], Loss: 70.4592\n",
      "Epoch [149/300], Step [31/172], Loss: 40.9928\n",
      "Epoch [149/300], Step [32/172], Loss: 42.9916\n",
      "Epoch [149/300], Step [33/172], Loss: 72.6082\n",
      "Epoch [149/300], Step [34/172], Loss: 3.6464\n",
      "Epoch [149/300], Step [35/172], Loss: 13.2206\n",
      "Epoch [149/300], Step [36/172], Loss: 18.2816\n",
      "Epoch [149/300], Step [37/172], Loss: 17.5255\n",
      "Epoch [149/300], Step [38/172], Loss: 28.9843\n",
      "Epoch [149/300], Step [39/172], Loss: 37.2319\n",
      "Epoch [149/300], Step [40/172], Loss: 20.1716\n",
      "Epoch [149/300], Step [41/172], Loss: 35.7080\n",
      "Epoch [149/300], Step [42/172], Loss: 39.4265\n",
      "Epoch [149/300], Step [43/172], Loss: 26.7320\n",
      "Epoch [149/300], Step [44/172], Loss: 20.0677\n",
      "Epoch [149/300], Step [45/172], Loss: 24.1565\n",
      "Epoch [149/300], Step [46/172], Loss: 18.2501\n",
      "Epoch [149/300], Step [47/172], Loss: 46.3518\n",
      "Epoch [149/300], Step [48/172], Loss: 57.0518\n",
      "Epoch [149/300], Step [49/172], Loss: 20.1004\n",
      "Epoch [149/300], Step [50/172], Loss: 46.4075\n",
      "Epoch [149/300], Step [51/172], Loss: 8.1112\n",
      "Epoch [149/300], Step [52/172], Loss: 17.9123\n",
      "Epoch [149/300], Step [53/172], Loss: 22.0522\n",
      "Epoch [149/300], Step [54/172], Loss: 13.1062\n",
      "Epoch [149/300], Step [55/172], Loss: 13.2384\n",
      "Epoch [149/300], Step [56/172], Loss: 15.7896\n",
      "Epoch [149/300], Step [57/172], Loss: 16.9074\n",
      "Epoch [149/300], Step [58/172], Loss: 14.4932\n",
      "Epoch [149/300], Step [59/172], Loss: 28.3094\n",
      "Epoch [149/300], Step [60/172], Loss: 33.5209\n",
      "Epoch [149/300], Step [61/172], Loss: 6.5934\n",
      "Epoch [149/300], Step [62/172], Loss: 21.4354\n",
      "Epoch [149/300], Step [63/172], Loss: 9.8603\n",
      "Epoch [149/300], Step [64/172], Loss: 9.8308\n",
      "Epoch [149/300], Step [65/172], Loss: 19.7902\n",
      "Epoch [149/300], Step [66/172], Loss: 5.8987\n",
      "Epoch [149/300], Step [67/172], Loss: 24.3078\n",
      "Epoch [149/300], Step [68/172], Loss: 5.4141\n",
      "Epoch [149/300], Step [69/172], Loss: 42.0361\n",
      "Epoch [149/300], Step [70/172], Loss: 46.1365\n",
      "Epoch [149/300], Step [71/172], Loss: 44.7296\n",
      "Epoch [149/300], Step [72/172], Loss: 47.1135\n",
      "Epoch [149/300], Step [73/172], Loss: 54.3220\n",
      "Epoch [149/300], Step [74/172], Loss: 29.0394\n",
      "Epoch [149/300], Step [75/172], Loss: 29.5016\n",
      "Epoch [149/300], Step [76/172], Loss: 31.7894\n",
      "Epoch [149/300], Step [77/172], Loss: 55.0781\n",
      "Epoch [149/300], Step [78/172], Loss: 42.5614\n",
      "Epoch [149/300], Step [79/172], Loss: 41.6722\n",
      "Epoch [149/300], Step [80/172], Loss: 54.6682\n",
      "Epoch [149/300], Step [81/172], Loss: 37.8175\n",
      "Epoch [149/300], Step [82/172], Loss: 38.1091\n",
      "Epoch [149/300], Step [83/172], Loss: 45.4494\n",
      "Epoch [149/300], Step [84/172], Loss: 35.0862\n",
      "Epoch [149/300], Step [85/172], Loss: 40.2704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [149/300], Step [86/172], Loss: 33.5836\n",
      "Epoch [149/300], Step [87/172], Loss: 27.3382\n",
      "Epoch [149/300], Step [88/172], Loss: 27.8818\n",
      "Epoch [149/300], Step [89/172], Loss: 26.5032\n",
      "Epoch [149/300], Step [90/172], Loss: 23.6172\n",
      "Epoch [149/300], Step [91/172], Loss: 27.3125\n",
      "Epoch [149/300], Step [92/172], Loss: 20.9395\n",
      "Epoch [149/300], Step [93/172], Loss: 21.3904\n",
      "Epoch [149/300], Step [94/172], Loss: 29.4340\n",
      "Epoch [149/300], Step [95/172], Loss: 21.5338\n",
      "Epoch [149/300], Step [96/172], Loss: 19.7729\n",
      "Epoch [149/300], Step [97/172], Loss: 27.4049\n",
      "Epoch [149/300], Step [98/172], Loss: 19.6067\n",
      "Epoch [149/300], Step [99/172], Loss: 18.8512\n",
      "Epoch [149/300], Step [100/172], Loss: 15.3888\n",
      "Epoch [149/300], Step [101/172], Loss: 17.8466\n",
      "Epoch [149/300], Step [102/172], Loss: 17.1093\n",
      "Epoch [149/300], Step [103/172], Loss: 12.8174\n",
      "Epoch [149/300], Step [104/172], Loss: 17.3881\n",
      "Epoch [149/300], Step [105/172], Loss: 18.5878\n",
      "Epoch [149/300], Step [106/172], Loss: 15.9471\n",
      "Epoch [149/300], Step [107/172], Loss: 15.2638\n",
      "Epoch [149/300], Step [108/172], Loss: 15.3464\n",
      "Epoch [149/300], Step [109/172], Loss: 15.3445\n",
      "Epoch [149/300], Step [110/172], Loss: 15.5966\n",
      "Epoch [149/300], Step [111/172], Loss: 14.2225\n",
      "Epoch [149/300], Step [112/172], Loss: 17.5921\n",
      "Epoch [149/300], Step [113/172], Loss: 12.1971\n",
      "Epoch [149/300], Step [114/172], Loss: 13.4125\n",
      "Epoch [149/300], Step [115/172], Loss: 19.5766\n",
      "Epoch [149/300], Step [116/172], Loss: 14.2728\n",
      "Epoch [149/300], Step [117/172], Loss: 10.9808\n",
      "Epoch [149/300], Step [118/172], Loss: 14.2585\n",
      "Epoch [149/300], Step [119/172], Loss: 15.3042\n",
      "Epoch [149/300], Step [120/172], Loss: 9.7971\n",
      "Epoch [149/300], Step [121/172], Loss: 9.6902\n",
      "Epoch [149/300], Step [122/172], Loss: 10.2718\n",
      "Epoch [149/300], Step [123/172], Loss: 9.9717\n",
      "Epoch [149/300], Step [124/172], Loss: 7.7858\n",
      "Epoch [149/300], Step [125/172], Loss: 12.1389\n",
      "Epoch [149/300], Step [126/172], Loss: 10.3281\n",
      "Epoch [149/300], Step [127/172], Loss: 10.7981\n",
      "Epoch [149/300], Step [128/172], Loss: 10.6557\n",
      "Epoch [149/300], Step [129/172], Loss: 7.9758\n",
      "Epoch [149/300], Step [130/172], Loss: 11.5955\n",
      "Epoch [149/300], Step [131/172], Loss: 7.5103\n",
      "Epoch [149/300], Step [132/172], Loss: 8.2290\n",
      "Epoch [149/300], Step [133/172], Loss: 8.9897\n",
      "Epoch [149/300], Step [134/172], Loss: 11.5310\n",
      "Epoch [149/300], Step [135/172], Loss: 8.5253\n",
      "Epoch [149/300], Step [136/172], Loss: 8.0113\n",
      "Epoch [149/300], Step [137/172], Loss: 9.2624\n",
      "Epoch [149/300], Step [138/172], Loss: 7.0371\n",
      "Epoch [149/300], Step [139/172], Loss: 9.2166\n",
      "Epoch [149/300], Step [140/172], Loss: 9.1628\n",
      "Epoch [149/300], Step [141/172], Loss: 10.0533\n",
      "Epoch [149/300], Step [142/172], Loss: 13.5049\n",
      "Epoch [149/300], Step [143/172], Loss: 9.8992\n",
      "Epoch [149/300], Step [144/172], Loss: 8.6295\n",
      "Epoch [149/300], Step [145/172], Loss: 9.3772\n",
      "Epoch [149/300], Step [146/172], Loss: 9.1568\n",
      "Epoch [149/300], Step [147/172], Loss: 5.0130\n",
      "Epoch [149/300], Step [148/172], Loss: 5.9459\n",
      "Epoch [149/300], Step [149/172], Loss: 6.7854\n",
      "Epoch [149/300], Step [150/172], Loss: 6.5121\n",
      "Epoch [149/300], Step [151/172], Loss: 5.7746\n",
      "Epoch [149/300], Step [152/172], Loss: 7.2023\n",
      "Epoch [149/300], Step [153/172], Loss: 6.4867\n",
      "Epoch [149/300], Step [154/172], Loss: 7.4678\n",
      "Epoch [149/300], Step [155/172], Loss: 6.2188\n",
      "Epoch [149/300], Step [156/172], Loss: 12.5276\n",
      "Epoch [149/300], Step [157/172], Loss: 9.3554\n",
      "Epoch [149/300], Step [158/172], Loss: 6.9929\n",
      "Epoch [149/300], Step [159/172], Loss: 8.8988\n",
      "Epoch [149/300], Step [160/172], Loss: 9.9240\n",
      "Epoch [149/300], Step [161/172], Loss: 7.2032\n",
      "Epoch [149/300], Step [162/172], Loss: 5.7731\n",
      "Epoch [149/300], Step [163/172], Loss: 6.3332\n",
      "Epoch [149/300], Step [164/172], Loss: 8.7357\n",
      "Epoch [149/300], Step [165/172], Loss: 5.8023\n",
      "Epoch [149/300], Step [166/172], Loss: 5.3766\n",
      "Epoch [149/300], Step [167/172], Loss: 9.5989\n",
      "Epoch [149/300], Step [168/172], Loss: 6.5902\n",
      "Epoch [149/300], Step [169/172], Loss: 6.5942\n",
      "Epoch [149/300], Step [170/172], Loss: 4.8642\n",
      "Epoch [149/300], Step [171/172], Loss: 7.0619\n",
      "Epoch [149/300], Step [172/172], Loss: 5.0445\n",
      "Epoch [150/300], Step [1/172], Loss: 59.9275\n",
      "Epoch [150/300], Step [2/172], Loss: 60.6537\n",
      "Epoch [150/300], Step [3/172], Loss: 56.1005\n",
      "Epoch [150/300], Step [4/172], Loss: 32.5552\n",
      "Epoch [150/300], Step [5/172], Loss: 57.5650\n",
      "Epoch [150/300], Step [6/172], Loss: 19.2986\n",
      "Epoch [150/300], Step [7/172], Loss: 29.8903\n",
      "Epoch [150/300], Step [8/172], Loss: 4.8315\n",
      "Epoch [150/300], Step [9/172], Loss: 35.6317\n",
      "Epoch [150/300], Step [10/172], Loss: 44.4194\n",
      "Epoch [150/300], Step [11/172], Loss: 66.4948\n",
      "Epoch [150/300], Step [12/172], Loss: 74.8874\n",
      "Epoch [150/300], Step [13/172], Loss: 39.7173\n",
      "Epoch [150/300], Step [14/172], Loss: 69.7382\n",
      "Epoch [150/300], Step [15/172], Loss: 62.7431\n",
      "Epoch [150/300], Step [16/172], Loss: 13.5960\n",
      "Epoch [150/300], Step [17/172], Loss: 50.0850\n",
      "Epoch [150/300], Step [18/172], Loss: 61.9738\n",
      "Epoch [150/300], Step [19/172], Loss: 84.8140\n",
      "Epoch [150/300], Step [20/172], Loss: 46.8575\n",
      "Epoch [150/300], Step [21/172], Loss: 88.9931\n",
      "Epoch [150/300], Step [22/172], Loss: 66.1198\n",
      "Epoch [150/300], Step [23/172], Loss: 2.1603\n",
      "Epoch [150/300], Step [24/172], Loss: 62.0851\n",
      "Epoch [150/300], Step [25/172], Loss: 43.7183\n",
      "Epoch [150/300], Step [26/172], Loss: 52.0769\n",
      "Epoch [150/300], Step [27/172], Loss: 66.9352\n",
      "Epoch [150/300], Step [28/172], Loss: 27.4788\n",
      "Epoch [150/300], Step [29/172], Loss: 19.5794\n",
      "Epoch [150/300], Step [30/172], Loss: 70.2433\n",
      "Epoch [150/300], Step [31/172], Loss: 41.0769\n",
      "Epoch [150/300], Step [32/172], Loss: 43.1390\n",
      "Epoch [150/300], Step [33/172], Loss: 72.8662\n",
      "Epoch [150/300], Step [34/172], Loss: 3.6990\n",
      "Epoch [150/300], Step [35/172], Loss: 13.3771\n",
      "Epoch [150/300], Step [36/172], Loss: 18.4825\n",
      "Epoch [150/300], Step [37/172], Loss: 17.7424\n",
      "Epoch [150/300], Step [38/172], Loss: 29.3774\n",
      "Epoch [150/300], Step [39/172], Loss: 37.5638\n",
      "Epoch [150/300], Step [40/172], Loss: 20.4896\n",
      "Epoch [150/300], Step [41/172], Loss: 35.9864\n",
      "Epoch [150/300], Step [42/172], Loss: 39.9773\n",
      "Epoch [150/300], Step [43/172], Loss: 27.0422\n",
      "Epoch [150/300], Step [44/172], Loss: 20.3278\n",
      "Epoch [150/300], Step [45/172], Loss: 24.6344\n",
      "Epoch [150/300], Step [46/172], Loss: 18.5141\n",
      "Epoch [150/300], Step [47/172], Loss: 46.6355\n",
      "Epoch [150/300], Step [48/172], Loss: 57.8206\n",
      "Epoch [150/300], Step [49/172], Loss: 20.3483\n",
      "Epoch [150/300], Step [50/172], Loss: 46.6025\n",
      "Epoch [150/300], Step [51/172], Loss: 8.1255\n",
      "Epoch [150/300], Step [52/172], Loss: 18.0268\n",
      "Epoch [150/300], Step [53/172], Loss: 22.1948\n",
      "Epoch [150/300], Step [54/172], Loss: 13.3893\n",
      "Epoch [150/300], Step [55/172], Loss: 13.3840\n",
      "Epoch [150/300], Step [56/172], Loss: 15.6564\n",
      "Epoch [150/300], Step [57/172], Loss: 16.8071\n",
      "Epoch [150/300], Step [58/172], Loss: 14.4651\n",
      "Epoch [150/300], Step [59/172], Loss: 28.1762\n",
      "Epoch [150/300], Step [60/172], Loss: 32.6261\n",
      "Epoch [150/300], Step [61/172], Loss: 6.5409\n",
      "Epoch [150/300], Step [62/172], Loss: 21.2344\n",
      "Epoch [150/300], Step [63/172], Loss: 9.6894\n",
      "Epoch [150/300], Step [64/172], Loss: 9.7476\n",
      "Epoch [150/300], Step [65/172], Loss: 19.6231\n",
      "Epoch [150/300], Step [66/172], Loss: 5.8929\n",
      "Epoch [150/300], Step [67/172], Loss: 24.0185\n",
      "Epoch [150/300], Step [68/172], Loss: 5.3307\n",
      "Epoch [150/300], Step [69/172], Loss: 41.5584\n",
      "Epoch [150/300], Step [70/172], Loss: 46.1088\n",
      "Epoch [150/300], Step [71/172], Loss: 44.8056\n",
      "Epoch [150/300], Step [72/172], Loss: 46.9948\n",
      "Epoch [150/300], Step [73/172], Loss: 54.3940\n",
      "Epoch [150/300], Step [74/172], Loss: 29.0091\n",
      "Epoch [150/300], Step [75/172], Loss: 29.3842\n",
      "Epoch [150/300], Step [76/172], Loss: 31.7504\n",
      "Epoch [150/300], Step [77/172], Loss: 54.6208\n",
      "Epoch [150/300], Step [78/172], Loss: 42.3011\n",
      "Epoch [150/300], Step [79/172], Loss: 41.3785\n",
      "Epoch [150/300], Step [80/172], Loss: 54.1929\n",
      "Epoch [150/300], Step [81/172], Loss: 37.5637\n",
      "Epoch [150/300], Step [82/172], Loss: 37.5990\n",
      "Epoch [150/300], Step [83/172], Loss: 45.3714\n",
      "Epoch [150/300], Step [84/172], Loss: 34.9534\n",
      "Epoch [150/300], Step [85/172], Loss: 40.1281\n",
      "Epoch [150/300], Step [86/172], Loss: 33.4731\n",
      "Epoch [150/300], Step [87/172], Loss: 27.2761\n",
      "Epoch [150/300], Step [88/172], Loss: 27.7307\n",
      "Epoch [150/300], Step [89/172], Loss: 26.4712\n",
      "Epoch [150/300], Step [90/172], Loss: 23.4572\n",
      "Epoch [150/300], Step [91/172], Loss: 27.2668\n",
      "Epoch [150/300], Step [92/172], Loss: 20.9113\n",
      "Epoch [150/300], Step [93/172], Loss: 21.3827\n",
      "Epoch [150/300], Step [94/172], Loss: 29.4118\n",
      "Epoch [150/300], Step [95/172], Loss: 21.5390\n",
      "Epoch [150/300], Step [96/172], Loss: 19.7906\n",
      "Epoch [150/300], Step [97/172], Loss: 27.4195\n",
      "Epoch [150/300], Step [98/172], Loss: 19.6069\n",
      "Epoch [150/300], Step [99/172], Loss: 18.8723\n",
      "Epoch [150/300], Step [100/172], Loss: 15.4027\n",
      "Epoch [150/300], Step [101/172], Loss: 17.8773\n",
      "Epoch [150/300], Step [102/172], Loss: 16.9479\n",
      "Epoch [150/300], Step [103/172], Loss: 12.7889\n",
      "Epoch [150/300], Step [104/172], Loss: 17.4666\n",
      "Epoch [150/300], Step [105/172], Loss: 18.4511\n",
      "Epoch [150/300], Step [106/172], Loss: 15.9559\n",
      "Epoch [150/300], Step [107/172], Loss: 15.2570\n",
      "Epoch [150/300], Step [108/172], Loss: 15.2671\n",
      "Epoch [150/300], Step [109/172], Loss: 15.2291\n",
      "Epoch [150/300], Step [110/172], Loss: 15.6374\n",
      "Epoch [150/300], Step [111/172], Loss: 14.2850\n",
      "Epoch [150/300], Step [112/172], Loss: 17.5843\n",
      "Epoch [150/300], Step [113/172], Loss: 12.1441\n",
      "Epoch [150/300], Step [114/172], Loss: 13.3973\n",
      "Epoch [150/300], Step [115/172], Loss: 19.5269\n",
      "Epoch [150/300], Step [116/172], Loss: 14.3191\n",
      "Epoch [150/300], Step [117/172], Loss: 10.9795\n",
      "Epoch [150/300], Step [118/172], Loss: 14.2391\n",
      "Epoch [150/300], Step [119/172], Loss: 15.3385\n",
      "Epoch [150/300], Step [120/172], Loss: 9.7730\n",
      "Epoch [150/300], Step [121/172], Loss: 9.7202\n",
      "Epoch [150/300], Step [122/172], Loss: 10.2588\n",
      "Epoch [150/300], Step [123/172], Loss: 9.9930\n",
      "Epoch [150/300], Step [124/172], Loss: 7.7929\n",
      "Epoch [150/300], Step [125/172], Loss: 12.1861\n",
      "Epoch [150/300], Step [126/172], Loss: 10.3725\n",
      "Epoch [150/300], Step [127/172], Loss: 10.8127\n",
      "Epoch [150/300], Step [128/172], Loss: 10.6800\n",
      "Epoch [150/300], Step [129/172], Loss: 8.0082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/300], Step [130/172], Loss: 11.6384\n",
      "Epoch [150/300], Step [131/172], Loss: 7.4922\n",
      "Epoch [150/300], Step [132/172], Loss: 8.2531\n",
      "Epoch [150/300], Step [133/172], Loss: 8.9657\n",
      "Epoch [150/300], Step [134/172], Loss: 11.5127\n",
      "Epoch [150/300], Step [135/172], Loss: 8.5356\n",
      "Epoch [150/300], Step [136/172], Loss: 8.0760\n",
      "Epoch [150/300], Step [137/172], Loss: 9.2777\n",
      "Epoch [150/300], Step [138/172], Loss: 7.0412\n",
      "Epoch [150/300], Step [139/172], Loss: 9.2400\n",
      "Epoch [150/300], Step [140/172], Loss: 9.2124\n",
      "Epoch [150/300], Step [141/172], Loss: 10.0013\n",
      "Epoch [150/300], Step [142/172], Loss: 13.5545\n",
      "Epoch [150/300], Step [143/172], Loss: 9.9325\n",
      "Epoch [150/300], Step [144/172], Loss: 8.6507\n",
      "Epoch [150/300], Step [145/172], Loss: 9.4128\n",
      "Epoch [150/300], Step [146/172], Loss: 9.1827\n",
      "Epoch [150/300], Step [147/172], Loss: 5.0254\n",
      "Epoch [150/300], Step [148/172], Loss: 5.9501\n",
      "Epoch [150/300], Step [149/172], Loss: 6.7930\n",
      "Epoch [150/300], Step [150/172], Loss: 6.4996\n",
      "Epoch [150/300], Step [151/172], Loss: 5.7422\n",
      "Epoch [150/300], Step [152/172], Loss: 7.2098\n",
      "Epoch [150/300], Step [153/172], Loss: 6.4885\n",
      "Epoch [150/300], Step [154/172], Loss: 7.4578\n",
      "Epoch [150/300], Step [155/172], Loss: 6.2367\n",
      "Epoch [150/300], Step [156/172], Loss: 12.5727\n",
      "Epoch [150/300], Step [157/172], Loss: 9.3686\n",
      "Epoch [150/300], Step [158/172], Loss: 6.9871\n",
      "Epoch [150/300], Step [159/172], Loss: 8.9963\n",
      "Epoch [150/300], Step [160/172], Loss: 9.8999\n",
      "Epoch [150/300], Step [161/172], Loss: 7.2403\n",
      "Epoch [150/300], Step [162/172], Loss: 5.7690\n",
      "Epoch [150/300], Step [163/172], Loss: 6.3540\n",
      "Epoch [150/300], Step [164/172], Loss: 8.5961\n",
      "Epoch [150/300], Step [165/172], Loss: 5.8080\n",
      "Epoch [150/300], Step [166/172], Loss: 5.3795\n",
      "Epoch [150/300], Step [167/172], Loss: 9.6482\n",
      "Epoch [150/300], Step [168/172], Loss: 6.6161\n",
      "Epoch [150/300], Step [169/172], Loss: 6.5916\n",
      "Epoch [150/300], Step [170/172], Loss: 4.8542\n",
      "Epoch [150/300], Step [171/172], Loss: 7.1060\n",
      "Epoch [150/300], Step [172/172], Loss: 5.0572\n",
      "Epoch [151/300], Step [1/172], Loss: 59.6363\n",
      "Epoch [151/300], Step [2/172], Loss: 60.2618\n",
      "Epoch [151/300], Step [3/172], Loss: 55.5640\n",
      "Epoch [151/300], Step [4/172], Loss: 32.4111\n",
      "Epoch [151/300], Step [5/172], Loss: 56.8208\n",
      "Epoch [151/300], Step [6/172], Loss: 19.3330\n",
      "Epoch [151/300], Step [7/172], Loss: 29.9934\n",
      "Epoch [151/300], Step [8/172], Loss: 5.5833\n",
      "Epoch [151/300], Step [9/172], Loss: 35.7246\n",
      "Epoch [151/300], Step [10/172], Loss: 44.3557\n",
      "Epoch [151/300], Step [11/172], Loss: 66.2937\n",
      "Epoch [151/300], Step [12/172], Loss: 74.7280\n",
      "Epoch [151/300], Step [13/172], Loss: 40.0087\n",
      "Epoch [151/300], Step [14/172], Loss: 69.8878\n",
      "Epoch [151/300], Step [15/172], Loss: 62.5750\n",
      "Epoch [151/300], Step [16/172], Loss: 12.7206\n",
      "Epoch [151/300], Step [17/172], Loss: 50.1418\n",
      "Epoch [151/300], Step [18/172], Loss: 61.9421\n",
      "Epoch [151/300], Step [19/172], Loss: 84.5918\n",
      "Epoch [151/300], Step [20/172], Loss: 46.6504\n",
      "Epoch [151/300], Step [21/172], Loss: 88.4380\n",
      "Epoch [151/300], Step [22/172], Loss: 65.8313\n",
      "Epoch [151/300], Step [23/172], Loss: 2.0213\n",
      "Epoch [151/300], Step [24/172], Loss: 61.7933\n",
      "Epoch [151/300], Step [25/172], Loss: 43.6600\n",
      "Epoch [151/300], Step [26/172], Loss: 51.8836\n",
      "Epoch [151/300], Step [27/172], Loss: 66.4341\n",
      "Epoch [151/300], Step [28/172], Loss: 27.3418\n",
      "Epoch [151/300], Step [29/172], Loss: 19.5857\n",
      "Epoch [151/300], Step [30/172], Loss: 69.9100\n",
      "Epoch [151/300], Step [31/172], Loss: 40.9361\n",
      "Epoch [151/300], Step [32/172], Loss: 43.1780\n",
      "Epoch [151/300], Step [33/172], Loss: 72.7566\n",
      "Epoch [151/300], Step [34/172], Loss: 3.8015\n",
      "Epoch [151/300], Step [35/172], Loss: 13.3198\n",
      "Epoch [151/300], Step [36/172], Loss: 18.5930\n",
      "Epoch [151/300], Step [37/172], Loss: 17.7459\n",
      "Epoch [151/300], Step [38/172], Loss: 29.3910\n",
      "Epoch [151/300], Step [39/172], Loss: 37.6037\n",
      "Epoch [151/300], Step [40/172], Loss: 20.5747\n",
      "Epoch [151/300], Step [41/172], Loss: 35.9634\n",
      "Epoch [151/300], Step [42/172], Loss: 39.9989\n",
      "Epoch [151/300], Step [43/172], Loss: 27.2048\n",
      "Epoch [151/300], Step [44/172], Loss: 20.4209\n",
      "Epoch [151/300], Step [45/172], Loss: 24.7772\n",
      "Epoch [151/300], Step [46/172], Loss: 18.5994\n",
      "Epoch [151/300], Step [47/172], Loss: 46.8751\n",
      "Epoch [151/300], Step [48/172], Loss: 58.1204\n",
      "Epoch [151/300], Step [49/172], Loss: 20.5366\n",
      "Epoch [151/300], Step [50/172], Loss: 46.6289\n",
      "Epoch [151/300], Step [51/172], Loss: 8.2845\n",
      "Epoch [151/300], Step [52/172], Loss: 18.1577\n",
      "Epoch [151/300], Step [53/172], Loss: 22.2661\n",
      "Epoch [151/300], Step [54/172], Loss: 13.4982\n",
      "Epoch [151/300], Step [55/172], Loss: 13.5171\n",
      "Epoch [151/300], Step [56/172], Loss: 15.6347\n",
      "Epoch [151/300], Step [57/172], Loss: 17.0130\n",
      "Epoch [151/300], Step [58/172], Loss: 14.5608\n",
      "Epoch [151/300], Step [59/172], Loss: 28.1265\n",
      "Epoch [151/300], Step [60/172], Loss: 32.6551\n",
      "Epoch [151/300], Step [61/172], Loss: 6.6216\n",
      "Epoch [151/300], Step [62/172], Loss: 21.1872\n",
      "Epoch [151/300], Step [63/172], Loss: 9.9137\n",
      "Epoch [151/300], Step [64/172], Loss: 9.8325\n",
      "Epoch [151/300], Step [65/172], Loss: 19.7483\n",
      "Epoch [151/300], Step [66/172], Loss: 5.9277\n",
      "Epoch [151/300], Step [67/172], Loss: 24.1110\n",
      "Epoch [151/300], Step [68/172], Loss: 5.2777\n",
      "Epoch [151/300], Step [69/172], Loss: 41.4651\n",
      "Epoch [151/300], Step [70/172], Loss: 45.8709\n",
      "Epoch [151/300], Step [71/172], Loss: 44.5833\n",
      "Epoch [151/300], Step [72/172], Loss: 46.8253\n",
      "Epoch [151/300], Step [73/172], Loss: 54.2898\n",
      "Epoch [151/300], Step [74/172], Loss: 28.9465\n",
      "Epoch [151/300], Step [75/172], Loss: 29.2883\n",
      "Epoch [151/300], Step [76/172], Loss: 31.7020\n",
      "Epoch [151/300], Step [77/172], Loss: 54.5955\n",
      "Epoch [151/300], Step [78/172], Loss: 42.4555\n",
      "Epoch [151/300], Step [79/172], Loss: 41.4178\n",
      "Epoch [151/300], Step [80/172], Loss: 54.3723\n",
      "Epoch [151/300], Step [81/172], Loss: 37.6193\n",
      "Epoch [151/300], Step [82/172], Loss: 38.0318\n",
      "Epoch [151/300], Step [83/172], Loss: 45.5296\n",
      "Epoch [151/300], Step [84/172], Loss: 34.9526\n",
      "Epoch [151/300], Step [85/172], Loss: 40.1461\n",
      "Epoch [151/300], Step [86/172], Loss: 33.6130\n",
      "Epoch [151/300], Step [87/172], Loss: 27.2980\n",
      "Epoch [151/300], Step [88/172], Loss: 27.7798\n",
      "Epoch [151/300], Step [89/172], Loss: 26.6106\n",
      "Epoch [151/300], Step [90/172], Loss: 23.4795\n",
      "Epoch [151/300], Step [91/172], Loss: 27.2579\n",
      "Epoch [151/300], Step [92/172], Loss: 20.9066\n",
      "Epoch [151/300], Step [93/172], Loss: 21.4326\n",
      "Epoch [151/300], Step [94/172], Loss: 29.5389\n",
      "Epoch [151/300], Step [95/172], Loss: 21.5713\n",
      "Epoch [151/300], Step [96/172], Loss: 19.8568\n",
      "Epoch [151/300], Step [97/172], Loss: 27.4868\n",
      "Epoch [151/300], Step [98/172], Loss: 19.6974\n",
      "Epoch [151/300], Step [99/172], Loss: 18.9379\n",
      "Epoch [151/300], Step [100/172], Loss: 15.4117\n",
      "Epoch [151/300], Step [101/172], Loss: 17.9516\n",
      "Epoch [151/300], Step [102/172], Loss: 17.1109\n",
      "Epoch [151/300], Step [103/172], Loss: 12.8198\n",
      "Epoch [151/300], Step [104/172], Loss: 17.5153\n",
      "Epoch [151/300], Step [105/172], Loss: 18.6390\n",
      "Epoch [151/300], Step [106/172], Loss: 15.9710\n",
      "Epoch [151/300], Step [107/172], Loss: 15.2179\n",
      "Epoch [151/300], Step [108/172], Loss: 15.2138\n",
      "Epoch [151/300], Step [109/172], Loss: 15.2859\n",
      "Epoch [151/300], Step [110/172], Loss: 15.6560\n",
      "Epoch [151/300], Step [111/172], Loss: 14.3442\n",
      "Epoch [151/300], Step [112/172], Loss: 17.6828\n",
      "Epoch [151/300], Step [113/172], Loss: 12.1057\n",
      "Epoch [151/300], Step [114/172], Loss: 13.4457\n",
      "Epoch [151/300], Step [115/172], Loss: 19.6302\n",
      "Epoch [151/300], Step [116/172], Loss: 14.3797\n",
      "Epoch [151/300], Step [117/172], Loss: 11.0557\n",
      "Epoch [151/300], Step [118/172], Loss: 14.2837\n",
      "Epoch [151/300], Step [119/172], Loss: 15.2426\n",
      "Epoch [151/300], Step [120/172], Loss: 9.8193\n",
      "Epoch [151/300], Step [121/172], Loss: 9.7110\n",
      "Epoch [151/300], Step [122/172], Loss: 10.3006\n",
      "Epoch [151/300], Step [123/172], Loss: 10.0336\n",
      "Epoch [151/300], Step [124/172], Loss: 7.7833\n",
      "Epoch [151/300], Step [125/172], Loss: 12.1180\n",
      "Epoch [151/300], Step [126/172], Loss: 10.3874\n",
      "Epoch [151/300], Step [127/172], Loss: 10.8493\n",
      "Epoch [151/300], Step [128/172], Loss: 10.6716\n",
      "Epoch [151/300], Step [129/172], Loss: 8.0165\n",
      "Epoch [151/300], Step [130/172], Loss: 11.6873\n",
      "Epoch [151/300], Step [131/172], Loss: 7.4981\n",
      "Epoch [151/300], Step [132/172], Loss: 8.2440\n",
      "Epoch [151/300], Step [133/172], Loss: 8.9736\n",
      "Epoch [151/300], Step [134/172], Loss: 11.4677\n",
      "Epoch [151/300], Step [135/172], Loss: 8.4992\n",
      "Epoch [151/300], Step [136/172], Loss: 8.0548\n",
      "Epoch [151/300], Step [137/172], Loss: 9.2952\n",
      "Epoch [151/300], Step [138/172], Loss: 7.0258\n",
      "Epoch [151/300], Step [139/172], Loss: 9.2090\n",
      "Epoch [151/300], Step [140/172], Loss: 9.2161\n",
      "Epoch [151/300], Step [141/172], Loss: 10.0193\n",
      "Epoch [151/300], Step [142/172], Loss: 13.6205\n",
      "Epoch [151/300], Step [143/172], Loss: 9.9644\n",
      "Epoch [151/300], Step [144/172], Loss: 8.6476\n",
      "Epoch [151/300], Step [145/172], Loss: 9.4698\n",
      "Epoch [151/300], Step [146/172], Loss: 9.1971\n",
      "Epoch [151/300], Step [147/172], Loss: 5.0218\n",
      "Epoch [151/300], Step [148/172], Loss: 5.9581\n",
      "Epoch [151/300], Step [149/172], Loss: 6.7886\n",
      "Epoch [151/300], Step [150/172], Loss: 6.4719\n",
      "Epoch [151/300], Step [151/172], Loss: 5.7508\n",
      "Epoch [151/300], Step [152/172], Loss: 7.1553\n",
      "Epoch [151/300], Step [153/172], Loss: 6.4794\n",
      "Epoch [151/300], Step [154/172], Loss: 7.4782\n",
      "Epoch [151/300], Step [155/172], Loss: 6.2234\n",
      "Epoch [151/300], Step [156/172], Loss: 12.5988\n",
      "Epoch [151/300], Step [157/172], Loss: 9.3872\n",
      "Epoch [151/300], Step [158/172], Loss: 7.0001\n",
      "Epoch [151/300], Step [159/172], Loss: 8.9815\n",
      "Epoch [151/300], Step [160/172], Loss: 9.8847\n",
      "Epoch [151/300], Step [161/172], Loss: 7.2807\n",
      "Epoch [151/300], Step [162/172], Loss: 5.7368\n",
      "Epoch [151/300], Step [163/172], Loss: 6.3769\n",
      "Epoch [151/300], Step [164/172], Loss: 8.7261\n",
      "Epoch [151/300], Step [165/172], Loss: 5.8153\n",
      "Epoch [151/300], Step [166/172], Loss: 5.4036\n",
      "Epoch [151/300], Step [167/172], Loss: 9.6617\n",
      "Epoch [151/300], Step [168/172], Loss: 6.6047\n",
      "Epoch [151/300], Step [169/172], Loss: 6.6309\n",
      "Epoch [151/300], Step [170/172], Loss: 4.8544\n",
      "Epoch [151/300], Step [171/172], Loss: 7.1148\n",
      "Epoch [151/300], Step [172/172], Loss: 5.0713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [152/300], Step [1/172], Loss: 59.4183\n",
      "Epoch [152/300], Step [2/172], Loss: 59.9484\n",
      "Epoch [152/300], Step [3/172], Loss: 55.1231\n",
      "Epoch [152/300], Step [4/172], Loss: 32.2596\n",
      "Epoch [152/300], Step [5/172], Loss: 56.6351\n",
      "Epoch [152/300], Step [6/172], Loss: 19.3375\n",
      "Epoch [152/300], Step [7/172], Loss: 29.7652\n",
      "Epoch [152/300], Step [8/172], Loss: 4.9140\n",
      "Epoch [152/300], Step [9/172], Loss: 35.4570\n",
      "Epoch [152/300], Step [10/172], Loss: 44.3996\n",
      "Epoch [152/300], Step [11/172], Loss: 66.0606\n",
      "Epoch [152/300], Step [12/172], Loss: 74.5434\n",
      "Epoch [152/300], Step [13/172], Loss: 39.6715\n",
      "Epoch [152/300], Step [14/172], Loss: 69.1264\n",
      "Epoch [152/300], Step [15/172], Loss: 62.5468\n",
      "Epoch [152/300], Step [16/172], Loss: 13.8072\n",
      "Epoch [152/300], Step [17/172], Loss: 50.0081\n",
      "Epoch [152/300], Step [18/172], Loss: 61.9632\n",
      "Epoch [152/300], Step [19/172], Loss: 85.0212\n",
      "Epoch [152/300], Step [20/172], Loss: 46.1133\n",
      "Epoch [152/300], Step [21/172], Loss: 88.8299\n",
      "Epoch [152/300], Step [22/172], Loss: 65.8528\n",
      "Epoch [152/300], Step [23/172], Loss: 2.1045\n",
      "Epoch [152/300], Step [24/172], Loss: 61.8701\n",
      "Epoch [152/300], Step [25/172], Loss: 43.5604\n",
      "Epoch [152/300], Step [26/172], Loss: 51.8263\n",
      "Epoch [152/300], Step [27/172], Loss: 66.6174\n",
      "Epoch [152/300], Step [28/172], Loss: 27.2408\n",
      "Epoch [152/300], Step [29/172], Loss: 19.3226\n",
      "Epoch [152/300], Step [30/172], Loss: 69.6799\n",
      "Epoch [152/300], Step [31/172], Loss: 40.8035\n",
      "Epoch [152/300], Step [32/172], Loss: 43.1343\n",
      "Epoch [152/300], Step [33/172], Loss: 72.7813\n",
      "Epoch [152/300], Step [34/172], Loss: 3.5427\n",
      "Epoch [152/300], Step [35/172], Loss: 13.2125\n",
      "Epoch [152/300], Step [36/172], Loss: 18.3524\n",
      "Epoch [152/300], Step [37/172], Loss: 17.6574\n",
      "Epoch [152/300], Step [38/172], Loss: 29.3419\n",
      "Epoch [152/300], Step [39/172], Loss: 37.2857\n",
      "Epoch [152/300], Step [40/172], Loss: 20.3644\n",
      "Epoch [152/300], Step [41/172], Loss: 35.6598\n",
      "Epoch [152/300], Step [42/172], Loss: 39.7294\n",
      "Epoch [152/300], Step [43/172], Loss: 27.0083\n",
      "Epoch [152/300], Step [44/172], Loss: 20.2568\n",
      "Epoch [152/300], Step [45/172], Loss: 24.5955\n",
      "Epoch [152/300], Step [46/172], Loss: 18.3188\n",
      "Epoch [152/300], Step [47/172], Loss: 46.5471\n",
      "Epoch [152/300], Step [48/172], Loss: 57.8946\n",
      "Epoch [152/300], Step [49/172], Loss: 20.2147\n",
      "Epoch [152/300], Step [50/172], Loss: 46.4626\n",
      "Epoch [152/300], Step [51/172], Loss: 8.1694\n",
      "Epoch [152/300], Step [52/172], Loss: 18.1004\n",
      "Epoch [152/300], Step [53/172], Loss: 22.1131\n",
      "Epoch [152/300], Step [54/172], Loss: 13.4436\n",
      "Epoch [152/300], Step [55/172], Loss: 13.4531\n",
      "Epoch [152/300], Step [56/172], Loss: 15.9053\n",
      "Epoch [152/300], Step [57/172], Loss: 16.8951\n",
      "Epoch [152/300], Step [58/172], Loss: 14.4037\n",
      "Epoch [152/300], Step [59/172], Loss: 28.1483\n",
      "Epoch [152/300], Step [60/172], Loss: 32.3166\n",
      "Epoch [152/300], Step [61/172], Loss: 6.5358\n",
      "Epoch [152/300], Step [62/172], Loss: 21.2773\n",
      "Epoch [152/300], Step [63/172], Loss: 9.8958\n",
      "Epoch [152/300], Step [64/172], Loss: 9.8908\n",
      "Epoch [152/300], Step [65/172], Loss: 19.7329\n",
      "Epoch [152/300], Step [66/172], Loss: 5.9569\n",
      "Epoch [152/300], Step [67/172], Loss: 24.1133\n",
      "Epoch [152/300], Step [68/172], Loss: 5.4172\n",
      "Epoch [152/300], Step [69/172], Loss: 41.0743\n",
      "Epoch [152/300], Step [70/172], Loss: 45.4842\n",
      "Epoch [152/300], Step [71/172], Loss: 44.3020\n",
      "Epoch [152/300], Step [72/172], Loss: 46.3889\n",
      "Epoch [152/300], Step [73/172], Loss: 53.8909\n",
      "Epoch [152/300], Step [74/172], Loss: 28.6377\n",
      "Epoch [152/300], Step [75/172], Loss: 29.0709\n",
      "Epoch [152/300], Step [76/172], Loss: 31.4665\n",
      "Epoch [152/300], Step [77/172], Loss: 54.2268\n",
      "Epoch [152/300], Step [78/172], Loss: 42.1188\n",
      "Epoch [152/300], Step [79/172], Loss: 41.1894\n",
      "Epoch [152/300], Step [80/172], Loss: 54.1448\n",
      "Epoch [152/300], Step [81/172], Loss: 37.4130\n",
      "Epoch [152/300], Step [82/172], Loss: 37.7380\n",
      "Epoch [152/300], Step [83/172], Loss: 45.3406\n",
      "Epoch [152/300], Step [84/172], Loss: 34.8487\n",
      "Epoch [152/300], Step [85/172], Loss: 40.0327\n",
      "Epoch [152/300], Step [86/172], Loss: 33.4987\n",
      "Epoch [152/300], Step [87/172], Loss: 27.2132\n",
      "Epoch [152/300], Step [88/172], Loss: 27.5849\n",
      "Epoch [152/300], Step [89/172], Loss: 26.5209\n",
      "Epoch [152/300], Step [90/172], Loss: 23.3046\n",
      "Epoch [152/300], Step [91/172], Loss: 27.1932\n",
      "Epoch [152/300], Step [92/172], Loss: 20.8370\n",
      "Epoch [152/300], Step [93/172], Loss: 21.3173\n",
      "Epoch [152/300], Step [94/172], Loss: 29.4426\n",
      "Epoch [152/300], Step [95/172], Loss: 21.4570\n",
      "Epoch [152/300], Step [96/172], Loss: 19.7959\n",
      "Epoch [152/300], Step [97/172], Loss: 27.4497\n",
      "Epoch [152/300], Step [98/172], Loss: 19.6202\n",
      "Epoch [152/300], Step [99/172], Loss: 18.8903\n",
      "Epoch [152/300], Step [100/172], Loss: 15.3700\n",
      "Epoch [152/300], Step [101/172], Loss: 17.9110\n",
      "Epoch [152/300], Step [102/172], Loss: 17.0250\n",
      "Epoch [152/300], Step [103/172], Loss: 12.7704\n",
      "Epoch [152/300], Step [104/172], Loss: 17.5110\n",
      "Epoch [152/300], Step [105/172], Loss: 18.5661\n",
      "Epoch [152/300], Step [106/172], Loss: 15.9245\n",
      "Epoch [152/300], Step [107/172], Loss: 15.2230\n",
      "Epoch [152/300], Step [108/172], Loss: 15.1547\n",
      "Epoch [152/300], Step [109/172], Loss: 15.1711\n",
      "Epoch [152/300], Step [110/172], Loss: 15.5943\n",
      "Epoch [152/300], Step [111/172], Loss: 14.3598\n",
      "Epoch [152/300], Step [112/172], Loss: 17.5664\n",
      "Epoch [152/300], Step [113/172], Loss: 12.0431\n",
      "Epoch [152/300], Step [114/172], Loss: 13.4149\n",
      "Epoch [152/300], Step [115/172], Loss: 19.5268\n",
      "Epoch [152/300], Step [116/172], Loss: 14.3236\n",
      "Epoch [152/300], Step [117/172], Loss: 11.0251\n",
      "Epoch [152/300], Step [118/172], Loss: 14.2450\n",
      "Epoch [152/300], Step [119/172], Loss: 15.2681\n",
      "Epoch [152/300], Step [120/172], Loss: 9.7777\n",
      "Epoch [152/300], Step [121/172], Loss: 9.6914\n",
      "Epoch [152/300], Step [122/172], Loss: 10.2432\n",
      "Epoch [152/300], Step [123/172], Loss: 10.0140\n",
      "Epoch [152/300], Step [124/172], Loss: 7.7527\n",
      "Epoch [152/300], Step [125/172], Loss: 12.1117\n",
      "Epoch [152/300], Step [126/172], Loss: 10.4165\n",
      "Epoch [152/300], Step [127/172], Loss: 10.8216\n",
      "Epoch [152/300], Step [128/172], Loss: 10.6471\n",
      "Epoch [152/300], Step [129/172], Loss: 8.0071\n",
      "Epoch [152/300], Step [130/172], Loss: 11.6846\n",
      "Epoch [152/300], Step [131/172], Loss: 7.4559\n",
      "Epoch [152/300], Step [132/172], Loss: 8.2333\n",
      "Epoch [152/300], Step [133/172], Loss: 8.9384\n",
      "Epoch [152/300], Step [134/172], Loss: 11.4671\n",
      "Epoch [152/300], Step [135/172], Loss: 8.4961\n",
      "Epoch [152/300], Step [136/172], Loss: 8.0510\n",
      "Epoch [152/300], Step [137/172], Loss: 9.2325\n",
      "Epoch [152/300], Step [138/172], Loss: 6.9982\n",
      "Epoch [152/300], Step [139/172], Loss: 9.2175\n",
      "Epoch [152/300], Step [140/172], Loss: 9.1983\n",
      "Epoch [152/300], Step [141/172], Loss: 9.9924\n",
      "Epoch [152/300], Step [142/172], Loss: 13.5854\n",
      "Epoch [152/300], Step [143/172], Loss: 9.9467\n",
      "Epoch [152/300], Step [144/172], Loss: 8.6464\n",
      "Epoch [152/300], Step [145/172], Loss: 9.4438\n",
      "Epoch [152/300], Step [146/172], Loss: 9.1671\n",
      "Epoch [152/300], Step [147/172], Loss: 5.0098\n",
      "Epoch [152/300], Step [148/172], Loss: 5.9351\n",
      "Epoch [152/300], Step [149/172], Loss: 6.7529\n",
      "Epoch [152/300], Step [150/172], Loss: 6.4401\n",
      "Epoch [152/300], Step [151/172], Loss: 5.6997\n",
      "Epoch [152/300], Step [152/172], Loss: 7.1701\n",
      "Epoch [152/300], Step [153/172], Loss: 6.4572\n",
      "Epoch [152/300], Step [154/172], Loss: 7.4195\n",
      "Epoch [152/300], Step [155/172], Loss: 6.1996\n",
      "Epoch [152/300], Step [156/172], Loss: 12.5718\n",
      "Epoch [152/300], Step [157/172], Loss: 9.3238\n",
      "Epoch [152/300], Step [158/172], Loss: 6.9591\n",
      "Epoch [152/300], Step [159/172], Loss: 8.9703\n",
      "Epoch [152/300], Step [160/172], Loss: 9.8475\n",
      "Epoch [152/300], Step [161/172], Loss: 7.2461\n",
      "Epoch [152/300], Step [162/172], Loss: 5.7060\n",
      "Epoch [152/300], Step [163/172], Loss: 6.3571\n",
      "Epoch [152/300], Step [164/172], Loss: 8.6533\n",
      "Epoch [152/300], Step [165/172], Loss: 5.7946\n",
      "Epoch [152/300], Step [166/172], Loss: 5.3628\n",
      "Epoch [152/300], Step [167/172], Loss: 9.6569\n",
      "Epoch [152/300], Step [168/172], Loss: 6.5763\n",
      "Epoch [152/300], Step [169/172], Loss: 6.5828\n",
      "Epoch [152/300], Step [170/172], Loss: 4.8231\n",
      "Epoch [152/300], Step [171/172], Loss: 7.1152\n",
      "Epoch [152/300], Step [172/172], Loss: 5.0346\n",
      "Epoch [153/300], Step [1/172], Loss: 59.2338\n",
      "Epoch [153/300], Step [2/172], Loss: 59.8872\n",
      "Epoch [153/300], Step [3/172], Loss: 54.6936\n",
      "Epoch [153/300], Step [4/172], Loss: 32.0586\n",
      "Epoch [153/300], Step [5/172], Loss: 56.3430\n",
      "Epoch [153/300], Step [6/172], Loss: 19.4566\n",
      "Epoch [153/300], Step [7/172], Loss: 30.2297\n",
      "Epoch [153/300], Step [8/172], Loss: 4.9881\n",
      "Epoch [153/300], Step [9/172], Loss: 35.3786\n",
      "Epoch [153/300], Step [10/172], Loss: 44.3224\n",
      "Epoch [153/300], Step [11/172], Loss: 65.9161\n",
      "Epoch [153/300], Step [12/172], Loss: 74.2617\n",
      "Epoch [153/300], Step [13/172], Loss: 39.6639\n",
      "Epoch [153/300], Step [14/172], Loss: 69.2139\n",
      "Epoch [153/300], Step [15/172], Loss: 62.2155\n",
      "Epoch [153/300], Step [16/172], Loss: 12.9605\n",
      "Epoch [153/300], Step [17/172], Loss: 49.8265\n",
      "Epoch [153/300], Step [18/172], Loss: 61.8714\n",
      "Epoch [153/300], Step [19/172], Loss: 84.6555\n",
      "Epoch [153/300], Step [20/172], Loss: 46.0810\n",
      "Epoch [153/300], Step [21/172], Loss: 88.4845\n",
      "Epoch [153/300], Step [22/172], Loss: 65.5986\n",
      "Epoch [153/300], Step [23/172], Loss: 2.1052\n",
      "Epoch [153/300], Step [24/172], Loss: 61.5064\n",
      "Epoch [153/300], Step [25/172], Loss: 43.3019\n",
      "Epoch [153/300], Step [26/172], Loss: 51.4957\n",
      "Epoch [153/300], Step [27/172], Loss: 66.3815\n",
      "Epoch [153/300], Step [28/172], Loss: 27.3528\n",
      "Epoch [153/300], Step [29/172], Loss: 19.1767\n",
      "Epoch [153/300], Step [30/172], Loss: 69.4146\n",
      "Epoch [153/300], Step [31/172], Loss: 40.7665\n",
      "Epoch [153/300], Step [32/172], Loss: 43.1362\n",
      "Epoch [153/300], Step [33/172], Loss: 72.6013\n",
      "Epoch [153/300], Step [34/172], Loss: 3.6216\n",
      "Epoch [153/300], Step [35/172], Loss: 13.3030\n",
      "Epoch [153/300], Step [36/172], Loss: 18.3716\n",
      "Epoch [153/300], Step [37/172], Loss: 17.6632\n",
      "Epoch [153/300], Step [38/172], Loss: 29.3350\n",
      "Epoch [153/300], Step [39/172], Loss: 37.4552\n",
      "Epoch [153/300], Step [40/172], Loss: 20.4936\n",
      "Epoch [153/300], Step [41/172], Loss: 35.7539\n",
      "Epoch [153/300], Step [42/172], Loss: 39.7301\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [153/300], Step [43/172], Loss: 27.1750\n",
      "Epoch [153/300], Step [44/172], Loss: 20.2647\n",
      "Epoch [153/300], Step [45/172], Loss: 24.8346\n",
      "Epoch [153/300], Step [46/172], Loss: 18.3791\n",
      "Epoch [153/300], Step [47/172], Loss: 46.7234\n",
      "Epoch [153/300], Step [48/172], Loss: 58.2004\n",
      "Epoch [153/300], Step [49/172], Loss: 20.5024\n",
      "Epoch [153/300], Step [50/172], Loss: 46.3563\n",
      "Epoch [153/300], Step [51/172], Loss: 8.2422\n",
      "Epoch [153/300], Step [52/172], Loss: 18.1913\n",
      "Epoch [153/300], Step [53/172], Loss: 22.1556\n",
      "Epoch [153/300], Step [54/172], Loss: 13.6023\n",
      "Epoch [153/300], Step [55/172], Loss: 13.4905\n",
      "Epoch [153/300], Step [56/172], Loss: 15.8586\n",
      "Epoch [153/300], Step [57/172], Loss: 16.8119\n",
      "Epoch [153/300], Step [58/172], Loss: 14.3029\n",
      "Epoch [153/300], Step [59/172], Loss: 28.0176\n",
      "Epoch [153/300], Step [60/172], Loss: 31.9265\n",
      "Epoch [153/300], Step [61/172], Loss: 6.5508\n",
      "Epoch [153/300], Step [62/172], Loss: 21.1058\n",
      "Epoch [153/300], Step [63/172], Loss: 9.9790\n",
      "Epoch [153/300], Step [64/172], Loss: 9.8969\n",
      "Epoch [153/300], Step [65/172], Loss: 19.7792\n",
      "Epoch [153/300], Step [66/172], Loss: 6.0060\n",
      "Epoch [153/300], Step [67/172], Loss: 23.9311\n",
      "Epoch [153/300], Step [68/172], Loss: 5.1634\n",
      "Epoch [153/300], Step [69/172], Loss: 40.6948\n",
      "Epoch [153/300], Step [70/172], Loss: 45.4418\n",
      "Epoch [153/300], Step [71/172], Loss: 44.2424\n",
      "Epoch [153/300], Step [72/172], Loss: 46.2721\n",
      "Epoch [153/300], Step [73/172], Loss: 53.8084\n",
      "Epoch [153/300], Step [74/172], Loss: 28.6568\n",
      "Epoch [153/300], Step [75/172], Loss: 29.1728\n",
      "Epoch [153/300], Step [76/172], Loss: 31.4961\n",
      "Epoch [153/300], Step [77/172], Loss: 54.0830\n",
      "Epoch [153/300], Step [78/172], Loss: 42.0119\n",
      "Epoch [153/300], Step [79/172], Loss: 41.0109\n",
      "Epoch [153/300], Step [80/172], Loss: 53.8238\n",
      "Epoch [153/300], Step [81/172], Loss: 37.3214\n",
      "Epoch [153/300], Step [82/172], Loss: 37.7633\n",
      "Epoch [153/300], Step [83/172], Loss: 45.3415\n",
      "Epoch [153/300], Step [84/172], Loss: 34.7533\n",
      "Epoch [153/300], Step [85/172], Loss: 39.9480\n",
      "Epoch [153/300], Step [86/172], Loss: 33.4712\n",
      "Epoch [153/300], Step [87/172], Loss: 27.1967\n",
      "Epoch [153/300], Step [88/172], Loss: 27.5788\n",
      "Epoch [153/300], Step [89/172], Loss: 26.5885\n",
      "Epoch [153/300], Step [90/172], Loss: 23.3376\n",
      "Epoch [153/300], Step [91/172], Loss: 27.2178\n",
      "Epoch [153/300], Step [92/172], Loss: 20.8662\n",
      "Epoch [153/300], Step [93/172], Loss: 21.3356\n",
      "Epoch [153/300], Step [94/172], Loss: 29.4528\n",
      "Epoch [153/300], Step [95/172], Loss: 21.5402\n",
      "Epoch [153/300], Step [96/172], Loss: 19.8863\n",
      "Epoch [153/300], Step [97/172], Loss: 27.5726\n",
      "Epoch [153/300], Step [98/172], Loss: 19.7016\n",
      "Epoch [153/300], Step [99/172], Loss: 18.9702\n",
      "Epoch [153/300], Step [100/172], Loss: 15.4466\n",
      "Epoch [153/300], Step [101/172], Loss: 18.0514\n",
      "Epoch [153/300], Step [102/172], Loss: 17.1219\n",
      "Epoch [153/300], Step [103/172], Loss: 12.8455\n",
      "Epoch [153/300], Step [104/172], Loss: 17.6080\n",
      "Epoch [153/300], Step [105/172], Loss: 18.6609\n",
      "Epoch [153/300], Step [106/172], Loss: 15.9764\n",
      "Epoch [153/300], Step [107/172], Loss: 15.2900\n",
      "Epoch [153/300], Step [108/172], Loss: 15.1583\n",
      "Epoch [153/300], Step [109/172], Loss: 15.2328\n",
      "Epoch [153/300], Step [110/172], Loss: 15.7063\n",
      "Epoch [153/300], Step [111/172], Loss: 14.4131\n",
      "Epoch [153/300], Step [112/172], Loss: 17.6112\n",
      "Epoch [153/300], Step [113/172], Loss: 12.0294\n",
      "Epoch [153/300], Step [114/172], Loss: 13.4458\n",
      "Epoch [153/300], Step [115/172], Loss: 19.5372\n",
      "Epoch [153/300], Step [116/172], Loss: 14.3960\n",
      "Epoch [153/300], Step [117/172], Loss: 11.0303\n",
      "Epoch [153/300], Step [118/172], Loss: 14.2584\n",
      "Epoch [153/300], Step [119/172], Loss: 15.3226\n",
      "Epoch [153/300], Step [120/172], Loss: 9.8093\n",
      "Epoch [153/300], Step [121/172], Loss: 9.7366\n",
      "Epoch [153/300], Step [122/172], Loss: 10.2877\n",
      "Epoch [153/300], Step [123/172], Loss: 10.0066\n",
      "Epoch [153/300], Step [124/172], Loss: 7.7680\n",
      "Epoch [153/300], Step [125/172], Loss: 12.1266\n",
      "Epoch [153/300], Step [126/172], Loss: 10.4504\n",
      "Epoch [153/300], Step [127/172], Loss: 10.8631\n",
      "Epoch [153/300], Step [128/172], Loss: 10.6670\n",
      "Epoch [153/300], Step [129/172], Loss: 8.0268\n",
      "Epoch [153/300], Step [130/172], Loss: 11.7242\n",
      "Epoch [153/300], Step [131/172], Loss: 7.4705\n",
      "Epoch [153/300], Step [132/172], Loss: 8.2728\n",
      "Epoch [153/300], Step [133/172], Loss: 8.9710\n",
      "Epoch [153/300], Step [134/172], Loss: 11.4810\n",
      "Epoch [153/300], Step [135/172], Loss: 8.5083\n",
      "Epoch [153/300], Step [136/172], Loss: 8.0329\n",
      "Epoch [153/300], Step [137/172], Loss: 9.2746\n",
      "Epoch [153/300], Step [138/172], Loss: 7.0143\n",
      "Epoch [153/300], Step [139/172], Loss: 9.2415\n",
      "Epoch [153/300], Step [140/172], Loss: 9.2321\n",
      "Epoch [153/300], Step [141/172], Loss: 9.9671\n",
      "Epoch [153/300], Step [142/172], Loss: 13.6160\n",
      "Epoch [153/300], Step [143/172], Loss: 9.9981\n",
      "Epoch [153/300], Step [144/172], Loss: 8.6764\n",
      "Epoch [153/300], Step [145/172], Loss: 9.4681\n",
      "Epoch [153/300], Step [146/172], Loss: 9.2159\n",
      "Epoch [153/300], Step [147/172], Loss: 5.0295\n",
      "Epoch [153/300], Step [148/172], Loss: 5.9521\n",
      "Epoch [153/300], Step [149/172], Loss: 6.7620\n",
      "Epoch [153/300], Step [150/172], Loss: 6.4457\n",
      "Epoch [153/300], Step [151/172], Loss: 5.7053\n",
      "Epoch [153/300], Step [152/172], Loss: 7.1832\n",
      "Epoch [153/300], Step [153/172], Loss: 6.4848\n",
      "Epoch [153/300], Step [154/172], Loss: 7.4506\n",
      "Epoch [153/300], Step [155/172], Loss: 6.2119\n",
      "Epoch [153/300], Step [156/172], Loss: 12.6087\n",
      "Epoch [153/300], Step [157/172], Loss: 9.3321\n",
      "Epoch [153/300], Step [158/172], Loss: 6.9732\n",
      "Epoch [153/300], Step [159/172], Loss: 9.0159\n",
      "Epoch [153/300], Step [160/172], Loss: 9.8659\n",
      "Epoch [153/300], Step [161/172], Loss: 7.2741\n",
      "Epoch [153/300], Step [162/172], Loss: 5.7082\n",
      "Epoch [153/300], Step [163/172], Loss: 6.3750\n",
      "Epoch [153/300], Step [164/172], Loss: 8.6806\n",
      "Epoch [153/300], Step [165/172], Loss: 5.8176\n",
      "Epoch [153/300], Step [166/172], Loss: 5.4026\n",
      "Epoch [153/300], Step [167/172], Loss: 9.6945\n",
      "Epoch [153/300], Step [168/172], Loss: 6.6052\n",
      "Epoch [153/300], Step [169/172], Loss: 6.6209\n",
      "Epoch [153/300], Step [170/172], Loss: 4.8423\n",
      "Epoch [153/300], Step [171/172], Loss: 7.1516\n",
      "Epoch [153/300], Step [172/172], Loss: 5.0678\n",
      "Epoch [154/300], Step [1/172], Loss: 58.9993\n",
      "Epoch [154/300], Step [2/172], Loss: 59.6258\n",
      "Epoch [154/300], Step [3/172], Loss: 54.4664\n",
      "Epoch [154/300], Step [4/172], Loss: 31.9078\n",
      "Epoch [154/300], Step [5/172], Loss: 55.9746\n",
      "Epoch [154/300], Step [6/172], Loss: 19.4424\n",
      "Epoch [154/300], Step [7/172], Loss: 30.1080\n",
      "Epoch [154/300], Step [8/172], Loss: 4.8843\n",
      "Epoch [154/300], Step [9/172], Loss: 35.2943\n",
      "Epoch [154/300], Step [10/172], Loss: 44.3653\n",
      "Epoch [154/300], Step [11/172], Loss: 65.6916\n",
      "Epoch [154/300], Step [12/172], Loss: 74.1577\n",
      "Epoch [154/300], Step [13/172], Loss: 39.6019\n",
      "Epoch [154/300], Step [14/172], Loss: 68.8335\n",
      "Epoch [154/300], Step [15/172], Loss: 62.1536\n",
      "Epoch [154/300], Step [16/172], Loss: 13.0814\n",
      "Epoch [154/300], Step [17/172], Loss: 49.8699\n",
      "Epoch [154/300], Step [18/172], Loss: 61.8960\n",
      "Epoch [154/300], Step [19/172], Loss: 85.0574\n",
      "Epoch [154/300], Step [20/172], Loss: 45.4979\n",
      "Epoch [154/300], Step [21/172], Loss: 88.6469\n",
      "Epoch [154/300], Step [22/172], Loss: 65.4909\n",
      "Epoch [154/300], Step [23/172], Loss: 2.0565\n",
      "Epoch [154/300], Step [24/172], Loss: 61.4627\n",
      "Epoch [154/300], Step [25/172], Loss: 43.2891\n",
      "Epoch [154/300], Step [26/172], Loss: 51.3633\n",
      "Epoch [154/300], Step [27/172], Loss: 66.2979\n",
      "Epoch [154/300], Step [28/172], Loss: 27.2250\n",
      "Epoch [154/300], Step [29/172], Loss: 19.0705\n",
      "Epoch [154/300], Step [30/172], Loss: 68.9666\n",
      "Epoch [154/300], Step [31/172], Loss: 40.4589\n",
      "Epoch [154/300], Step [32/172], Loss: 43.0184\n",
      "Epoch [154/300], Step [33/172], Loss: 72.4544\n",
      "Epoch [154/300], Step [34/172], Loss: 3.4873\n",
      "Epoch [154/300], Step [35/172], Loss: 13.2449\n",
      "Epoch [154/300], Step [36/172], Loss: 18.2199\n",
      "Epoch [154/300], Step [37/172], Loss: 17.6376\n",
      "Epoch [154/300], Step [38/172], Loss: 29.2035\n",
      "Epoch [154/300], Step [39/172], Loss: 37.1357\n",
      "Epoch [154/300], Step [40/172], Loss: 20.3323\n",
      "Epoch [154/300], Step [41/172], Loss: 35.4882\n",
      "Epoch [154/300], Step [42/172], Loss: 39.5598\n",
      "Epoch [154/300], Step [43/172], Loss: 27.0511\n",
      "Epoch [154/300], Step [44/172], Loss: 20.1910\n",
      "Epoch [154/300], Step [45/172], Loss: 24.7345\n",
      "Epoch [154/300], Step [46/172], Loss: 18.2188\n",
      "Epoch [154/300], Step [47/172], Loss: 46.4595\n",
      "Epoch [154/300], Step [48/172], Loss: 58.0197\n",
      "Epoch [154/300], Step [49/172], Loss: 20.3497\n",
      "Epoch [154/300], Step [50/172], Loss: 46.4448\n",
      "Epoch [154/300], Step [51/172], Loss: 8.2070\n",
      "Epoch [154/300], Step [52/172], Loss: 18.1356\n",
      "Epoch [154/300], Step [53/172], Loss: 22.0497\n",
      "Epoch [154/300], Step [54/172], Loss: 13.6050\n",
      "Epoch [154/300], Step [55/172], Loss: 13.4670\n",
      "Epoch [154/300], Step [56/172], Loss: 16.1023\n",
      "Epoch [154/300], Step [57/172], Loss: 17.0448\n",
      "Epoch [154/300], Step [58/172], Loss: 14.1619\n",
      "Epoch [154/300], Step [59/172], Loss: 27.9709\n",
      "Epoch [154/300], Step [60/172], Loss: 31.8378\n",
      "Epoch [154/300], Step [61/172], Loss: 6.5494\n",
      "Epoch [154/300], Step [62/172], Loss: 21.1533\n",
      "Epoch [154/300], Step [63/172], Loss: 9.9153\n",
      "Epoch [154/300], Step [64/172], Loss: 9.9371\n",
      "Epoch [154/300], Step [65/172], Loss: 19.8288\n",
      "Epoch [154/300], Step [66/172], Loss: 6.0030\n",
      "Epoch [154/300], Step [67/172], Loss: 23.9932\n",
      "Epoch [154/300], Step [68/172], Loss: 5.3426\n",
      "Epoch [154/300], Step [69/172], Loss: 40.6157\n",
      "Epoch [154/300], Step [70/172], Loss: 45.0916\n",
      "Epoch [154/300], Step [71/172], Loss: 44.0508\n",
      "Epoch [154/300], Step [72/172], Loss: 46.0954\n",
      "Epoch [154/300], Step [73/172], Loss: 53.7619\n",
      "Epoch [154/300], Step [74/172], Loss: 28.5302\n",
      "Epoch [154/300], Step [75/172], Loss: 29.1985\n",
      "Epoch [154/300], Step [76/172], Loss: 31.4425\n",
      "Epoch [154/300], Step [77/172], Loss: 54.0171\n",
      "Epoch [154/300], Step [78/172], Loss: 41.9375\n",
      "Epoch [154/300], Step [79/172], Loss: 41.0731\n",
      "Epoch [154/300], Step [80/172], Loss: 54.0668\n",
      "Epoch [154/300], Step [81/172], Loss: 37.3368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [154/300], Step [82/172], Loss: 37.7256\n",
      "Epoch [154/300], Step [83/172], Loss: 45.3775\n",
      "Epoch [154/300], Step [84/172], Loss: 34.9041\n",
      "Epoch [154/300], Step [85/172], Loss: 40.0990\n",
      "Epoch [154/300], Step [86/172], Loss: 33.6059\n",
      "Epoch [154/300], Step [87/172], Loss: 27.3065\n",
      "Epoch [154/300], Step [88/172], Loss: 27.5430\n",
      "Epoch [154/300], Step [89/172], Loss: 26.7302\n",
      "Epoch [154/300], Step [90/172], Loss: 23.2587\n",
      "Epoch [154/300], Step [91/172], Loss: 27.2884\n",
      "Epoch [154/300], Step [92/172], Loss: 20.9262\n",
      "Epoch [154/300], Step [93/172], Loss: 21.3690\n",
      "Epoch [154/300], Step [94/172], Loss: 29.5353\n",
      "Epoch [154/300], Step [95/172], Loss: 21.4871\n",
      "Epoch [154/300], Step [96/172], Loss: 19.9283\n",
      "Epoch [154/300], Step [97/172], Loss: 27.6465\n",
      "Epoch [154/300], Step [98/172], Loss: 19.7586\n",
      "Epoch [154/300], Step [99/172], Loss: 19.0209\n",
      "Epoch [154/300], Step [100/172], Loss: 15.4956\n",
      "Epoch [154/300], Step [101/172], Loss: 18.0957\n",
      "Epoch [154/300], Step [102/172], Loss: 17.1511\n",
      "Epoch [154/300], Step [103/172], Loss: 12.8263\n",
      "Epoch [154/300], Step [104/172], Loss: 17.6367\n",
      "Epoch [154/300], Step [105/172], Loss: 18.6589\n",
      "Epoch [154/300], Step [106/172], Loss: 15.9612\n",
      "Epoch [154/300], Step [107/172], Loss: 15.3463\n",
      "Epoch [154/300], Step [108/172], Loss: 15.1280\n",
      "Epoch [154/300], Step [109/172], Loss: 15.1774\n",
      "Epoch [154/300], Step [110/172], Loss: 15.6653\n",
      "Epoch [154/300], Step [111/172], Loss: 14.4632\n",
      "Epoch [154/300], Step [112/172], Loss: 17.5271\n",
      "Epoch [154/300], Step [113/172], Loss: 11.9608\n",
      "Epoch [154/300], Step [114/172], Loss: 13.4264\n",
      "Epoch [154/300], Step [115/172], Loss: 19.4620\n",
      "Epoch [154/300], Step [116/172], Loss: 14.3696\n",
      "Epoch [154/300], Step [117/172], Loss: 11.0294\n",
      "Epoch [154/300], Step [118/172], Loss: 14.2816\n",
      "Epoch [154/300], Step [119/172], Loss: 15.3314\n",
      "Epoch [154/300], Step [120/172], Loss: 9.7744\n",
      "Epoch [154/300], Step [121/172], Loss: 9.7212\n",
      "Epoch [154/300], Step [122/172], Loss: 10.2965\n",
      "Epoch [154/300], Step [123/172], Loss: 9.9608\n",
      "Epoch [154/300], Step [124/172], Loss: 7.7427\n",
      "Epoch [154/300], Step [125/172], Loss: 12.1049\n",
      "Epoch [154/300], Step [126/172], Loss: 10.4679\n",
      "Epoch [154/300], Step [127/172], Loss: 10.8323\n",
      "Epoch [154/300], Step [128/172], Loss: 10.6200\n",
      "Epoch [154/300], Step [129/172], Loss: 8.0116\n",
      "Epoch [154/300], Step [130/172], Loss: 11.7510\n",
      "Epoch [154/300], Step [131/172], Loss: 7.4515\n",
      "Epoch [154/300], Step [132/172], Loss: 8.2578\n",
      "Epoch [154/300], Step [133/172], Loss: 8.9627\n",
      "Epoch [154/300], Step [134/172], Loss: 11.4761\n",
      "Epoch [154/300], Step [135/172], Loss: 8.5117\n",
      "Epoch [154/300], Step [136/172], Loss: 8.0572\n",
      "Epoch [154/300], Step [137/172], Loss: 9.2566\n",
      "Epoch [154/300], Step [138/172], Loss: 7.0026\n",
      "Epoch [154/300], Step [139/172], Loss: 9.2421\n",
      "Epoch [154/300], Step [140/172], Loss: 9.2249\n",
      "Epoch [154/300], Step [141/172], Loss: 9.9670\n",
      "Epoch [154/300], Step [142/172], Loss: 13.6040\n",
      "Epoch [154/300], Step [143/172], Loss: 10.0175\n",
      "Epoch [154/300], Step [144/172], Loss: 8.6925\n",
      "Epoch [154/300], Step [145/172], Loss: 9.4548\n",
      "Epoch [154/300], Step [146/172], Loss: 9.2041\n",
      "Epoch [154/300], Step [147/172], Loss: 5.0304\n",
      "Epoch [154/300], Step [148/172], Loss: 5.9425\n",
      "Epoch [154/300], Step [149/172], Loss: 6.7539\n",
      "Epoch [154/300], Step [150/172], Loss: 6.4224\n",
      "Epoch [154/300], Step [151/172], Loss: 5.6551\n",
      "Epoch [154/300], Step [152/172], Loss: 7.1926\n",
      "Epoch [154/300], Step [153/172], Loss: 6.4787\n",
      "Epoch [154/300], Step [154/172], Loss: 7.4372\n",
      "Epoch [154/300], Step [155/172], Loss: 6.2021\n",
      "Epoch [154/300], Step [156/172], Loss: 12.6335\n",
      "Epoch [154/300], Step [157/172], Loss: 9.3187\n",
      "Epoch [154/300], Step [158/172], Loss: 6.9574\n",
      "Epoch [154/300], Step [159/172], Loss: 8.9734\n",
      "Epoch [154/300], Step [160/172], Loss: 9.8581\n",
      "Epoch [154/300], Step [161/172], Loss: 7.2541\n",
      "Epoch [154/300], Step [162/172], Loss: 5.6855\n",
      "Epoch [154/300], Step [163/172], Loss: 6.3494\n",
      "Epoch [154/300], Step [164/172], Loss: 8.6230\n",
      "Epoch [154/300], Step [165/172], Loss: 5.8151\n",
      "Epoch [154/300], Step [166/172], Loss: 5.3908\n",
      "Epoch [154/300], Step [167/172], Loss: 9.6866\n",
      "Epoch [154/300], Step [168/172], Loss: 6.6008\n",
      "Epoch [154/300], Step [169/172], Loss: 6.6081\n",
      "Epoch [154/300], Step [170/172], Loss: 4.8142\n",
      "Epoch [154/300], Step [171/172], Loss: 7.1473\n",
      "Epoch [154/300], Step [172/172], Loss: 5.0561\n",
      "Epoch [155/300], Step [1/172], Loss: 58.8515\n",
      "Epoch [155/300], Step [2/172], Loss: 59.5890\n",
      "Epoch [155/300], Step [3/172], Loss: 54.2176\n",
      "Epoch [155/300], Step [4/172], Loss: 31.8057\n",
      "Epoch [155/300], Step [5/172], Loss: 55.8008\n",
      "Epoch [155/300], Step [6/172], Loss: 19.3547\n",
      "Epoch [155/300], Step [7/172], Loss: 29.9345\n",
      "Epoch [155/300], Step [8/172], Loss: 5.0000\n",
      "Epoch [155/300], Step [9/172], Loss: 35.1302\n",
      "Epoch [155/300], Step [10/172], Loss: 44.1651\n",
      "Epoch [155/300], Step [11/172], Loss: 65.4997\n",
      "Epoch [155/300], Step [12/172], Loss: 73.6860\n",
      "Epoch [155/300], Step [13/172], Loss: 39.4028\n",
      "Epoch [155/300], Step [14/172], Loss: 68.5339\n",
      "Epoch [155/300], Step [15/172], Loss: 61.8121\n",
      "Epoch [155/300], Step [16/172], Loss: 12.8110\n",
      "Epoch [155/300], Step [17/172], Loss: 49.5359\n",
      "Epoch [155/300], Step [18/172], Loss: 61.7437\n",
      "Epoch [155/300], Step [19/172], Loss: 84.8190\n",
      "Epoch [155/300], Step [20/172], Loss: 45.0402\n",
      "Epoch [155/300], Step [21/172], Loss: 88.4417\n",
      "Epoch [155/300], Step [22/172], Loss: 65.4508\n",
      "Epoch [155/300], Step [23/172], Loss: 2.1097\n",
      "Epoch [155/300], Step [24/172], Loss: 61.2339\n",
      "Epoch [155/300], Step [25/172], Loss: 43.2419\n",
      "Epoch [155/300], Step [26/172], Loss: 51.1483\n",
      "Epoch [155/300], Step [27/172], Loss: 66.0739\n",
      "Epoch [155/300], Step [28/172], Loss: 27.2586\n",
      "Epoch [155/300], Step [29/172], Loss: 18.9303\n",
      "Epoch [155/300], Step [30/172], Loss: 69.0345\n",
      "Epoch [155/300], Step [31/172], Loss: 40.4486\n",
      "Epoch [155/300], Step [32/172], Loss: 43.0669\n",
      "Epoch [155/300], Step [33/172], Loss: 72.4475\n",
      "Epoch [155/300], Step [34/172], Loss: 3.5375\n",
      "Epoch [155/300], Step [35/172], Loss: 13.2601\n",
      "Epoch [155/300], Step [36/172], Loss: 18.2527\n",
      "Epoch [155/300], Step [37/172], Loss: 17.7520\n",
      "Epoch [155/300], Step [38/172], Loss: 29.5259\n",
      "Epoch [155/300], Step [39/172], Loss: 37.1837\n",
      "Epoch [155/300], Step [40/172], Loss: 20.4490\n",
      "Epoch [155/300], Step [41/172], Loss: 35.6109\n",
      "Epoch [155/300], Step [42/172], Loss: 39.7327\n",
      "Epoch [155/300], Step [43/172], Loss: 27.1740\n",
      "Epoch [155/300], Step [44/172], Loss: 20.3507\n",
      "Epoch [155/300], Step [45/172], Loss: 24.8682\n",
      "Epoch [155/300], Step [46/172], Loss: 18.2682\n",
      "Epoch [155/300], Step [47/172], Loss: 46.7318\n",
      "Epoch [155/300], Step [48/172], Loss: 58.2961\n",
      "Epoch [155/300], Step [49/172], Loss: 20.4917\n",
      "Epoch [155/300], Step [50/172], Loss: 46.3737\n",
      "Epoch [155/300], Step [51/172], Loss: 8.2418\n",
      "Epoch [155/300], Step [52/172], Loss: 18.2071\n",
      "Epoch [155/300], Step [53/172], Loss: 22.0859\n",
      "Epoch [155/300], Step [54/172], Loss: 13.6722\n",
      "Epoch [155/300], Step [55/172], Loss: 13.5269\n",
      "Epoch [155/300], Step [56/172], Loss: 16.0141\n",
      "Epoch [155/300], Step [57/172], Loss: 17.0164\n",
      "Epoch [155/300], Step [58/172], Loss: 14.2390\n",
      "Epoch [155/300], Step [59/172], Loss: 28.0561\n",
      "Epoch [155/300], Step [60/172], Loss: 31.6623\n",
      "Epoch [155/300], Step [61/172], Loss: 6.4981\n",
      "Epoch [155/300], Step [62/172], Loss: 21.1564\n",
      "Epoch [155/300], Step [63/172], Loss: 9.9373\n",
      "Epoch [155/300], Step [64/172], Loss: 9.9889\n",
      "Epoch [155/300], Step [65/172], Loss: 19.7801\n",
      "Epoch [155/300], Step [66/172], Loss: 6.0013\n",
      "Epoch [155/300], Step [67/172], Loss: 23.9935\n",
      "Epoch [155/300], Step [68/172], Loss: 5.2198\n",
      "Epoch [155/300], Step [69/172], Loss: 40.2247\n",
      "Epoch [155/300], Step [70/172], Loss: 45.0290\n",
      "Epoch [155/300], Step [71/172], Loss: 43.9030\n",
      "Epoch [155/300], Step [72/172], Loss: 45.9698\n",
      "Epoch [155/300], Step [73/172], Loss: 53.7137\n",
      "Epoch [155/300], Step [74/172], Loss: 28.4624\n",
      "Epoch [155/300], Step [75/172], Loss: 29.0936\n",
      "Epoch [155/300], Step [76/172], Loss: 31.4498\n",
      "Epoch [155/300], Step [77/172], Loss: 53.8996\n",
      "Epoch [155/300], Step [78/172], Loss: 41.8544\n",
      "Epoch [155/300], Step [79/172], Loss: 41.0244\n",
      "Epoch [155/300], Step [80/172], Loss: 53.9651\n",
      "Epoch [155/300], Step [81/172], Loss: 37.1980\n",
      "Epoch [155/300], Step [82/172], Loss: 37.8092\n",
      "Epoch [155/300], Step [83/172], Loss: 45.3179\n",
      "Epoch [155/300], Step [84/172], Loss: 34.8013\n",
      "Epoch [155/300], Step [85/172], Loss: 39.9966\n",
      "Epoch [155/300], Step [86/172], Loss: 33.6002\n",
      "Epoch [155/300], Step [87/172], Loss: 27.2235\n",
      "Epoch [155/300], Step [88/172], Loss: 27.4391\n",
      "Epoch [155/300], Step [89/172], Loss: 26.7542\n",
      "Epoch [155/300], Step [90/172], Loss: 23.2032\n",
      "Epoch [155/300], Step [91/172], Loss: 27.2474\n",
      "Epoch [155/300], Step [92/172], Loss: 20.8409\n",
      "Epoch [155/300], Step [93/172], Loss: 21.3076\n",
      "Epoch [155/300], Step [94/172], Loss: 29.4677\n",
      "Epoch [155/300], Step [95/172], Loss: 21.4652\n",
      "Epoch [155/300], Step [96/172], Loss: 19.8866\n",
      "Epoch [155/300], Step [97/172], Loss: 27.6504\n",
      "Epoch [155/300], Step [98/172], Loss: 19.7307\n",
      "Epoch [155/300], Step [99/172], Loss: 19.0107\n",
      "Epoch [155/300], Step [100/172], Loss: 15.4682\n",
      "Epoch [155/300], Step [101/172], Loss: 18.0786\n",
      "Epoch [155/300], Step [102/172], Loss: 17.1503\n",
      "Epoch [155/300], Step [103/172], Loss: 12.8048\n",
      "Epoch [155/300], Step [104/172], Loss: 17.6595\n",
      "Epoch [155/300], Step [105/172], Loss: 18.6929\n",
      "Epoch [155/300], Step [106/172], Loss: 15.9220\n",
      "Epoch [155/300], Step [107/172], Loss: 15.3401\n",
      "Epoch [155/300], Step [108/172], Loss: 15.0867\n",
      "Epoch [155/300], Step [109/172], Loss: 15.1701\n",
      "Epoch [155/300], Step [110/172], Loss: 15.6946\n",
      "Epoch [155/300], Step [111/172], Loss: 14.4619\n",
      "Epoch [155/300], Step [112/172], Loss: 17.5706\n",
      "Epoch [155/300], Step [113/172], Loss: 11.9258\n",
      "Epoch [155/300], Step [114/172], Loss: 13.4295\n",
      "Epoch [155/300], Step [115/172], Loss: 19.4342\n",
      "Epoch [155/300], Step [116/172], Loss: 14.3817\n",
      "Epoch [155/300], Step [117/172], Loss: 11.0529\n",
      "Epoch [155/300], Step [118/172], Loss: 14.2590\n",
      "Epoch [155/300], Step [119/172], Loss: 15.3401\n",
      "Epoch [155/300], Step [120/172], Loss: 9.7889\n",
      "Epoch [155/300], Step [121/172], Loss: 9.7254\n",
      "Epoch [155/300], Step [122/172], Loss: 10.3389\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [155/300], Step [123/172], Loss: 9.9679\n",
      "Epoch [155/300], Step [124/172], Loss: 7.7103\n",
      "Epoch [155/300], Step [125/172], Loss: 12.0778\n",
      "Epoch [155/300], Step [126/172], Loss: 10.4606\n",
      "Epoch [155/300], Step [127/172], Loss: 10.8222\n",
      "Epoch [155/300], Step [128/172], Loss: 10.5843\n",
      "Epoch [155/300], Step [129/172], Loss: 8.0096\n",
      "Epoch [155/300], Step [130/172], Loss: 11.7668\n",
      "Epoch [155/300], Step [131/172], Loss: 7.4394\n",
      "Epoch [155/300], Step [132/172], Loss: 8.2498\n",
      "Epoch [155/300], Step [133/172], Loss: 8.9667\n",
      "Epoch [155/300], Step [134/172], Loss: 11.4751\n",
      "Epoch [155/300], Step [135/172], Loss: 8.4947\n",
      "Epoch [155/300], Step [136/172], Loss: 7.9973\n",
      "Epoch [155/300], Step [137/172], Loss: 9.2334\n",
      "Epoch [155/300], Step [138/172], Loss: 6.9674\n",
      "Epoch [155/300], Step [139/172], Loss: 9.2198\n",
      "Epoch [155/300], Step [140/172], Loss: 9.2145\n",
      "Epoch [155/300], Step [141/172], Loss: 9.9170\n",
      "Epoch [155/300], Step [142/172], Loss: 13.6367\n",
      "Epoch [155/300], Step [143/172], Loss: 10.0332\n",
      "Epoch [155/300], Step [144/172], Loss: 8.6725\n",
      "Epoch [155/300], Step [145/172], Loss: 9.4586\n",
      "Epoch [155/300], Step [146/172], Loss: 9.1990\n",
      "Epoch [155/300], Step [147/172], Loss: 5.0158\n",
      "Epoch [155/300], Step [148/172], Loss: 5.9265\n",
      "Epoch [155/300], Step [149/172], Loss: 6.7271\n",
      "Epoch [155/300], Step [150/172], Loss: 6.3809\n",
      "Epoch [155/300], Step [151/172], Loss: 5.6399\n",
      "Epoch [155/300], Step [152/172], Loss: 7.1593\n",
      "Epoch [155/300], Step [153/172], Loss: 6.4625\n",
      "Epoch [155/300], Step [154/172], Loss: 7.4223\n",
      "Epoch [155/300], Step [155/172], Loss: 6.1749\n",
      "Epoch [155/300], Step [156/172], Loss: 12.6348\n",
      "Epoch [155/300], Step [157/172], Loss: 9.3133\n",
      "Epoch [155/300], Step [158/172], Loss: 6.9473\n",
      "Epoch [155/300], Step [159/172], Loss: 8.9668\n",
      "Epoch [155/300], Step [160/172], Loss: 9.8517\n",
      "Epoch [155/300], Step [161/172], Loss: 7.2302\n",
      "Epoch [155/300], Step [162/172], Loss: 5.6420\n",
      "Epoch [155/300], Step [163/172], Loss: 6.3451\n",
      "Epoch [155/300], Step [164/172], Loss: 8.6278\n",
      "Epoch [155/300], Step [165/172], Loss: 5.8025\n",
      "Epoch [155/300], Step [166/172], Loss: 5.3687\n",
      "Epoch [155/300], Step [167/172], Loss: 9.6738\n",
      "Epoch [155/300], Step [168/172], Loss: 6.5844\n",
      "Epoch [155/300], Step [169/172], Loss: 6.6009\n",
      "Epoch [155/300], Step [170/172], Loss: 4.8005\n",
      "Epoch [155/300], Step [171/172], Loss: 7.1464\n",
      "Epoch [155/300], Step [172/172], Loss: 5.0535\n",
      "Epoch [156/300], Step [1/172], Loss: 58.6215\n",
      "Epoch [156/300], Step [2/172], Loss: 59.3390\n",
      "Epoch [156/300], Step [3/172], Loss: 54.0148\n",
      "Epoch [156/300], Step [4/172], Loss: 31.6989\n",
      "Epoch [156/300], Step [5/172], Loss: 55.7848\n",
      "Epoch [156/300], Step [6/172], Loss: 19.4032\n",
      "Epoch [156/300], Step [7/172], Loss: 29.8233\n",
      "Epoch [156/300], Step [8/172], Loss: 4.8993\n",
      "Epoch [156/300], Step [9/172], Loss: 35.1373\n",
      "Epoch [156/300], Step [10/172], Loss: 44.2782\n",
      "Epoch [156/300], Step [11/172], Loss: 65.4213\n",
      "Epoch [156/300], Step [12/172], Loss: 73.6467\n",
      "Epoch [156/300], Step [13/172], Loss: 39.3060\n",
      "Epoch [156/300], Step [14/172], Loss: 68.3733\n",
      "Epoch [156/300], Step [15/172], Loss: 61.7084\n",
      "Epoch [156/300], Step [16/172], Loss: 12.9255\n",
      "Epoch [156/300], Step [17/172], Loss: 49.4736\n",
      "Epoch [156/300], Step [18/172], Loss: 61.7548\n",
      "Epoch [156/300], Step [19/172], Loss: 84.8267\n",
      "Epoch [156/300], Step [20/172], Loss: 44.9666\n",
      "Epoch [156/300], Step [21/172], Loss: 88.3793\n",
      "Epoch [156/300], Step [22/172], Loss: 65.2993\n",
      "Epoch [156/300], Step [23/172], Loss: 2.1062\n",
      "Epoch [156/300], Step [24/172], Loss: 61.1087\n",
      "Epoch [156/300], Step [25/172], Loss: 43.2003\n",
      "Epoch [156/300], Step [26/172], Loss: 50.9672\n",
      "Epoch [156/300], Step [27/172], Loss: 65.8879\n",
      "Epoch [156/300], Step [28/172], Loss: 27.0950\n",
      "Epoch [156/300], Step [29/172], Loss: 18.8538\n",
      "Epoch [156/300], Step [30/172], Loss: 68.6417\n",
      "Epoch [156/300], Step [31/172], Loss: 40.4273\n",
      "Epoch [156/300], Step [32/172], Loss: 43.0256\n",
      "Epoch [156/300], Step [33/172], Loss: 72.3829\n",
      "Epoch [156/300], Step [34/172], Loss: 3.4373\n",
      "Epoch [156/300], Step [35/172], Loss: 13.1603\n",
      "Epoch [156/300], Step [36/172], Loss: 18.2052\n",
      "Epoch [156/300], Step [37/172], Loss: 17.7055\n",
      "Epoch [156/300], Step [38/172], Loss: 29.3973\n",
      "Epoch [156/300], Step [39/172], Loss: 36.9776\n",
      "Epoch [156/300], Step [40/172], Loss: 20.3895\n",
      "Epoch [156/300], Step [41/172], Loss: 35.4231\n",
      "Epoch [156/300], Step [42/172], Loss: 39.6433\n",
      "Epoch [156/300], Step [43/172], Loss: 27.1392\n",
      "Epoch [156/300], Step [44/172], Loss: 20.3009\n",
      "Epoch [156/300], Step [45/172], Loss: 24.8810\n",
      "Epoch [156/300], Step [46/172], Loss: 18.0689\n",
      "Epoch [156/300], Step [47/172], Loss: 46.6784\n",
      "Epoch [156/300], Step [48/172], Loss: 58.0350\n",
      "Epoch [156/300], Step [49/172], Loss: 20.4479\n",
      "Epoch [156/300], Step [50/172], Loss: 46.3843\n",
      "Epoch [156/300], Step [51/172], Loss: 8.2484\n",
      "Epoch [156/300], Step [52/172], Loss: 18.2659\n",
      "Epoch [156/300], Step [53/172], Loss: 22.1547\n",
      "Epoch [156/300], Step [54/172], Loss: 13.7444\n",
      "Epoch [156/300], Step [55/172], Loss: 13.5656\n",
      "Epoch [156/300], Step [56/172], Loss: 16.2352\n",
      "Epoch [156/300], Step [57/172], Loss: 17.0388\n",
      "Epoch [156/300], Step [58/172], Loss: 14.0895\n",
      "Epoch [156/300], Step [59/172], Loss: 28.0003\n",
      "Epoch [156/300], Step [60/172], Loss: 31.4571\n",
      "Epoch [156/300], Step [61/172], Loss: 6.6022\n",
      "Epoch [156/300], Step [62/172], Loss: 21.1155\n",
      "Epoch [156/300], Step [63/172], Loss: 9.9863\n",
      "Epoch [156/300], Step [64/172], Loss: 10.0389\n",
      "Epoch [156/300], Step [65/172], Loss: 19.8953\n",
      "Epoch [156/300], Step [66/172], Loss: 6.0855\n",
      "Epoch [156/300], Step [67/172], Loss: 24.1498\n",
      "Epoch [156/300], Step [68/172], Loss: 5.4509\n",
      "Epoch [156/300], Step [69/172], Loss: 40.2405\n",
      "Epoch [156/300], Step [70/172], Loss: 44.6177\n",
      "Epoch [156/300], Step [71/172], Loss: 43.6356\n",
      "Epoch [156/300], Step [72/172], Loss: 45.5540\n",
      "Epoch [156/300], Step [73/172], Loss: 53.3332\n",
      "Epoch [156/300], Step [74/172], Loss: 28.2649\n",
      "Epoch [156/300], Step [75/172], Loss: 28.9383\n",
      "Epoch [156/300], Step [76/172], Loss: 31.2724\n",
      "Epoch [156/300], Step [77/172], Loss: 53.5778\n",
      "Epoch [156/300], Step [78/172], Loss: 41.6000\n",
      "Epoch [156/300], Step [79/172], Loss: 40.7106\n",
      "Epoch [156/300], Step [80/172], Loss: 53.6220\n",
      "Epoch [156/300], Step [81/172], Loss: 37.0068\n",
      "Epoch [156/300], Step [82/172], Loss: 37.6138\n",
      "Epoch [156/300], Step [83/172], Loss: 45.2011\n",
      "Epoch [156/300], Step [84/172], Loss: 34.6832\n",
      "Epoch [156/300], Step [85/172], Loss: 39.9420\n",
      "Epoch [156/300], Step [86/172], Loss: 33.5716\n",
      "Epoch [156/300], Step [87/172], Loss: 27.1963\n",
      "Epoch [156/300], Step [88/172], Loss: 27.3224\n",
      "Epoch [156/300], Step [89/172], Loss: 26.8003\n",
      "Epoch [156/300], Step [90/172], Loss: 23.1067\n",
      "Epoch [156/300], Step [91/172], Loss: 27.2419\n",
      "Epoch [156/300], Step [92/172], Loss: 20.8365\n",
      "Epoch [156/300], Step [93/172], Loss: 21.3377\n",
      "Epoch [156/300], Step [94/172], Loss: 29.4454\n",
      "Epoch [156/300], Step [95/172], Loss: 21.4671\n",
      "Epoch [156/300], Step [96/172], Loss: 19.9730\n",
      "Epoch [156/300], Step [97/172], Loss: 27.7197\n",
      "Epoch [156/300], Step [98/172], Loss: 19.8011\n",
      "Epoch [156/300], Step [99/172], Loss: 19.0671\n",
      "Epoch [156/300], Step [100/172], Loss: 15.5355\n",
      "Epoch [156/300], Step [101/172], Loss: 18.1661\n",
      "Epoch [156/300], Step [102/172], Loss: 17.1904\n",
      "Epoch [156/300], Step [103/172], Loss: 12.8611\n",
      "Epoch [156/300], Step [104/172], Loss: 17.7605\n",
      "Epoch [156/300], Step [105/172], Loss: 18.7964\n",
      "Epoch [156/300], Step [106/172], Loss: 15.9787\n",
      "Epoch [156/300], Step [107/172], Loss: 15.4218\n",
      "Epoch [156/300], Step [108/172], Loss: 15.1462\n",
      "Epoch [156/300], Step [109/172], Loss: 15.1710\n",
      "Epoch [156/300], Step [110/172], Loss: 15.7790\n",
      "Epoch [156/300], Step [111/172], Loss: 14.5993\n",
      "Epoch [156/300], Step [112/172], Loss: 17.6222\n",
      "Epoch [156/300], Step [113/172], Loss: 11.9492\n",
      "Epoch [156/300], Step [114/172], Loss: 13.5100\n",
      "Epoch [156/300], Step [115/172], Loss: 19.4566\n",
      "Epoch [156/300], Step [116/172], Loss: 14.4411\n",
      "Epoch [156/300], Step [117/172], Loss: 11.1208\n",
      "Epoch [156/300], Step [118/172], Loss: 14.2620\n",
      "Epoch [156/300], Step [119/172], Loss: 15.4590\n",
      "Epoch [156/300], Step [120/172], Loss: 9.8108\n",
      "Epoch [156/300], Step [121/172], Loss: 9.7976\n",
      "Epoch [156/300], Step [122/172], Loss: 10.3739\n",
      "Epoch [156/300], Step [123/172], Loss: 10.0043\n",
      "Epoch [156/300], Step [124/172], Loss: 7.7381\n",
      "Epoch [156/300], Step [125/172], Loss: 12.1303\n",
      "Epoch [156/300], Step [126/172], Loss: 10.5302\n",
      "Epoch [156/300], Step [127/172], Loss: 10.8777\n",
      "Epoch [156/300], Step [128/172], Loss: 10.6430\n",
      "Epoch [156/300], Step [129/172], Loss: 8.0537\n",
      "Epoch [156/300], Step [130/172], Loss: 11.8262\n",
      "Epoch [156/300], Step [131/172], Loss: 7.4609\n",
      "Epoch [156/300], Step [132/172], Loss: 8.3011\n",
      "Epoch [156/300], Step [133/172], Loss: 9.0013\n",
      "Epoch [156/300], Step [134/172], Loss: 11.5625\n",
      "Epoch [156/300], Step [135/172], Loss: 8.5391\n",
      "Epoch [156/300], Step [136/172], Loss: 8.0423\n",
      "Epoch [156/300], Step [137/172], Loss: 9.2699\n",
      "Epoch [156/300], Step [138/172], Loss: 6.9970\n",
      "Epoch [156/300], Step [139/172], Loss: 9.2743\n",
      "Epoch [156/300], Step [140/172], Loss: 9.2421\n",
      "Epoch [156/300], Step [141/172], Loss: 9.9442\n",
      "Epoch [156/300], Step [142/172], Loss: 13.7057\n",
      "Epoch [156/300], Step [143/172], Loss: 10.0613\n",
      "Epoch [156/300], Step [144/172], Loss: 8.7281\n",
      "Epoch [156/300], Step [145/172], Loss: 9.5065\n",
      "Epoch [156/300], Step [146/172], Loss: 9.2428\n",
      "Epoch [156/300], Step [147/172], Loss: 5.0394\n",
      "Epoch [156/300], Step [148/172], Loss: 5.9477\n",
      "Epoch [156/300], Step [149/172], Loss: 6.7453\n",
      "Epoch [156/300], Step [150/172], Loss: 6.3987\n",
      "Epoch [156/300], Step [151/172], Loss: 5.6347\n",
      "Epoch [156/300], Step [152/172], Loss: 7.2061\n",
      "Epoch [156/300], Step [153/172], Loss: 6.4803\n",
      "Epoch [156/300], Step [154/172], Loss: 7.4794\n",
      "Epoch [156/300], Step [155/172], Loss: 6.2027\n",
      "Epoch [156/300], Step [156/172], Loss: 12.6868\n",
      "Epoch [156/300], Step [157/172], Loss: 9.3167\n",
      "Epoch [156/300], Step [158/172], Loss: 6.9473\n",
      "Epoch [156/300], Step [159/172], Loss: 8.9730\n",
      "Epoch [156/300], Step [160/172], Loss: 9.8764\n",
      "Epoch [156/300], Step [161/172], Loss: 7.2357\n",
      "Epoch [156/300], Step [162/172], Loss: 5.6391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [156/300], Step [163/172], Loss: 6.3625\n",
      "Epoch [156/300], Step [164/172], Loss: 8.6115\n",
      "Epoch [156/300], Step [165/172], Loss: 5.8265\n",
      "Epoch [156/300], Step [166/172], Loss: 5.4077\n",
      "Epoch [156/300], Step [167/172], Loss: 9.7293\n",
      "Epoch [156/300], Step [168/172], Loss: 6.6248\n",
      "Epoch [156/300], Step [169/172], Loss: 6.6224\n",
      "Epoch [156/300], Step [170/172], Loss: 4.8268\n",
      "Epoch [156/300], Step [171/172], Loss: 7.2255\n",
      "Epoch [156/300], Step [172/172], Loss: 5.0704\n",
      "Epoch [157/300], Step [1/172], Loss: 58.4241\n",
      "Epoch [157/300], Step [2/172], Loss: 59.3787\n",
      "Epoch [157/300], Step [3/172], Loss: 53.8055\n",
      "Epoch [157/300], Step [4/172], Loss: 31.5195\n",
      "Epoch [157/300], Step [5/172], Loss: 55.4930\n",
      "Epoch [157/300], Step [6/172], Loss: 19.3557\n",
      "Epoch [157/300], Step [7/172], Loss: 29.7972\n",
      "Epoch [157/300], Step [8/172], Loss: 4.9650\n",
      "Epoch [157/300], Step [9/172], Loss: 35.0958\n",
      "Epoch [157/300], Step [10/172], Loss: 44.1730\n",
      "Epoch [157/300], Step [11/172], Loss: 65.1196\n",
      "Epoch [157/300], Step [12/172], Loss: 73.3649\n",
      "Epoch [157/300], Step [13/172], Loss: 39.4232\n",
      "Epoch [157/300], Step [14/172], Loss: 68.1619\n",
      "Epoch [157/300], Step [15/172], Loss: 61.3737\n",
      "Epoch [157/300], Step [16/172], Loss: 12.6756\n",
      "Epoch [157/300], Step [17/172], Loss: 49.3977\n",
      "Epoch [157/300], Step [18/172], Loss: 61.6193\n",
      "Epoch [157/300], Step [19/172], Loss: 84.7045\n",
      "Epoch [157/300], Step [20/172], Loss: 44.3785\n",
      "Epoch [157/300], Step [21/172], Loss: 88.0804\n",
      "Epoch [157/300], Step [22/172], Loss: 65.2296\n",
      "Epoch [157/300], Step [23/172], Loss: 2.0454\n",
      "Epoch [157/300], Step [24/172], Loss: 60.9780\n",
      "Epoch [157/300], Step [25/172], Loss: 43.2586\n",
      "Epoch [157/300], Step [26/172], Loss: 50.9986\n",
      "Epoch [157/300], Step [27/172], Loss: 65.7589\n",
      "Epoch [157/300], Step [28/172], Loss: 27.2229\n",
      "Epoch [157/300], Step [29/172], Loss: 18.8849\n",
      "Epoch [157/300], Step [30/172], Loss: 68.5628\n",
      "Epoch [157/300], Step [31/172], Loss: 40.4609\n",
      "Epoch [157/300], Step [32/172], Loss: 43.2206\n",
      "Epoch [157/300], Step [33/172], Loss: 72.5780\n",
      "Epoch [157/300], Step [34/172], Loss: 3.5422\n",
      "Epoch [157/300], Step [35/172], Loss: 13.1800\n",
      "Epoch [157/300], Step [36/172], Loss: 18.3478\n",
      "Epoch [157/300], Step [37/172], Loss: 17.7906\n",
      "Epoch [157/300], Step [38/172], Loss: 29.6630\n",
      "Epoch [157/300], Step [39/172], Loss: 37.1497\n",
      "Epoch [157/300], Step [40/172], Loss: 20.5553\n",
      "Epoch [157/300], Step [41/172], Loss: 35.4690\n",
      "Epoch [157/300], Step [42/172], Loss: 39.8521\n",
      "Epoch [157/300], Step [43/172], Loss: 27.2785\n",
      "Epoch [157/300], Step [44/172], Loss: 20.4962\n",
      "Epoch [157/300], Step [45/172], Loss: 25.1242\n",
      "Epoch [157/300], Step [46/172], Loss: 18.2860\n",
      "Epoch [157/300], Step [47/172], Loss: 46.9325\n",
      "Epoch [157/300], Step [48/172], Loss: 58.7860\n",
      "Epoch [157/300], Step [49/172], Loss: 20.6170\n",
      "Epoch [157/300], Step [50/172], Loss: 46.4711\n",
      "Epoch [157/300], Step [51/172], Loss: 8.3292\n",
      "Epoch [157/300], Step [52/172], Loss: 18.3940\n",
      "Epoch [157/300], Step [53/172], Loss: 22.2291\n",
      "Epoch [157/300], Step [54/172], Loss: 13.8848\n",
      "Epoch [157/300], Step [55/172], Loss: 13.6413\n",
      "Epoch [157/300], Step [56/172], Loss: 16.0644\n",
      "Epoch [157/300], Step [57/172], Loss: 17.1578\n",
      "Epoch [157/300], Step [58/172], Loss: 14.0040\n",
      "Epoch [157/300], Step [59/172], Loss: 27.9858\n",
      "Epoch [157/300], Step [60/172], Loss: 31.3724\n",
      "Epoch [157/300], Step [61/172], Loss: 6.5527\n",
      "Epoch [157/300], Step [62/172], Loss: 21.0466\n",
      "Epoch [157/300], Step [63/172], Loss: 9.9553\n",
      "Epoch [157/300], Step [64/172], Loss: 10.0402\n",
      "Epoch [157/300], Step [65/172], Loss: 19.8597\n",
      "Epoch [157/300], Step [66/172], Loss: 6.0808\n",
      "Epoch [157/300], Step [67/172], Loss: 24.0389\n",
      "Epoch [157/300], Step [68/172], Loss: 5.3456\n",
      "Epoch [157/300], Step [69/172], Loss: 39.8997\n",
      "Epoch [157/300], Step [70/172], Loss: 44.5466\n",
      "Epoch [157/300], Step [71/172], Loss: 43.5275\n",
      "Epoch [157/300], Step [72/172], Loss: 45.5112\n",
      "Epoch [157/300], Step [73/172], Loss: 53.3844\n",
      "Epoch [157/300], Step [74/172], Loss: 28.1780\n",
      "Epoch [157/300], Step [75/172], Loss: 28.8684\n",
      "Epoch [157/300], Step [76/172], Loss: 31.2556\n",
      "Epoch [157/300], Step [77/172], Loss: 53.5302\n",
      "Epoch [157/300], Step [78/172], Loss: 41.5432\n",
      "Epoch [157/300], Step [79/172], Loss: 40.6098\n",
      "Epoch [157/300], Step [80/172], Loss: 53.6418\n",
      "Epoch [157/300], Step [81/172], Loss: 36.8811\n",
      "Epoch [157/300], Step [82/172], Loss: 37.6262\n",
      "Epoch [157/300], Step [83/172], Loss: 45.2414\n",
      "Epoch [157/300], Step [84/172], Loss: 34.6449\n",
      "Epoch [157/300], Step [85/172], Loss: 39.9195\n",
      "Epoch [157/300], Step [86/172], Loss: 33.5380\n",
      "Epoch [157/300], Step [87/172], Loss: 27.1468\n",
      "Epoch [157/300], Step [88/172], Loss: 27.1985\n",
      "Epoch [157/300], Step [89/172], Loss: 26.8179\n",
      "Epoch [157/300], Step [90/172], Loss: 23.0319\n",
      "Epoch [157/300], Step [91/172], Loss: 27.2627\n",
      "Epoch [157/300], Step [92/172], Loss: 20.7818\n",
      "Epoch [157/300], Step [93/172], Loss: 21.2912\n",
      "Epoch [157/300], Step [94/172], Loss: 29.4835\n",
      "Epoch [157/300], Step [95/172], Loss: 21.4114\n",
      "Epoch [157/300], Step [96/172], Loss: 19.9690\n",
      "Epoch [157/300], Step [97/172], Loss: 27.7483\n",
      "Epoch [157/300], Step [98/172], Loss: 19.7982\n",
      "Epoch [157/300], Step [99/172], Loss: 19.0556\n",
      "Epoch [157/300], Step [100/172], Loss: 15.5534\n",
      "Epoch [157/300], Step [101/172], Loss: 18.1770\n",
      "Epoch [157/300], Step [102/172], Loss: 17.2239\n",
      "Epoch [157/300], Step [103/172], Loss: 12.8387\n",
      "Epoch [157/300], Step [104/172], Loss: 17.7756\n",
      "Epoch [157/300], Step [105/172], Loss: 18.8558\n",
      "Epoch [157/300], Step [106/172], Loss: 15.9362\n",
      "Epoch [157/300], Step [107/172], Loss: 15.4421\n",
      "Epoch [157/300], Step [108/172], Loss: 15.1403\n",
      "Epoch [157/300], Step [109/172], Loss: 15.2006\n",
      "Epoch [157/300], Step [110/172], Loss: 15.7691\n",
      "Epoch [157/300], Step [111/172], Loss: 14.6483\n",
      "Epoch [157/300], Step [112/172], Loss: 17.6055\n",
      "Epoch [157/300], Step [113/172], Loss: 11.9376\n",
      "Epoch [157/300], Step [114/172], Loss: 13.5039\n",
      "Epoch [157/300], Step [115/172], Loss: 19.4892\n",
      "Epoch [157/300], Step [116/172], Loss: 14.4253\n",
      "Epoch [157/300], Step [117/172], Loss: 11.1351\n",
      "Epoch [157/300], Step [118/172], Loss: 14.3128\n",
      "Epoch [157/300], Step [119/172], Loss: 15.4466\n",
      "Epoch [157/300], Step [120/172], Loss: 9.8380\n",
      "Epoch [157/300], Step [121/172], Loss: 9.7947\n",
      "Epoch [157/300], Step [122/172], Loss: 10.4292\n",
      "Epoch [157/300], Step [123/172], Loss: 10.0210\n",
      "Epoch [157/300], Step [124/172], Loss: 7.7240\n",
      "Epoch [157/300], Step [125/172], Loss: 12.1033\n",
      "Epoch [157/300], Step [126/172], Loss: 10.5351\n",
      "Epoch [157/300], Step [127/172], Loss: 10.8621\n",
      "Epoch [157/300], Step [128/172], Loss: 10.6061\n",
      "Epoch [157/300], Step [129/172], Loss: 8.0421\n",
      "Epoch [157/300], Step [130/172], Loss: 11.8400\n",
      "Epoch [157/300], Step [131/172], Loss: 7.4569\n",
      "Epoch [157/300], Step [132/172], Loss: 8.2902\n",
      "Epoch [157/300], Step [133/172], Loss: 9.0176\n",
      "Epoch [157/300], Step [134/172], Loss: 11.5522\n",
      "Epoch [157/300], Step [135/172], Loss: 8.5169\n",
      "Epoch [157/300], Step [136/172], Loss: 8.0385\n",
      "Epoch [157/300], Step [137/172], Loss: 9.2403\n",
      "Epoch [157/300], Step [138/172], Loss: 6.9703\n",
      "Epoch [157/300], Step [139/172], Loss: 9.2659\n",
      "Epoch [157/300], Step [140/172], Loss: 9.2362\n",
      "Epoch [157/300], Step [141/172], Loss: 9.9140\n",
      "Epoch [157/300], Step [142/172], Loss: 13.7419\n",
      "Epoch [157/300], Step [143/172], Loss: 10.0688\n",
      "Epoch [157/300], Step [144/172], Loss: 8.7203\n",
      "Epoch [157/300], Step [145/172], Loss: 9.5338\n",
      "Epoch [157/300], Step [146/172], Loss: 9.2315\n",
      "Epoch [157/300], Step [147/172], Loss: 5.0322\n",
      "Epoch [157/300], Step [148/172], Loss: 5.9402\n",
      "Epoch [157/300], Step [149/172], Loss: 6.7236\n",
      "Epoch [157/300], Step [150/172], Loss: 6.3909\n",
      "Epoch [157/300], Step [151/172], Loss: 5.6169\n",
      "Epoch [157/300], Step [152/172], Loss: 7.1919\n",
      "Epoch [157/300], Step [153/172], Loss: 6.4742\n",
      "Epoch [157/300], Step [154/172], Loss: 7.4303\n",
      "Epoch [157/300], Step [155/172], Loss: 6.1941\n",
      "Epoch [157/300], Step [156/172], Loss: 12.7052\n",
      "Epoch [157/300], Step [157/172], Loss: 9.3168\n",
      "Epoch [157/300], Step [158/172], Loss: 6.9475\n",
      "Epoch [157/300], Step [159/172], Loss: 9.0158\n",
      "Epoch [157/300], Step [160/172], Loss: 9.8837\n",
      "Epoch [157/300], Step [161/172], Loss: 7.1676\n",
      "Epoch [157/300], Step [162/172], Loss: 5.6052\n",
      "Epoch [157/300], Step [163/172], Loss: 6.3396\n",
      "Epoch [157/300], Step [164/172], Loss: 8.6028\n",
      "Epoch [157/300], Step [165/172], Loss: 5.8244\n",
      "Epoch [157/300], Step [166/172], Loss: 5.4008\n",
      "Epoch [157/300], Step [167/172], Loss: 9.7529\n",
      "Epoch [157/300], Step [168/172], Loss: 6.6101\n",
      "Epoch [157/300], Step [169/172], Loss: 6.6223\n",
      "Epoch [157/300], Step [170/172], Loss: 4.7989\n",
      "Epoch [157/300], Step [171/172], Loss: 7.2415\n",
      "Epoch [157/300], Step [172/172], Loss: 5.0699\n",
      "Epoch [158/300], Step [1/172], Loss: 58.3010\n",
      "Epoch [158/300], Step [2/172], Loss: 59.1356\n",
      "Epoch [158/300], Step [3/172], Loss: 53.2121\n",
      "Epoch [158/300], Step [4/172], Loss: 31.4347\n",
      "Epoch [158/300], Step [5/172], Loss: 54.9926\n",
      "Epoch [158/300], Step [6/172], Loss: 19.3455\n",
      "Epoch [158/300], Step [7/172], Loss: 29.6089\n",
      "Epoch [158/300], Step [8/172], Loss: 5.0137\n",
      "Epoch [158/300], Step [9/172], Loss: 34.9851\n",
      "Epoch [158/300], Step [10/172], Loss: 43.9929\n",
      "Epoch [158/300], Step [11/172], Loss: 64.7426\n",
      "Epoch [158/300], Step [12/172], Loss: 72.7618\n",
      "Epoch [158/300], Step [13/172], Loss: 39.0734\n",
      "Epoch [158/300], Step [14/172], Loss: 67.7317\n",
      "Epoch [158/300], Step [15/172], Loss: 60.9506\n",
      "Epoch [158/300], Step [16/172], Loss: 12.6638\n",
      "Epoch [158/300], Step [17/172], Loss: 48.8891\n",
      "Epoch [158/300], Step [18/172], Loss: 61.4080\n",
      "Epoch [158/300], Step [19/172], Loss: 84.2265\n",
      "Epoch [158/300], Step [20/172], Loss: 44.2769\n",
      "Epoch [158/300], Step [21/172], Loss: 87.7649\n",
      "Epoch [158/300], Step [22/172], Loss: 65.1503\n",
      "Epoch [158/300], Step [23/172], Loss: 2.1168\n",
      "Epoch [158/300], Step [24/172], Loss: 60.7260\n",
      "Epoch [158/300], Step [25/172], Loss: 43.1980\n",
      "Epoch [158/300], Step [26/172], Loss: 50.8456\n",
      "Epoch [158/300], Step [27/172], Loss: 65.9005\n",
      "Epoch [158/300], Step [28/172], Loss: 27.0684\n",
      "Epoch [158/300], Step [29/172], Loss: 18.8393\n",
      "Epoch [158/300], Step [30/172], Loss: 68.4411\n",
      "Epoch [158/300], Step [31/172], Loss: 40.4321\n",
      "Epoch [158/300], Step [32/172], Loss: 43.2927\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [158/300], Step [33/172], Loss: 72.6870\n",
      "Epoch [158/300], Step [34/172], Loss: 3.4393\n",
      "Epoch [158/300], Step [35/172], Loss: 13.1725\n",
      "Epoch [158/300], Step [36/172], Loss: 18.3274\n",
      "Epoch [158/300], Step [37/172], Loss: 17.8113\n",
      "Epoch [158/300], Step [38/172], Loss: 29.7983\n",
      "Epoch [158/300], Step [39/172], Loss: 37.1222\n",
      "Epoch [158/300], Step [40/172], Loss: 20.5777\n",
      "Epoch [158/300], Step [41/172], Loss: 35.3464\n",
      "Epoch [158/300], Step [42/172], Loss: 39.7722\n",
      "Epoch [158/300], Step [43/172], Loss: 27.3114\n",
      "Epoch [158/300], Step [44/172], Loss: 20.5061\n",
      "Epoch [158/300], Step [45/172], Loss: 25.1774\n",
      "Epoch [158/300], Step [46/172], Loss: 18.2758\n",
      "Epoch [158/300], Step [47/172], Loss: 47.1486\n",
      "Epoch [158/300], Step [48/172], Loss: 58.9198\n",
      "Epoch [158/300], Step [49/172], Loss: 20.4698\n",
      "Epoch [158/300], Step [50/172], Loss: 46.5531\n",
      "Epoch [158/300], Step [51/172], Loss: 8.3494\n",
      "Epoch [158/300], Step [52/172], Loss: 18.5385\n",
      "Epoch [158/300], Step [53/172], Loss: 22.3596\n",
      "Epoch [158/300], Step [54/172], Loss: 13.9841\n",
      "Epoch [158/300], Step [55/172], Loss: 13.7717\n",
      "Epoch [158/300], Step [56/172], Loss: 16.4316\n",
      "Epoch [158/300], Step [57/172], Loss: 17.1328\n",
      "Epoch [158/300], Step [58/172], Loss: 14.1167\n",
      "Epoch [158/300], Step [59/172], Loss: 28.0846\n",
      "Epoch [158/300], Step [60/172], Loss: 30.9696\n",
      "Epoch [158/300], Step [61/172], Loss: 6.6157\n",
      "Epoch [158/300], Step [62/172], Loss: 21.1274\n",
      "Epoch [158/300], Step [63/172], Loss: 10.1114\n",
      "Epoch [158/300], Step [64/172], Loss: 10.1942\n",
      "Epoch [158/300], Step [65/172], Loss: 19.9366\n",
      "Epoch [158/300], Step [66/172], Loss: 6.1767\n",
      "Epoch [158/300], Step [67/172], Loss: 24.1055\n",
      "Epoch [158/300], Step [68/172], Loss: 5.4308\n",
      "Epoch [158/300], Step [69/172], Loss: 39.7201\n",
      "Epoch [158/300], Step [70/172], Loss: 44.0860\n",
      "Epoch [158/300], Step [71/172], Loss: 43.1994\n",
      "Epoch [158/300], Step [72/172], Loss: 45.0665\n",
      "Epoch [158/300], Step [73/172], Loss: 52.9530\n",
      "Epoch [158/300], Step [74/172], Loss: 27.9168\n",
      "Epoch [158/300], Step [75/172], Loss: 28.5547\n",
      "Epoch [158/300], Step [76/172], Loss: 31.0412\n",
      "Epoch [158/300], Step [77/172], Loss: 53.2100\n",
      "Epoch [158/300], Step [78/172], Loss: 41.3518\n",
      "Epoch [158/300], Step [79/172], Loss: 40.3631\n",
      "Epoch [158/300], Step [80/172], Loss: 53.4055\n",
      "Epoch [158/300], Step [81/172], Loss: 36.7357\n",
      "Epoch [158/300], Step [82/172], Loss: 37.5585\n",
      "Epoch [158/300], Step [83/172], Loss: 45.1241\n",
      "Epoch [158/300], Step [84/172], Loss: 34.5693\n",
      "Epoch [158/300], Step [85/172], Loss: 39.8483\n",
      "Epoch [158/300], Step [86/172], Loss: 33.4851\n",
      "Epoch [158/300], Step [87/172], Loss: 27.0862\n",
      "Epoch [158/300], Step [88/172], Loss: 27.1000\n",
      "Epoch [158/300], Step [89/172], Loss: 26.8518\n",
      "Epoch [158/300], Step [90/172], Loss: 22.9763\n",
      "Epoch [158/300], Step [91/172], Loss: 27.2167\n",
      "Epoch [158/300], Step [92/172], Loss: 20.7371\n",
      "Epoch [158/300], Step [93/172], Loss: 21.2495\n",
      "Epoch [158/300], Step [94/172], Loss: 29.4189\n",
      "Epoch [158/300], Step [95/172], Loss: 21.3769\n",
      "Epoch [158/300], Step [96/172], Loss: 19.9461\n",
      "Epoch [158/300], Step [97/172], Loss: 27.7759\n",
      "Epoch [158/300], Step [98/172], Loss: 19.7740\n",
      "Epoch [158/300], Step [99/172], Loss: 19.0483\n",
      "Epoch [158/300], Step [100/172], Loss: 15.5476\n",
      "Epoch [158/300], Step [101/172], Loss: 18.1816\n",
      "Epoch [158/300], Step [102/172], Loss: 17.2554\n",
      "Epoch [158/300], Step [103/172], Loss: 12.8325\n",
      "Epoch [158/300], Step [104/172], Loss: 17.7858\n",
      "Epoch [158/300], Step [105/172], Loss: 18.8996\n",
      "Epoch [158/300], Step [106/172], Loss: 15.9319\n",
      "Epoch [158/300], Step [107/172], Loss: 15.4835\n",
      "Epoch [158/300], Step [108/172], Loss: 15.1198\n",
      "Epoch [158/300], Step [109/172], Loss: 15.2205\n",
      "Epoch [158/300], Step [110/172], Loss: 15.7836\n",
      "Epoch [158/300], Step [111/172], Loss: 14.6957\n",
      "Epoch [158/300], Step [112/172], Loss: 17.5888\n",
      "Epoch [158/300], Step [113/172], Loss: 11.9216\n",
      "Epoch [158/300], Step [114/172], Loss: 13.4858\n",
      "Epoch [158/300], Step [115/172], Loss: 19.5351\n",
      "Epoch [158/300], Step [116/172], Loss: 14.4217\n",
      "Epoch [158/300], Step [117/172], Loss: 11.1647\n",
      "Epoch [158/300], Step [118/172], Loss: 14.3104\n",
      "Epoch [158/300], Step [119/172], Loss: 15.4984\n",
      "Epoch [158/300], Step [120/172], Loss: 9.8481\n",
      "Epoch [158/300], Step [121/172], Loss: 9.7827\n",
      "Epoch [158/300], Step [122/172], Loss: 10.4581\n",
      "Epoch [158/300], Step [123/172], Loss: 10.0365\n",
      "Epoch [158/300], Step [124/172], Loss: 7.6937\n",
      "Epoch [158/300], Step [125/172], Loss: 12.0713\n",
      "Epoch [158/300], Step [126/172], Loss: 10.5425\n",
      "Epoch [158/300], Step [127/172], Loss: 10.8368\n",
      "Epoch [158/300], Step [128/172], Loss: 10.5444\n",
      "Epoch [158/300], Step [129/172], Loss: 8.0166\n",
      "Epoch [158/300], Step [130/172], Loss: 11.8339\n",
      "Epoch [158/300], Step [131/172], Loss: 7.4356\n",
      "Epoch [158/300], Step [132/172], Loss: 8.2717\n",
      "Epoch [158/300], Step [133/172], Loss: 9.0073\n",
      "Epoch [158/300], Step [134/172], Loss: 11.6103\n",
      "Epoch [158/300], Step [135/172], Loss: 8.5412\n",
      "Epoch [158/300], Step [136/172], Loss: 8.0030\n",
      "Epoch [158/300], Step [137/172], Loss: 9.2297\n",
      "Epoch [158/300], Step [138/172], Loss: 6.9512\n",
      "Epoch [158/300], Step [139/172], Loss: 9.2725\n",
      "Epoch [158/300], Step [140/172], Loss: 9.2318\n",
      "Epoch [158/300], Step [141/172], Loss: 9.9074\n",
      "Epoch [158/300], Step [142/172], Loss: 13.7706\n",
      "Epoch [158/300], Step [143/172], Loss: 10.0803\n",
      "Epoch [158/300], Step [144/172], Loss: 8.7287\n",
      "Epoch [158/300], Step [145/172], Loss: 9.5559\n",
      "Epoch [158/300], Step [146/172], Loss: 9.2207\n",
      "Epoch [158/300], Step [147/172], Loss: 5.0268\n",
      "Epoch [158/300], Step [148/172], Loss: 5.9295\n",
      "Epoch [158/300], Step [149/172], Loss: 6.7143\n",
      "Epoch [158/300], Step [150/172], Loss: 6.3553\n",
      "Epoch [158/300], Step [151/172], Loss: 5.6012\n",
      "Epoch [158/300], Step [152/172], Loss: 7.1996\n",
      "Epoch [158/300], Step [153/172], Loss: 6.4568\n",
      "Epoch [158/300], Step [154/172], Loss: 7.4439\n",
      "Epoch [158/300], Step [155/172], Loss: 6.1849\n",
      "Epoch [158/300], Step [156/172], Loss: 12.7296\n",
      "Epoch [158/300], Step [157/172], Loss: 9.2928\n",
      "Epoch [158/300], Step [158/172], Loss: 6.9387\n",
      "Epoch [158/300], Step [159/172], Loss: 8.9665\n",
      "Epoch [158/300], Step [160/172], Loss: 9.8759\n",
      "Epoch [158/300], Step [161/172], Loss: 7.1936\n",
      "Epoch [158/300], Step [162/172], Loss: 5.5835\n",
      "Epoch [158/300], Step [163/172], Loss: 6.3428\n",
      "Epoch [158/300], Step [164/172], Loss: 8.6405\n",
      "Epoch [158/300], Step [165/172], Loss: 5.8226\n",
      "Epoch [158/300], Step [166/172], Loss: 5.3896\n",
      "Epoch [158/300], Step [167/172], Loss: 9.7713\n",
      "Epoch [158/300], Step [168/172], Loss: 6.5678\n",
      "Epoch [158/300], Step [169/172], Loss: 6.6269\n",
      "Epoch [158/300], Step [170/172], Loss: 4.7772\n",
      "Epoch [158/300], Step [171/172], Loss: 7.2481\n",
      "Epoch [158/300], Step [172/172], Loss: 5.0566\n",
      "Epoch [159/300], Step [1/172], Loss: 58.1374\n",
      "Epoch [159/300], Step [2/172], Loss: 59.0274\n",
      "Epoch [159/300], Step [3/172], Loss: 53.1358\n",
      "Epoch [159/300], Step [4/172], Loss: 31.3415\n",
      "Epoch [159/300], Step [5/172], Loss: 54.8929\n",
      "Epoch [159/300], Step [6/172], Loss: 19.1899\n",
      "Epoch [159/300], Step [7/172], Loss: 29.0866\n",
      "Epoch [159/300], Step [8/172], Loss: 4.7892\n",
      "Epoch [159/300], Step [9/172], Loss: 34.9608\n",
      "Epoch [159/300], Step [10/172], Loss: 44.0785\n",
      "Epoch [159/300], Step [11/172], Loss: 64.5974\n",
      "Epoch [159/300], Step [12/172], Loss: 72.6773\n",
      "Epoch [159/300], Step [13/172], Loss: 39.1300\n",
      "Epoch [159/300], Step [14/172], Loss: 67.6452\n",
      "Epoch [159/300], Step [15/172], Loss: 60.6689\n",
      "Epoch [159/300], Step [16/172], Loss: 12.9165\n",
      "Epoch [159/300], Step [17/172], Loss: 48.8731\n",
      "Epoch [159/300], Step [18/172], Loss: 61.3379\n",
      "Epoch [159/300], Step [19/172], Loss: 84.3410\n",
      "Epoch [159/300], Step [20/172], Loss: 43.6111\n",
      "Epoch [159/300], Step [21/172], Loss: 87.6967\n",
      "Epoch [159/300], Step [22/172], Loss: 65.0467\n",
      "Epoch [159/300], Step [23/172], Loss: 2.1392\n",
      "Epoch [159/300], Step [24/172], Loss: 60.5475\n",
      "Epoch [159/300], Step [25/172], Loss: 43.3495\n",
      "Epoch [159/300], Step [26/172], Loss: 50.8232\n",
      "Epoch [159/300], Step [27/172], Loss: 65.5274\n",
      "Epoch [159/300], Step [28/172], Loss: 27.0720\n",
      "Epoch [159/300], Step [29/172], Loss: 18.7598\n",
      "Epoch [159/300], Step [30/172], Loss: 68.2876\n",
      "Epoch [159/300], Step [31/172], Loss: 40.4485\n",
      "Epoch [159/300], Step [32/172], Loss: 43.2267\n",
      "Epoch [159/300], Step [33/172], Loss: 72.4479\n",
      "Epoch [159/300], Step [34/172], Loss: 3.4734\n",
      "Epoch [159/300], Step [35/172], Loss: 13.1837\n",
      "Epoch [159/300], Step [36/172], Loss: 18.1839\n",
      "Epoch [159/300], Step [37/172], Loss: 17.7255\n",
      "Epoch [159/300], Step [38/172], Loss: 29.6818\n",
      "Epoch [159/300], Step [39/172], Loss: 37.0831\n",
      "Epoch [159/300], Step [40/172], Loss: 20.6019\n",
      "Epoch [159/300], Step [41/172], Loss: 35.3690\n",
      "Epoch [159/300], Step [42/172], Loss: 39.6929\n",
      "Epoch [159/300], Step [43/172], Loss: 27.3076\n",
      "Epoch [159/300], Step [44/172], Loss: 20.5388\n",
      "Epoch [159/300], Step [45/172], Loss: 25.2441\n",
      "Epoch [159/300], Step [46/172], Loss: 18.2238\n",
      "Epoch [159/300], Step [47/172], Loss: 47.1083\n",
      "Epoch [159/300], Step [48/172], Loss: 59.0800\n",
      "Epoch [159/300], Step [49/172], Loss: 20.6096\n",
      "Epoch [159/300], Step [50/172], Loss: 46.3224\n",
      "Epoch [159/300], Step [51/172], Loss: 8.3865\n",
      "Epoch [159/300], Step [52/172], Loss: 18.5302\n",
      "Epoch [159/300], Step [53/172], Loss: 22.2986\n",
      "Epoch [159/300], Step [54/172], Loss: 14.1108\n",
      "Epoch [159/300], Step [55/172], Loss: 13.8411\n",
      "Epoch [159/300], Step [56/172], Loss: 16.5840\n",
      "Epoch [159/300], Step [57/172], Loss: 17.2161\n",
      "Epoch [159/300], Step [58/172], Loss: 14.1733\n",
      "Epoch [159/300], Step [59/172], Loss: 28.1639\n",
      "Epoch [159/300], Step [60/172], Loss: 31.0581\n",
      "Epoch [159/300], Step [61/172], Loss: 6.5742\n",
      "Epoch [159/300], Step [62/172], Loss: 21.0587\n",
      "Epoch [159/300], Step [63/172], Loss: 10.0997\n",
      "Epoch [159/300], Step [64/172], Loss: 10.2075\n",
      "Epoch [159/300], Step [65/172], Loss: 19.9884\n",
      "Epoch [159/300], Step [66/172], Loss: 6.1545\n",
      "Epoch [159/300], Step [67/172], Loss: 24.0555\n",
      "Epoch [159/300], Step [68/172], Loss: 5.4473\n",
      "Epoch [159/300], Step [69/172], Loss: 39.5358\n",
      "Epoch [159/300], Step [70/172], Loss: 43.8079\n",
      "Epoch [159/300], Step [71/172], Loss: 43.0711\n",
      "Epoch [159/300], Step [72/172], Loss: 44.9664\n",
      "Epoch [159/300], Step [73/172], Loss: 52.7934\n",
      "Epoch [159/300], Step [74/172], Loss: 27.8356\n",
      "Epoch [159/300], Step [75/172], Loss: 28.5637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [159/300], Step [76/172], Loss: 30.8404\n",
      "Epoch [159/300], Step [77/172], Loss: 52.9765\n",
      "Epoch [159/300], Step [78/172], Loss: 41.2418\n",
      "Epoch [159/300], Step [79/172], Loss: 40.2610\n",
      "Epoch [159/300], Step [80/172], Loss: 53.3097\n",
      "Epoch [159/300], Step [81/172], Loss: 36.7061\n",
      "Epoch [159/300], Step [82/172], Loss: 37.4291\n",
      "Epoch [159/300], Step [83/172], Loss: 45.1611\n",
      "Epoch [159/300], Step [84/172], Loss: 34.5746\n",
      "Epoch [159/300], Step [85/172], Loss: 39.7735\n",
      "Epoch [159/300], Step [86/172], Loss: 33.5529\n",
      "Epoch [159/300], Step [87/172], Loss: 27.1112\n",
      "Epoch [159/300], Step [88/172], Loss: 27.0363\n",
      "Epoch [159/300], Step [89/172], Loss: 26.9148\n",
      "Epoch [159/300], Step [90/172], Loss: 22.8095\n",
      "Epoch [159/300], Step [91/172], Loss: 27.2566\n",
      "Epoch [159/300], Step [92/172], Loss: 20.7400\n",
      "Epoch [159/300], Step [93/172], Loss: 21.2196\n",
      "Epoch [159/300], Step [94/172], Loss: 29.3663\n",
      "Epoch [159/300], Step [95/172], Loss: 21.3764\n",
      "Epoch [159/300], Step [96/172], Loss: 20.0053\n",
      "Epoch [159/300], Step [97/172], Loss: 27.8514\n",
      "Epoch [159/300], Step [98/172], Loss: 19.8151\n",
      "Epoch [159/300], Step [99/172], Loss: 19.0546\n",
      "Epoch [159/300], Step [100/172], Loss: 15.5519\n",
      "Epoch [159/300], Step [101/172], Loss: 18.2175\n",
      "Epoch [159/300], Step [102/172], Loss: 17.1872\n",
      "Epoch [159/300], Step [103/172], Loss: 12.8110\n",
      "Epoch [159/300], Step [104/172], Loss: 17.7978\n",
      "Epoch [159/300], Step [105/172], Loss: 18.8818\n",
      "Epoch [159/300], Step [106/172], Loss: 15.8881\n",
      "Epoch [159/300], Step [107/172], Loss: 15.5014\n",
      "Epoch [159/300], Step [108/172], Loss: 15.0906\n",
      "Epoch [159/300], Step [109/172], Loss: 15.1373\n",
      "Epoch [159/300], Step [110/172], Loss: 15.7486\n",
      "Epoch [159/300], Step [111/172], Loss: 14.7488\n",
      "Epoch [159/300], Step [112/172], Loss: 17.5535\n",
      "Epoch [159/300], Step [113/172], Loss: 11.9057\n",
      "Epoch [159/300], Step [114/172], Loss: 13.4902\n",
      "Epoch [159/300], Step [115/172], Loss: 19.4656\n",
      "Epoch [159/300], Step [116/172], Loss: 14.3964\n",
      "Epoch [159/300], Step [117/172], Loss: 11.1529\n",
      "Epoch [159/300], Step [118/172], Loss: 14.3117\n",
      "Epoch [159/300], Step [119/172], Loss: 15.5341\n",
      "Epoch [159/300], Step [120/172], Loss: 9.8615\n",
      "Epoch [159/300], Step [121/172], Loss: 9.7581\n",
      "Epoch [159/300], Step [122/172], Loss: 10.4768\n",
      "Epoch [159/300], Step [123/172], Loss: 10.0146\n",
      "Epoch [159/300], Step [124/172], Loss: 7.6732\n",
      "Epoch [159/300], Step [125/172], Loss: 12.0506\n",
      "Epoch [159/300], Step [126/172], Loss: 10.5476\n",
      "Epoch [159/300], Step [127/172], Loss: 10.8425\n",
      "Epoch [159/300], Step [128/172], Loss: 10.5222\n",
      "Epoch [159/300], Step [129/172], Loss: 7.9976\n",
      "Epoch [159/300], Step [130/172], Loss: 11.8382\n",
      "Epoch [159/300], Step [131/172], Loss: 7.4309\n",
      "Epoch [159/300], Step [132/172], Loss: 8.2593\n",
      "Epoch [159/300], Step [133/172], Loss: 9.0238\n",
      "Epoch [159/300], Step [134/172], Loss: 11.6631\n",
      "Epoch [159/300], Step [135/172], Loss: 8.5419\n",
      "Epoch [159/300], Step [136/172], Loss: 8.0242\n",
      "Epoch [159/300], Step [137/172], Loss: 9.1977\n",
      "Epoch [159/300], Step [138/172], Loss: 6.9072\n",
      "Epoch [159/300], Step [139/172], Loss: 9.2941\n",
      "Epoch [159/300], Step [140/172], Loss: 9.2310\n",
      "Epoch [159/300], Step [141/172], Loss: 9.8479\n",
      "Epoch [159/300], Step [142/172], Loss: 13.7608\n",
      "Epoch [159/300], Step [143/172], Loss: 10.0793\n",
      "Epoch [159/300], Step [144/172], Loss: 8.7048\n",
      "Epoch [159/300], Step [145/172], Loss: 9.5602\n",
      "Epoch [159/300], Step [146/172], Loss: 9.1972\n",
      "Epoch [159/300], Step [147/172], Loss: 5.0140\n",
      "Epoch [159/300], Step [148/172], Loss: 5.9199\n",
      "Epoch [159/300], Step [149/172], Loss: 6.6826\n",
      "Epoch [159/300], Step [150/172], Loss: 6.3237\n",
      "Epoch [159/300], Step [151/172], Loss: 5.5944\n",
      "Epoch [159/300], Step [152/172], Loss: 7.2193\n",
      "Epoch [159/300], Step [153/172], Loss: 6.4287\n",
      "Epoch [159/300], Step [154/172], Loss: 7.4085\n",
      "Epoch [159/300], Step [155/172], Loss: 6.1506\n",
      "Epoch [159/300], Step [156/172], Loss: 12.7277\n",
      "Epoch [159/300], Step [157/172], Loss: 9.2967\n",
      "Epoch [159/300], Step [158/172], Loss: 6.9272\n",
      "Epoch [159/300], Step [159/172], Loss: 8.9557\n",
      "Epoch [159/300], Step [160/172], Loss: 9.8692\n",
      "Epoch [159/300], Step [161/172], Loss: 7.1384\n",
      "Epoch [159/300], Step [162/172], Loss: 5.5743\n",
      "Epoch [159/300], Step [163/172], Loss: 6.2942\n",
      "Epoch [159/300], Step [164/172], Loss: 8.5878\n",
      "Epoch [159/300], Step [165/172], Loss: 5.8153\n",
      "Epoch [159/300], Step [166/172], Loss: 5.3807\n",
      "Epoch [159/300], Step [167/172], Loss: 9.8265\n",
      "Epoch [159/300], Step [168/172], Loss: 6.5592\n",
      "Epoch [159/300], Step [169/172], Loss: 6.6723\n",
      "Epoch [159/300], Step [170/172], Loss: 4.7597\n",
      "Epoch [159/300], Step [171/172], Loss: 7.2730\n",
      "Epoch [159/300], Step [172/172], Loss: 5.0501\n",
      "Epoch [160/300], Step [1/172], Loss: 57.9608\n",
      "Epoch [160/300], Step [2/172], Loss: 59.0220\n",
      "Epoch [160/300], Step [3/172], Loss: 53.2740\n",
      "Epoch [160/300], Step [4/172], Loss: 31.1920\n",
      "Epoch [160/300], Step [5/172], Loss: 54.6674\n",
      "Epoch [160/300], Step [6/172], Loss: 19.1378\n",
      "Epoch [160/300], Step [7/172], Loss: 29.3386\n",
      "Epoch [160/300], Step [8/172], Loss: 5.0007\n",
      "Epoch [160/300], Step [9/172], Loss: 34.9609\n",
      "Epoch [160/300], Step [10/172], Loss: 43.9726\n",
      "Epoch [160/300], Step [11/172], Loss: 64.4822\n",
      "Epoch [160/300], Step [12/172], Loss: 72.5996\n",
      "Epoch [160/300], Step [13/172], Loss: 39.2018\n",
      "Epoch [160/300], Step [14/172], Loss: 67.8706\n",
      "Epoch [160/300], Step [15/172], Loss: 60.4907\n",
      "Epoch [160/300], Step [16/172], Loss: 12.2972\n",
      "Epoch [160/300], Step [17/172], Loss: 48.7414\n",
      "Epoch [160/300], Step [18/172], Loss: 61.2019\n",
      "Epoch [160/300], Step [19/172], Loss: 84.0803\n",
      "Epoch [160/300], Step [20/172], Loss: 43.1082\n",
      "Epoch [160/300], Step [21/172], Loss: 87.1948\n",
      "Epoch [160/300], Step [22/172], Loss: 64.9642\n",
      "Epoch [160/300], Step [23/172], Loss: 2.1154\n",
      "Epoch [160/300], Step [24/172], Loss: 60.2359\n",
      "Epoch [160/300], Step [25/172], Loss: 43.1297\n",
      "Epoch [160/300], Step [26/172], Loss: 50.6656\n",
      "Epoch [160/300], Step [27/172], Loss: 64.9925\n",
      "Epoch [160/300], Step [28/172], Loss: 27.0042\n",
      "Epoch [160/300], Step [29/172], Loss: 18.8339\n",
      "Epoch [160/300], Step [30/172], Loss: 67.8250\n",
      "Epoch [160/300], Step [31/172], Loss: 40.2962\n",
      "Epoch [160/300], Step [32/172], Loss: 43.1774\n",
      "Epoch [160/300], Step [33/172], Loss: 72.2816\n",
      "Epoch [160/300], Step [34/172], Loss: 3.6699\n",
      "Epoch [160/300], Step [35/172], Loss: 13.2525\n",
      "Epoch [160/300], Step [36/172], Loss: 18.4719\n",
      "Epoch [160/300], Step [37/172], Loss: 17.8445\n",
      "Epoch [160/300], Step [38/172], Loss: 29.8256\n",
      "Epoch [160/300], Step [39/172], Loss: 37.2721\n",
      "Epoch [160/300], Step [40/172], Loss: 20.8340\n",
      "Epoch [160/300], Step [41/172], Loss: 35.5052\n",
      "Epoch [160/300], Step [42/172], Loss: 40.0237\n",
      "Epoch [160/300], Step [43/172], Loss: 27.6844\n",
      "Epoch [160/300], Step [44/172], Loss: 20.7594\n",
      "Epoch [160/300], Step [45/172], Loss: 25.6709\n",
      "Epoch [160/300], Step [46/172], Loss: 18.4904\n",
      "Epoch [160/300], Step [47/172], Loss: 47.4806\n",
      "Epoch [160/300], Step [48/172], Loss: 59.5538\n",
      "Epoch [160/300], Step [49/172], Loss: 20.7981\n",
      "Epoch [160/300], Step [50/172], Loss: 46.5125\n",
      "Epoch [160/300], Step [51/172], Loss: 8.4799\n",
      "Epoch [160/300], Step [52/172], Loss: 18.7737\n",
      "Epoch [160/300], Step [53/172], Loss: 22.5449\n",
      "Epoch [160/300], Step [54/172], Loss: 14.3304\n",
      "Epoch [160/300], Step [55/172], Loss: 14.0084\n",
      "Epoch [160/300], Step [56/172], Loss: 16.2523\n",
      "Epoch [160/300], Step [57/172], Loss: 17.2481\n",
      "Epoch [160/300], Step [58/172], Loss: 14.2158\n",
      "Epoch [160/300], Step [59/172], Loss: 28.1005\n",
      "Epoch [160/300], Step [60/172], Loss: 30.4391\n",
      "Epoch [160/300], Step [61/172], Loss: 6.5815\n",
      "Epoch [160/300], Step [62/172], Loss: 21.0696\n",
      "Epoch [160/300], Step [63/172], Loss: 10.1190\n",
      "Epoch [160/300], Step [64/172], Loss: 10.2344\n",
      "Epoch [160/300], Step [65/172], Loss: 19.9288\n",
      "Epoch [160/300], Step [66/172], Loss: 6.2137\n",
      "Epoch [160/300], Step [67/172], Loss: 24.0082\n",
      "Epoch [160/300], Step [68/172], Loss: 5.3976\n",
      "Epoch [160/300], Step [69/172], Loss: 39.3153\n",
      "Epoch [160/300], Step [70/172], Loss: 43.6844\n",
      "Epoch [160/300], Step [71/172], Loss: 42.9776\n",
      "Epoch [160/300], Step [72/172], Loss: 44.8182\n",
      "Epoch [160/300], Step [73/172], Loss: 52.6790\n",
      "Epoch [160/300], Step [74/172], Loss: 27.7870\n",
      "Epoch [160/300], Step [75/172], Loss: 28.4493\n",
      "Epoch [160/300], Step [76/172], Loss: 30.7852\n",
      "Epoch [160/300], Step [77/172], Loss: 52.8459\n",
      "Epoch [160/300], Step [78/172], Loss: 41.0706\n",
      "Epoch [160/300], Step [79/172], Loss: 40.0159\n",
      "Epoch [160/300], Step [80/172], Loss: 53.1481\n",
      "Epoch [160/300], Step [81/172], Loss: 36.4621\n",
      "Epoch [160/300], Step [82/172], Loss: 37.4395\n",
      "Epoch [160/300], Step [83/172], Loss: 45.2060\n",
      "Epoch [160/300], Step [84/172], Loss: 34.3961\n",
      "Epoch [160/300], Step [85/172], Loss: 39.6303\n",
      "Epoch [160/300], Step [86/172], Loss: 33.4999\n",
      "Epoch [160/300], Step [87/172], Loss: 26.9970\n",
      "Epoch [160/300], Step [88/172], Loss: 26.8943\n",
      "Epoch [160/300], Step [89/172], Loss: 26.8509\n",
      "Epoch [160/300], Step [90/172], Loss: 22.7684\n",
      "Epoch [160/300], Step [91/172], Loss: 27.2088\n",
      "Epoch [160/300], Step [92/172], Loss: 20.6649\n",
      "Epoch [160/300], Step [93/172], Loss: 21.2284\n",
      "Epoch [160/300], Step [94/172], Loss: 29.4509\n",
      "Epoch [160/300], Step [95/172], Loss: 21.3397\n",
      "Epoch [160/300], Step [96/172], Loss: 20.0126\n",
      "Epoch [160/300], Step [97/172], Loss: 27.7912\n",
      "Epoch [160/300], Step [98/172], Loss: 19.8026\n",
      "Epoch [160/300], Step [99/172], Loss: 19.0678\n",
      "Epoch [160/300], Step [100/172], Loss: 15.5357\n",
      "Epoch [160/300], Step [101/172], Loss: 18.2456\n",
      "Epoch [160/300], Step [102/172], Loss: 17.2625\n",
      "Epoch [160/300], Step [103/172], Loss: 12.7984\n",
      "Epoch [160/300], Step [104/172], Loss: 17.8228\n",
      "Epoch [160/300], Step [105/172], Loss: 18.9895\n",
      "Epoch [160/300], Step [106/172], Loss: 15.8741\n",
      "Epoch [160/300], Step [107/172], Loss: 15.5110\n",
      "Epoch [160/300], Step [108/172], Loss: 15.1148\n",
      "Epoch [160/300], Step [109/172], Loss: 15.2515\n",
      "Epoch [160/300], Step [110/172], Loss: 15.7832\n",
      "Epoch [160/300], Step [111/172], Loss: 14.8486\n",
      "Epoch [160/300], Step [112/172], Loss: 17.6627\n",
      "Epoch [160/300], Step [113/172], Loss: 11.9579\n",
      "Epoch [160/300], Step [114/172], Loss: 13.5599\n",
      "Epoch [160/300], Step [115/172], Loss: 19.6162\n",
      "Epoch [160/300], Step [116/172], Loss: 14.4737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/300], Step [117/172], Loss: 11.2199\n",
      "Epoch [160/300], Step [118/172], Loss: 14.3696\n",
      "Epoch [160/300], Step [119/172], Loss: 15.5109\n",
      "Epoch [160/300], Step [120/172], Loss: 9.9463\n",
      "Epoch [160/300], Step [121/172], Loss: 9.7674\n",
      "Epoch [160/300], Step [122/172], Loss: 10.5313\n",
      "Epoch [160/300], Step [123/172], Loss: 10.0950\n",
      "Epoch [160/300], Step [124/172], Loss: 7.6890\n",
      "Epoch [160/300], Step [125/172], Loss: 12.0492\n",
      "Epoch [160/300], Step [126/172], Loss: 10.5865\n",
      "Epoch [160/300], Step [127/172], Loss: 10.8740\n",
      "Epoch [160/300], Step [128/172], Loss: 10.5290\n",
      "Epoch [160/300], Step [129/172], Loss: 8.0019\n",
      "Epoch [160/300], Step [130/172], Loss: 11.8516\n",
      "Epoch [160/300], Step [131/172], Loss: 7.4328\n",
      "Epoch [160/300], Step [132/172], Loss: 8.2590\n",
      "Epoch [160/300], Step [133/172], Loss: 9.0185\n",
      "Epoch [160/300], Step [134/172], Loss: 11.6413\n",
      "Epoch [160/300], Step [135/172], Loss: 8.5091\n",
      "Epoch [160/300], Step [136/172], Loss: 8.0115\n",
      "Epoch [160/300], Step [137/172], Loss: 9.2424\n",
      "Epoch [160/300], Step [138/172], Loss: 6.9154\n",
      "Epoch [160/300], Step [139/172], Loss: 9.2856\n",
      "Epoch [160/300], Step [140/172], Loss: 9.2484\n",
      "Epoch [160/300], Step [141/172], Loss: 9.8464\n",
      "Epoch [160/300], Step [142/172], Loss: 13.8265\n",
      "Epoch [160/300], Step [143/172], Loss: 10.1179\n",
      "Epoch [160/300], Step [144/172], Loss: 8.7217\n",
      "Epoch [160/300], Step [145/172], Loss: 9.6465\n",
      "Epoch [160/300], Step [146/172], Loss: 9.2162\n",
      "Epoch [160/300], Step [147/172], Loss: 5.0320\n",
      "Epoch [160/300], Step [148/172], Loss: 5.9420\n",
      "Epoch [160/300], Step [149/172], Loss: 6.6942\n",
      "Epoch [160/300], Step [150/172], Loss: 6.3179\n",
      "Epoch [160/300], Step [151/172], Loss: 5.6168\n",
      "Epoch [160/300], Step [152/172], Loss: 7.2239\n",
      "Epoch [160/300], Step [153/172], Loss: 6.4398\n",
      "Epoch [160/300], Step [154/172], Loss: 7.4505\n",
      "Epoch [160/300], Step [155/172], Loss: 6.1672\n",
      "Epoch [160/300], Step [156/172], Loss: 12.7094\n",
      "Epoch [160/300], Step [157/172], Loss: 9.2807\n",
      "Epoch [160/300], Step [158/172], Loss: 6.9216\n",
      "Epoch [160/300], Step [159/172], Loss: 9.0044\n",
      "Epoch [160/300], Step [160/172], Loss: 9.8527\n",
      "Epoch [160/300], Step [161/172], Loss: 7.1469\n",
      "Epoch [160/300], Step [162/172], Loss: 5.5641\n",
      "Epoch [160/300], Step [163/172], Loss: 6.3132\n",
      "Epoch [160/300], Step [164/172], Loss: 8.6641\n",
      "Epoch [160/300], Step [165/172], Loss: 5.8386\n",
      "Epoch [160/300], Step [166/172], Loss: 5.4071\n",
      "Epoch [160/300], Step [167/172], Loss: 9.9362\n",
      "Epoch [160/300], Step [168/172], Loss: 6.5706\n",
      "Epoch [160/300], Step [169/172], Loss: 6.7194\n",
      "Epoch [160/300], Step [170/172], Loss: 4.7542\n",
      "Epoch [160/300], Step [171/172], Loss: 7.2936\n",
      "Epoch [160/300], Step [172/172], Loss: 5.0718\n",
      "Epoch [161/300], Step [1/172], Loss: 57.6765\n",
      "Epoch [161/300], Step [2/172], Loss: 58.5402\n",
      "Epoch [161/300], Step [3/172], Loss: 52.7101\n",
      "Epoch [161/300], Step [4/172], Loss: 31.0249\n",
      "Epoch [161/300], Step [5/172], Loss: 54.0996\n",
      "Epoch [161/300], Step [6/172], Loss: 19.2616\n",
      "Epoch [161/300], Step [7/172], Loss: 29.5699\n",
      "Epoch [161/300], Step [8/172], Loss: 4.6843\n",
      "Epoch [161/300], Step [9/172], Loss: 34.7633\n",
      "Epoch [161/300], Step [10/172], Loss: 43.8920\n",
      "Epoch [161/300], Step [11/172], Loss: 64.1933\n",
      "Epoch [161/300], Step [12/172], Loss: 72.2204\n",
      "Epoch [161/300], Step [13/172], Loss: 38.6363\n",
      "Epoch [161/300], Step [14/172], Loss: 66.9290\n",
      "Epoch [161/300], Step [15/172], Loss: 60.1732\n",
      "Epoch [161/300], Step [16/172], Loss: 12.8316\n",
      "Epoch [161/300], Step [17/172], Loss: 48.4209\n",
      "Epoch [161/300], Step [18/172], Loss: 60.9325\n",
      "Epoch [161/300], Step [19/172], Loss: 83.8943\n",
      "Epoch [161/300], Step [20/172], Loss: 42.6550\n",
      "Epoch [161/300], Step [21/172], Loss: 86.9973\n",
      "Epoch [161/300], Step [22/172], Loss: 64.5330\n",
      "Epoch [161/300], Step [23/172], Loss: 1.9426\n",
      "Epoch [161/300], Step [24/172], Loss: 60.1214\n",
      "Epoch [161/300], Step [25/172], Loss: 42.7289\n",
      "Epoch [161/300], Step [26/172], Loss: 50.2997\n",
      "Epoch [161/300], Step [27/172], Loss: 65.0042\n",
      "Epoch [161/300], Step [28/172], Loss: 26.7844\n",
      "Epoch [161/300], Step [29/172], Loss: 18.6103\n",
      "Epoch [161/300], Step [30/172], Loss: 67.6624\n",
      "Epoch [161/300], Step [31/172], Loss: 40.0286\n",
      "Epoch [161/300], Step [32/172], Loss: 43.1455\n",
      "Epoch [161/300], Step [33/172], Loss: 72.1835\n",
      "Epoch [161/300], Step [34/172], Loss: 3.4055\n",
      "Epoch [161/300], Step [35/172], Loss: 13.1245\n",
      "Epoch [161/300], Step [36/172], Loss: 18.4403\n",
      "Epoch [161/300], Step [37/172], Loss: 17.7533\n",
      "Epoch [161/300], Step [38/172], Loss: 29.4904\n",
      "Epoch [161/300], Step [39/172], Loss: 37.0203\n",
      "Epoch [161/300], Step [40/172], Loss: 20.7100\n",
      "Epoch [161/300], Step [41/172], Loss: 35.2186\n",
      "Epoch [161/300], Step [42/172], Loss: 39.5617\n",
      "Epoch [161/300], Step [43/172], Loss: 27.4542\n",
      "Epoch [161/300], Step [44/172], Loss: 20.4566\n",
      "Epoch [161/300], Step [45/172], Loss: 25.5308\n",
      "Epoch [161/300], Step [46/172], Loss: 18.2953\n",
      "Epoch [161/300], Step [47/172], Loss: 47.3234\n",
      "Epoch [161/300], Step [48/172], Loss: 59.0162\n",
      "Epoch [161/300], Step [49/172], Loss: 20.4935\n",
      "Epoch [161/300], Step [50/172], Loss: 46.5732\n",
      "Epoch [161/300], Step [51/172], Loss: 8.4362\n",
      "Epoch [161/300], Step [52/172], Loss: 18.7573\n",
      "Epoch [161/300], Step [53/172], Loss: 22.4907\n",
      "Epoch [161/300], Step [54/172], Loss: 14.1691\n",
      "Epoch [161/300], Step [55/172], Loss: 14.1138\n",
      "Epoch [161/300], Step [56/172], Loss: 16.7859\n",
      "Epoch [161/300], Step [57/172], Loss: 17.1931\n",
      "Epoch [161/300], Step [58/172], Loss: 14.2521\n",
      "Epoch [161/300], Step [59/172], Loss: 28.1569\n",
      "Epoch [161/300], Step [60/172], Loss: 30.0516\n",
      "Epoch [161/300], Step [61/172], Loss: 6.5422\n",
      "Epoch [161/300], Step [62/172], Loss: 21.0185\n",
      "Epoch [161/300], Step [63/172], Loss: 10.2532\n",
      "Epoch [161/300], Step [64/172], Loss: 10.3943\n",
      "Epoch [161/300], Step [65/172], Loss: 20.0112\n",
      "Epoch [161/300], Step [66/172], Loss: 6.2306\n",
      "Epoch [161/300], Step [67/172], Loss: 24.1615\n",
      "Epoch [161/300], Step [68/172], Loss: 5.6334\n",
      "Epoch [161/300], Step [69/172], Loss: 38.8857\n",
      "Epoch [161/300], Step [70/172], Loss: 43.0592\n",
      "Epoch [161/300], Step [71/172], Loss: 42.5709\n",
      "Epoch [161/300], Step [72/172], Loss: 44.3759\n",
      "Epoch [161/300], Step [73/172], Loss: 52.2627\n",
      "Epoch [161/300], Step [74/172], Loss: 27.5157\n",
      "Epoch [161/300], Step [75/172], Loss: 28.3200\n",
      "Epoch [161/300], Step [76/172], Loss: 30.4479\n",
      "Epoch [161/300], Step [77/172], Loss: 52.5141\n",
      "Epoch [161/300], Step [78/172], Loss: 40.7345\n",
      "Epoch [161/300], Step [79/172], Loss: 39.8102\n",
      "Epoch [161/300], Step [80/172], Loss: 52.7961\n",
      "Epoch [161/300], Step [81/172], Loss: 36.3162\n",
      "Epoch [161/300], Step [82/172], Loss: 36.9961\n",
      "Epoch [161/300], Step [83/172], Loss: 44.9453\n",
      "Epoch [161/300], Step [84/172], Loss: 34.3622\n",
      "Epoch [161/300], Step [85/172], Loss: 39.5836\n",
      "Epoch [161/300], Step [86/172], Loss: 33.4189\n",
      "Epoch [161/300], Step [87/172], Loss: 26.9546\n",
      "Epoch [161/300], Step [88/172], Loss: 26.7807\n",
      "Epoch [161/300], Step [89/172], Loss: 26.9017\n",
      "Epoch [161/300], Step [90/172], Loss: 22.6288\n",
      "Epoch [161/300], Step [91/172], Loss: 27.1885\n",
      "Epoch [161/300], Step [92/172], Loss: 20.6320\n",
      "Epoch [161/300], Step [93/172], Loss: 21.2213\n",
      "Epoch [161/300], Step [94/172], Loss: 29.4036\n",
      "Epoch [161/300], Step [95/172], Loss: 21.3009\n",
      "Epoch [161/300], Step [96/172], Loss: 20.0087\n",
      "Epoch [161/300], Step [97/172], Loss: 27.8075\n",
      "Epoch [161/300], Step [98/172], Loss: 19.7447\n",
      "Epoch [161/300], Step [99/172], Loss: 19.0491\n",
      "Epoch [161/300], Step [100/172], Loss: 15.5071\n",
      "Epoch [161/300], Step [101/172], Loss: 18.2406\n",
      "Epoch [161/300], Step [102/172], Loss: 17.0404\n",
      "Epoch [161/300], Step [103/172], Loss: 12.7832\n",
      "Epoch [161/300], Step [104/172], Loss: 17.8178\n",
      "Epoch [161/300], Step [105/172], Loss: 18.8361\n",
      "Epoch [161/300], Step [106/172], Loss: 15.8725\n",
      "Epoch [161/300], Step [107/172], Loss: 15.4942\n",
      "Epoch [161/300], Step [108/172], Loss: 14.9960\n",
      "Epoch [161/300], Step [109/172], Loss: 15.1393\n",
      "Epoch [161/300], Step [110/172], Loss: 15.7201\n",
      "Epoch [161/300], Step [111/172], Loss: 14.7357\n",
      "Epoch [161/300], Step [112/172], Loss: 17.6127\n",
      "Epoch [161/300], Step [113/172], Loss: 11.9499\n",
      "Epoch [161/300], Step [114/172], Loss: 13.5624\n",
      "Epoch [161/300], Step [115/172], Loss: 19.4228\n",
      "Epoch [161/300], Step [116/172], Loss: 14.4613\n",
      "Epoch [161/300], Step [117/172], Loss: 11.1765\n",
      "Epoch [161/300], Step [118/172], Loss: 14.3948\n",
      "Epoch [161/300], Step [119/172], Loss: 15.5391\n",
      "Epoch [161/300], Step [120/172], Loss: 9.9079\n",
      "Epoch [161/300], Step [121/172], Loss: 9.7493\n",
      "Epoch [161/300], Step [122/172], Loss: 10.3426\n",
      "Epoch [161/300], Step [123/172], Loss: 10.0956\n",
      "Epoch [161/300], Step [124/172], Loss: 7.6446\n",
      "Epoch [161/300], Step [125/172], Loss: 12.0255\n",
      "Epoch [161/300], Step [126/172], Loss: 10.5785\n",
      "Epoch [161/300], Step [127/172], Loss: 10.8377\n",
      "Epoch [161/300], Step [128/172], Loss: 10.5172\n",
      "Epoch [161/300], Step [129/172], Loss: 7.9690\n",
      "Epoch [161/300], Step [130/172], Loss: 11.8915\n",
      "Epoch [161/300], Step [131/172], Loss: 7.3742\n",
      "Epoch [161/300], Step [132/172], Loss: 8.2456\n",
      "Epoch [161/300], Step [133/172], Loss: 8.9411\n",
      "Epoch [161/300], Step [134/172], Loss: 11.5447\n",
      "Epoch [161/300], Step [135/172], Loss: 8.4853\n",
      "Epoch [161/300], Step [136/172], Loss: 8.0189\n",
      "Epoch [161/300], Step [137/172], Loss: 9.1722\n",
      "Epoch [161/300], Step [138/172], Loss: 6.8674\n",
      "Epoch [161/300], Step [139/172], Loss: 9.2595\n",
      "Epoch [161/300], Step [140/172], Loss: 9.2328\n",
      "Epoch [161/300], Step [141/172], Loss: 9.7815\n",
      "Epoch [161/300], Step [142/172], Loss: 13.7108\n",
      "Epoch [161/300], Step [143/172], Loss: 10.1039\n",
      "Epoch [161/300], Step [144/172], Loss: 8.7102\n",
      "Epoch [161/300], Step [145/172], Loss: 9.5845\n",
      "Epoch [161/300], Step [146/172], Loss: 9.1836\n",
      "Epoch [161/300], Step [147/172], Loss: 4.9995\n",
      "Epoch [161/300], Step [148/172], Loss: 5.9025\n",
      "Epoch [161/300], Step [149/172], Loss: 6.6510\n",
      "Epoch [161/300], Step [150/172], Loss: 6.2508\n",
      "Epoch [161/300], Step [151/172], Loss: 5.5889\n",
      "Epoch [161/300], Step [152/172], Loss: 7.1953\n",
      "Epoch [161/300], Step [153/172], Loss: 6.3962\n",
      "Epoch [161/300], Step [154/172], Loss: 7.3765\n",
      "Epoch [161/300], Step [155/172], Loss: 6.0973\n",
      "Epoch [161/300], Step [156/172], Loss: 12.6666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [161/300], Step [157/172], Loss: 9.2200\n",
      "Epoch [161/300], Step [158/172], Loss: 6.8707\n",
      "Epoch [161/300], Step [159/172], Loss: 8.9879\n",
      "Epoch [161/300], Step [160/172], Loss: 9.7836\n",
      "Epoch [161/300], Step [161/172], Loss: 7.0943\n",
      "Epoch [161/300], Step [162/172], Loss: 5.5382\n",
      "Epoch [161/300], Step [163/172], Loss: 6.2987\n",
      "Epoch [161/300], Step [164/172], Loss: 8.5279\n",
      "Epoch [161/300], Step [165/172], Loss: 5.8090\n",
      "Epoch [161/300], Step [166/172], Loss: 5.3552\n",
      "Epoch [161/300], Step [167/172], Loss: 9.8033\n",
      "Epoch [161/300], Step [168/172], Loss: 6.5211\n",
      "Epoch [161/300], Step [169/172], Loss: 6.6005\n",
      "Epoch [161/300], Step [170/172], Loss: 4.7087\n",
      "Epoch [161/300], Step [171/172], Loss: 7.2989\n",
      "Epoch [161/300], Step [172/172], Loss: 5.0346\n",
      "Epoch [162/300], Step [1/172], Loss: 57.5635\n",
      "Epoch [162/300], Step [2/172], Loss: 58.6274\n",
      "Epoch [162/300], Step [3/172], Loss: 52.9443\n",
      "Epoch [162/300], Step [4/172], Loss: 30.8508\n",
      "Epoch [162/300], Step [5/172], Loss: 54.2726\n",
      "Epoch [162/300], Step [6/172], Loss: 19.3063\n",
      "Epoch [162/300], Step [7/172], Loss: 30.0722\n",
      "Epoch [162/300], Step [8/172], Loss: 5.2195\n",
      "Epoch [162/300], Step [9/172], Loss: 34.8094\n",
      "Epoch [162/300], Step [10/172], Loss: 43.8207\n",
      "Epoch [162/300], Step [11/172], Loss: 64.2159\n",
      "Epoch [162/300], Step [12/172], Loss: 72.3374\n",
      "Epoch [162/300], Step [13/172], Loss: 38.9319\n",
      "Epoch [162/300], Step [14/172], Loss: 67.2809\n",
      "Epoch [162/300], Step [15/172], Loss: 60.1840\n",
      "Epoch [162/300], Step [16/172], Loss: 11.9974\n",
      "Epoch [162/300], Step [17/172], Loss: 48.6392\n",
      "Epoch [162/300], Step [18/172], Loss: 60.9596\n",
      "Epoch [162/300], Step [19/172], Loss: 83.7969\n",
      "Epoch [162/300], Step [20/172], Loss: 42.6259\n",
      "Epoch [162/300], Step [21/172], Loss: 86.6146\n",
      "Epoch [162/300], Step [22/172], Loss: 64.2451\n",
      "Epoch [162/300], Step [23/172], Loss: 1.9756\n",
      "Epoch [162/300], Step [24/172], Loss: 59.6374\n",
      "Epoch [162/300], Step [25/172], Loss: 42.9330\n",
      "Epoch [162/300], Step [26/172], Loss: 50.2983\n",
      "Epoch [162/300], Step [27/172], Loss: 64.5107\n",
      "Epoch [162/300], Step [28/172], Loss: 26.7325\n",
      "Epoch [162/300], Step [29/172], Loss: 18.6681\n",
      "Epoch [162/300], Step [30/172], Loss: 67.0957\n",
      "Epoch [162/300], Step [31/172], Loss: 39.9279\n",
      "Epoch [162/300], Step [32/172], Loss: 43.0662\n",
      "Epoch [162/300], Step [33/172], Loss: 71.9386\n",
      "Epoch [162/300], Step [34/172], Loss: 3.5356\n",
      "Epoch [162/300], Step [35/172], Loss: 13.2991\n",
      "Epoch [162/300], Step [36/172], Loss: 18.4840\n",
      "Epoch [162/300], Step [37/172], Loss: 17.8293\n",
      "Epoch [162/300], Step [38/172], Loss: 29.6248\n",
      "Epoch [162/300], Step [39/172], Loss: 37.3279\n",
      "Epoch [162/300], Step [40/172], Loss: 21.0155\n",
      "Epoch [162/300], Step [41/172], Loss: 35.4959\n",
      "Epoch [162/300], Step [42/172], Loss: 39.8047\n",
      "Epoch [162/300], Step [43/172], Loss: 27.7402\n",
      "Epoch [162/300], Step [44/172], Loss: 20.7528\n",
      "Epoch [162/300], Step [45/172], Loss: 26.0479\n",
      "Epoch [162/300], Step [46/172], Loss: 18.5772\n",
      "Epoch [162/300], Step [47/172], Loss: 47.7912\n",
      "Epoch [162/300], Step [48/172], Loss: 59.6489\n",
      "Epoch [162/300], Step [49/172], Loss: 20.9838\n",
      "Epoch [162/300], Step [50/172], Loss: 46.4663\n",
      "Epoch [162/300], Step [51/172], Loss: 8.5044\n",
      "Epoch [162/300], Step [52/172], Loss: 18.9764\n",
      "Epoch [162/300], Step [53/172], Loss: 22.5677\n",
      "Epoch [162/300], Step [54/172], Loss: 14.4284\n",
      "Epoch [162/300], Step [55/172], Loss: 14.1926\n",
      "Epoch [162/300], Step [56/172], Loss: 16.3504\n",
      "Epoch [162/300], Step [57/172], Loss: 17.0954\n",
      "Epoch [162/300], Step [58/172], Loss: 14.0347\n",
      "Epoch [162/300], Step [59/172], Loss: 28.0050\n",
      "Epoch [162/300], Step [60/172], Loss: 29.8739\n",
      "Epoch [162/300], Step [61/172], Loss: 6.6519\n",
      "Epoch [162/300], Step [62/172], Loss: 20.7746\n",
      "Epoch [162/300], Step [63/172], Loss: 10.3114\n",
      "Epoch [162/300], Step [64/172], Loss: 10.3193\n",
      "Epoch [162/300], Step [65/172], Loss: 20.0465\n",
      "Epoch [162/300], Step [66/172], Loss: 6.2845\n",
      "Epoch [162/300], Step [67/172], Loss: 23.9063\n",
      "Epoch [162/300], Step [68/172], Loss: 5.3517\n",
      "Epoch [162/300], Step [69/172], Loss: 38.6521\n",
      "Epoch [162/300], Step [70/172], Loss: 43.1444\n",
      "Epoch [162/300], Step [71/172], Loss: 42.4531\n",
      "Epoch [162/300], Step [72/172], Loss: 44.1520\n",
      "Epoch [162/300], Step [73/172], Loss: 51.9870\n",
      "Epoch [162/300], Step [74/172], Loss: 27.3173\n",
      "Epoch [162/300], Step [75/172], Loss: 28.1856\n",
      "Epoch [162/300], Step [76/172], Loss: 30.4080\n",
      "Epoch [162/300], Step [77/172], Loss: 52.1687\n",
      "Epoch [162/300], Step [78/172], Loss: 40.4377\n",
      "Epoch [162/300], Step [79/172], Loss: 39.3083\n",
      "Epoch [162/300], Step [80/172], Loss: 52.1215\n",
      "Epoch [162/300], Step [81/172], Loss: 35.9938\n",
      "Epoch [162/300], Step [82/172], Loss: 37.1161\n",
      "Epoch [162/300], Step [83/172], Loss: 44.7265\n",
      "Epoch [162/300], Step [84/172], Loss: 34.0513\n",
      "Epoch [162/300], Step [85/172], Loss: 39.2538\n",
      "Epoch [162/300], Step [86/172], Loss: 33.2482\n",
      "Epoch [162/300], Step [87/172], Loss: 26.8339\n",
      "Epoch [162/300], Step [88/172], Loss: 26.7363\n",
      "Epoch [162/300], Step [89/172], Loss: 26.8216\n",
      "Epoch [162/300], Step [90/172], Loss: 22.5765\n",
      "Epoch [162/300], Step [91/172], Loss: 27.1664\n",
      "Epoch [162/300], Step [92/172], Loss: 20.5785\n",
      "Epoch [162/300], Step [93/172], Loss: 21.2118\n",
      "Epoch [162/300], Step [94/172], Loss: 29.4646\n",
      "Epoch [162/300], Step [95/172], Loss: 21.3695\n",
      "Epoch [162/300], Step [96/172], Loss: 20.0285\n",
      "Epoch [162/300], Step [97/172], Loss: 27.8593\n",
      "Epoch [162/300], Step [98/172], Loss: 19.8300\n",
      "Epoch [162/300], Step [99/172], Loss: 19.1287\n",
      "Epoch [162/300], Step [100/172], Loss: 15.5491\n",
      "Epoch [162/300], Step [101/172], Loss: 18.3622\n",
      "Epoch [162/300], Step [102/172], Loss: 17.2594\n",
      "Epoch [162/300], Step [103/172], Loss: 12.7859\n",
      "Epoch [162/300], Step [104/172], Loss: 17.8989\n",
      "Epoch [162/300], Step [105/172], Loss: 19.0511\n",
      "Epoch [162/300], Step [106/172], Loss: 15.8828\n",
      "Epoch [162/300], Step [107/172], Loss: 15.4990\n",
      "Epoch [162/300], Step [108/172], Loss: 15.0512\n",
      "Epoch [162/300], Step [109/172], Loss: 15.2167\n",
      "Epoch [162/300], Step [110/172], Loss: 15.7535\n",
      "Epoch [162/300], Step [111/172], Loss: 14.8732\n",
      "Epoch [162/300], Step [112/172], Loss: 17.6733\n",
      "Epoch [162/300], Step [113/172], Loss: 11.9107\n",
      "Epoch [162/300], Step [114/172], Loss: 13.5976\n",
      "Epoch [162/300], Step [115/172], Loss: 19.6007\n",
      "Epoch [162/300], Step [116/172], Loss: 14.4501\n",
      "Epoch [162/300], Step [117/172], Loss: 11.2681\n",
      "Epoch [162/300], Step [118/172], Loss: 14.4155\n",
      "Epoch [162/300], Step [119/172], Loss: 15.4892\n",
      "Epoch [162/300], Step [120/172], Loss: 9.9254\n",
      "Epoch [162/300], Step [121/172], Loss: 9.7186\n",
      "Epoch [162/300], Step [122/172], Loss: 10.4908\n",
      "Epoch [162/300], Step [123/172], Loss: 10.1518\n",
      "Epoch [162/300], Step [124/172], Loss: 7.6559\n",
      "Epoch [162/300], Step [125/172], Loss: 11.9858\n",
      "Epoch [162/300], Step [126/172], Loss: 10.6287\n",
      "Epoch [162/300], Step [127/172], Loss: 10.8300\n",
      "Epoch [162/300], Step [128/172], Loss: 10.4484\n",
      "Epoch [162/300], Step [129/172], Loss: 7.9688\n",
      "Epoch [162/300], Step [130/172], Loss: 11.8943\n",
      "Epoch [162/300], Step [131/172], Loss: 7.3939\n",
      "Epoch [162/300], Step [132/172], Loss: 8.2363\n",
      "Epoch [162/300], Step [133/172], Loss: 8.9638\n",
      "Epoch [162/300], Step [134/172], Loss: 11.5499\n",
      "Epoch [162/300], Step [135/172], Loss: 8.4492\n",
      "Epoch [162/300], Step [136/172], Loss: 8.0868\n",
      "Epoch [162/300], Step [137/172], Loss: 9.2176\n",
      "Epoch [162/300], Step [138/172], Loss: 6.8879\n",
      "Epoch [162/300], Step [139/172], Loss: 9.2548\n",
      "Epoch [162/300], Step [140/172], Loss: 9.2607\n",
      "Epoch [162/300], Step [141/172], Loss: 9.7532\n",
      "Epoch [162/300], Step [142/172], Loss: 13.7649\n",
      "Epoch [162/300], Step [143/172], Loss: 10.1692\n",
      "Epoch [162/300], Step [144/172], Loss: 8.7190\n",
      "Epoch [162/300], Step [145/172], Loss: 9.6360\n",
      "Epoch [162/300], Step [146/172], Loss: 9.1786\n",
      "Epoch [162/300], Step [147/172], Loss: 5.0196\n",
      "Epoch [162/300], Step [148/172], Loss: 5.9231\n",
      "Epoch [162/300], Step [149/172], Loss: 6.6680\n",
      "Epoch [162/300], Step [150/172], Loss: 6.2581\n",
      "Epoch [162/300], Step [151/172], Loss: 5.5918\n",
      "Epoch [162/300], Step [152/172], Loss: 7.1817\n",
      "Epoch [162/300], Step [153/172], Loss: 6.3917\n",
      "Epoch [162/300], Step [154/172], Loss: 7.4147\n",
      "Epoch [162/300], Step [155/172], Loss: 6.1053\n",
      "Epoch [162/300], Step [156/172], Loss: 12.6914\n",
      "Epoch [162/300], Step [157/172], Loss: 9.2423\n",
      "Epoch [162/300], Step [158/172], Loss: 6.9089\n",
      "Epoch [162/300], Step [159/172], Loss: 9.0248\n",
      "Epoch [162/300], Step [160/172], Loss: 9.8030\n",
      "Epoch [162/300], Step [161/172], Loss: 7.1333\n",
      "Epoch [162/300], Step [162/172], Loss: 5.5084\n",
      "Epoch [162/300], Step [163/172], Loss: 6.2933\n",
      "Epoch [162/300], Step [164/172], Loss: 8.6423\n",
      "Epoch [162/300], Step [165/172], Loss: 5.8260\n",
      "Epoch [162/300], Step [166/172], Loss: 5.3616\n",
      "Epoch [162/300], Step [167/172], Loss: 9.9401\n",
      "Epoch [162/300], Step [168/172], Loss: 6.5159\n",
      "Epoch [162/300], Step [169/172], Loss: 6.6908\n",
      "Epoch [162/300], Step [170/172], Loss: 4.7177\n",
      "Epoch [162/300], Step [171/172], Loss: 7.3322\n",
      "Epoch [162/300], Step [172/172], Loss: 5.0697\n",
      "Epoch [163/300], Step [1/172], Loss: 57.2242\n",
      "Epoch [163/300], Step [2/172], Loss: 58.2569\n",
      "Epoch [163/300], Step [3/172], Loss: 51.9281\n",
      "Epoch [163/300], Step [4/172], Loss: 30.7238\n",
      "Epoch [163/300], Step [5/172], Loss: 53.7216\n",
      "Epoch [163/300], Step [6/172], Loss: 19.3774\n",
      "Epoch [163/300], Step [7/172], Loss: 29.9069\n",
      "Epoch [163/300], Step [8/172], Loss: 4.6797\n",
      "Epoch [163/300], Step [9/172], Loss: 34.5387\n",
      "Epoch [163/300], Step [10/172], Loss: 43.8427\n",
      "Epoch [163/300], Step [11/172], Loss: 63.9053\n",
      "Epoch [163/300], Step [12/172], Loss: 71.9179\n",
      "Epoch [163/300], Step [13/172], Loss: 38.2646\n",
      "Epoch [163/300], Step [14/172], Loss: 66.3972\n",
      "Epoch [163/300], Step [15/172], Loss: 60.0180\n",
      "Epoch [163/300], Step [16/172], Loss: 12.8458\n",
      "Epoch [163/300], Step [17/172], Loss: 48.2769\n",
      "Epoch [163/300], Step [18/172], Loss: 60.8269\n",
      "Epoch [163/300], Step [19/172], Loss: 83.9505\n",
      "Epoch [163/300], Step [20/172], Loss: 42.0988\n",
      "Epoch [163/300], Step [21/172], Loss: 86.9541\n",
      "Epoch [163/300], Step [22/172], Loss: 64.1318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [163/300], Step [23/172], Loss: 1.9845\n",
      "Epoch [163/300], Step [24/172], Loss: 59.7779\n",
      "Epoch [163/300], Step [25/172], Loss: 42.5873\n",
      "Epoch [163/300], Step [26/172], Loss: 50.0783\n",
      "Epoch [163/300], Step [27/172], Loss: 64.6119\n",
      "Epoch [163/300], Step [28/172], Loss: 26.3447\n",
      "Epoch [163/300], Step [29/172], Loss: 18.2184\n",
      "Epoch [163/300], Step [30/172], Loss: 66.9656\n",
      "Epoch [163/300], Step [31/172], Loss: 39.4790\n",
      "Epoch [163/300], Step [32/172], Loss: 42.9640\n",
      "Epoch [163/300], Step [33/172], Loss: 71.7889\n",
      "Epoch [163/300], Step [34/172], Loss: 3.3297\n",
      "Epoch [163/300], Step [35/172], Loss: 13.1270\n",
      "Epoch [163/300], Step [36/172], Loss: 18.1986\n",
      "Epoch [163/300], Step [37/172], Loss: 17.5030\n",
      "Epoch [163/300], Step [38/172], Loss: 29.3591\n",
      "Epoch [163/300], Step [39/172], Loss: 36.6814\n",
      "Epoch [163/300], Step [40/172], Loss: 20.5183\n",
      "Epoch [163/300], Step [41/172], Loss: 34.8450\n",
      "Epoch [163/300], Step [42/172], Loss: 39.1743\n",
      "Epoch [163/300], Step [43/172], Loss: 27.1717\n",
      "Epoch [163/300], Step [44/172], Loss: 20.3646\n",
      "Epoch [163/300], Step [45/172], Loss: 25.5415\n",
      "Epoch [163/300], Step [46/172], Loss: 18.1136\n",
      "Epoch [163/300], Step [47/172], Loss: 47.0196\n",
      "Epoch [163/300], Step [48/172], Loss: 59.1931\n",
      "Epoch [163/300], Step [49/172], Loss: 20.5875\n",
      "Epoch [163/300], Step [50/172], Loss: 46.1047\n",
      "Epoch [163/300], Step [51/172], Loss: 8.3850\n",
      "Epoch [163/300], Step [52/172], Loss: 18.7357\n",
      "Epoch [163/300], Step [53/172], Loss: 22.3295\n",
      "Epoch [163/300], Step [54/172], Loss: 14.2469\n",
      "Epoch [163/300], Step [55/172], Loss: 14.1318\n",
      "Epoch [163/300], Step [56/172], Loss: 16.9503\n",
      "Epoch [163/300], Step [57/172], Loss: 17.0992\n",
      "Epoch [163/300], Step [58/172], Loss: 14.0520\n",
      "Epoch [163/300], Step [59/172], Loss: 28.0300\n",
      "Epoch [163/300], Step [60/172], Loss: 29.3966\n",
      "Epoch [163/300], Step [61/172], Loss: 6.5838\n",
      "Epoch [163/300], Step [62/172], Loss: 20.9577\n",
      "Epoch [163/300], Step [63/172], Loss: 10.2881\n",
      "Epoch [163/300], Step [64/172], Loss: 10.3889\n",
      "Epoch [163/300], Step [65/172], Loss: 20.0329\n",
      "Epoch [163/300], Step [66/172], Loss: 6.3373\n",
      "Epoch [163/300], Step [67/172], Loss: 24.0075\n",
      "Epoch [163/300], Step [68/172], Loss: 5.4617\n",
      "Epoch [163/300], Step [69/172], Loss: 38.5716\n",
      "Epoch [163/300], Step [70/172], Loss: 42.6892\n",
      "Epoch [163/300], Step [71/172], Loss: 42.2319\n",
      "Epoch [163/300], Step [72/172], Loss: 43.9604\n",
      "Epoch [163/300], Step [73/172], Loss: 51.8642\n",
      "Epoch [163/300], Step [74/172], Loss: 27.3087\n",
      "Epoch [163/300], Step [75/172], Loss: 28.2133\n",
      "Epoch [163/300], Step [76/172], Loss: 30.2741\n",
      "Epoch [163/300], Step [77/172], Loss: 52.1280\n",
      "Epoch [163/300], Step [78/172], Loss: 40.5021\n",
      "Epoch [163/300], Step [79/172], Loss: 39.4437\n",
      "Epoch [163/300], Step [80/172], Loss: 52.5241\n",
      "Epoch [163/300], Step [81/172], Loss: 36.1119\n",
      "Epoch [163/300], Step [82/172], Loss: 36.9579\n",
      "Epoch [163/300], Step [83/172], Loss: 44.9548\n",
      "Epoch [163/300], Step [84/172], Loss: 34.2949\n",
      "Epoch [163/300], Step [85/172], Loss: 39.6592\n",
      "Epoch [163/300], Step [86/172], Loss: 33.5197\n",
      "Epoch [163/300], Step [87/172], Loss: 26.9678\n",
      "Epoch [163/300], Step [88/172], Loss: 26.7188\n",
      "Epoch [163/300], Step [89/172], Loss: 27.0141\n",
      "Epoch [163/300], Step [90/172], Loss: 22.6113\n",
      "Epoch [163/300], Step [91/172], Loss: 27.1999\n",
      "Epoch [163/300], Step [92/172], Loss: 20.6504\n",
      "Epoch [163/300], Step [93/172], Loss: 21.2253\n",
      "Epoch [163/300], Step [94/172], Loss: 29.5177\n",
      "Epoch [163/300], Step [95/172], Loss: 21.2857\n",
      "Epoch [163/300], Step [96/172], Loss: 20.0470\n",
      "Epoch [163/300], Step [97/172], Loss: 27.9268\n",
      "Epoch [163/300], Step [98/172], Loss: 19.7757\n",
      "Epoch [163/300], Step [99/172], Loss: 19.1071\n",
      "Epoch [163/300], Step [100/172], Loss: 15.5053\n",
      "Epoch [163/300], Step [101/172], Loss: 18.3320\n",
      "Epoch [163/300], Step [102/172], Loss: 17.1182\n",
      "Epoch [163/300], Step [103/172], Loss: 12.7425\n",
      "Epoch [163/300], Step [104/172], Loss: 17.8849\n",
      "Epoch [163/300], Step [105/172], Loss: 18.8743\n",
      "Epoch [163/300], Step [106/172], Loss: 15.8472\n",
      "Epoch [163/300], Step [107/172], Loss: 15.5069\n",
      "Epoch [163/300], Step [108/172], Loss: 14.9566\n",
      "Epoch [163/300], Step [109/172], Loss: 15.1380\n",
      "Epoch [163/300], Step [110/172], Loss: 15.7053\n",
      "Epoch [163/300], Step [111/172], Loss: 14.8221\n",
      "Epoch [163/300], Step [112/172], Loss: 17.5592\n",
      "Epoch [163/300], Step [113/172], Loss: 11.8611\n",
      "Epoch [163/300], Step [114/172], Loss: 13.5423\n",
      "Epoch [163/300], Step [115/172], Loss: 19.5922\n",
      "Epoch [163/300], Step [116/172], Loss: 14.3949\n",
      "Epoch [163/300], Step [117/172], Loss: 11.1848\n",
      "Epoch [163/300], Step [118/172], Loss: 14.3943\n",
      "Epoch [163/300], Step [119/172], Loss: 15.5007\n",
      "Epoch [163/300], Step [120/172], Loss: 9.8728\n",
      "Epoch [163/300], Step [121/172], Loss: 9.6623\n",
      "Epoch [163/300], Step [122/172], Loss: 10.4504\n",
      "Epoch [163/300], Step [123/172], Loss: 10.1224\n",
      "Epoch [163/300], Step [124/172], Loss: 7.6193\n",
      "Epoch [163/300], Step [125/172], Loss: 11.9423\n",
      "Epoch [163/300], Step [126/172], Loss: 10.6427\n",
      "Epoch [163/300], Step [127/172], Loss: 10.8023\n",
      "Epoch [163/300], Step [128/172], Loss: 10.4248\n",
      "Epoch [163/300], Step [129/172], Loss: 7.9456\n",
      "Epoch [163/300], Step [130/172], Loss: 11.9070\n",
      "Epoch [163/300], Step [131/172], Loss: 7.3608\n",
      "Epoch [163/300], Step [132/172], Loss: 8.1993\n",
      "Epoch [163/300], Step [133/172], Loss: 8.9396\n",
      "Epoch [163/300], Step [134/172], Loss: 11.5440\n",
      "Epoch [163/300], Step [135/172], Loss: 8.4671\n",
      "Epoch [163/300], Step [136/172], Loss: 8.0347\n",
      "Epoch [163/300], Step [137/172], Loss: 9.1734\n",
      "Epoch [163/300], Step [138/172], Loss: 6.8801\n",
      "Epoch [163/300], Step [139/172], Loss: 9.2658\n",
      "Epoch [163/300], Step [140/172], Loss: 9.2515\n",
      "Epoch [163/300], Step [141/172], Loss: 9.7330\n",
      "Epoch [163/300], Step [142/172], Loss: 13.7485\n",
      "Epoch [163/300], Step [143/172], Loss: 10.1774\n",
      "Epoch [163/300], Step [144/172], Loss: 8.7356\n",
      "Epoch [163/300], Step [145/172], Loss: 9.6116\n",
      "Epoch [163/300], Step [146/172], Loss: 9.1822\n",
      "Epoch [163/300], Step [147/172], Loss: 5.0111\n",
      "Epoch [163/300], Step [148/172], Loss: 5.8990\n",
      "Epoch [163/300], Step [149/172], Loss: 6.6699\n",
      "Epoch [163/300], Step [150/172], Loss: 6.2056\n",
      "Epoch [163/300], Step [151/172], Loss: 5.5589\n",
      "Epoch [163/300], Step [152/172], Loss: 7.1964\n",
      "Epoch [163/300], Step [153/172], Loss: 6.3767\n",
      "Epoch [163/300], Step [154/172], Loss: 7.3840\n",
      "Epoch [163/300], Step [155/172], Loss: 6.0903\n",
      "Epoch [163/300], Step [156/172], Loss: 12.7094\n",
      "Epoch [163/300], Step [157/172], Loss: 9.2273\n",
      "Epoch [163/300], Step [158/172], Loss: 6.8945\n",
      "Epoch [163/300], Step [159/172], Loss: 9.0107\n",
      "Epoch [163/300], Step [160/172], Loss: 9.8181\n",
      "Epoch [163/300], Step [161/172], Loss: 7.1944\n",
      "Epoch [163/300], Step [162/172], Loss: 5.4923\n",
      "Epoch [163/300], Step [163/172], Loss: 6.2989\n",
      "Epoch [163/300], Step [164/172], Loss: 8.5498\n",
      "Epoch [163/300], Step [165/172], Loss: 5.8043\n",
      "Epoch [163/300], Step [166/172], Loss: 5.3379\n",
      "Epoch [163/300], Step [167/172], Loss: 9.9214\n",
      "Epoch [163/300], Step [168/172], Loss: 6.4725\n",
      "Epoch [163/300], Step [169/172], Loss: 6.6589\n",
      "Epoch [163/300], Step [170/172], Loss: 4.6936\n",
      "Epoch [163/300], Step [171/172], Loss: 7.3547\n",
      "Epoch [163/300], Step [172/172], Loss: 5.0754\n",
      "Epoch [164/300], Step [1/172], Loss: 57.1512\n",
      "Epoch [164/300], Step [2/172], Loss: 58.2574\n",
      "Epoch [164/300], Step [3/172], Loss: 51.6695\n",
      "Epoch [164/300], Step [4/172], Loss: 30.5822\n",
      "Epoch [164/300], Step [5/172], Loss: 53.4447\n",
      "Epoch [164/300], Step [6/172], Loss: 19.2625\n",
      "Epoch [164/300], Step [7/172], Loss: 29.8393\n",
      "Epoch [164/300], Step [8/172], Loss: 5.0450\n",
      "Epoch [164/300], Step [9/172], Loss: 34.5061\n",
      "Epoch [164/300], Step [10/172], Loss: 43.9116\n",
      "Epoch [164/300], Step [11/172], Loss: 63.7071\n",
      "Epoch [164/300], Step [12/172], Loss: 71.7818\n",
      "Epoch [164/300], Step [13/172], Loss: 38.3196\n",
      "Epoch [164/300], Step [14/172], Loss: 66.3087\n",
      "Epoch [164/300], Step [15/172], Loss: 59.7275\n",
      "Epoch [164/300], Step [16/172], Loss: 11.9179\n",
      "Epoch [164/300], Step [17/172], Loss: 48.2348\n",
      "Epoch [164/300], Step [18/172], Loss: 60.8071\n",
      "Epoch [164/300], Step [19/172], Loss: 83.5930\n",
      "Epoch [164/300], Step [20/172], Loss: 41.9236\n",
      "Epoch [164/300], Step [21/172], Loss: 86.5560\n",
      "Epoch [164/300], Step [22/172], Loss: 63.8729\n",
      "Epoch [164/300], Step [23/172], Loss: 1.9671\n",
      "Epoch [164/300], Step [24/172], Loss: 59.4898\n",
      "Epoch [164/300], Step [25/172], Loss: 42.6121\n",
      "Epoch [164/300], Step [26/172], Loss: 49.9705\n",
      "Epoch [164/300], Step [27/172], Loss: 64.6786\n",
      "Epoch [164/300], Step [28/172], Loss: 26.4608\n",
      "Epoch [164/300], Step [29/172], Loss: 18.2662\n",
      "Epoch [164/300], Step [30/172], Loss: 66.7470\n",
      "Epoch [164/300], Step [31/172], Loss: 39.5608\n",
      "Epoch [164/300], Step [32/172], Loss: 42.9158\n",
      "Epoch [164/300], Step [33/172], Loss: 71.7127\n",
      "Epoch [164/300], Step [34/172], Loss: 3.2937\n",
      "Epoch [164/300], Step [35/172], Loss: 13.3339\n",
      "Epoch [164/300], Step [36/172], Loss: 18.2425\n",
      "Epoch [164/300], Step [37/172], Loss: 17.6088\n",
      "Epoch [164/300], Step [38/172], Loss: 29.6125\n",
      "Epoch [164/300], Step [39/172], Loss: 36.8508\n",
      "Epoch [164/300], Step [40/172], Loss: 20.6456\n",
      "Epoch [164/300], Step [41/172], Loss: 34.9989\n",
      "Epoch [164/300], Step [42/172], Loss: 39.4188\n",
      "Epoch [164/300], Step [43/172], Loss: 27.4371\n",
      "Epoch [164/300], Step [44/172], Loss: 20.5082\n",
      "Epoch [164/300], Step [45/172], Loss: 25.7864\n",
      "Epoch [164/300], Step [46/172], Loss: 18.3024\n",
      "Epoch [164/300], Step [47/172], Loss: 47.2361\n",
      "Epoch [164/300], Step [48/172], Loss: 59.5908\n",
      "Epoch [164/300], Step [49/172], Loss: 20.7575\n",
      "Epoch [164/300], Step [50/172], Loss: 46.1522\n",
      "Epoch [164/300], Step [51/172], Loss: 8.4512\n",
      "Epoch [164/300], Step [52/172], Loss: 18.8181\n",
      "Epoch [164/300], Step [53/172], Loss: 22.3778\n",
      "Epoch [164/300], Step [54/172], Loss: 14.3662\n",
      "Epoch [164/300], Step [55/172], Loss: 14.1730\n",
      "Epoch [164/300], Step [56/172], Loss: 17.0070\n",
      "Epoch [164/300], Step [57/172], Loss: 16.9972\n",
      "Epoch [164/300], Step [58/172], Loss: 13.8733\n",
      "Epoch [164/300], Step [59/172], Loss: 27.9061\n",
      "Epoch [164/300], Step [60/172], Loss: 29.0673\n",
      "Epoch [164/300], Step [61/172], Loss: 6.5816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [164/300], Step [62/172], Loss: 20.9091\n",
      "Epoch [164/300], Step [63/172], Loss: 10.3203\n",
      "Epoch [164/300], Step [64/172], Loss: 10.3365\n",
      "Epoch [164/300], Step [65/172], Loss: 19.9966\n",
      "Epoch [164/300], Step [66/172], Loss: 6.3047\n",
      "Epoch [164/300], Step [67/172], Loss: 23.7823\n",
      "Epoch [164/300], Step [68/172], Loss: 5.3806\n",
      "Epoch [164/300], Step [69/172], Loss: 38.2219\n",
      "Epoch [164/300], Step [70/172], Loss: 42.5903\n",
      "Epoch [164/300], Step [71/172], Loss: 42.1539\n",
      "Epoch [164/300], Step [72/172], Loss: 43.8226\n",
      "Epoch [164/300], Step [73/172], Loss: 51.7775\n",
      "Epoch [164/300], Step [74/172], Loss: 27.3202\n",
      "Epoch [164/300], Step [75/172], Loss: 28.1470\n",
      "Epoch [164/300], Step [76/172], Loss: 30.1772\n",
      "Epoch [164/300], Step [77/172], Loss: 51.9186\n",
      "Epoch [164/300], Step [78/172], Loss: 40.3521\n",
      "Epoch [164/300], Step [79/172], Loss: 39.2852\n",
      "Epoch [164/300], Step [80/172], Loss: 52.4445\n",
      "Epoch [164/300], Step [81/172], Loss: 35.9547\n",
      "Epoch [164/300], Step [82/172], Loss: 37.0337\n",
      "Epoch [164/300], Step [83/172], Loss: 44.7829\n",
      "Epoch [164/300], Step [84/172], Loss: 34.1000\n",
      "Epoch [164/300], Step [85/172], Loss: 39.4131\n",
      "Epoch [164/300], Step [86/172], Loss: 33.3539\n",
      "Epoch [164/300], Step [87/172], Loss: 26.9069\n",
      "Epoch [164/300], Step [88/172], Loss: 26.6509\n",
      "Epoch [164/300], Step [89/172], Loss: 26.9601\n",
      "Epoch [164/300], Step [90/172], Loss: 22.5569\n",
      "Epoch [164/300], Step [91/172], Loss: 27.1743\n",
      "Epoch [164/300], Step [92/172], Loss: 20.6228\n",
      "Epoch [164/300], Step [93/172], Loss: 21.1943\n",
      "Epoch [164/300], Step [94/172], Loss: 29.4742\n",
      "Epoch [164/300], Step [95/172], Loss: 21.3701\n",
      "Epoch [164/300], Step [96/172], Loss: 20.0716\n",
      "Epoch [164/300], Step [97/172], Loss: 27.9893\n",
      "Epoch [164/300], Step [98/172], Loss: 19.8029\n",
      "Epoch [164/300], Step [99/172], Loss: 19.1692\n",
      "Epoch [164/300], Step [100/172], Loss: 15.5123\n",
      "Epoch [164/300], Step [101/172], Loss: 18.4087\n",
      "Epoch [164/300], Step [102/172], Loss: 17.2715\n",
      "Epoch [164/300], Step [103/172], Loss: 12.8065\n",
      "Epoch [164/300], Step [104/172], Loss: 17.9726\n",
      "Epoch [164/300], Step [105/172], Loss: 19.0594\n",
      "Epoch [164/300], Step [106/172], Loss: 15.8846\n",
      "Epoch [164/300], Step [107/172], Loss: 15.5563\n",
      "Epoch [164/300], Step [108/172], Loss: 14.9983\n",
      "Epoch [164/300], Step [109/172], Loss: 15.1495\n",
      "Epoch [164/300], Step [110/172], Loss: 15.8545\n",
      "Epoch [164/300], Step [111/172], Loss: 14.9212\n",
      "Epoch [164/300], Step [112/172], Loss: 17.6510\n",
      "Epoch [164/300], Step [113/172], Loss: 11.8896\n",
      "Epoch [164/300], Step [114/172], Loss: 13.6219\n",
      "Epoch [164/300], Step [115/172], Loss: 19.6521\n",
      "Epoch [164/300], Step [116/172], Loss: 14.5145\n",
      "Epoch [164/300], Step [117/172], Loss: 11.2401\n",
      "Epoch [164/300], Step [118/172], Loss: 14.4238\n",
      "Epoch [164/300], Step [119/172], Loss: 15.6191\n",
      "Epoch [164/300], Step [120/172], Loss: 9.9271\n",
      "Epoch [164/300], Step [121/172], Loss: 9.7024\n",
      "Epoch [164/300], Step [122/172], Loss: 10.4966\n",
      "Epoch [164/300], Step [123/172], Loss: 10.1663\n",
      "Epoch [164/300], Step [124/172], Loss: 7.6321\n",
      "Epoch [164/300], Step [125/172], Loss: 11.9378\n",
      "Epoch [164/300], Step [126/172], Loss: 10.7184\n",
      "Epoch [164/300], Step [127/172], Loss: 10.8799\n",
      "Epoch [164/300], Step [128/172], Loss: 10.5113\n",
      "Epoch [164/300], Step [129/172], Loss: 7.9718\n",
      "Epoch [164/300], Step [130/172], Loss: 11.9803\n",
      "Epoch [164/300], Step [131/172], Loss: 7.3973\n",
      "Epoch [164/300], Step [132/172], Loss: 8.2488\n",
      "Epoch [164/300], Step [133/172], Loss: 8.9818\n",
      "Epoch [164/300], Step [134/172], Loss: 11.6092\n",
      "Epoch [164/300], Step [135/172], Loss: 8.4895\n",
      "Epoch [164/300], Step [136/172], Loss: 8.0143\n",
      "Epoch [164/300], Step [137/172], Loss: 9.1920\n",
      "Epoch [164/300], Step [138/172], Loss: 6.9236\n",
      "Epoch [164/300], Step [139/172], Loss: 9.2417\n",
      "Epoch [164/300], Step [140/172], Loss: 9.2913\n",
      "Epoch [164/300], Step [141/172], Loss: 9.7367\n",
      "Epoch [164/300], Step [142/172], Loss: 13.8517\n",
      "Epoch [164/300], Step [143/172], Loss: 10.2410\n",
      "Epoch [164/300], Step [144/172], Loss: 8.7773\n",
      "Epoch [164/300], Step [145/172], Loss: 9.6218\n",
      "Epoch [164/300], Step [146/172], Loss: 9.2656\n",
      "Epoch [164/300], Step [147/172], Loss: 4.9921\n",
      "Epoch [164/300], Step [148/172], Loss: 5.9017\n",
      "Epoch [164/300], Step [149/172], Loss: 6.6189\n",
      "Epoch [164/300], Step [150/172], Loss: 6.1979\n",
      "Epoch [164/300], Step [151/172], Loss: 5.5829\n",
      "Epoch [164/300], Step [152/172], Loss: 7.2266\n",
      "Epoch [164/300], Step [153/172], Loss: 6.3535\n",
      "Epoch [164/300], Step [154/172], Loss: 7.4360\n",
      "Epoch [164/300], Step [155/172], Loss: 6.0522\n",
      "Epoch [164/300], Step [156/172], Loss: 12.7596\n",
      "Epoch [164/300], Step [157/172], Loss: 9.2648\n",
      "Epoch [164/300], Step [158/172], Loss: 6.9151\n",
      "Epoch [164/300], Step [159/172], Loss: 9.0194\n",
      "Epoch [164/300], Step [160/172], Loss: 9.8587\n",
      "Epoch [164/300], Step [161/172], Loss: 7.1380\n",
      "Epoch [164/300], Step [162/172], Loss: 5.4772\n",
      "Epoch [164/300], Step [163/172], Loss: 6.3279\n",
      "Epoch [164/300], Step [164/172], Loss: 8.6510\n",
      "Epoch [164/300], Step [165/172], Loss: 5.8058\n",
      "Epoch [164/300], Step [166/172], Loss: 5.2993\n",
      "Epoch [164/300], Step [167/172], Loss: 9.8854\n",
      "Epoch [164/300], Step [168/172], Loss: 6.4022\n",
      "Epoch [164/300], Step [169/172], Loss: 6.6498\n",
      "Epoch [164/300], Step [170/172], Loss: 4.6794\n",
      "Epoch [164/300], Step [171/172], Loss: 7.3709\n",
      "Epoch [164/300], Step [172/172], Loss: 5.0058\n",
      "Epoch [165/300], Step [1/172], Loss: 57.0248\n",
      "Epoch [165/300], Step [2/172], Loss: 57.9189\n",
      "Epoch [165/300], Step [3/172], Loss: 51.2538\n",
      "Epoch [165/300], Step [4/172], Loss: 30.3407\n",
      "Epoch [165/300], Step [5/172], Loss: 53.8283\n",
      "Epoch [165/300], Step [6/172], Loss: 19.2084\n",
      "Epoch [165/300], Step [7/172], Loss: 29.6190\n",
      "Epoch [165/300], Step [8/172], Loss: 4.6109\n",
      "Epoch [165/300], Step [9/172], Loss: 34.2344\n",
      "Epoch [165/300], Step [10/172], Loss: 43.6758\n",
      "Epoch [165/300], Step [11/172], Loss: 63.1451\n",
      "Epoch [165/300], Step [12/172], Loss: 71.1021\n",
      "Epoch [165/300], Step [13/172], Loss: 37.8723\n",
      "Epoch [165/300], Step [14/172], Loss: 65.6661\n",
      "Epoch [165/300], Step [15/172], Loss: 59.4003\n",
      "Epoch [165/300], Step [16/172], Loss: 12.5262\n",
      "Epoch [165/300], Step [17/172], Loss: 47.6207\n",
      "Epoch [165/300], Step [18/172], Loss: 60.4623\n",
      "Epoch [165/300], Step [19/172], Loss: 83.2726\n",
      "Epoch [165/300], Step [20/172], Loss: 41.3533\n",
      "Epoch [165/300], Step [21/172], Loss: 86.1232\n",
      "Epoch [165/300], Step [22/172], Loss: 63.8162\n",
      "Epoch [165/300], Step [23/172], Loss: 2.0627\n",
      "Epoch [165/300], Step [24/172], Loss: 59.1976\n",
      "Epoch [165/300], Step [25/172], Loss: 42.4362\n",
      "Epoch [165/300], Step [26/172], Loss: 49.6953\n",
      "Epoch [165/300], Step [27/172], Loss: 64.4746\n",
      "Epoch [165/300], Step [28/172], Loss: 26.1614\n",
      "Epoch [165/300], Step [29/172], Loss: 18.1173\n",
      "Epoch [165/300], Step [30/172], Loss: 66.5021\n",
      "Epoch [165/300], Step [31/172], Loss: 39.5425\n",
      "Epoch [165/300], Step [32/172], Loss: 42.7588\n",
      "Epoch [165/300], Step [33/172], Loss: 71.4447\n",
      "Epoch [165/300], Step [34/172], Loss: 3.2697\n",
      "Epoch [165/300], Step [35/172], Loss: 13.2917\n",
      "Epoch [165/300], Step [36/172], Loss: 18.2231\n",
      "Epoch [165/300], Step [37/172], Loss: 17.4572\n",
      "Epoch [165/300], Step [38/172], Loss: 29.5387\n",
      "Epoch [165/300], Step [39/172], Loss: 36.8892\n",
      "Epoch [165/300], Step [40/172], Loss: 20.6982\n",
      "Epoch [165/300], Step [41/172], Loss: 35.1339\n",
      "Epoch [165/300], Step [42/172], Loss: 39.3166\n",
      "Epoch [165/300], Step [43/172], Loss: 27.4985\n",
      "Epoch [165/300], Step [44/172], Loss: 20.5828\n",
      "Epoch [165/300], Step [45/172], Loss: 25.9082\n",
      "Epoch [165/300], Step [46/172], Loss: 18.0447\n",
      "Epoch [165/300], Step [47/172], Loss: 47.2889\n",
      "Epoch [165/300], Step [48/172], Loss: 59.9289\n",
      "Epoch [165/300], Step [49/172], Loss: 20.7301\n",
      "Epoch [165/300], Step [50/172], Loss: 46.3041\n",
      "Epoch [165/300], Step [51/172], Loss: 8.4644\n",
      "Epoch [165/300], Step [52/172], Loss: 18.9456\n",
      "Epoch [165/300], Step [53/172], Loss: 22.4466\n",
      "Epoch [165/300], Step [54/172], Loss: 14.4392\n",
      "Epoch [165/300], Step [55/172], Loss: 14.1737\n",
      "Epoch [165/300], Step [56/172], Loss: 16.9431\n",
      "Epoch [165/300], Step [57/172], Loss: 16.9721\n",
      "Epoch [165/300], Step [58/172], Loss: 13.8608\n",
      "Epoch [165/300], Step [59/172], Loss: 27.9581\n",
      "Epoch [165/300], Step [60/172], Loss: 29.1172\n",
      "Epoch [165/300], Step [61/172], Loss: 6.5534\n",
      "Epoch [165/300], Step [62/172], Loss: 20.7477\n",
      "Epoch [165/300], Step [63/172], Loss: 10.3283\n",
      "Epoch [165/300], Step [64/172], Loss: 10.4033\n",
      "Epoch [165/300], Step [65/172], Loss: 20.0476\n",
      "Epoch [165/300], Step [66/172], Loss: 6.3538\n",
      "Epoch [165/300], Step [67/172], Loss: 23.7239\n",
      "Epoch [165/300], Step [68/172], Loss: 5.2759\n",
      "Epoch [165/300], Step [69/172], Loss: 37.9372\n",
      "Epoch [165/300], Step [70/172], Loss: 42.3759\n",
      "Epoch [165/300], Step [71/172], Loss: 42.1093\n",
      "Epoch [165/300], Step [72/172], Loss: 43.6384\n",
      "Epoch [165/300], Step [73/172], Loss: 51.5522\n",
      "Epoch [165/300], Step [74/172], Loss: 27.1286\n",
      "Epoch [165/300], Step [75/172], Loss: 28.1739\n",
      "Epoch [165/300], Step [76/172], Loss: 30.1850\n",
      "Epoch [165/300], Step [77/172], Loss: 51.8837\n",
      "Epoch [165/300], Step [78/172], Loss: 40.1500\n",
      "Epoch [165/300], Step [79/172], Loss: 39.1338\n",
      "Epoch [165/300], Step [80/172], Loss: 52.0948\n",
      "Epoch [165/300], Step [81/172], Loss: 35.8745\n",
      "Epoch [165/300], Step [82/172], Loss: 36.8068\n",
      "Epoch [165/300], Step [83/172], Loss: 44.6885\n",
      "Epoch [165/300], Step [84/172], Loss: 33.9220\n",
      "Epoch [165/300], Step [85/172], Loss: 39.2276\n",
      "Epoch [165/300], Step [86/172], Loss: 33.2829\n",
      "Epoch [165/300], Step [87/172], Loss: 26.8810\n",
      "Epoch [165/300], Step [88/172], Loss: 26.5687\n",
      "Epoch [165/300], Step [89/172], Loss: 26.9126\n",
      "Epoch [165/300], Step [90/172], Loss: 22.4103\n",
      "Epoch [165/300], Step [91/172], Loss: 27.1966\n",
      "Epoch [165/300], Step [92/172], Loss: 20.5909\n",
      "Epoch [165/300], Step [93/172], Loss: 21.1671\n",
      "Epoch [165/300], Step [94/172], Loss: 29.4983\n",
      "Epoch [165/300], Step [95/172], Loss: 21.3143\n",
      "Epoch [165/300], Step [96/172], Loss: 20.0652\n",
      "Epoch [165/300], Step [97/172], Loss: 28.0349\n",
      "Epoch [165/300], Step [98/172], Loss: 19.8424\n",
      "Epoch [165/300], Step [99/172], Loss: 19.1578\n",
      "Epoch [165/300], Step [100/172], Loss: 15.5499\n",
      "Epoch [165/300], Step [101/172], Loss: 18.4702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [165/300], Step [102/172], Loss: 17.1977\n",
      "Epoch [165/300], Step [103/172], Loss: 12.7847\n",
      "Epoch [165/300], Step [104/172], Loss: 18.0119\n",
      "Epoch [165/300], Step [105/172], Loss: 19.0025\n",
      "Epoch [165/300], Step [106/172], Loss: 15.8611\n",
      "Epoch [165/300], Step [107/172], Loss: 15.5629\n",
      "Epoch [165/300], Step [108/172], Loss: 14.9838\n",
      "Epoch [165/300], Step [109/172], Loss: 15.0660\n",
      "Epoch [165/300], Step [110/172], Loss: 15.8245\n",
      "Epoch [165/300], Step [111/172], Loss: 14.9296\n",
      "Epoch [165/300], Step [112/172], Loss: 17.5702\n",
      "Epoch [165/300], Step [113/172], Loss: 11.8575\n",
      "Epoch [165/300], Step [114/172], Loss: 13.6277\n",
      "Epoch [165/300], Step [115/172], Loss: 19.5429\n",
      "Epoch [165/300], Step [116/172], Loss: 14.4619\n",
      "Epoch [165/300], Step [117/172], Loss: 11.2116\n",
      "Epoch [165/300], Step [118/172], Loss: 14.3918\n",
      "Epoch [165/300], Step [119/172], Loss: 15.6565\n",
      "Epoch [165/300], Step [120/172], Loss: 9.8926\n",
      "Epoch [165/300], Step [121/172], Loss: 9.6670\n",
      "Epoch [165/300], Step [122/172], Loss: 10.4927\n",
      "Epoch [165/300], Step [123/172], Loss: 10.0893\n",
      "Epoch [165/300], Step [124/172], Loss: 7.5951\n",
      "Epoch [165/300], Step [125/172], Loss: 11.8953\n",
      "Epoch [165/300], Step [126/172], Loss: 10.7072\n",
      "Epoch [165/300], Step [127/172], Loss: 10.8326\n",
      "Epoch [165/300], Step [128/172], Loss: 10.4624\n",
      "Epoch [165/300], Step [129/172], Loss: 7.9536\n",
      "Epoch [165/300], Step [130/172], Loss: 11.9850\n",
      "Epoch [165/300], Step [131/172], Loss: 7.3602\n",
      "Epoch [165/300], Step [132/172], Loss: 8.2356\n",
      "Epoch [165/300], Step [133/172], Loss: 8.9783\n",
      "Epoch [165/300], Step [134/172], Loss: 11.6118\n",
      "Epoch [165/300], Step [135/172], Loss: 8.4834\n",
      "Epoch [165/300], Step [136/172], Loss: 7.9787\n",
      "Epoch [165/300], Step [137/172], Loss: 9.1577\n",
      "Epoch [165/300], Step [138/172], Loss: 6.8912\n",
      "Epoch [165/300], Step [139/172], Loss: 9.2746\n",
      "Epoch [165/300], Step [140/172], Loss: 9.2785\n",
      "Epoch [165/300], Step [141/172], Loss: 9.6726\n",
      "Epoch [165/300], Step [142/172], Loss: 13.8153\n",
      "Epoch [165/300], Step [143/172], Loss: 10.2384\n",
      "Epoch [165/300], Step [144/172], Loss: 8.7892\n",
      "Epoch [165/300], Step [145/172], Loss: 9.6051\n",
      "Epoch [165/300], Step [146/172], Loss: 9.2262\n",
      "Epoch [165/300], Step [147/172], Loss: 4.9918\n",
      "Epoch [165/300], Step [148/172], Loss: 5.8855\n",
      "Epoch [165/300], Step [149/172], Loss: 6.5740\n",
      "Epoch [165/300], Step [150/172], Loss: 6.1726\n",
      "Epoch [165/300], Step [151/172], Loss: 5.5399\n",
      "Epoch [165/300], Step [152/172], Loss: 7.2365\n",
      "Epoch [165/300], Step [153/172], Loss: 6.3344\n",
      "Epoch [165/300], Step [154/172], Loss: 7.3757\n",
      "Epoch [165/300], Step [155/172], Loss: 6.0539\n",
      "Epoch [165/300], Step [156/172], Loss: 12.7723\n",
      "Epoch [165/300], Step [157/172], Loss: 9.2726\n",
      "Epoch [165/300], Step [158/172], Loss: 6.8957\n",
      "Epoch [165/300], Step [159/172], Loss: 8.9895\n",
      "Epoch [165/300], Step [160/172], Loss: 9.8669\n",
      "Epoch [165/300], Step [161/172], Loss: 7.0423\n",
      "Epoch [165/300], Step [162/172], Loss: 5.4463\n",
      "Epoch [165/300], Step [163/172], Loss: 6.2990\n",
      "Epoch [165/300], Step [164/172], Loss: 8.5497\n",
      "Epoch [165/300], Step [165/172], Loss: 5.7941\n",
      "Epoch [165/300], Step [166/172], Loss: 5.3592\n",
      "Epoch [165/300], Step [167/172], Loss: 9.9233\n",
      "Epoch [165/300], Step [168/172], Loss: 6.4299\n",
      "Epoch [165/300], Step [169/172], Loss: 6.6199\n",
      "Epoch [165/300], Step [170/172], Loss: 4.6532\n",
      "Epoch [165/300], Step [171/172], Loss: 7.3829\n",
      "Epoch [165/300], Step [172/172], Loss: 5.0049\n",
      "Epoch [166/300], Step [1/172], Loss: 56.7464\n",
      "Epoch [166/300], Step [2/172], Loss: 57.8062\n",
      "Epoch [166/300], Step [3/172], Loss: 51.2401\n",
      "Epoch [166/300], Step [4/172], Loss: 30.2314\n",
      "Epoch [166/300], Step [5/172], Loss: 53.1323\n",
      "Epoch [166/300], Step [6/172], Loss: 19.1406\n",
      "Epoch [166/300], Step [7/172], Loss: 29.5017\n",
      "Epoch [166/300], Step [8/172], Loss: 4.7954\n",
      "Epoch [166/300], Step [9/172], Loss: 34.3918\n",
      "Epoch [166/300], Step [10/172], Loss: 43.7124\n",
      "Epoch [166/300], Step [11/172], Loss: 63.2316\n",
      "Epoch [166/300], Step [12/172], Loss: 71.3329\n",
      "Epoch [166/300], Step [13/172], Loss: 38.1104\n",
      "Epoch [166/300], Step [14/172], Loss: 66.3211\n",
      "Epoch [166/300], Step [15/172], Loss: 59.5065\n",
      "Epoch [166/300], Step [16/172], Loss: 11.5060\n",
      "Epoch [166/300], Step [17/172], Loss: 47.8322\n",
      "Epoch [166/300], Step [18/172], Loss: 60.7159\n",
      "Epoch [166/300], Step [19/172], Loss: 83.3609\n",
      "Epoch [166/300], Step [20/172], Loss: 41.1379\n",
      "Epoch [166/300], Step [21/172], Loss: 85.9667\n",
      "Epoch [166/300], Step [22/172], Loss: 63.4967\n",
      "Epoch [166/300], Step [23/172], Loss: 1.8476\n",
      "Epoch [166/300], Step [24/172], Loss: 58.8550\n",
      "Epoch [166/300], Step [25/172], Loss: 42.3556\n",
      "Epoch [166/300], Step [26/172], Loss: 49.5098\n",
      "Epoch [166/300], Step [27/172], Loss: 63.8722\n",
      "Epoch [166/300], Step [28/172], Loss: 26.0753\n",
      "Epoch [166/300], Step [29/172], Loss: 17.9158\n",
      "Epoch [166/300], Step [30/172], Loss: 66.3050\n",
      "Epoch [166/300], Step [31/172], Loss: 39.3324\n",
      "Epoch [166/300], Step [32/172], Loss: 42.6074\n",
      "Epoch [166/300], Step [33/172], Loss: 71.0978\n",
      "Epoch [166/300], Step [34/172], Loss: 3.3804\n",
      "Epoch [166/300], Step [35/172], Loss: 13.2342\n",
      "Epoch [166/300], Step [36/172], Loss: 18.1156\n",
      "Epoch [166/300], Step [37/172], Loss: 17.3501\n",
      "Epoch [166/300], Step [38/172], Loss: 29.4391\n",
      "Epoch [166/300], Step [39/172], Loss: 36.6624\n",
      "Epoch [166/300], Step [40/172], Loss: 20.6166\n",
      "Epoch [166/300], Step [41/172], Loss: 34.9142\n",
      "Epoch [166/300], Step [42/172], Loss: 39.2808\n",
      "Epoch [166/300], Step [43/172], Loss: 27.5149\n",
      "Epoch [166/300], Step [44/172], Loss: 20.6015\n",
      "Epoch [166/300], Step [45/172], Loss: 25.9662\n",
      "Epoch [166/300], Step [46/172], Loss: 18.0369\n",
      "Epoch [166/300], Step [47/172], Loss: 47.0836\n",
      "Epoch [166/300], Step [48/172], Loss: 59.7769\n",
      "Epoch [166/300], Step [49/172], Loss: 20.7683\n",
      "Epoch [166/300], Step [50/172], Loss: 46.2552\n",
      "Epoch [166/300], Step [51/172], Loss: 8.4929\n",
      "Epoch [166/300], Step [52/172], Loss: 18.9372\n",
      "Epoch [166/300], Step [53/172], Loss: 22.4086\n",
      "Epoch [166/300], Step [54/172], Loss: 14.5944\n",
      "Epoch [166/300], Step [55/172], Loss: 14.2856\n",
      "Epoch [166/300], Step [56/172], Loss: 16.7660\n",
      "Epoch [166/300], Step [57/172], Loss: 16.9706\n",
      "Epoch [166/300], Step [58/172], Loss: 13.8583\n",
      "Epoch [166/300], Step [59/172], Loss: 28.0026\n",
      "Epoch [166/300], Step [60/172], Loss: 28.7187\n",
      "Epoch [166/300], Step [61/172], Loss: 6.5349\n",
      "Epoch [166/300], Step [62/172], Loss: 20.6482\n",
      "Epoch [166/300], Step [63/172], Loss: 10.3137\n",
      "Epoch [166/300], Step [64/172], Loss: 10.3981\n",
      "Epoch [166/300], Step [65/172], Loss: 20.0723\n",
      "Epoch [166/300], Step [66/172], Loss: 6.3231\n",
      "Epoch [166/300], Step [67/172], Loss: 23.6661\n",
      "Epoch [166/300], Step [68/172], Loss: 5.4673\n",
      "Epoch [166/300], Step [69/172], Loss: 37.7917\n",
      "Epoch [166/300], Step [70/172], Loss: 42.0663\n",
      "Epoch [166/300], Step [71/172], Loss: 42.0093\n",
      "Epoch [166/300], Step [72/172], Loss: 43.5878\n",
      "Epoch [166/300], Step [73/172], Loss: 51.4840\n",
      "Epoch [166/300], Step [74/172], Loss: 27.1819\n",
      "Epoch [166/300], Step [75/172], Loss: 28.2416\n",
      "Epoch [166/300], Step [76/172], Loss: 30.1840\n",
      "Epoch [166/300], Step [77/172], Loss: 51.8059\n",
      "Epoch [166/300], Step [78/172], Loss: 40.1040\n",
      "Epoch [166/300], Step [79/172], Loss: 39.2088\n",
      "Epoch [166/300], Step [80/172], Loss: 52.2340\n",
      "Epoch [166/300], Step [81/172], Loss: 35.8881\n",
      "Epoch [166/300], Step [82/172], Loss: 36.9521\n",
      "Epoch [166/300], Step [83/172], Loss: 44.8053\n",
      "Epoch [166/300], Step [84/172], Loss: 33.9445\n",
      "Epoch [166/300], Step [85/172], Loss: 39.2222\n",
      "Epoch [166/300], Step [86/172], Loss: 33.3169\n",
      "Epoch [166/300], Step [87/172], Loss: 26.8834\n",
      "Epoch [166/300], Step [88/172], Loss: 26.4777\n",
      "Epoch [166/300], Step [89/172], Loss: 26.9080\n",
      "Epoch [166/300], Step [90/172], Loss: 22.2698\n",
      "Epoch [166/300], Step [91/172], Loss: 27.1282\n",
      "Epoch [166/300], Step [92/172], Loss: 20.5765\n",
      "Epoch [166/300], Step [93/172], Loss: 21.1319\n",
      "Epoch [166/300], Step [94/172], Loss: 29.5167\n",
      "Epoch [166/300], Step [95/172], Loss: 21.3044\n",
      "Epoch [166/300], Step [96/172], Loss: 20.0679\n",
      "Epoch [166/300], Step [97/172], Loss: 28.0544\n",
      "Epoch [166/300], Step [98/172], Loss: 19.7957\n",
      "Epoch [166/300], Step [99/172], Loss: 19.1820\n",
      "Epoch [166/300], Step [100/172], Loss: 15.5142\n",
      "Epoch [166/300], Step [101/172], Loss: 18.4662\n",
      "Epoch [166/300], Step [102/172], Loss: 17.1949\n",
      "Epoch [166/300], Step [103/172], Loss: 12.7554\n",
      "Epoch [166/300], Step [104/172], Loss: 18.0566\n",
      "Epoch [166/300], Step [105/172], Loss: 19.0216\n",
      "Epoch [166/300], Step [106/172], Loss: 15.8471\n",
      "Epoch [166/300], Step [107/172], Loss: 15.5173\n",
      "Epoch [166/300], Step [108/172], Loss: 14.9445\n",
      "Epoch [166/300], Step [109/172], Loss: 15.0548\n",
      "Epoch [166/300], Step [110/172], Loss: 15.8446\n",
      "Epoch [166/300], Step [111/172], Loss: 14.9538\n",
      "Epoch [166/300], Step [112/172], Loss: 17.5501\n",
      "Epoch [166/300], Step [113/172], Loss: 11.8967\n",
      "Epoch [166/300], Step [114/172], Loss: 13.6725\n",
      "Epoch [166/300], Step [115/172], Loss: 19.4837\n",
      "Epoch [166/300], Step [116/172], Loss: 14.5538\n",
      "Epoch [166/300], Step [117/172], Loss: 11.2451\n",
      "Epoch [166/300], Step [118/172], Loss: 14.3486\n",
      "Epoch [166/300], Step [119/172], Loss: 15.6120\n",
      "Epoch [166/300], Step [120/172], Loss: 9.8880\n",
      "Epoch [166/300], Step [121/172], Loss: 9.6486\n",
      "Epoch [166/300], Step [122/172], Loss: 10.4493\n",
      "Epoch [166/300], Step [123/172], Loss: 10.1501\n",
      "Epoch [166/300], Step [124/172], Loss: 7.5850\n",
      "Epoch [166/300], Step [125/172], Loss: 11.8626\n",
      "Epoch [166/300], Step [126/172], Loss: 10.7459\n",
      "Epoch [166/300], Step [127/172], Loss: 10.8981\n",
      "Epoch [166/300], Step [128/172], Loss: 10.5578\n",
      "Epoch [166/300], Step [129/172], Loss: 7.9814\n",
      "Epoch [166/300], Step [130/172], Loss: 12.0129\n",
      "Epoch [166/300], Step [131/172], Loss: 7.3631\n",
      "Epoch [166/300], Step [132/172], Loss: 8.2705\n",
      "Epoch [166/300], Step [133/172], Loss: 8.9734\n",
      "Epoch [166/300], Step [134/172], Loss: 11.5029\n",
      "Epoch [166/300], Step [135/172], Loss: 8.4387\n",
      "Epoch [166/300], Step [136/172], Loss: 8.0028\n",
      "Epoch [166/300], Step [137/172], Loss: 9.1613\n",
      "Epoch [166/300], Step [138/172], Loss: 6.9144\n",
      "Epoch [166/300], Step [139/172], Loss: 9.2526\n",
      "Epoch [166/300], Step [140/172], Loss: 9.3291\n",
      "Epoch [166/300], Step [141/172], Loss: 9.6577\n",
      "Epoch [166/300], Step [142/172], Loss: 13.8741\n",
      "Epoch [166/300], Step [143/172], Loss: 10.2756\n",
      "Epoch [166/300], Step [144/172], Loss: 8.7941\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [166/300], Step [145/172], Loss: 9.6186\n",
      "Epoch [166/300], Step [146/172], Loss: 9.2661\n",
      "Epoch [166/300], Step [147/172], Loss: 4.9801\n",
      "Epoch [166/300], Step [148/172], Loss: 5.8937\n",
      "Epoch [166/300], Step [149/172], Loss: 6.5602\n",
      "Epoch [166/300], Step [150/172], Loss: 6.1628\n",
      "Epoch [166/300], Step [151/172], Loss: 5.5165\n",
      "Epoch [166/300], Step [152/172], Loss: 7.2051\n",
      "Epoch [166/300], Step [153/172], Loss: 6.3248\n",
      "Epoch [166/300], Step [154/172], Loss: 7.3709\n",
      "Epoch [166/300], Step [155/172], Loss: 6.0511\n",
      "Epoch [166/300], Step [156/172], Loss: 12.7781\n",
      "Epoch [166/300], Step [157/172], Loss: 9.2810\n",
      "Epoch [166/300], Step [158/172], Loss: 6.9119\n",
      "Epoch [166/300], Step [159/172], Loss: 9.0328\n",
      "Epoch [166/300], Step [160/172], Loss: 9.8730\n",
      "Epoch [166/300], Step [161/172], Loss: 7.0154\n",
      "Epoch [166/300], Step [162/172], Loss: 5.4184\n",
      "Epoch [166/300], Step [163/172], Loss: 6.2836\n",
      "Epoch [166/300], Step [164/172], Loss: 8.5392\n",
      "Epoch [166/300], Step [165/172], Loss: 5.7934\n",
      "Epoch [166/300], Step [166/172], Loss: 5.3625\n",
      "Epoch [166/300], Step [167/172], Loss: 9.8806\n",
      "Epoch [166/300], Step [168/172], Loss: 6.4272\n",
      "Epoch [166/300], Step [169/172], Loss: 6.5606\n",
      "Epoch [166/300], Step [170/172], Loss: 4.6408\n",
      "Epoch [166/300], Step [171/172], Loss: 7.4075\n",
      "Epoch [166/300], Step [172/172], Loss: 4.9730\n",
      "Epoch [167/300], Step [1/172], Loss: 56.3984\n",
      "Epoch [167/300], Step [2/172], Loss: 57.6887\n",
      "Epoch [167/300], Step [3/172], Loss: 50.9468\n",
      "Epoch [167/300], Step [4/172], Loss: 30.0805\n",
      "Epoch [167/300], Step [5/172], Loss: 52.9674\n",
      "Epoch [167/300], Step [6/172], Loss: 19.1821\n",
      "Epoch [167/300], Step [7/172], Loss: 29.5954\n",
      "Epoch [167/300], Step [8/172], Loss: 4.7394\n",
      "Epoch [167/300], Step [9/172], Loss: 34.3191\n",
      "Epoch [167/300], Step [10/172], Loss: 43.5337\n",
      "Epoch [167/300], Step [11/172], Loss: 62.8327\n",
      "Epoch [167/300], Step [12/172], Loss: 70.7684\n",
      "Epoch [167/300], Step [13/172], Loss: 37.6086\n",
      "Epoch [167/300], Step [14/172], Loss: 65.3076\n",
      "Epoch [167/300], Step [15/172], Loss: 59.1807\n",
      "Epoch [167/300], Step [16/172], Loss: 11.9655\n",
      "Epoch [167/300], Step [17/172], Loss: 47.4618\n",
      "Epoch [167/300], Step [18/172], Loss: 60.5139\n",
      "Epoch [167/300], Step [19/172], Loss: 83.2711\n",
      "Epoch [167/300], Step [20/172], Loss: 40.6380\n",
      "Epoch [167/300], Step [21/172], Loss: 86.3027\n",
      "Epoch [167/300], Step [22/172], Loss: 63.1322\n",
      "Epoch [167/300], Step [23/172], Loss: 1.9745\n",
      "Epoch [167/300], Step [24/172], Loss: 58.7603\n",
      "Epoch [167/300], Step [25/172], Loss: 42.2034\n",
      "Epoch [167/300], Step [26/172], Loss: 49.3615\n",
      "Epoch [167/300], Step [27/172], Loss: 63.9131\n",
      "Epoch [167/300], Step [28/172], Loss: 25.8324\n",
      "Epoch [167/300], Step [29/172], Loss: 17.8173\n",
      "Epoch [167/300], Step [30/172], Loss: 66.1763\n",
      "Epoch [167/300], Step [31/172], Loss: 39.0298\n",
      "Epoch [167/300], Step [32/172], Loss: 42.6208\n",
      "Epoch [167/300], Step [33/172], Loss: 71.0803\n",
      "Epoch [167/300], Step [34/172], Loss: 3.2113\n",
      "Epoch [167/300], Step [35/172], Loss: 13.3141\n",
      "Epoch [167/300], Step [36/172], Loss: 18.0927\n",
      "Epoch [167/300], Step [37/172], Loss: 17.3120\n",
      "Epoch [167/300], Step [38/172], Loss: 29.4310\n",
      "Epoch [167/300], Step [39/172], Loss: 36.5398\n",
      "Epoch [167/300], Step [40/172], Loss: 20.4985\n",
      "Epoch [167/300], Step [41/172], Loss: 34.6575\n",
      "Epoch [167/300], Step [42/172], Loss: 39.1286\n",
      "Epoch [167/300], Step [43/172], Loss: 27.3710\n",
      "Epoch [167/300], Step [44/172], Loss: 20.4774\n",
      "Epoch [167/300], Step [45/172], Loss: 25.8697\n",
      "Epoch [167/300], Step [46/172], Loss: 18.0253\n",
      "Epoch [167/300], Step [47/172], Loss: 47.1513\n",
      "Epoch [167/300], Step [48/172], Loss: 59.9962\n",
      "Epoch [167/300], Step [49/172], Loss: 20.6528\n",
      "Epoch [167/300], Step [50/172], Loss: 46.3606\n",
      "Epoch [167/300], Step [51/172], Loss: 8.4596\n",
      "Epoch [167/300], Step [52/172], Loss: 18.9411\n",
      "Epoch [167/300], Step [53/172], Loss: 22.3135\n",
      "Epoch [167/300], Step [54/172], Loss: 14.4084\n",
      "Epoch [167/300], Step [55/172], Loss: 14.2404\n",
      "Epoch [167/300], Step [56/172], Loss: 17.2023\n",
      "Epoch [167/300], Step [57/172], Loss: 17.0024\n",
      "Epoch [167/300], Step [58/172], Loss: 13.8556\n",
      "Epoch [167/300], Step [59/172], Loss: 28.1448\n",
      "Epoch [167/300], Step [60/172], Loss: 28.6565\n",
      "Epoch [167/300], Step [61/172], Loss: 6.4606\n",
      "Epoch [167/300], Step [62/172], Loss: 20.7121\n",
      "Epoch [167/300], Step [63/172], Loss: 10.3885\n",
      "Epoch [167/300], Step [64/172], Loss: 10.4977\n",
      "Epoch [167/300], Step [65/172], Loss: 20.1208\n",
      "Epoch [167/300], Step [66/172], Loss: 6.3247\n",
      "Epoch [167/300], Step [67/172], Loss: 23.8418\n",
      "Epoch [167/300], Step [68/172], Loss: 5.5402\n",
      "Epoch [167/300], Step [69/172], Loss: 37.5124\n",
      "Epoch [167/300], Step [70/172], Loss: 41.7661\n",
      "Epoch [167/300], Step [71/172], Loss: 41.7372\n",
      "Epoch [167/300], Step [72/172], Loss: 43.2795\n",
      "Epoch [167/300], Step [73/172], Loss: 51.2978\n",
      "Epoch [167/300], Step [74/172], Loss: 26.9540\n",
      "Epoch [167/300], Step [75/172], Loss: 28.1246\n",
      "Epoch [167/300], Step [76/172], Loss: 30.0068\n",
      "Epoch [167/300], Step [77/172], Loss: 51.5006\n",
      "Epoch [167/300], Step [78/172], Loss: 39.8090\n",
      "Epoch [167/300], Step [79/172], Loss: 39.0371\n",
      "Epoch [167/300], Step [80/172], Loss: 52.0533\n",
      "Epoch [167/300], Step [81/172], Loss: 35.7514\n",
      "Epoch [167/300], Step [82/172], Loss: 36.7319\n",
      "Epoch [167/300], Step [83/172], Loss: 44.6253\n",
      "Epoch [167/300], Step [84/172], Loss: 33.9296\n",
      "Epoch [167/300], Step [85/172], Loss: 39.2383\n",
      "Epoch [167/300], Step [86/172], Loss: 33.3250\n",
      "Epoch [167/300], Step [87/172], Loss: 26.8714\n",
      "Epoch [167/300], Step [88/172], Loss: 26.3838\n",
      "Epoch [167/300], Step [89/172], Loss: 26.9794\n",
      "Epoch [167/300], Step [90/172], Loss: 22.1045\n",
      "Epoch [167/300], Step [91/172], Loss: 27.1210\n",
      "Epoch [167/300], Step [92/172], Loss: 20.5278\n",
      "Epoch [167/300], Step [93/172], Loss: 21.0621\n",
      "Epoch [167/300], Step [94/172], Loss: 29.4800\n",
      "Epoch [167/300], Step [95/172], Loss: 21.2416\n",
      "Epoch [167/300], Step [96/172], Loss: 19.9942\n",
      "Epoch [167/300], Step [97/172], Loss: 27.9758\n",
      "Epoch [167/300], Step [98/172], Loss: 19.7354\n",
      "Epoch [167/300], Step [99/172], Loss: 19.1126\n",
      "Epoch [167/300], Step [100/172], Loss: 15.4500\n",
      "Epoch [167/300], Step [101/172], Loss: 18.4240\n",
      "Epoch [167/300], Step [102/172], Loss: 17.1674\n",
      "Epoch [167/300], Step [103/172], Loss: 12.6720\n",
      "Epoch [167/300], Step [104/172], Loss: 18.0123\n",
      "Epoch [167/300], Step [105/172], Loss: 18.9169\n",
      "Epoch [167/300], Step [106/172], Loss: 15.7599\n",
      "Epoch [167/300], Step [107/172], Loss: 15.4903\n",
      "Epoch [167/300], Step [108/172], Loss: 14.8209\n",
      "Epoch [167/300], Step [109/172], Loss: 14.9391\n",
      "Epoch [167/300], Step [110/172], Loss: 15.7243\n",
      "Epoch [167/300], Step [111/172], Loss: 14.9129\n",
      "Epoch [167/300], Step [112/172], Loss: 17.5100\n",
      "Epoch [167/300], Step [113/172], Loss: 11.7738\n",
      "Epoch [167/300], Step [114/172], Loss: 13.6128\n",
      "Epoch [167/300], Step [115/172], Loss: 19.4155\n",
      "Epoch [167/300], Step [116/172], Loss: 14.3733\n",
      "Epoch [167/300], Step [117/172], Loss: 11.1975\n",
      "Epoch [167/300], Step [118/172], Loss: 14.3413\n",
      "Epoch [167/300], Step [119/172], Loss: 15.6056\n",
      "Epoch [167/300], Step [120/172], Loss: 9.8406\n",
      "Epoch [167/300], Step [121/172], Loss: 9.5585\n",
      "Epoch [167/300], Step [122/172], Loss: 10.4430\n",
      "Epoch [167/300], Step [123/172], Loss: 10.1126\n",
      "Epoch [167/300], Step [124/172], Loss: 7.4984\n",
      "Epoch [167/300], Step [125/172], Loss: 11.7707\n",
      "Epoch [167/300], Step [126/172], Loss: 10.6621\n",
      "Epoch [167/300], Step [127/172], Loss: 10.8134\n",
      "Epoch [167/300], Step [128/172], Loss: 10.3964\n",
      "Epoch [167/300], Step [129/172], Loss: 7.9107\n",
      "Epoch [167/300], Step [130/172], Loss: 11.9686\n",
      "Epoch [167/300], Step [131/172], Loss: 7.3029\n",
      "Epoch [167/300], Step [132/172], Loss: 8.1775\n",
      "Epoch [167/300], Step [133/172], Loss: 8.9487\n",
      "Epoch [167/300], Step [134/172], Loss: 11.5547\n",
      "Epoch [167/300], Step [135/172], Loss: 8.4121\n",
      "Epoch [167/300], Step [136/172], Loss: 7.9834\n",
      "Epoch [167/300], Step [137/172], Loss: 9.1201\n",
      "Epoch [167/300], Step [138/172], Loss: 6.8496\n",
      "Epoch [167/300], Step [139/172], Loss: 9.2110\n",
      "Epoch [167/300], Step [140/172], Loss: 9.2550\n",
      "Epoch [167/300], Step [141/172], Loss: 9.6033\n",
      "Epoch [167/300], Step [142/172], Loss: 13.8159\n",
      "Epoch [167/300], Step [143/172], Loss: 10.2480\n",
      "Epoch [167/300], Step [144/172], Loss: 8.7400\n",
      "Epoch [167/300], Step [145/172], Loss: 9.5813\n",
      "Epoch [167/300], Step [146/172], Loss: 9.1744\n",
      "Epoch [167/300], Step [147/172], Loss: 4.9440\n",
      "Epoch [167/300], Step [148/172], Loss: 5.8371\n",
      "Epoch [167/300], Step [149/172], Loss: 6.4974\n",
      "Epoch [167/300], Step [150/172], Loss: 6.0873\n",
      "Epoch [167/300], Step [151/172], Loss: 5.4851\n",
      "Epoch [167/300], Step [152/172], Loss: 7.1798\n",
      "Epoch [167/300], Step [153/172], Loss: 6.2524\n",
      "Epoch [167/300], Step [154/172], Loss: 7.3501\n",
      "Epoch [167/300], Step [155/172], Loss: 5.9778\n",
      "Epoch [167/300], Step [156/172], Loss: 12.7674\n",
      "Epoch [167/300], Step [157/172], Loss: 9.2820\n",
      "Epoch [167/300], Step [158/172], Loss: 6.8815\n",
      "Epoch [167/300], Step [159/172], Loss: 8.9566\n",
      "Epoch [167/300], Step [160/172], Loss: 9.8213\n",
      "Epoch [167/300], Step [161/172], Loss: 6.9794\n",
      "Epoch [167/300], Step [162/172], Loss: 5.3681\n",
      "Epoch [167/300], Step [163/172], Loss: 6.2306\n",
      "Epoch [167/300], Step [164/172], Loss: 8.5265\n",
      "Epoch [167/300], Step [165/172], Loss: 5.7508\n",
      "Epoch [167/300], Step [166/172], Loss: 5.3300\n",
      "Epoch [167/300], Step [167/172], Loss: 9.9279\n",
      "Epoch [167/300], Step [168/172], Loss: 6.3839\n",
      "Epoch [167/300], Step [169/172], Loss: 6.5525\n",
      "Epoch [167/300], Step [170/172], Loss: 4.6019\n",
      "Epoch [167/300], Step [171/172], Loss: 7.3950\n",
      "Epoch [167/300], Step [172/172], Loss: 4.9377\n",
      "Epoch [168/300], Step [1/172], Loss: 56.3330\n",
      "Epoch [168/300], Step [2/172], Loss: 57.5971\n",
      "Epoch [168/300], Step [3/172], Loss: 51.4803\n",
      "Epoch [168/300], Step [4/172], Loss: 29.9604\n",
      "Epoch [168/300], Step [5/172], Loss: 52.7716\n",
      "Epoch [168/300], Step [6/172], Loss: 18.9895\n",
      "Epoch [168/300], Step [7/172], Loss: 29.2760\n",
      "Epoch [168/300], Step [8/172], Loss: 4.6798\n",
      "Epoch [168/300], Step [9/172], Loss: 34.0644\n",
      "Epoch [168/300], Step [10/172], Loss: 43.3300\n",
      "Epoch [168/300], Step [11/172], Loss: 62.5996\n",
      "Epoch [168/300], Step [12/172], Loss: 70.4191\n",
      "Epoch [168/300], Step [13/172], Loss: 37.4954\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [168/300], Step [14/172], Loss: 65.0878\n",
      "Epoch [168/300], Step [15/172], Loss: 58.8405\n",
      "Epoch [168/300], Step [16/172], Loss: 11.5386\n",
      "Epoch [168/300], Step [17/172], Loss: 47.0964\n",
      "Epoch [168/300], Step [18/172], Loss: 60.2995\n",
      "Epoch [168/300], Step [19/172], Loss: 82.8168\n",
      "Epoch [168/300], Step [20/172], Loss: 40.4152\n",
      "Epoch [168/300], Step [21/172], Loss: 86.0575\n",
      "Epoch [168/300], Step [22/172], Loss: 62.9052\n",
      "Epoch [168/300], Step [23/172], Loss: 1.9690\n",
      "Epoch [168/300], Step [24/172], Loss: 58.4997\n",
      "Epoch [168/300], Step [25/172], Loss: 41.9572\n",
      "Epoch [168/300], Step [26/172], Loss: 49.2795\n",
      "Epoch [168/300], Step [27/172], Loss: 63.6105\n",
      "Epoch [168/300], Step [28/172], Loss: 25.6012\n",
      "Epoch [168/300], Step [29/172], Loss: 17.7198\n",
      "Epoch [168/300], Step [30/172], Loss: 66.1447\n",
      "Epoch [168/300], Step [31/172], Loss: 39.0144\n",
      "Epoch [168/300], Step [32/172], Loss: 42.5236\n",
      "Epoch [168/300], Step [33/172], Loss: 71.0608\n",
      "Epoch [168/300], Step [34/172], Loss: 3.1216\n",
      "Epoch [168/300], Step [35/172], Loss: 13.2624\n",
      "Epoch [168/300], Step [36/172], Loss: 17.9772\n",
      "Epoch [168/300], Step [37/172], Loss: 17.2292\n",
      "Epoch [168/300], Step [38/172], Loss: 29.3696\n",
      "Epoch [168/300], Step [39/172], Loss: 36.5598\n",
      "Epoch [168/300], Step [40/172], Loss: 20.4723\n",
      "Epoch [168/300], Step [41/172], Loss: 34.5637\n",
      "Epoch [168/300], Step [42/172], Loss: 39.0227\n",
      "Epoch [168/300], Step [43/172], Loss: 27.3195\n",
      "Epoch [168/300], Step [44/172], Loss: 20.5604\n",
      "Epoch [168/300], Step [45/172], Loss: 25.8795\n",
      "Epoch [168/300], Step [46/172], Loss: 18.0804\n",
      "Epoch [168/300], Step [47/172], Loss: 47.2694\n",
      "Epoch [168/300], Step [48/172], Loss: 60.2577\n",
      "Epoch [168/300], Step [49/172], Loss: 20.5828\n",
      "Epoch [168/300], Step [50/172], Loss: 46.4964\n",
      "Epoch [168/300], Step [51/172], Loss: 8.5013\n",
      "Epoch [168/300], Step [52/172], Loss: 19.0132\n",
      "Epoch [168/300], Step [53/172], Loss: 22.4932\n",
      "Epoch [168/300], Step [54/172], Loss: 14.5040\n",
      "Epoch [168/300], Step [55/172], Loss: 14.3125\n",
      "Epoch [168/300], Step [56/172], Loss: 17.2401\n",
      "Epoch [168/300], Step [57/172], Loss: 16.9298\n",
      "Epoch [168/300], Step [58/172], Loss: 13.8884\n",
      "Epoch [168/300], Step [59/172], Loss: 28.1197\n",
      "Epoch [168/300], Step [60/172], Loss: 28.4116\n",
      "Epoch [168/300], Step [61/172], Loss: 6.5316\n",
      "Epoch [168/300], Step [62/172], Loss: 20.7949\n",
      "Epoch [168/300], Step [63/172], Loss: 10.4672\n",
      "Epoch [168/300], Step [64/172], Loss: 10.5387\n",
      "Epoch [168/300], Step [65/172], Loss: 20.0610\n",
      "Epoch [168/300], Step [66/172], Loss: 6.2958\n",
      "Epoch [168/300], Step [67/172], Loss: 23.7516\n",
      "Epoch [168/300], Step [68/172], Loss: 5.3518\n",
      "Epoch [168/300], Step [69/172], Loss: 37.4256\n",
      "Epoch [168/300], Step [70/172], Loss: 41.6001\n",
      "Epoch [168/300], Step [71/172], Loss: 41.6599\n",
      "Epoch [168/300], Step [72/172], Loss: 43.1855\n",
      "Epoch [168/300], Step [73/172], Loss: 51.1688\n",
      "Epoch [168/300], Step [74/172], Loss: 26.9217\n",
      "Epoch [168/300], Step [75/172], Loss: 28.1747\n",
      "Epoch [168/300], Step [76/172], Loss: 29.8615\n",
      "Epoch [168/300], Step [77/172], Loss: 51.4636\n",
      "Epoch [168/300], Step [78/172], Loss: 39.6790\n",
      "Epoch [168/300], Step [79/172], Loss: 38.9325\n",
      "Epoch [168/300], Step [80/172], Loss: 51.8885\n",
      "Epoch [168/300], Step [81/172], Loss: 35.5462\n",
      "Epoch [168/300], Step [82/172], Loss: 36.6157\n",
      "Epoch [168/300], Step [83/172], Loss: 44.4433\n",
      "Epoch [168/300], Step [84/172], Loss: 33.7196\n",
      "Epoch [168/300], Step [85/172], Loss: 38.9481\n",
      "Epoch [168/300], Step [86/172], Loss: 33.2098\n",
      "Epoch [168/300], Step [87/172], Loss: 26.7855\n",
      "Epoch [168/300], Step [88/172], Loss: 26.2633\n",
      "Epoch [168/300], Step [89/172], Loss: 26.8222\n",
      "Epoch [168/300], Step [90/172], Loss: 21.9929\n",
      "Epoch [168/300], Step [91/172], Loss: 27.0428\n",
      "Epoch [168/300], Step [92/172], Loss: 20.4912\n",
      "Epoch [168/300], Step [93/172], Loss: 21.0284\n",
      "Epoch [168/300], Step [94/172], Loss: 29.4553\n",
      "Epoch [168/300], Step [95/172], Loss: 21.2400\n",
      "Epoch [168/300], Step [96/172], Loss: 19.9689\n",
      "Epoch [168/300], Step [97/172], Loss: 27.9233\n",
      "Epoch [168/300], Step [98/172], Loss: 19.6367\n",
      "Epoch [168/300], Step [99/172], Loss: 19.0583\n",
      "Epoch [168/300], Step [100/172], Loss: 15.3631\n",
      "Epoch [168/300], Step [101/172], Loss: 18.3720\n",
      "Epoch [168/300], Step [102/172], Loss: 17.0756\n",
      "Epoch [168/300], Step [103/172], Loss: 12.6289\n",
      "Epoch [168/300], Step [104/172], Loss: 17.9824\n",
      "Epoch [168/300], Step [105/172], Loss: 18.8613\n",
      "Epoch [168/300], Step [106/172], Loss: 15.7041\n",
      "Epoch [168/300], Step [107/172], Loss: 15.4223\n",
      "Epoch [168/300], Step [108/172], Loss: 14.7665\n",
      "Epoch [168/300], Step [109/172], Loss: 14.8060\n",
      "Epoch [168/300], Step [110/172], Loss: 15.6650\n",
      "Epoch [168/300], Step [111/172], Loss: 14.9198\n",
      "Epoch [168/300], Step [112/172], Loss: 17.5401\n",
      "Epoch [168/300], Step [113/172], Loss: 11.7821\n",
      "Epoch [168/300], Step [114/172], Loss: 13.6365\n",
      "Epoch [168/300], Step [115/172], Loss: 19.3212\n",
      "Epoch [168/300], Step [116/172], Loss: 14.3768\n",
      "Epoch [168/300], Step [117/172], Loss: 11.1634\n",
      "Epoch [168/300], Step [118/172], Loss: 14.2868\n",
      "Epoch [168/300], Step [119/172], Loss: 15.5635\n",
      "Epoch [168/300], Step [120/172], Loss: 9.8267\n",
      "Epoch [168/300], Step [121/172], Loss: 9.4855\n",
      "Epoch [168/300], Step [122/172], Loss: 10.3744\n",
      "Epoch [168/300], Step [123/172], Loss: 10.1315\n",
      "Epoch [168/300], Step [124/172], Loss: 7.4804\n",
      "Epoch [168/300], Step [125/172], Loss: 11.7101\n",
      "Epoch [168/300], Step [126/172], Loss: 10.6994\n",
      "Epoch [168/300], Step [127/172], Loss: 10.8182\n",
      "Epoch [168/300], Step [128/172], Loss: 10.4243\n",
      "Epoch [168/300], Step [129/172], Loss: 7.8911\n",
      "Epoch [168/300], Step [130/172], Loss: 11.9889\n",
      "Epoch [168/300], Step [131/172], Loss: 7.2844\n",
      "Epoch [168/300], Step [132/172], Loss: 8.1519\n",
      "Epoch [168/300], Step [133/172], Loss: 8.9344\n",
      "Epoch [168/300], Step [134/172], Loss: 11.4634\n",
      "Epoch [168/300], Step [135/172], Loss: 8.3912\n",
      "Epoch [168/300], Step [136/172], Loss: 8.0539\n",
      "Epoch [168/300], Step [137/172], Loss: 9.0845\n",
      "Epoch [168/300], Step [138/172], Loss: 6.8392\n",
      "Epoch [168/300], Step [139/172], Loss: 9.1951\n",
      "Epoch [168/300], Step [140/172], Loss: 9.2814\n",
      "Epoch [168/300], Step [141/172], Loss: 9.5949\n",
      "Epoch [168/300], Step [142/172], Loss: 13.7628\n",
      "Epoch [168/300], Step [143/172], Loss: 10.2723\n",
      "Epoch [168/300], Step [144/172], Loss: 8.7557\n",
      "Epoch [168/300], Step [145/172], Loss: 9.5209\n",
      "Epoch [168/300], Step [146/172], Loss: 9.1768\n",
      "Epoch [168/300], Step [147/172], Loss: 4.9395\n",
      "Epoch [168/300], Step [148/172], Loss: 5.8256\n",
      "Epoch [168/300], Step [149/172], Loss: 6.4671\n",
      "Epoch [168/300], Step [150/172], Loss: 6.0659\n",
      "Epoch [168/300], Step [151/172], Loss: 5.4705\n",
      "Epoch [168/300], Step [152/172], Loss: 7.1659\n",
      "Epoch [168/300], Step [153/172], Loss: 6.2303\n",
      "Epoch [168/300], Step [154/172], Loss: 7.3146\n",
      "Epoch [168/300], Step [155/172], Loss: 5.9604\n",
      "Epoch [168/300], Step [156/172], Loss: 12.7744\n",
      "Epoch [168/300], Step [157/172], Loss: 9.2799\n",
      "Epoch [168/300], Step [158/172], Loss: 6.8772\n",
      "Epoch [168/300], Step [159/172], Loss: 8.9909\n",
      "Epoch [168/300], Step [160/172], Loss: 9.8195\n",
      "Epoch [168/300], Step [161/172], Loss: 6.9362\n",
      "Epoch [168/300], Step [162/172], Loss: 5.3455\n",
      "Epoch [168/300], Step [163/172], Loss: 6.2360\n",
      "Epoch [168/300], Step [164/172], Loss: 8.4932\n",
      "Epoch [168/300], Step [165/172], Loss: 5.7385\n",
      "Epoch [168/300], Step [166/172], Loss: 5.2948\n",
      "Epoch [168/300], Step [167/172], Loss: 9.8978\n",
      "Epoch [168/300], Step [168/172], Loss: 6.3391\n",
      "Epoch [168/300], Step [169/172], Loss: 6.4940\n",
      "Epoch [168/300], Step [170/172], Loss: 4.5800\n",
      "Epoch [168/300], Step [171/172], Loss: 7.4006\n",
      "Epoch [168/300], Step [172/172], Loss: 4.9227\n",
      "Epoch [169/300], Step [1/172], Loss: 56.1058\n",
      "Epoch [169/300], Step [2/172], Loss: 57.2800\n",
      "Epoch [169/300], Step [3/172], Loss: 51.1523\n",
      "Epoch [169/300], Step [4/172], Loss: 29.7645\n",
      "Epoch [169/300], Step [5/172], Loss: 52.5043\n",
      "Epoch [169/300], Step [6/172], Loss: 19.0067\n",
      "Epoch [169/300], Step [7/172], Loss: 29.7286\n",
      "Epoch [169/300], Step [8/172], Loss: 4.6530\n",
      "Epoch [169/300], Step [9/172], Loss: 33.9635\n",
      "Epoch [169/300], Step [10/172], Loss: 43.3017\n",
      "Epoch [169/300], Step [11/172], Loss: 62.5234\n",
      "Epoch [169/300], Step [12/172], Loss: 70.1634\n",
      "Epoch [169/300], Step [13/172], Loss: 37.2683\n",
      "Epoch [169/300], Step [14/172], Loss: 64.6489\n",
      "Epoch [169/300], Step [15/172], Loss: 58.6752\n",
      "Epoch [169/300], Step [16/172], Loss: 11.5919\n",
      "Epoch [169/300], Step [17/172], Loss: 46.8934\n",
      "Epoch [169/300], Step [18/172], Loss: 60.1440\n",
      "Epoch [169/300], Step [19/172], Loss: 82.5890\n",
      "Epoch [169/300], Step [20/172], Loss: 40.1836\n",
      "Epoch [169/300], Step [21/172], Loss: 85.6009\n",
      "Epoch [169/300], Step [22/172], Loss: 62.6609\n",
      "Epoch [169/300], Step [23/172], Loss: 2.0146\n",
      "Epoch [169/300], Step [24/172], Loss: 58.1941\n",
      "Epoch [169/300], Step [25/172], Loss: 41.7548\n",
      "Epoch [169/300], Step [26/172], Loss: 48.9810\n",
      "Epoch [169/300], Step [27/172], Loss: 63.4043\n",
      "Epoch [169/300], Step [28/172], Loss: 25.3788\n",
      "Epoch [169/300], Step [29/172], Loss: 17.5342\n",
      "Epoch [169/300], Step [30/172], Loss: 65.6127\n",
      "Epoch [169/300], Step [31/172], Loss: 38.7043\n",
      "Epoch [169/300], Step [32/172], Loss: 42.4017\n",
      "Epoch [169/300], Step [33/172], Loss: 70.8316\n",
      "Epoch [169/300], Step [34/172], Loss: 3.1688\n",
      "Epoch [169/300], Step [35/172], Loss: 13.3688\n",
      "Epoch [169/300], Step [36/172], Loss: 18.1484\n",
      "Epoch [169/300], Step [37/172], Loss: 17.1736\n",
      "Epoch [169/300], Step [38/172], Loss: 29.5036\n",
      "Epoch [169/300], Step [39/172], Loss: 36.4841\n",
      "Epoch [169/300], Step [40/172], Loss: 20.4559\n",
      "Epoch [169/300], Step [41/172], Loss: 34.4979\n",
      "Epoch [169/300], Step [42/172], Loss: 39.0947\n",
      "Epoch [169/300], Step [43/172], Loss: 27.2792\n",
      "Epoch [169/300], Step [44/172], Loss: 20.5903\n",
      "Epoch [169/300], Step [45/172], Loss: 26.0356\n",
      "Epoch [169/300], Step [46/172], Loss: 18.0142\n",
      "Epoch [169/300], Step [47/172], Loss: 47.1781\n",
      "Epoch [169/300], Step [48/172], Loss: 60.4075\n",
      "Epoch [169/300], Step [49/172], Loss: 20.7387\n",
      "Epoch [169/300], Step [50/172], Loss: 46.5628\n",
      "Epoch [169/300], Step [51/172], Loss: 8.4641\n",
      "Epoch [169/300], Step [52/172], Loss: 18.9745\n",
      "Epoch [169/300], Step [53/172], Loss: 22.3961\n",
      "Epoch [169/300], Step [54/172], Loss: 14.4820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [169/300], Step [55/172], Loss: 14.2572\n",
      "Epoch [169/300], Step [56/172], Loss: 16.9032\n",
      "Epoch [169/300], Step [57/172], Loss: 16.9901\n",
      "Epoch [169/300], Step [58/172], Loss: 13.8248\n",
      "Epoch [169/300], Step [59/172], Loss: 28.0844\n",
      "Epoch [169/300], Step [60/172], Loss: 28.3778\n",
      "Epoch [169/300], Step [61/172], Loss: 6.4928\n",
      "Epoch [169/300], Step [62/172], Loss: 20.6026\n",
      "Epoch [169/300], Step [63/172], Loss: 10.3710\n",
      "Epoch [169/300], Step [64/172], Loss: 10.4380\n",
      "Epoch [169/300], Step [65/172], Loss: 20.0902\n",
      "Epoch [169/300], Step [66/172], Loss: 6.2898\n",
      "Epoch [169/300], Step [67/172], Loss: 23.6744\n",
      "Epoch [169/300], Step [68/172], Loss: 5.3583\n",
      "Epoch [169/300], Step [69/172], Loss: 37.0698\n",
      "Epoch [169/300], Step [70/172], Loss: 41.5416\n",
      "Epoch [169/300], Step [71/172], Loss: 41.6966\n",
      "Epoch [169/300], Step [72/172], Loss: 43.0950\n",
      "Epoch [169/300], Step [73/172], Loss: 51.1801\n",
      "Epoch [169/300], Step [74/172], Loss: 26.8132\n",
      "Epoch [169/300], Step [75/172], Loss: 28.3745\n",
      "Epoch [169/300], Step [76/172], Loss: 29.9894\n",
      "Epoch [169/300], Step [77/172], Loss: 51.4251\n",
      "Epoch [169/300], Step [78/172], Loss: 39.6595\n",
      "Epoch [169/300], Step [79/172], Loss: 38.9689\n",
      "Epoch [169/300], Step [80/172], Loss: 51.8974\n",
      "Epoch [169/300], Step [81/172], Loss: 35.5155\n",
      "Epoch [169/300], Step [82/172], Loss: 36.6702\n",
      "Epoch [169/300], Step [83/172], Loss: 44.5127\n",
      "Epoch [169/300], Step [84/172], Loss: 33.7601\n",
      "Epoch [169/300], Step [85/172], Loss: 39.0337\n",
      "Epoch [169/300], Step [86/172], Loss: 33.3021\n",
      "Epoch [169/300], Step [87/172], Loss: 26.8293\n",
      "Epoch [169/300], Step [88/172], Loss: 26.2998\n",
      "Epoch [169/300], Step [89/172], Loss: 26.8877\n",
      "Epoch [169/300], Step [90/172], Loss: 22.0363\n",
      "Epoch [169/300], Step [91/172], Loss: 27.1358\n",
      "Epoch [169/300], Step [92/172], Loss: 20.5484\n",
      "Epoch [169/300], Step [93/172], Loss: 21.0399\n",
      "Epoch [169/300], Step [94/172], Loss: 29.5663\n",
      "Epoch [169/300], Step [95/172], Loss: 21.2255\n",
      "Epoch [169/300], Step [96/172], Loss: 20.0309\n",
      "Epoch [169/300], Step [97/172], Loss: 28.0606\n",
      "Epoch [169/300], Step [98/172], Loss: 19.7269\n",
      "Epoch [169/300], Step [99/172], Loss: 19.1312\n",
      "Epoch [169/300], Step [100/172], Loss: 15.4573\n",
      "Epoch [169/300], Step [101/172], Loss: 18.4744\n",
      "Epoch [169/300], Step [102/172], Loss: 17.1591\n",
      "Epoch [169/300], Step [103/172], Loss: 12.6377\n",
      "Epoch [169/300], Step [104/172], Loss: 18.1385\n",
      "Epoch [169/300], Step [105/172], Loss: 18.9090\n",
      "Epoch [169/300], Step [106/172], Loss: 15.7380\n",
      "Epoch [169/300], Step [107/172], Loss: 15.4945\n",
      "Epoch [169/300], Step [108/172], Loss: 14.7667\n",
      "Epoch [169/300], Step [109/172], Loss: 14.7938\n",
      "Epoch [169/300], Step [110/172], Loss: 15.6760\n",
      "Epoch [169/300], Step [111/172], Loss: 14.9816\n",
      "Epoch [169/300], Step [112/172], Loss: 17.5311\n",
      "Epoch [169/300], Step [113/172], Loss: 11.7405\n",
      "Epoch [169/300], Step [114/172], Loss: 13.6258\n",
      "Epoch [169/300], Step [115/172], Loss: 19.3204\n",
      "Epoch [169/300], Step [116/172], Loss: 14.3772\n",
      "Epoch [169/300], Step [117/172], Loss: 11.1962\n",
      "Epoch [169/300], Step [118/172], Loss: 14.2726\n",
      "Epoch [169/300], Step [119/172], Loss: 15.6240\n",
      "Epoch [169/300], Step [120/172], Loss: 9.8202\n",
      "Epoch [169/300], Step [121/172], Loss: 9.5264\n",
      "Epoch [169/300], Step [122/172], Loss: 10.4115\n",
      "Epoch [169/300], Step [123/172], Loss: 10.1089\n",
      "Epoch [169/300], Step [124/172], Loss: 7.5332\n",
      "Epoch [169/300], Step [125/172], Loss: 11.8260\n",
      "Epoch [169/300], Step [126/172], Loss: 10.7580\n",
      "Epoch [169/300], Step [127/172], Loss: 10.7974\n",
      "Epoch [169/300], Step [128/172], Loss: 10.3870\n",
      "Epoch [169/300], Step [129/172], Loss: 7.9005\n",
      "Epoch [169/300], Step [130/172], Loss: 11.9976\n",
      "Epoch [169/300], Step [131/172], Loss: 7.2731\n",
      "Epoch [169/300], Step [132/172], Loss: 8.1651\n",
      "Epoch [169/300], Step [133/172], Loss: 8.9305\n",
      "Epoch [169/300], Step [134/172], Loss: 11.5138\n",
      "Epoch [169/300], Step [135/172], Loss: 8.4626\n",
      "Epoch [169/300], Step [136/172], Loss: 8.0449\n",
      "Epoch [169/300], Step [137/172], Loss: 9.1485\n",
      "Epoch [169/300], Step [138/172], Loss: 6.8380\n",
      "Epoch [169/300], Step [139/172], Loss: 9.3503\n",
      "Epoch [169/300], Step [140/172], Loss: 9.2801\n",
      "Epoch [169/300], Step [141/172], Loss: 9.5688\n",
      "Epoch [169/300], Step [142/172], Loss: 13.7815\n",
      "Epoch [169/300], Step [143/172], Loss: 10.2977\n",
      "Epoch [169/300], Step [144/172], Loss: 8.8138\n",
      "Epoch [169/300], Step [145/172], Loss: 9.5646\n",
      "Epoch [169/300], Step [146/172], Loss: 9.1651\n",
      "Epoch [169/300], Step [147/172], Loss: 4.9838\n",
      "Epoch [169/300], Step [148/172], Loss: 5.8323\n",
      "Epoch [169/300], Step [149/172], Loss: 6.4975\n",
      "Epoch [169/300], Step [150/172], Loss: 6.0746\n",
      "Epoch [169/300], Step [151/172], Loss: 5.4408\n",
      "Epoch [169/300], Step [152/172], Loss: 7.1943\n",
      "Epoch [169/300], Step [153/172], Loss: 6.2975\n",
      "Epoch [169/300], Step [154/172], Loss: 7.3191\n",
      "Epoch [169/300], Step [155/172], Loss: 6.0562\n",
      "Epoch [169/300], Step [156/172], Loss: 12.8090\n",
      "Epoch [169/300], Step [157/172], Loss: 9.3006\n",
      "Epoch [169/300], Step [158/172], Loss: 6.8906\n",
      "Epoch [169/300], Step [159/172], Loss: 8.9681\n",
      "Epoch [169/300], Step [160/172], Loss: 9.8368\n",
      "Epoch [169/300], Step [161/172], Loss: 6.9939\n",
      "Epoch [169/300], Step [162/172], Loss: 5.3379\n",
      "Epoch [169/300], Step [163/172], Loss: 6.2454\n",
      "Epoch [169/300], Step [164/172], Loss: 8.4893\n",
      "Epoch [169/300], Step [165/172], Loss: 5.7754\n",
      "Epoch [169/300], Step [166/172], Loss: 5.3224\n",
      "Epoch [169/300], Step [167/172], Loss: 10.0178\n",
      "Epoch [169/300], Step [168/172], Loss: 6.3910\n",
      "Epoch [169/300], Step [169/172], Loss: 6.5307\n",
      "Epoch [169/300], Step [170/172], Loss: 4.6116\n",
      "Epoch [169/300], Step [171/172], Loss: 7.4743\n",
      "Epoch [169/300], Step [172/172], Loss: 5.0229\n",
      "Epoch [170/300], Step [1/172], Loss: 55.9167\n",
      "Epoch [170/300], Step [2/172], Loss: 57.0205\n",
      "Epoch [170/300], Step [3/172], Loss: 51.1121\n",
      "Epoch [170/300], Step [4/172], Loss: 29.6161\n",
      "Epoch [170/300], Step [5/172], Loss: 52.2337\n",
      "Epoch [170/300], Step [6/172], Loss: 19.0098\n",
      "Epoch [170/300], Step [7/172], Loss: 29.7137\n",
      "Epoch [170/300], Step [8/172], Loss: 4.6283\n",
      "Epoch [170/300], Step [9/172], Loss: 33.8819\n",
      "Epoch [170/300], Step [10/172], Loss: 43.3604\n",
      "Epoch [170/300], Step [11/172], Loss: 62.3398\n",
      "Epoch [170/300], Step [12/172], Loss: 70.0974\n",
      "Epoch [170/300], Step [13/172], Loss: 37.2262\n",
      "Epoch [170/300], Step [14/172], Loss: 64.7002\n",
      "Epoch [170/300], Step [15/172], Loss: 58.6283\n",
      "Epoch [170/300], Step [16/172], Loss: 11.2745\n",
      "Epoch [170/300], Step [17/172], Loss: 46.8956\n",
      "Epoch [170/300], Step [18/172], Loss: 60.1675\n",
      "Epoch [170/300], Step [19/172], Loss: 82.5401\n",
      "Epoch [170/300], Step [20/172], Loss: 39.8023\n",
      "Epoch [170/300], Step [21/172], Loss: 85.3467\n",
      "Epoch [170/300], Step [22/172], Loss: 62.6504\n",
      "Epoch [170/300], Step [23/172], Loss: 1.7920\n",
      "Epoch [170/300], Step [24/172], Loss: 57.9470\n",
      "Epoch [170/300], Step [25/172], Loss: 41.4979\n",
      "Epoch [170/300], Step [26/172], Loss: 48.7532\n",
      "Epoch [170/300], Step [27/172], Loss: 63.2330\n",
      "Epoch [170/300], Step [28/172], Loss: 25.2846\n",
      "Epoch [170/300], Step [29/172], Loss: 17.5547\n",
      "Epoch [170/300], Step [30/172], Loss: 65.5806\n",
      "Epoch [170/300], Step [31/172], Loss: 38.5164\n",
      "Epoch [170/300], Step [32/172], Loss: 42.1508\n",
      "Epoch [170/300], Step [33/172], Loss: 70.3980\n",
      "Epoch [170/300], Step [34/172], Loss: 3.0749\n",
      "Epoch [170/300], Step [35/172], Loss: 13.3659\n",
      "Epoch [170/300], Step [36/172], Loss: 17.9175\n",
      "Epoch [170/300], Step [37/172], Loss: 17.1044\n",
      "Epoch [170/300], Step [38/172], Loss: 29.3688\n",
      "Epoch [170/300], Step [39/172], Loss: 36.3224\n",
      "Epoch [170/300], Step [40/172], Loss: 20.3447\n",
      "Epoch [170/300], Step [41/172], Loss: 34.2689\n",
      "Epoch [170/300], Step [42/172], Loss: 38.7185\n",
      "Epoch [170/300], Step [43/172], Loss: 27.1746\n",
      "Epoch [170/300], Step [44/172], Loss: 20.5755\n",
      "Epoch [170/300], Step [45/172], Loss: 25.9277\n",
      "Epoch [170/300], Step [46/172], Loss: 18.2338\n",
      "Epoch [170/300], Step [47/172], Loss: 47.2204\n",
      "Epoch [170/300], Step [48/172], Loss: 60.3224\n",
      "Epoch [170/300], Step [49/172], Loss: 20.7644\n",
      "Epoch [170/300], Step [50/172], Loss: 46.7621\n",
      "Epoch [170/300], Step [51/172], Loss: 8.4461\n",
      "Epoch [170/300], Step [52/172], Loss: 18.9786\n",
      "Epoch [170/300], Step [53/172], Loss: 22.5504\n",
      "Epoch [170/300], Step [54/172], Loss: 14.4047\n",
      "Epoch [170/300], Step [55/172], Loss: 14.2935\n",
      "Epoch [170/300], Step [56/172], Loss: 16.9764\n",
      "Epoch [170/300], Step [57/172], Loss: 16.9705\n",
      "Epoch [170/300], Step [58/172], Loss: 13.8095\n",
      "Epoch [170/300], Step [59/172], Loss: 27.9590\n",
      "Epoch [170/300], Step [60/172], Loss: 28.4470\n",
      "Epoch [170/300], Step [61/172], Loss: 6.4843\n",
      "Epoch [170/300], Step [62/172], Loss: 20.5719\n",
      "Epoch [170/300], Step [63/172], Loss: 10.2856\n",
      "Epoch [170/300], Step [64/172], Loss: 10.4905\n",
      "Epoch [170/300], Step [65/172], Loss: 20.0346\n",
      "Epoch [170/300], Step [66/172], Loss: 6.3751\n",
      "Epoch [170/300], Step [67/172], Loss: 23.7587\n",
      "Epoch [170/300], Step [68/172], Loss: 5.4922\n",
      "Epoch [170/300], Step [69/172], Loss: 37.0157\n",
      "Epoch [170/300], Step [70/172], Loss: 41.1692\n",
      "Epoch [170/300], Step [71/172], Loss: 41.4763\n",
      "Epoch [170/300], Step [72/172], Loss: 42.8836\n",
      "Epoch [170/300], Step [73/172], Loss: 50.8707\n",
      "Epoch [170/300], Step [74/172], Loss: 26.7893\n",
      "Epoch [170/300], Step [75/172], Loss: 28.0905\n",
      "Epoch [170/300], Step [76/172], Loss: 29.8286\n",
      "Epoch [170/300], Step [77/172], Loss: 51.2103\n",
      "Epoch [170/300], Step [78/172], Loss: 39.4750\n",
      "Epoch [170/300], Step [79/172], Loss: 38.7933\n",
      "Epoch [170/300], Step [80/172], Loss: 51.7833\n",
      "Epoch [170/300], Step [81/172], Loss: 35.3942\n",
      "Epoch [170/300], Step [82/172], Loss: 36.5725\n",
      "Epoch [170/300], Step [83/172], Loss: 44.3994\n",
      "Epoch [170/300], Step [84/172], Loss: 33.6425\n",
      "Epoch [170/300], Step [85/172], Loss: 38.8356\n",
      "Epoch [170/300], Step [86/172], Loss: 33.2269\n",
      "Epoch [170/300], Step [87/172], Loss: 26.7276\n",
      "Epoch [170/300], Step [88/172], Loss: 26.1194\n",
      "Epoch [170/300], Step [89/172], Loss: 26.8394\n",
      "Epoch [170/300], Step [90/172], Loss: 21.9377\n",
      "Epoch [170/300], Step [91/172], Loss: 27.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/300], Step [92/172], Loss: 20.4708\n",
      "Epoch [170/300], Step [93/172], Loss: 21.0545\n",
      "Epoch [170/300], Step [94/172], Loss: 29.5121\n",
      "Epoch [170/300], Step [95/172], Loss: 21.1858\n",
      "Epoch [170/300], Step [96/172], Loss: 19.9912\n",
      "Epoch [170/300], Step [97/172], Loss: 27.9053\n",
      "Epoch [170/300], Step [98/172], Loss: 19.5912\n",
      "Epoch [170/300], Step [99/172], Loss: 19.0353\n",
      "Epoch [170/300], Step [100/172], Loss: 15.3546\n",
      "Epoch [170/300], Step [101/172], Loss: 18.3650\n",
      "Epoch [170/300], Step [102/172], Loss: 17.0415\n",
      "Epoch [170/300], Step [103/172], Loss: 12.5931\n",
      "Epoch [170/300], Step [104/172], Loss: 18.0631\n",
      "Epoch [170/300], Step [105/172], Loss: 18.8173\n",
      "Epoch [170/300], Step [106/172], Loss: 15.7224\n",
      "Epoch [170/300], Step [107/172], Loss: 15.4359\n",
      "Epoch [170/300], Step [108/172], Loss: 14.7461\n",
      "Epoch [170/300], Step [109/172], Loss: 14.7100\n",
      "Epoch [170/300], Step [110/172], Loss: 15.7152\n",
      "Epoch [170/300], Step [111/172], Loss: 14.9867\n",
      "Epoch [170/300], Step [112/172], Loss: 17.5661\n",
      "Epoch [170/300], Step [113/172], Loss: 11.8118\n",
      "Epoch [170/300], Step [114/172], Loss: 13.6465\n",
      "Epoch [170/300], Step [115/172], Loss: 19.2706\n",
      "Epoch [170/300], Step [116/172], Loss: 14.4548\n",
      "Epoch [170/300], Step [117/172], Loss: 11.2034\n",
      "Epoch [170/300], Step [118/172], Loss: 14.2524\n",
      "Epoch [170/300], Step [119/172], Loss: 15.6712\n",
      "Epoch [170/300], Step [120/172], Loss: 9.8353\n",
      "Epoch [170/300], Step [121/172], Loss: 9.4961\n",
      "Epoch [170/300], Step [122/172], Loss: 10.3600\n",
      "Epoch [170/300], Step [123/172], Loss: 10.2143\n",
      "Epoch [170/300], Step [124/172], Loss: 7.4921\n",
      "Epoch [170/300], Step [125/172], Loss: 11.7296\n",
      "Epoch [170/300], Step [126/172], Loss: 10.7585\n",
      "Epoch [170/300], Step [127/172], Loss: 10.8764\n",
      "Epoch [170/300], Step [128/172], Loss: 10.5097\n",
      "Epoch [170/300], Step [129/172], Loss: 7.9324\n",
      "Epoch [170/300], Step [130/172], Loss: 12.0840\n",
      "Epoch [170/300], Step [131/172], Loss: 7.2796\n",
      "Epoch [170/300], Step [132/172], Loss: 8.1654\n",
      "Epoch [170/300], Step [133/172], Loss: 8.9267\n",
      "Epoch [170/300], Step [134/172], Loss: 11.4934\n",
      "Epoch [170/300], Step [135/172], Loss: 8.4446\n",
      "Epoch [170/300], Step [136/172], Loss: 8.0975\n",
      "Epoch [170/300], Step [137/172], Loss: 9.0983\n",
      "Epoch [170/300], Step [138/172], Loss: 6.8304\n",
      "Epoch [170/300], Step [139/172], Loss: 9.2884\n",
      "Epoch [170/300], Step [140/172], Loss: 9.3412\n",
      "Epoch [170/300], Step [141/172], Loss: 9.5610\n",
      "Epoch [170/300], Step [142/172], Loss: 13.8579\n",
      "Epoch [170/300], Step [143/172], Loss: 10.3422\n",
      "Epoch [170/300], Step [144/172], Loss: 8.7787\n",
      "Epoch [170/300], Step [145/172], Loss: 9.5801\n",
      "Epoch [170/300], Step [146/172], Loss: 9.2077\n",
      "Epoch [170/300], Step [147/172], Loss: 4.9532\n",
      "Epoch [170/300], Step [148/172], Loss: 5.8300\n",
      "Epoch [170/300], Step [149/172], Loss: 6.4632\n",
      "Epoch [170/300], Step [150/172], Loss: 6.0444\n",
      "Epoch [170/300], Step [151/172], Loss: 5.4516\n",
      "Epoch [170/300], Step [152/172], Loss: 7.2062\n",
      "Epoch [170/300], Step [153/172], Loss: 6.2621\n",
      "Epoch [170/300], Step [154/172], Loss: 7.3010\n",
      "Epoch [170/300], Step [155/172], Loss: 5.9987\n",
      "Epoch [170/300], Step [156/172], Loss: 12.8530\n",
      "Epoch [170/300], Step [157/172], Loss: 9.2956\n",
      "Epoch [170/300], Step [158/172], Loss: 6.8984\n",
      "Epoch [170/300], Step [159/172], Loss: 9.0904\n",
      "Epoch [170/300], Step [160/172], Loss: 9.8376\n",
      "Epoch [170/300], Step [161/172], Loss: 6.9527\n",
      "Epoch [170/300], Step [162/172], Loss: 5.3154\n",
      "Epoch [170/300], Step [163/172], Loss: 6.2732\n",
      "Epoch [170/300], Step [164/172], Loss: 8.4788\n",
      "Epoch [170/300], Step [165/172], Loss: 5.7637\n",
      "Epoch [170/300], Step [166/172], Loss: 5.2949\n",
      "Epoch [170/300], Step [167/172], Loss: 9.9813\n",
      "Epoch [170/300], Step [168/172], Loss: 6.3388\n",
      "Epoch [170/300], Step [169/172], Loss: 6.4491\n",
      "Epoch [170/300], Step [170/172], Loss: 4.6011\n",
      "Epoch [170/300], Step [171/172], Loss: 7.4976\n",
      "Epoch [170/300], Step [172/172], Loss: 4.9768\n",
      "Epoch [171/300], Step [1/172], Loss: 55.5625\n",
      "Epoch [171/300], Step [2/172], Loss: 56.7231\n",
      "Epoch [171/300], Step [3/172], Loss: 50.6802\n",
      "Epoch [171/300], Step [4/172], Loss: 29.4408\n",
      "Epoch [171/300], Step [5/172], Loss: 51.7543\n",
      "Epoch [171/300], Step [6/172], Loss: 18.9852\n",
      "Epoch [171/300], Step [7/172], Loss: 29.6717\n",
      "Epoch [171/300], Step [8/172], Loss: 4.8454\n",
      "Epoch [171/300], Step [9/172], Loss: 33.8114\n",
      "Epoch [171/300], Step [10/172], Loss: 43.3021\n",
      "Epoch [171/300], Step [11/172], Loss: 62.2670\n",
      "Epoch [171/300], Step [12/172], Loss: 69.8247\n",
      "Epoch [171/300], Step [13/172], Loss: 37.1609\n",
      "Epoch [171/300], Step [14/172], Loss: 64.4294\n",
      "Epoch [171/300], Step [15/172], Loss: 58.4727\n",
      "Epoch [171/300], Step [16/172], Loss: 11.0511\n",
      "Epoch [171/300], Step [17/172], Loss: 46.8783\n",
      "Epoch [171/300], Step [18/172], Loss: 60.2557\n",
      "Epoch [171/300], Step [19/172], Loss: 82.6375\n",
      "Epoch [171/300], Step [20/172], Loss: 39.5653\n",
      "Epoch [171/300], Step [21/172], Loss: 85.5270\n",
      "Epoch [171/300], Step [22/172], Loss: 62.3672\n",
      "Epoch [171/300], Step [23/172], Loss: 1.9426\n",
      "Epoch [171/300], Step [24/172], Loss: 57.7885\n",
      "Epoch [171/300], Step [25/172], Loss: 41.6202\n",
      "Epoch [171/300], Step [26/172], Loss: 48.7082\n",
      "Epoch [171/300], Step [27/172], Loss: 62.9728\n",
      "Epoch [171/300], Step [28/172], Loss: 25.0511\n",
      "Epoch [171/300], Step [29/172], Loss: 17.4343\n",
      "Epoch [171/300], Step [30/172], Loss: 65.0723\n",
      "Epoch [171/300], Step [31/172], Loss: 38.1812\n",
      "Epoch [171/300], Step [32/172], Loss: 42.2613\n",
      "Epoch [171/300], Step [33/172], Loss: 70.6219\n",
      "Epoch [171/300], Step [34/172], Loss: 3.0498\n",
      "Epoch [171/300], Step [35/172], Loss: 13.3813\n",
      "Epoch [171/300], Step [36/172], Loss: 18.0090\n",
      "Epoch [171/300], Step [37/172], Loss: 16.9708\n",
      "Epoch [171/300], Step [38/172], Loss: 29.3622\n",
      "Epoch [171/300], Step [39/172], Loss: 36.2946\n",
      "Epoch [171/300], Step [40/172], Loss: 20.2715\n",
      "Epoch [171/300], Step [41/172], Loss: 34.2438\n",
      "Epoch [171/300], Step [42/172], Loss: 38.8315\n",
      "Epoch [171/300], Step [43/172], Loss: 27.1343\n",
      "Epoch [171/300], Step [44/172], Loss: 20.5894\n",
      "Epoch [171/300], Step [45/172], Loss: 25.9418\n",
      "Epoch [171/300], Step [46/172], Loss: 18.0624\n",
      "Epoch [171/300], Step [47/172], Loss: 47.0712\n",
      "Epoch [171/300], Step [48/172], Loss: 60.9806\n",
      "Epoch [171/300], Step [49/172], Loss: 20.8701\n",
      "Epoch [171/300], Step [50/172], Loss: 46.8408\n",
      "Epoch [171/300], Step [51/172], Loss: 8.4544\n",
      "Epoch [171/300], Step [52/172], Loss: 19.0250\n",
      "Epoch [171/300], Step [53/172], Loss: 22.4165\n",
      "Epoch [171/300], Step [54/172], Loss: 14.5178\n",
      "Epoch [171/300], Step [55/172], Loss: 14.3658\n",
      "Epoch [171/300], Step [56/172], Loss: 16.8636\n",
      "Epoch [171/300], Step [57/172], Loss: 16.9990\n",
      "Epoch [171/300], Step [58/172], Loss: 13.6311\n",
      "Epoch [171/300], Step [59/172], Loss: 27.9560\n",
      "Epoch [171/300], Step [60/172], Loss: 27.8932\n",
      "Epoch [171/300], Step [61/172], Loss: 6.4638\n",
      "Epoch [171/300], Step [62/172], Loss: 20.6398\n",
      "Epoch [171/300], Step [63/172], Loss: 10.3479\n",
      "Epoch [171/300], Step [64/172], Loss: 10.4483\n",
      "Epoch [171/300], Step [65/172], Loss: 20.1436\n",
      "Epoch [171/300], Step [66/172], Loss: 6.4519\n",
      "Epoch [171/300], Step [67/172], Loss: 23.5732\n",
      "Epoch [171/300], Step [68/172], Loss: 5.5395\n",
      "Epoch [171/300], Step [69/172], Loss: 36.7700\n",
      "Epoch [171/300], Step [70/172], Loss: 41.2025\n",
      "Epoch [171/300], Step [71/172], Loss: 41.6024\n",
      "Epoch [171/300], Step [72/172], Loss: 42.8167\n",
      "Epoch [171/300], Step [73/172], Loss: 50.8785\n",
      "Epoch [171/300], Step [74/172], Loss: 26.9771\n",
      "Epoch [171/300], Step [75/172], Loss: 28.4000\n",
      "Epoch [171/300], Step [76/172], Loss: 29.9795\n",
      "Epoch [171/300], Step [77/172], Loss: 51.1143\n",
      "Epoch [171/300], Step [78/172], Loss: 39.5678\n",
      "Epoch [171/300], Step [79/172], Loss: 38.6083\n",
      "Epoch [171/300], Step [80/172], Loss: 51.8145\n",
      "Epoch [171/300], Step [81/172], Loss: 35.2581\n",
      "Epoch [171/300], Step [82/172], Loss: 36.7108\n",
      "Epoch [171/300], Step [83/172], Loss: 44.5750\n",
      "Epoch [171/300], Step [84/172], Loss: 33.6105\n",
      "Epoch [171/300], Step [85/172], Loss: 38.9503\n",
      "Epoch [171/300], Step [86/172], Loss: 33.2886\n",
      "Epoch [171/300], Step [87/172], Loss: 26.7667\n",
      "Epoch [171/300], Step [88/172], Loss: 26.1735\n",
      "Epoch [171/300], Step [89/172], Loss: 26.9058\n",
      "Epoch [171/300], Step [90/172], Loss: 22.0347\n",
      "Epoch [171/300], Step [91/172], Loss: 27.0263\n",
      "Epoch [171/300], Step [92/172], Loss: 20.5128\n",
      "Epoch [171/300], Step [93/172], Loss: 21.1133\n",
      "Epoch [171/300], Step [94/172], Loss: 29.6112\n",
      "Epoch [171/300], Step [95/172], Loss: 21.2478\n",
      "Epoch [171/300], Step [96/172], Loss: 20.0815\n",
      "Epoch [171/300], Step [97/172], Loss: 28.0614\n",
      "Epoch [171/300], Step [98/172], Loss: 19.6410\n",
      "Epoch [171/300], Step [99/172], Loss: 19.1644\n",
      "Epoch [171/300], Step [100/172], Loss: 15.3766\n",
      "Epoch [171/300], Step [101/172], Loss: 18.4960\n",
      "Epoch [171/300], Step [102/172], Loss: 17.1474\n",
      "Epoch [171/300], Step [103/172], Loss: 12.6456\n",
      "Epoch [171/300], Step [104/172], Loss: 18.2380\n",
      "Epoch [171/300], Step [105/172], Loss: 18.9817\n",
      "Epoch [171/300], Step [106/172], Loss: 15.7009\n",
      "Epoch [171/300], Step [107/172], Loss: 15.4528\n",
      "Epoch [171/300], Step [108/172], Loss: 14.7759\n",
      "Epoch [171/300], Step [109/172], Loss: 14.7746\n",
      "Epoch [171/300], Step [110/172], Loss: 15.8612\n",
      "Epoch [171/300], Step [111/172], Loss: 15.0418\n",
      "Epoch [171/300], Step [112/172], Loss: 17.6427\n",
      "Epoch [171/300], Step [113/172], Loss: 11.8643\n",
      "Epoch [171/300], Step [114/172], Loss: 13.6808\n",
      "Epoch [171/300], Step [115/172], Loss: 19.3717\n",
      "Epoch [171/300], Step [116/172], Loss: 14.6111\n",
      "Epoch [171/300], Step [117/172], Loss: 11.2929\n",
      "Epoch [171/300], Step [118/172], Loss: 14.2373\n",
      "Epoch [171/300], Step [119/172], Loss: 15.7179\n",
      "Epoch [171/300], Step [120/172], Loss: 9.8695\n",
      "Epoch [171/300], Step [121/172], Loss: 9.5654\n",
      "Epoch [171/300], Step [122/172], Loss: 10.4304\n",
      "Epoch [171/300], Step [123/172], Loss: 10.2223\n",
      "Epoch [171/300], Step [124/172], Loss: 7.5718\n",
      "Epoch [171/300], Step [125/172], Loss: 11.8850\n",
      "Epoch [171/300], Step [126/172], Loss: 10.8757\n",
      "Epoch [171/300], Step [127/172], Loss: 10.9233\n",
      "Epoch [171/300], Step [128/172], Loss: 10.5580\n",
      "Epoch [171/300], Step [129/172], Loss: 7.9620\n",
      "Epoch [171/300], Step [130/172], Loss: 12.0890\n",
      "Epoch [171/300], Step [131/172], Loss: 7.3072\n",
      "Epoch [171/300], Step [132/172], Loss: 8.1967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/300], Step [133/172], Loss: 8.9410\n",
      "Epoch [171/300], Step [134/172], Loss: 11.4817\n",
      "Epoch [171/300], Step [135/172], Loss: 8.4619\n",
      "Epoch [171/300], Step [136/172], Loss: 8.1044\n",
      "Epoch [171/300], Step [137/172], Loss: 9.1863\n",
      "Epoch [171/300], Step [138/172], Loss: 6.8751\n",
      "Epoch [171/300], Step [139/172], Loss: 9.3919\n",
      "Epoch [171/300], Step [140/172], Loss: 9.4017\n",
      "Epoch [171/300], Step [141/172], Loss: 9.5918\n",
      "Epoch [171/300], Step [142/172], Loss: 14.0253\n",
      "Epoch [171/300], Step [143/172], Loss: 10.4066\n",
      "Epoch [171/300], Step [144/172], Loss: 8.8441\n",
      "Epoch [171/300], Step [145/172], Loss: 9.6786\n",
      "Epoch [171/300], Step [146/172], Loss: 9.2907\n",
      "Epoch [171/300], Step [147/172], Loss: 5.0181\n",
      "Epoch [171/300], Step [148/172], Loss: 5.8692\n",
      "Epoch [171/300], Step [149/172], Loss: 6.5212\n",
      "Epoch [171/300], Step [150/172], Loss: 6.0578\n",
      "Epoch [171/300], Step [151/172], Loss: 5.4704\n",
      "Epoch [171/300], Step [152/172], Loss: 7.2278\n",
      "Epoch [171/300], Step [153/172], Loss: 6.3237\n",
      "Epoch [171/300], Step [154/172], Loss: 7.3407\n",
      "Epoch [171/300], Step [155/172], Loss: 6.0800\n",
      "Epoch [171/300], Step [156/172], Loss: 12.8950\n",
      "Epoch [171/300], Step [157/172], Loss: 9.3266\n",
      "Epoch [171/300], Step [158/172], Loss: 6.9628\n",
      "Epoch [171/300], Step [159/172], Loss: 9.1282\n",
      "Epoch [171/300], Step [160/172], Loss: 9.9006\n",
      "Epoch [171/300], Step [161/172], Loss: 7.0308\n",
      "Epoch [171/300], Step [162/172], Loss: 5.3094\n",
      "Epoch [171/300], Step [163/172], Loss: 6.2832\n",
      "Epoch [171/300], Step [164/172], Loss: 8.5521\n",
      "Epoch [171/300], Step [165/172], Loss: 5.8153\n",
      "Epoch [171/300], Step [166/172], Loss: 5.3468\n",
      "Epoch [171/300], Step [167/172], Loss: 10.0601\n",
      "Epoch [171/300], Step [168/172], Loss: 6.3885\n",
      "Epoch [171/300], Step [169/172], Loss: 6.4973\n",
      "Epoch [171/300], Step [170/172], Loss: 4.6495\n",
      "Epoch [171/300], Step [171/172], Loss: 7.5323\n",
      "Epoch [171/300], Step [172/172], Loss: 5.0364\n",
      "Epoch [172/300], Step [1/172], Loss: 55.2641\n",
      "Epoch [172/300], Step [2/172], Loss: 56.4079\n",
      "Epoch [172/300], Step [3/172], Loss: 50.4230\n",
      "Epoch [172/300], Step [4/172], Loss: 29.3152\n",
      "Epoch [172/300], Step [5/172], Loss: 51.8090\n",
      "Epoch [172/300], Step [6/172], Loss: 19.0323\n",
      "Epoch [172/300], Step [7/172], Loss: 29.5409\n",
      "Epoch [172/300], Step [8/172], Loss: 4.4709\n",
      "Epoch [172/300], Step [9/172], Loss: 33.5591\n",
      "Epoch [172/300], Step [10/172], Loss: 43.3165\n",
      "Epoch [172/300], Step [11/172], Loss: 61.8747\n",
      "Epoch [172/300], Step [12/172], Loss: 69.4788\n",
      "Epoch [172/300], Step [13/172], Loss: 36.6389\n",
      "Epoch [172/300], Step [14/172], Loss: 64.0352\n",
      "Epoch [172/300], Step [15/172], Loss: 58.4372\n",
      "Epoch [172/300], Step [16/172], Loss: 11.5930\n",
      "Epoch [172/300], Step [17/172], Loss: 46.5691\n",
      "Epoch [172/300], Step [18/172], Loss: 60.0357\n",
      "Epoch [172/300], Step [19/172], Loss: 82.4695\n",
      "Epoch [172/300], Step [20/172], Loss: 39.3885\n",
      "Epoch [172/300], Step [21/172], Loss: 85.3949\n",
      "Epoch [172/300], Step [22/172], Loss: 62.1597\n",
      "Epoch [172/300], Step [23/172], Loss: 1.8332\n",
      "Epoch [172/300], Step [24/172], Loss: 57.5405\n",
      "Epoch [172/300], Step [25/172], Loss: 41.3414\n",
      "Epoch [172/300], Step [26/172], Loss: 48.4802\n",
      "Epoch [172/300], Step [27/172], Loss: 62.8869\n",
      "Epoch [172/300], Step [28/172], Loss: 24.9377\n",
      "Epoch [172/300], Step [29/172], Loss: 17.4703\n",
      "Epoch [172/300], Step [30/172], Loss: 65.1005\n",
      "Epoch [172/300], Step [31/172], Loss: 38.1739\n",
      "Epoch [172/300], Step [32/172], Loss: 42.0063\n",
      "Epoch [172/300], Step [33/172], Loss: 70.1669\n",
      "Epoch [172/300], Step [34/172], Loss: 2.9688\n",
      "Epoch [172/300], Step [35/172], Loss: 13.3260\n",
      "Epoch [172/300], Step [36/172], Loss: 17.8532\n",
      "Epoch [172/300], Step [37/172], Loss: 16.8768\n",
      "Epoch [172/300], Step [38/172], Loss: 29.1231\n",
      "Epoch [172/300], Step [39/172], Loss: 36.0073\n",
      "Epoch [172/300], Step [40/172], Loss: 20.1051\n",
      "Epoch [172/300], Step [41/172], Loss: 33.9386\n",
      "Epoch [172/300], Step [42/172], Loss: 38.5465\n",
      "Epoch [172/300], Step [43/172], Loss: 26.9906\n",
      "Epoch [172/300], Step [44/172], Loss: 20.4814\n",
      "Epoch [172/300], Step [45/172], Loss: 25.8803\n",
      "Epoch [172/300], Step [46/172], Loss: 17.8003\n",
      "Epoch [172/300], Step [47/172], Loss: 46.8052\n",
      "Epoch [172/300], Step [48/172], Loss: 60.4543\n",
      "Epoch [172/300], Step [49/172], Loss: 20.8079\n",
      "Epoch [172/300], Step [50/172], Loss: 47.3526\n",
      "Epoch [172/300], Step [51/172], Loss: 8.4553\n",
      "Epoch [172/300], Step [52/172], Loss: 18.9673\n",
      "Epoch [172/300], Step [53/172], Loss: 22.4426\n",
      "Epoch [172/300], Step [54/172], Loss: 14.3992\n",
      "Epoch [172/300], Step [55/172], Loss: 14.3149\n",
      "Epoch [172/300], Step [56/172], Loss: 16.8963\n",
      "Epoch [172/300], Step [57/172], Loss: 17.5123\n",
      "Epoch [172/300], Step [58/172], Loss: 13.5590\n",
      "Epoch [172/300], Step [59/172], Loss: 27.8600\n",
      "Epoch [172/300], Step [60/172], Loss: 28.4012\n",
      "Epoch [172/300], Step [61/172], Loss: 6.4374\n",
      "Epoch [172/300], Step [62/172], Loss: 20.5849\n",
      "Epoch [172/300], Step [63/172], Loss: 10.4340\n",
      "Epoch [172/300], Step [64/172], Loss: 10.5399\n",
      "Epoch [172/300], Step [65/172], Loss: 20.2194\n",
      "Epoch [172/300], Step [66/172], Loss: 6.4529\n",
      "Epoch [172/300], Step [67/172], Loss: 23.7994\n",
      "Epoch [172/300], Step [68/172], Loss: 5.4850\n",
      "Epoch [172/300], Step [69/172], Loss: 36.9013\n",
      "Epoch [172/300], Step [70/172], Loss: 40.9965\n",
      "Epoch [172/300], Step [71/172], Loss: 41.4458\n",
      "Epoch [172/300], Step [72/172], Loss: 42.6376\n",
      "Epoch [172/300], Step [73/172], Loss: 50.6635\n",
      "Epoch [172/300], Step [74/172], Loss: 26.7589\n",
      "Epoch [172/300], Step [75/172], Loss: 28.2843\n",
      "Epoch [172/300], Step [76/172], Loss: 29.8611\n",
      "Epoch [172/300], Step [77/172], Loss: 50.9401\n",
      "Epoch [172/300], Step [78/172], Loss: 39.3227\n",
      "Epoch [172/300], Step [79/172], Loss: 38.4886\n",
      "Epoch [172/300], Step [80/172], Loss: 51.5322\n",
      "Epoch [172/300], Step [81/172], Loss: 35.0848\n",
      "Epoch [172/300], Step [82/172], Loss: 36.3689\n",
      "Epoch [172/300], Step [83/172], Loss: 44.3639\n",
      "Epoch [172/300], Step [84/172], Loss: 33.4297\n",
      "Epoch [172/300], Step [85/172], Loss: 38.8203\n",
      "Epoch [172/300], Step [86/172], Loss: 33.0865\n",
      "Epoch [172/300], Step [87/172], Loss: 26.6431\n",
      "Epoch [172/300], Step [88/172], Loss: 25.9884\n",
      "Epoch [172/300], Step [89/172], Loss: 26.7166\n",
      "Epoch [172/300], Step [90/172], Loss: 21.7610\n",
      "Epoch [172/300], Step [91/172], Loss: 26.9974\n",
      "Epoch [172/300], Step [92/172], Loss: 20.4677\n",
      "Epoch [172/300], Step [93/172], Loss: 20.9945\n",
      "Epoch [172/300], Step [94/172], Loss: 29.4985\n",
      "Epoch [172/300], Step [95/172], Loss: 21.1320\n",
      "Epoch [172/300], Step [96/172], Loss: 19.9906\n",
      "Epoch [172/300], Step [97/172], Loss: 27.9605\n",
      "Epoch [172/300], Step [98/172], Loss: 19.5652\n",
      "Epoch [172/300], Step [99/172], Loss: 19.0713\n",
      "Epoch [172/300], Step [100/172], Loss: 15.3706\n",
      "Epoch [172/300], Step [101/172], Loss: 18.4441\n",
      "Epoch [172/300], Step [102/172], Loss: 17.0450\n",
      "Epoch [172/300], Step [103/172], Loss: 12.5987\n",
      "Epoch [172/300], Step [104/172], Loss: 18.2413\n",
      "Epoch [172/300], Step [105/172], Loss: 18.8428\n",
      "Epoch [172/300], Step [106/172], Loss: 15.6977\n",
      "Epoch [172/300], Step [107/172], Loss: 15.4373\n",
      "Epoch [172/300], Step [108/172], Loss: 14.7998\n",
      "Epoch [172/300], Step [109/172], Loss: 14.6501\n",
      "Epoch [172/300], Step [110/172], Loss: 15.7849\n",
      "Epoch [172/300], Step [111/172], Loss: 15.0671\n",
      "Epoch [172/300], Step [112/172], Loss: 17.6122\n",
      "Epoch [172/300], Step [113/172], Loss: 11.8247\n",
      "Epoch [172/300], Step [114/172], Loss: 13.6921\n",
      "Epoch [172/300], Step [115/172], Loss: 19.2358\n",
      "Epoch [172/300], Step [116/172], Loss: 14.5177\n",
      "Epoch [172/300], Step [117/172], Loss: 11.2350\n",
      "Epoch [172/300], Step [118/172], Loss: 14.1668\n",
      "Epoch [172/300], Step [119/172], Loss: 15.7263\n",
      "Epoch [172/300], Step [120/172], Loss: 9.8832\n",
      "Epoch [172/300], Step [121/172], Loss: 9.5553\n",
      "Epoch [172/300], Step [122/172], Loss: 10.4833\n",
      "Epoch [172/300], Step [123/172], Loss: 10.2469\n",
      "Epoch [172/300], Step [124/172], Loss: 7.5532\n",
      "Epoch [172/300], Step [125/172], Loss: 11.8398\n",
      "Epoch [172/300], Step [126/172], Loss: 10.8402\n",
      "Epoch [172/300], Step [127/172], Loss: 10.8997\n",
      "Epoch [172/300], Step [128/172], Loss: 10.4884\n",
      "Epoch [172/300], Step [129/172], Loss: 7.9388\n",
      "Epoch [172/300], Step [130/172], Loss: 12.0627\n",
      "Epoch [172/300], Step [131/172], Loss: 7.2871\n",
      "Epoch [172/300], Step [132/172], Loss: 8.1748\n",
      "Epoch [172/300], Step [133/172], Loss: 8.9514\n",
      "Epoch [172/300], Step [134/172], Loss: 11.4772\n",
      "Epoch [172/300], Step [135/172], Loss: 8.4670\n",
      "Epoch [172/300], Step [136/172], Loss: 8.1556\n",
      "Epoch [172/300], Step [137/172], Loss: 9.1232\n",
      "Epoch [172/300], Step [138/172], Loss: 6.8478\n",
      "Epoch [172/300], Step [139/172], Loss: 9.4062\n",
      "Epoch [172/300], Step [140/172], Loss: 9.4032\n",
      "Epoch [172/300], Step [141/172], Loss: 9.5500\n",
      "Epoch [172/300], Step [142/172], Loss: 13.9859\n",
      "Epoch [172/300], Step [143/172], Loss: 10.3810\n",
      "Epoch [172/300], Step [144/172], Loss: 8.8316\n",
      "Epoch [172/300], Step [145/172], Loss: 9.6648\n",
      "Epoch [172/300], Step [146/172], Loss: 9.2612\n",
      "Epoch [172/300], Step [147/172], Loss: 5.0377\n",
      "Epoch [172/300], Step [148/172], Loss: 5.8691\n",
      "Epoch [172/300], Step [149/172], Loss: 6.5079\n",
      "Epoch [172/300], Step [150/172], Loss: 6.0463\n",
      "Epoch [172/300], Step [151/172], Loss: 5.4350\n",
      "Epoch [172/300], Step [152/172], Loss: 7.2480\n",
      "Epoch [172/300], Step [153/172], Loss: 6.3255\n",
      "Epoch [172/300], Step [154/172], Loss: 7.2369\n",
      "Epoch [172/300], Step [155/172], Loss: 6.1003\n",
      "Epoch [172/300], Step [156/172], Loss: 12.9124\n",
      "Epoch [172/300], Step [157/172], Loss: 9.3462\n",
      "Epoch [172/300], Step [158/172], Loss: 6.9912\n",
      "Epoch [172/300], Step [159/172], Loss: 9.1940\n",
      "Epoch [172/300], Step [160/172], Loss: 9.9453\n",
      "Epoch [172/300], Step [161/172], Loss: 7.0460\n",
      "Epoch [172/300], Step [162/172], Loss: 5.2995\n",
      "Epoch [172/300], Step [163/172], Loss: 6.2507\n",
      "Epoch [172/300], Step [164/172], Loss: 8.4700\n",
      "Epoch [172/300], Step [165/172], Loss: 5.8275\n",
      "Epoch [172/300], Step [166/172], Loss: 5.3111\n",
      "Epoch [172/300], Step [167/172], Loss: 10.1295\n",
      "Epoch [172/300], Step [168/172], Loss: 6.3678\n",
      "Epoch [172/300], Step [169/172], Loss: 6.4707\n",
      "Epoch [172/300], Step [170/172], Loss: 4.6286\n",
      "Epoch [172/300], Step [171/172], Loss: 7.5581\n",
      "Epoch [172/300], Step [172/172], Loss: 5.0911\n",
      "Epoch [173/300], Step [1/172], Loss: 55.1911\n",
      "Epoch [173/300], Step [2/172], Loss: 56.2455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [173/300], Step [3/172], Loss: 49.6664\n",
      "Epoch [173/300], Step [4/172], Loss: 29.1445\n",
      "Epoch [173/300], Step [5/172], Loss: 50.6538\n",
      "Epoch [173/300], Step [6/172], Loss: 18.9908\n",
      "Epoch [173/300], Step [7/172], Loss: 29.3332\n",
      "Epoch [173/300], Step [8/172], Loss: 4.7604\n",
      "Epoch [173/300], Step [9/172], Loss: 33.5516\n",
      "Epoch [173/300], Step [10/172], Loss: 43.1727\n",
      "Epoch [173/300], Step [11/172], Loss: 61.5622\n",
      "Epoch [173/300], Step [12/172], Loss: 69.1006\n",
      "Epoch [173/300], Step [13/172], Loss: 36.3782\n",
      "Epoch [173/300], Step [14/172], Loss: 63.6092\n",
      "Epoch [173/300], Step [15/172], Loss: 58.2989\n",
      "Epoch [173/300], Step [16/172], Loss: 10.7815\n",
      "Epoch [173/300], Step [17/172], Loss: 46.5114\n",
      "Epoch [173/300], Step [18/172], Loss: 60.1834\n",
      "Epoch [173/300], Step [19/172], Loss: 82.3526\n",
      "Epoch [173/300], Step [20/172], Loss: 39.0736\n",
      "Epoch [173/300], Step [21/172], Loss: 85.1964\n",
      "Epoch [173/300], Step [22/172], Loss: 62.0462\n",
      "Epoch [173/300], Step [23/172], Loss: 1.8517\n",
      "Epoch [173/300], Step [24/172], Loss: 57.0725\n",
      "Epoch [173/300], Step [25/172], Loss: 41.5633\n",
      "Epoch [173/300], Step [26/172], Loss: 48.2440\n",
      "Epoch [173/300], Step [27/172], Loss: 62.5795\n",
      "Epoch [173/300], Step [28/172], Loss: 24.7575\n",
      "Epoch [173/300], Step [29/172], Loss: 17.2011\n",
      "Epoch [173/300], Step [30/172], Loss: 64.9990\n",
      "Epoch [173/300], Step [31/172], Loss: 37.8913\n",
      "Epoch [173/300], Step [32/172], Loss: 41.8366\n",
      "Epoch [173/300], Step [33/172], Loss: 69.9526\n",
      "Epoch [173/300], Step [34/172], Loss: 3.0981\n",
      "Epoch [173/300], Step [35/172], Loss: 13.3116\n",
      "Epoch [173/300], Step [36/172], Loss: 17.8904\n",
      "Epoch [173/300], Step [37/172], Loss: 16.9156\n",
      "Epoch [173/300], Step [38/172], Loss: 29.2722\n",
      "Epoch [173/300], Step [39/172], Loss: 35.9218\n",
      "Epoch [173/300], Step [40/172], Loss: 20.1341\n",
      "Epoch [173/300], Step [41/172], Loss: 33.9311\n",
      "Epoch [173/300], Step [42/172], Loss: 38.5319\n",
      "Epoch [173/300], Step [43/172], Loss: 27.0001\n",
      "Epoch [173/300], Step [44/172], Loss: 20.4751\n",
      "Epoch [173/300], Step [45/172], Loss: 25.9160\n",
      "Epoch [173/300], Step [46/172], Loss: 17.8006\n",
      "Epoch [173/300], Step [47/172], Loss: 46.8760\n",
      "Epoch [173/300], Step [48/172], Loss: 60.3265\n",
      "Epoch [173/300], Step [49/172], Loss: 20.8762\n",
      "Epoch [173/300], Step [50/172], Loss: 47.1239\n",
      "Epoch [173/300], Step [51/172], Loss: 8.4415\n",
      "Epoch [173/300], Step [52/172], Loss: 18.9026\n",
      "Epoch [173/300], Step [53/172], Loss: 22.3962\n",
      "Epoch [173/300], Step [54/172], Loss: 14.4606\n",
      "Epoch [173/300], Step [55/172], Loss: 14.3145\n",
      "Epoch [173/300], Step [56/172], Loss: 16.6849\n",
      "Epoch [173/300], Step [57/172], Loss: 17.3029\n",
      "Epoch [173/300], Step [58/172], Loss: 13.6678\n",
      "Epoch [173/300], Step [59/172], Loss: 27.8173\n",
      "Epoch [173/300], Step [60/172], Loss: 28.4764\n",
      "Epoch [173/300], Step [61/172], Loss: 6.4855\n",
      "Epoch [173/300], Step [62/172], Loss: 20.5521\n",
      "Epoch [173/300], Step [63/172], Loss: 10.3007\n",
      "Epoch [173/300], Step [64/172], Loss: 10.4660\n",
      "Epoch [173/300], Step [65/172], Loss: 20.2647\n",
      "Epoch [173/300], Step [66/172], Loss: 6.4377\n",
      "Epoch [173/300], Step [67/172], Loss: 23.6987\n",
      "Epoch [173/300], Step [68/172], Loss: 5.7386\n",
      "Epoch [173/300], Step [69/172], Loss: 36.8948\n",
      "Epoch [173/300], Step [70/172], Loss: 40.8049\n",
      "Epoch [173/300], Step [71/172], Loss: 41.3587\n",
      "Epoch [173/300], Step [72/172], Loss: 42.5225\n",
      "Epoch [173/300], Step [73/172], Loss: 50.6063\n",
      "Epoch [173/300], Step [74/172], Loss: 26.6436\n",
      "Epoch [173/300], Step [75/172], Loss: 28.0322\n",
      "Epoch [173/300], Step [76/172], Loss: 29.8282\n",
      "Epoch [173/300], Step [77/172], Loss: 50.7845\n",
      "Epoch [173/300], Step [78/172], Loss: 39.3743\n",
      "Epoch [173/300], Step [79/172], Loss: 38.4767\n",
      "Epoch [173/300], Step [80/172], Loss: 51.6453\n",
      "Epoch [173/300], Step [81/172], Loss: 35.1835\n",
      "Epoch [173/300], Step [82/172], Loss: 36.6447\n",
      "Epoch [173/300], Step [83/172], Loss: 44.3988\n",
      "Epoch [173/300], Step [84/172], Loss: 33.5540\n",
      "Epoch [173/300], Step [85/172], Loss: 38.8462\n",
      "Epoch [173/300], Step [86/172], Loss: 33.2323\n",
      "Epoch [173/300], Step [87/172], Loss: 26.6112\n",
      "Epoch [173/300], Step [88/172], Loss: 25.9561\n",
      "Epoch [173/300], Step [89/172], Loss: 26.8287\n",
      "Epoch [173/300], Step [90/172], Loss: 21.8039\n",
      "Epoch [173/300], Step [91/172], Loss: 26.9781\n",
      "Epoch [173/300], Step [92/172], Loss: 20.3991\n",
      "Epoch [173/300], Step [93/172], Loss: 20.9366\n",
      "Epoch [173/300], Step [94/172], Loss: 29.5244\n",
      "Epoch [173/300], Step [95/172], Loss: 21.0236\n",
      "Epoch [173/300], Step [96/172], Loss: 20.0013\n",
      "Epoch [173/300], Step [97/172], Loss: 28.0272\n",
      "Epoch [173/300], Step [98/172], Loss: 19.5647\n",
      "Epoch [173/300], Step [99/172], Loss: 19.0601\n",
      "Epoch [173/300], Step [100/172], Loss: 15.3536\n",
      "Epoch [173/300], Step [101/172], Loss: 18.4638\n",
      "Epoch [173/300], Step [102/172], Loss: 17.0824\n",
      "Epoch [173/300], Step [103/172], Loss: 12.5649\n",
      "Epoch [173/300], Step [104/172], Loss: 18.1985\n",
      "Epoch [173/300], Step [105/172], Loss: 18.9010\n",
      "Epoch [173/300], Step [106/172], Loss: 15.6198\n",
      "Epoch [173/300], Step [107/172], Loss: 15.4114\n",
      "Epoch [173/300], Step [108/172], Loss: 14.7261\n",
      "Epoch [173/300], Step [109/172], Loss: 14.7227\n",
      "Epoch [173/300], Step [110/172], Loss: 15.7042\n",
      "Epoch [173/300], Step [111/172], Loss: 15.0578\n",
      "Epoch [173/300], Step [112/172], Loss: 17.5580\n",
      "Epoch [173/300], Step [113/172], Loss: 11.7323\n",
      "Epoch [173/300], Step [114/172], Loss: 13.6853\n",
      "Epoch [173/300], Step [115/172], Loss: 19.2379\n",
      "Epoch [173/300], Step [116/172], Loss: 14.4639\n",
      "Epoch [173/300], Step [117/172], Loss: 11.2661\n",
      "Epoch [173/300], Step [118/172], Loss: 14.1609\n",
      "Epoch [173/300], Step [119/172], Loss: 15.6463\n",
      "Epoch [173/300], Step [120/172], Loss: 9.8727\n",
      "Epoch [173/300], Step [121/172], Loss: 9.5072\n",
      "Epoch [173/300], Step [122/172], Loss: 10.4721\n",
      "Epoch [173/300], Step [123/172], Loss: 10.2218\n",
      "Epoch [173/300], Step [124/172], Loss: 7.5324\n",
      "Epoch [173/300], Step [125/172], Loss: 11.7578\n",
      "Epoch [173/300], Step [126/172], Loss: 10.8183\n",
      "Epoch [173/300], Step [127/172], Loss: 10.8684\n",
      "Epoch [173/300], Step [128/172], Loss: 10.4307\n",
      "Epoch [173/300], Step [129/172], Loss: 7.9166\n",
      "Epoch [173/300], Step [130/172], Loss: 12.0811\n",
      "Epoch [173/300], Step [131/172], Loss: 7.2656\n",
      "Epoch [173/300], Step [132/172], Loss: 8.1643\n",
      "Epoch [173/300], Step [133/172], Loss: 8.9481\n",
      "Epoch [173/300], Step [134/172], Loss: 11.3828\n",
      "Epoch [173/300], Step [135/172], Loss: 8.3621\n",
      "Epoch [173/300], Step [136/172], Loss: 8.1213\n",
      "Epoch [173/300], Step [137/172], Loss: 9.0984\n",
      "Epoch [173/300], Step [138/172], Loss: 6.7989\n",
      "Epoch [173/300], Step [139/172], Loss: 9.3097\n",
      "Epoch [173/300], Step [140/172], Loss: 9.3834\n",
      "Epoch [173/300], Step [141/172], Loss: 9.5151\n",
      "Epoch [173/300], Step [142/172], Loss: 13.9779\n",
      "Epoch [173/300], Step [143/172], Loss: 10.3971\n",
      "Epoch [173/300], Step [144/172], Loss: 8.8028\n",
      "Epoch [173/300], Step [145/172], Loss: 9.6987\n",
      "Epoch [173/300], Step [146/172], Loss: 9.2141\n",
      "Epoch [173/300], Step [147/172], Loss: 4.9951\n",
      "Epoch [173/300], Step [148/172], Loss: 5.8504\n",
      "Epoch [173/300], Step [149/172], Loss: 6.4641\n",
      "Epoch [173/300], Step [150/172], Loss: 6.0191\n",
      "Epoch [173/300], Step [151/172], Loss: 5.4146\n",
      "Epoch [173/300], Step [152/172], Loss: 7.1947\n",
      "Epoch [173/300], Step [153/172], Loss: 6.2454\n",
      "Epoch [173/300], Step [154/172], Loss: 7.2476\n",
      "Epoch [173/300], Step [155/172], Loss: 6.0146\n",
      "Epoch [173/300], Step [156/172], Loss: 12.9324\n",
      "Epoch [173/300], Step [157/172], Loss: 9.3708\n",
      "Epoch [173/300], Step [158/172], Loss: 6.9832\n",
      "Epoch [173/300], Step [159/172], Loss: 9.1638\n",
      "Epoch [173/300], Step [160/172], Loss: 9.9161\n",
      "Epoch [173/300], Step [161/172], Loss: 6.9983\n",
      "Epoch [173/300], Step [162/172], Loss: 5.2617\n",
      "Epoch [173/300], Step [163/172], Loss: 6.2448\n",
      "Epoch [173/300], Step [164/172], Loss: 8.5008\n",
      "Epoch [173/300], Step [165/172], Loss: 5.8200\n",
      "Epoch [173/300], Step [166/172], Loss: 5.3190\n",
      "Epoch [173/300], Step [167/172], Loss: 10.1675\n",
      "Epoch [173/300], Step [168/172], Loss: 6.3071\n",
      "Epoch [173/300], Step [169/172], Loss: 6.5210\n",
      "Epoch [173/300], Step [170/172], Loss: 4.6017\n",
      "Epoch [173/300], Step [171/172], Loss: 7.5565\n",
      "Epoch [173/300], Step [172/172], Loss: 5.0229\n",
      "Epoch [174/300], Step [1/172], Loss: 54.9401\n",
      "Epoch [174/300], Step [2/172], Loss: 55.8894\n",
      "Epoch [174/300], Step [3/172], Loss: 49.1614\n",
      "Epoch [174/300], Step [4/172], Loss: 29.0675\n",
      "Epoch [174/300], Step [5/172], Loss: 50.4497\n",
      "Epoch [174/300], Step [6/172], Loss: 18.7860\n",
      "Epoch [174/300], Step [7/172], Loss: 29.0507\n",
      "Epoch [174/300], Step [8/172], Loss: 4.5222\n",
      "Epoch [174/300], Step [9/172], Loss: 33.4975\n",
      "Epoch [174/300], Step [10/172], Loss: 43.1823\n",
      "Epoch [174/300], Step [11/172], Loss: 61.5030\n",
      "Epoch [174/300], Step [12/172], Loss: 68.9169\n",
      "Epoch [174/300], Step [13/172], Loss: 36.0591\n",
      "Epoch [174/300], Step [14/172], Loss: 63.1265\n",
      "Epoch [174/300], Step [15/172], Loss: 58.0030\n",
      "Epoch [174/300], Step [16/172], Loss: 11.5983\n",
      "Epoch [174/300], Step [17/172], Loss: 46.2762\n",
      "Epoch [174/300], Step [18/172], Loss: 59.9969\n",
      "Epoch [174/300], Step [19/172], Loss: 82.5984\n",
      "Epoch [174/300], Step [20/172], Loss: 38.9201\n",
      "Epoch [174/300], Step [21/172], Loss: 85.2398\n",
      "Epoch [174/300], Step [22/172], Loss: 61.8131\n",
      "Epoch [174/300], Step [23/172], Loss: 1.6727\n",
      "Epoch [174/300], Step [24/172], Loss: 57.3934\n",
      "Epoch [174/300], Step [25/172], Loss: 41.3309\n",
      "Epoch [174/300], Step [26/172], Loss: 48.4233\n",
      "Epoch [174/300], Step [27/172], Loss: 62.6250\n",
      "Epoch [174/300], Step [28/172], Loss: 24.7251\n",
      "Epoch [174/300], Step [29/172], Loss: 17.2844\n",
      "Epoch [174/300], Step [30/172], Loss: 64.8209\n",
      "Epoch [174/300], Step [31/172], Loss: 37.8190\n",
      "Epoch [174/300], Step [32/172], Loss: 42.2635\n",
      "Epoch [174/300], Step [33/172], Loss: 70.4757\n",
      "Epoch [174/300], Step [34/172], Loss: 2.8999\n",
      "Epoch [174/300], Step [35/172], Loss: 13.2242\n",
      "Epoch [174/300], Step [36/172], Loss: 18.0995\n",
      "Epoch [174/300], Step [37/172], Loss: 16.7596\n",
      "Epoch [174/300], Step [38/172], Loss: 29.1534\n",
      "Epoch [174/300], Step [39/172], Loss: 35.7749\n",
      "Epoch [174/300], Step [40/172], Loss: 19.9958\n",
      "Epoch [174/300], Step [41/172], Loss: 33.7517\n",
      "Epoch [174/300], Step [42/172], Loss: 38.3918\n",
      "Epoch [174/300], Step [43/172], Loss: 26.9197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [174/300], Step [44/172], Loss: 20.4709\n",
      "Epoch [174/300], Step [45/172], Loss: 25.6357\n",
      "Epoch [174/300], Step [46/172], Loss: 17.6426\n",
      "Epoch [174/300], Step [47/172], Loss: 46.7445\n",
      "Epoch [174/300], Step [48/172], Loss: 60.4610\n",
      "Epoch [174/300], Step [49/172], Loss: 20.6334\n",
      "Epoch [174/300], Step [50/172], Loss: 47.0287\n",
      "Epoch [174/300], Step [51/172], Loss: 8.3870\n",
      "Epoch [174/300], Step [52/172], Loss: 18.8546\n",
      "Epoch [174/300], Step [53/172], Loss: 22.3432\n",
      "Epoch [174/300], Step [54/172], Loss: 14.4022\n",
      "Epoch [174/300], Step [55/172], Loss: 14.3218\n",
      "Epoch [174/300], Step [56/172], Loss: 17.1402\n",
      "Epoch [174/300], Step [57/172], Loss: 17.2776\n",
      "Epoch [174/300], Step [58/172], Loss: 13.5798\n",
      "Epoch [174/300], Step [59/172], Loss: 27.9075\n",
      "Epoch [174/300], Step [60/172], Loss: 27.8056\n",
      "Epoch [174/300], Step [61/172], Loss: 6.4087\n",
      "Epoch [174/300], Step [62/172], Loss: 20.4634\n",
      "Epoch [174/300], Step [63/172], Loss: 10.3093\n",
      "Epoch [174/300], Step [64/172], Loss: 10.5581\n",
      "Epoch [174/300], Step [65/172], Loss: 20.1779\n",
      "Epoch [174/300], Step [66/172], Loss: 6.4019\n",
      "Epoch [174/300], Step [67/172], Loss: 23.7721\n",
      "Epoch [174/300], Step [68/172], Loss: 5.4808\n",
      "Epoch [174/300], Step [69/172], Loss: 36.6150\n",
      "Epoch [174/300], Step [70/172], Loss: 40.6008\n",
      "Epoch [174/300], Step [71/172], Loss: 41.3108\n",
      "Epoch [174/300], Step [72/172], Loss: 42.4744\n",
      "Epoch [174/300], Step [73/172], Loss: 50.5279\n",
      "Epoch [174/300], Step [74/172], Loss: 26.6266\n",
      "Epoch [174/300], Step [75/172], Loss: 28.1472\n",
      "Epoch [174/300], Step [76/172], Loss: 29.6517\n",
      "Epoch [174/300], Step [77/172], Loss: 50.8004\n",
      "Epoch [174/300], Step [78/172], Loss: 39.2926\n",
      "Epoch [174/300], Step [79/172], Loss: 38.4804\n",
      "Epoch [174/300], Step [80/172], Loss: 51.7895\n",
      "Epoch [174/300], Step [81/172], Loss: 35.1237\n",
      "Epoch [174/300], Step [82/172], Loss: 36.5322\n",
      "Epoch [174/300], Step [83/172], Loss: 44.2867\n",
      "Epoch [174/300], Step [84/172], Loss: 33.5086\n",
      "Epoch [174/300], Step [85/172], Loss: 38.9399\n",
      "Epoch [174/300], Step [86/172], Loss: 33.2584\n",
      "Epoch [174/300], Step [87/172], Loss: 26.6205\n",
      "Epoch [174/300], Step [88/172], Loss: 25.9193\n",
      "Epoch [174/300], Step [89/172], Loss: 26.8771\n",
      "Epoch [174/300], Step [90/172], Loss: 21.6890\n",
      "Epoch [174/300], Step [91/172], Loss: 26.9412\n",
      "Epoch [174/300], Step [92/172], Loss: 20.3253\n",
      "Epoch [174/300], Step [93/172], Loss: 20.9749\n",
      "Epoch [174/300], Step [94/172], Loss: 29.5195\n",
      "Epoch [174/300], Step [95/172], Loss: 21.0551\n",
      "Epoch [174/300], Step [96/172], Loss: 20.0093\n",
      "Epoch [174/300], Step [97/172], Loss: 28.0248\n",
      "Epoch [174/300], Step [98/172], Loss: 19.5573\n",
      "Epoch [174/300], Step [99/172], Loss: 19.0840\n",
      "Epoch [174/300], Step [100/172], Loss: 15.3573\n",
      "Epoch [174/300], Step [101/172], Loss: 18.4894\n",
      "Epoch [174/300], Step [102/172], Loss: 16.9984\n",
      "Epoch [174/300], Step [103/172], Loss: 12.5547\n",
      "Epoch [174/300], Step [104/172], Loss: 18.2018\n",
      "Epoch [174/300], Step [105/172], Loss: 18.8584\n",
      "Epoch [174/300], Step [106/172], Loss: 15.6337\n",
      "Epoch [174/300], Step [107/172], Loss: 15.4012\n",
      "Epoch [174/300], Step [108/172], Loss: 14.7046\n",
      "Epoch [174/300], Step [109/172], Loss: 14.5988\n",
      "Epoch [174/300], Step [110/172], Loss: 15.7099\n",
      "Epoch [174/300], Step [111/172], Loss: 15.0550\n",
      "Epoch [174/300], Step [112/172], Loss: 17.5718\n",
      "Epoch [174/300], Step [113/172], Loss: 11.7544\n",
      "Epoch [174/300], Step [114/172], Loss: 13.6670\n",
      "Epoch [174/300], Step [115/172], Loss: 19.1298\n",
      "Epoch [174/300], Step [116/172], Loss: 14.4554\n",
      "Epoch [174/300], Step [117/172], Loss: 11.2225\n",
      "Epoch [174/300], Step [118/172], Loss: 14.1821\n",
      "Epoch [174/300], Step [119/172], Loss: 15.6694\n",
      "Epoch [174/300], Step [120/172], Loss: 9.8455\n",
      "Epoch [174/300], Step [121/172], Loss: 9.4582\n",
      "Epoch [174/300], Step [122/172], Loss: 10.4338\n",
      "Epoch [174/300], Step [123/172], Loss: 10.1842\n",
      "Epoch [174/300], Step [124/172], Loss: 7.4596\n",
      "Epoch [174/300], Step [125/172], Loss: 11.6554\n",
      "Epoch [174/300], Step [126/172], Loss: 10.7585\n",
      "Epoch [174/300], Step [127/172], Loss: 10.8816\n",
      "Epoch [174/300], Step [128/172], Loss: 10.4192\n",
      "Epoch [174/300], Step [129/172], Loss: 7.9118\n",
      "Epoch [174/300], Step [130/172], Loss: 12.1260\n",
      "Epoch [174/300], Step [131/172], Loss: 7.2442\n",
      "Epoch [174/300], Step [132/172], Loss: 8.1564\n",
      "Epoch [174/300], Step [133/172], Loss: 8.9245\n",
      "Epoch [174/300], Step [134/172], Loss: 11.3483\n",
      "Epoch [174/300], Step [135/172], Loss: 8.3625\n",
      "Epoch [174/300], Step [136/172], Loss: 8.0956\n",
      "Epoch [174/300], Step [137/172], Loss: 9.0742\n",
      "Epoch [174/300], Step [138/172], Loss: 6.7960\n",
      "Epoch [174/300], Step [139/172], Loss: 9.2967\n",
      "Epoch [174/300], Step [140/172], Loss: 9.3822\n",
      "Epoch [174/300], Step [141/172], Loss: 9.4030\n",
      "Epoch [174/300], Step [142/172], Loss: 13.8721\n",
      "Epoch [174/300], Step [143/172], Loss: 10.4238\n",
      "Epoch [174/300], Step [144/172], Loss: 8.7636\n",
      "Epoch [174/300], Step [145/172], Loss: 9.5982\n",
      "Epoch [174/300], Step [146/172], Loss: 9.1971\n",
      "Epoch [174/300], Step [147/172], Loss: 4.9838\n",
      "Epoch [174/300], Step [148/172], Loss: 5.8343\n",
      "Epoch [174/300], Step [149/172], Loss: 6.4337\n",
      "Epoch [174/300], Step [150/172], Loss: 5.9606\n",
      "Epoch [174/300], Step [151/172], Loss: 5.3928\n",
      "Epoch [174/300], Step [152/172], Loss: 7.2022\n",
      "Epoch [174/300], Step [153/172], Loss: 6.2325\n",
      "Epoch [174/300], Step [154/172], Loss: 7.2723\n",
      "Epoch [174/300], Step [155/172], Loss: 5.9572\n",
      "Epoch [174/300], Step [156/172], Loss: 12.9833\n",
      "Epoch [174/300], Step [157/172], Loss: 9.4117\n",
      "Epoch [174/300], Step [158/172], Loss: 6.9879\n",
      "Epoch [174/300], Step [159/172], Loss: 9.1080\n",
      "Epoch [174/300], Step [160/172], Loss: 9.9039\n",
      "Epoch [174/300], Step [161/172], Loss: 6.9474\n",
      "Epoch [174/300], Step [162/172], Loss: 5.2194\n",
      "Epoch [174/300], Step [163/172], Loss: 6.2710\n",
      "Epoch [174/300], Step [164/172], Loss: 8.4744\n",
      "Epoch [174/300], Step [165/172], Loss: 5.8049\n",
      "Epoch [174/300], Step [166/172], Loss: 5.3510\n",
      "Epoch [174/300], Step [167/172], Loss: 10.0281\n",
      "Epoch [174/300], Step [168/172], Loss: 6.3138\n",
      "Epoch [174/300], Step [169/172], Loss: 6.4195\n",
      "Epoch [174/300], Step [170/172], Loss: 4.5968\n",
      "Epoch [174/300], Step [171/172], Loss: 7.5390\n",
      "Epoch [174/300], Step [172/172], Loss: 4.9697\n",
      "Epoch [175/300], Step [1/172], Loss: 54.7358\n",
      "Epoch [175/300], Step [2/172], Loss: 56.8876\n",
      "Epoch [175/300], Step [3/172], Loss: 49.9900\n",
      "Epoch [175/300], Step [4/172], Loss: 28.9472\n",
      "Epoch [175/300], Step [5/172], Loss: 49.4738\n",
      "Epoch [175/300], Step [6/172], Loss: 18.7628\n",
      "Epoch [175/300], Step [7/172], Loss: 28.8104\n",
      "Epoch [175/300], Step [8/172], Loss: 4.5826\n",
      "Epoch [175/300], Step [9/172], Loss: 33.2731\n",
      "Epoch [175/300], Step [10/172], Loss: 42.8761\n",
      "Epoch [175/300], Step [11/172], Loss: 61.3133\n",
      "Epoch [175/300], Step [12/172], Loss: 68.6349\n",
      "Epoch [175/300], Step [13/172], Loss: 36.0179\n",
      "Epoch [175/300], Step [14/172], Loss: 62.9548\n",
      "Epoch [175/300], Step [15/172], Loss: 57.8338\n",
      "Epoch [175/300], Step [16/172], Loss: 10.9410\n",
      "Epoch [175/300], Step [17/172], Loss: 46.1282\n",
      "Epoch [175/300], Step [18/172], Loss: 59.9743\n",
      "Epoch [175/300], Step [19/172], Loss: 82.5300\n",
      "Epoch [175/300], Step [20/172], Loss: 38.5485\n",
      "Epoch [175/300], Step [21/172], Loss: 85.0177\n",
      "Epoch [175/300], Step [22/172], Loss: 61.6136\n",
      "Epoch [175/300], Step [23/172], Loss: 1.7915\n",
      "Epoch [175/300], Step [24/172], Loss: 57.2768\n",
      "Epoch [175/300], Step [25/172], Loss: 41.3389\n",
      "Epoch [175/300], Step [26/172], Loss: 48.1349\n",
      "Epoch [175/300], Step [27/172], Loss: 62.3117\n",
      "Epoch [175/300], Step [28/172], Loss: 24.7922\n",
      "Epoch [175/300], Step [29/172], Loss: 17.1876\n",
      "Epoch [175/300], Step [30/172], Loss: 64.8853\n",
      "Epoch [175/300], Step [31/172], Loss: 37.8878\n",
      "Epoch [175/300], Step [32/172], Loss: 42.2892\n",
      "Epoch [175/300], Step [33/172], Loss: 70.4421\n",
      "Epoch [175/300], Step [34/172], Loss: 2.9794\n",
      "Epoch [175/300], Step [35/172], Loss: 13.2478\n",
      "Epoch [175/300], Step [36/172], Loss: 17.8890\n",
      "Epoch [175/300], Step [37/172], Loss: 16.7587\n",
      "Epoch [175/300], Step [38/172], Loss: 29.2187\n",
      "Epoch [175/300], Step [39/172], Loss: 35.9207\n",
      "Epoch [175/300], Step [40/172], Loss: 20.0635\n",
      "Epoch [175/300], Step [41/172], Loss: 33.7219\n",
      "Epoch [175/300], Step [42/172], Loss: 38.3448\n",
      "Epoch [175/300], Step [43/172], Loss: 26.9171\n",
      "Epoch [175/300], Step [44/172], Loss: 20.5071\n",
      "Epoch [175/300], Step [45/172], Loss: 25.8256\n",
      "Epoch [175/300], Step [46/172], Loss: 17.8276\n",
      "Epoch [175/300], Step [47/172], Loss: 46.9960\n",
      "Epoch [175/300], Step [48/172], Loss: 60.7719\n",
      "Epoch [175/300], Step [49/172], Loss: 20.8188\n",
      "Epoch [175/300], Step [50/172], Loss: 47.0542\n",
      "Epoch [175/300], Step [51/172], Loss: 8.4148\n",
      "Epoch [175/300], Step [52/172], Loss: 18.9003\n",
      "Epoch [175/300], Step [53/172], Loss: 22.3509\n",
      "Epoch [175/300], Step [54/172], Loss: 14.3518\n",
      "Epoch [175/300], Step [55/172], Loss: 14.2852\n",
      "Epoch [175/300], Step [56/172], Loss: 17.0231\n",
      "Epoch [175/300], Step [57/172], Loss: 17.4390\n",
      "Epoch [175/300], Step [58/172], Loss: 13.5576\n",
      "Epoch [175/300], Step [59/172], Loss: 27.8219\n",
      "Epoch [175/300], Step [60/172], Loss: 27.6965\n",
      "Epoch [175/300], Step [61/172], Loss: 6.3886\n",
      "Epoch [175/300], Step [62/172], Loss: 20.4740\n",
      "Epoch [175/300], Step [63/172], Loss: 10.2682\n",
      "Epoch [175/300], Step [64/172], Loss: 10.5458\n",
      "Epoch [175/300], Step [65/172], Loss: 20.1221\n",
      "Epoch [175/300], Step [66/172], Loss: 6.4257\n",
      "Epoch [175/300], Step [67/172], Loss: 23.5655\n",
      "Epoch [175/300], Step [68/172], Loss: 5.2051\n",
      "Epoch [175/300], Step [69/172], Loss: 36.5956\n",
      "Epoch [175/300], Step [70/172], Loss: 40.5311\n",
      "Epoch [175/300], Step [71/172], Loss: 41.2195\n",
      "Epoch [175/300], Step [72/172], Loss: 42.3065\n",
      "Epoch [175/300], Step [73/172], Loss: 50.4960\n",
      "Epoch [175/300], Step [74/172], Loss: 26.5520\n",
      "Epoch [175/300], Step [75/172], Loss: 28.1729\n",
      "Epoch [175/300], Step [76/172], Loss: 29.5255\n",
      "Epoch [175/300], Step [77/172], Loss: 50.5168\n",
      "Epoch [175/300], Step [78/172], Loss: 39.0504\n",
      "Epoch [175/300], Step [79/172], Loss: 38.1456\n",
      "Epoch [175/300], Step [80/172], Loss: 51.3812\n",
      "Epoch [175/300], Step [81/172], Loss: 34.8708\n",
      "Epoch [175/300], Step [82/172], Loss: 36.3562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [175/300], Step [83/172], Loss: 44.1513\n",
      "Epoch [175/300], Step [84/172], Loss: 33.3123\n",
      "Epoch [175/300], Step [85/172], Loss: 38.6595\n",
      "Epoch [175/300], Step [86/172], Loss: 33.1247\n",
      "Epoch [175/300], Step [87/172], Loss: 26.5400\n",
      "Epoch [175/300], Step [88/172], Loss: 25.7087\n",
      "Epoch [175/300], Step [89/172], Loss: 26.7158\n",
      "Epoch [175/300], Step [90/172], Loss: 21.6261\n",
      "Epoch [175/300], Step [91/172], Loss: 26.8655\n",
      "Epoch [175/300], Step [92/172], Loss: 20.2175\n",
      "Epoch [175/300], Step [93/172], Loss: 20.8736\n",
      "Epoch [175/300], Step [94/172], Loss: 29.4539\n",
      "Epoch [175/300], Step [95/172], Loss: 21.0122\n",
      "Epoch [175/300], Step [96/172], Loss: 19.9609\n",
      "Epoch [175/300], Step [97/172], Loss: 27.9981\n",
      "Epoch [175/300], Step [98/172], Loss: 19.4881\n",
      "Epoch [175/300], Step [99/172], Loss: 19.0299\n",
      "Epoch [175/300], Step [100/172], Loss: 15.3166\n",
      "Epoch [175/300], Step [101/172], Loss: 18.4604\n",
      "Epoch [175/300], Step [102/172], Loss: 16.9987\n",
      "Epoch [175/300], Step [103/172], Loss: 12.5093\n",
      "Epoch [175/300], Step [104/172], Loss: 18.2392\n",
      "Epoch [175/300], Step [105/172], Loss: 18.8716\n",
      "Epoch [175/300], Step [106/172], Loss: 15.5810\n",
      "Epoch [175/300], Step [107/172], Loss: 15.3767\n",
      "Epoch [175/300], Step [108/172], Loss: 14.6239\n",
      "Epoch [175/300], Step [109/172], Loss: 14.6160\n",
      "Epoch [175/300], Step [110/172], Loss: 15.6503\n",
      "Epoch [175/300], Step [111/172], Loss: 15.0839\n",
      "Epoch [175/300], Step [112/172], Loss: 17.5553\n",
      "Epoch [175/300], Step [113/172], Loss: 11.6876\n",
      "Epoch [175/300], Step [114/172], Loss: 13.6462\n",
      "Epoch [175/300], Step [115/172], Loss: 19.2027\n",
      "Epoch [175/300], Step [116/172], Loss: 14.4120\n",
      "Epoch [175/300], Step [117/172], Loss: 11.2483\n",
      "Epoch [175/300], Step [118/172], Loss: 14.1035\n",
      "Epoch [175/300], Step [119/172], Loss: 15.6382\n",
      "Epoch [175/300], Step [120/172], Loss: 9.8709\n",
      "Epoch [175/300], Step [121/172], Loss: 9.4587\n",
      "Epoch [175/300], Step [122/172], Loss: 10.4477\n",
      "Epoch [175/300], Step [123/172], Loss: 10.1684\n",
      "Epoch [175/300], Step [124/172], Loss: 7.4947\n",
      "Epoch [175/300], Step [125/172], Loss: 11.7236\n",
      "Epoch [175/300], Step [126/172], Loss: 10.7931\n",
      "Epoch [175/300], Step [127/172], Loss: 10.7899\n",
      "Epoch [175/300], Step [128/172], Loss: 10.3251\n",
      "Epoch [175/300], Step [129/172], Loss: 7.8860\n",
      "Epoch [175/300], Step [130/172], Loss: 12.0589\n",
      "Epoch [175/300], Step [131/172], Loss: 7.2187\n",
      "Epoch [175/300], Step [132/172], Loss: 8.1391\n",
      "Epoch [175/300], Step [133/172], Loss: 8.8989\n",
      "Epoch [175/300], Step [134/172], Loss: 11.3458\n",
      "Epoch [175/300], Step [135/172], Loss: 8.3717\n",
      "Epoch [175/300], Step [136/172], Loss: 8.0890\n",
      "Epoch [175/300], Step [137/172], Loss: 9.1533\n",
      "Epoch [175/300], Step [138/172], Loss: 6.7917\n",
      "Epoch [175/300], Step [139/172], Loss: 9.3732\n",
      "Epoch [175/300], Step [140/172], Loss: 9.3540\n",
      "Epoch [175/300], Step [141/172], Loss: 9.4117\n",
      "Epoch [175/300], Step [142/172], Loss: 13.8322\n",
      "Epoch [175/300], Step [143/172], Loss: 10.4090\n",
      "Epoch [175/300], Step [144/172], Loss: 8.7780\n",
      "Epoch [175/300], Step [145/172], Loss: 9.5972\n",
      "Epoch [175/300], Step [146/172], Loss: 9.1468\n",
      "Epoch [175/300], Step [147/172], Loss: 5.0094\n",
      "Epoch [175/300], Step [148/172], Loss: 5.8305\n",
      "Epoch [175/300], Step [149/172], Loss: 6.4462\n",
      "Epoch [175/300], Step [150/172], Loss: 5.9785\n",
      "Epoch [175/300], Step [151/172], Loss: 5.3728\n",
      "Epoch [175/300], Step [152/172], Loss: 7.1891\n",
      "Epoch [175/300], Step [153/172], Loss: 6.2534\n",
      "Epoch [175/300], Step [154/172], Loss: 7.3010\n",
      "Epoch [175/300], Step [155/172], Loss: 6.0334\n",
      "Epoch [175/300], Step [156/172], Loss: 12.9118\n",
      "Epoch [175/300], Step [157/172], Loss: 9.3770\n",
      "Epoch [175/300], Step [158/172], Loss: 6.9575\n",
      "Epoch [175/300], Step [159/172], Loss: 9.1239\n",
      "Epoch [175/300], Step [160/172], Loss: 9.8511\n",
      "Epoch [175/300], Step [161/172], Loss: 6.9855\n",
      "Epoch [175/300], Step [162/172], Loss: 5.2002\n",
      "Epoch [175/300], Step [163/172], Loss: 6.2490\n",
      "Epoch [175/300], Step [164/172], Loss: 8.4579\n",
      "Epoch [175/300], Step [165/172], Loss: 5.8215\n",
      "Epoch [175/300], Step [166/172], Loss: 5.3257\n",
      "Epoch [175/300], Step [167/172], Loss: 10.1891\n",
      "Epoch [175/300], Step [168/172], Loss: 6.3366\n",
      "Epoch [175/300], Step [169/172], Loss: 6.4779\n",
      "Epoch [175/300], Step [170/172], Loss: 4.5978\n",
      "Epoch [175/300], Step [171/172], Loss: 7.5737\n",
      "Epoch [175/300], Step [172/172], Loss: 5.0570\n",
      "Epoch [176/300], Step [1/172], Loss: 54.5883\n",
      "Epoch [176/300], Step [2/172], Loss: 55.5977\n",
      "Epoch [176/300], Step [3/172], Loss: 49.4636\n",
      "Epoch [176/300], Step [4/172], Loss: 28.7990\n",
      "Epoch [176/300], Step [5/172], Loss: 50.0275\n",
      "Epoch [176/300], Step [6/172], Loss: 19.0887\n",
      "Epoch [176/300], Step [7/172], Loss: 29.5946\n",
      "Epoch [176/300], Step [8/172], Loss: 4.5271\n",
      "Epoch [176/300], Step [9/172], Loss: 33.3140\n",
      "Epoch [176/300], Step [10/172], Loss: 42.9883\n",
      "Epoch [176/300], Step [11/172], Loss: 60.9900\n",
      "Epoch [176/300], Step [12/172], Loss: 68.2788\n",
      "Epoch [176/300], Step [13/172], Loss: 35.7739\n",
      "Epoch [176/300], Step [14/172], Loss: 62.8277\n",
      "Epoch [176/300], Step [15/172], Loss: 57.7708\n",
      "Epoch [176/300], Step [16/172], Loss: 11.0888\n",
      "Epoch [176/300], Step [17/172], Loss: 46.0229\n",
      "Epoch [176/300], Step [18/172], Loss: 60.0407\n",
      "Epoch [176/300], Step [19/172], Loss: 82.3015\n",
      "Epoch [176/300], Step [20/172], Loss: 38.6238\n",
      "Epoch [176/300], Step [21/172], Loss: 84.9609\n",
      "Epoch [176/300], Step [22/172], Loss: 61.8071\n",
      "Epoch [176/300], Step [23/172], Loss: 1.9111\n",
      "Epoch [176/300], Step [24/172], Loss: 57.0779\n",
      "Epoch [176/300], Step [25/172], Loss: 41.1747\n",
      "Epoch [176/300], Step [26/172], Loss: 48.1055\n",
      "Epoch [176/300], Step [27/172], Loss: 62.6978\n",
      "Epoch [176/300], Step [28/172], Loss: 24.7124\n",
      "Epoch [176/300], Step [29/172], Loss: 17.3492\n",
      "Epoch [176/300], Step [30/172], Loss: 64.5360\n",
      "Epoch [176/300], Step [31/172], Loss: 37.7306\n",
      "Epoch [176/300], Step [32/172], Loss: 42.0438\n",
      "Epoch [176/300], Step [33/172], Loss: 70.1875\n",
      "Epoch [176/300], Step [34/172], Loss: 2.8903\n",
      "Epoch [176/300], Step [35/172], Loss: 13.2669\n",
      "Epoch [176/300], Step [36/172], Loss: 18.1648\n",
      "Epoch [176/300], Step [37/172], Loss: 16.7154\n",
      "Epoch [176/300], Step [38/172], Loss: 29.1152\n",
      "Epoch [176/300], Step [39/172], Loss: 35.6565\n",
      "Epoch [176/300], Step [40/172], Loss: 20.0161\n",
      "Epoch [176/300], Step [41/172], Loss: 33.5497\n",
      "Epoch [176/300], Step [42/172], Loss: 38.0864\n",
      "Epoch [176/300], Step [43/172], Loss: 26.7437\n",
      "Epoch [176/300], Step [44/172], Loss: 20.3239\n",
      "Epoch [176/300], Step [45/172], Loss: 25.7161\n",
      "Epoch [176/300], Step [46/172], Loss: 17.6553\n",
      "Epoch [176/300], Step [47/172], Loss: 46.8245\n",
      "Epoch [176/300], Step [48/172], Loss: 60.5125\n",
      "Epoch [176/300], Step [49/172], Loss: 20.6530\n",
      "Epoch [176/300], Step [50/172], Loss: 46.8588\n",
      "Epoch [176/300], Step [51/172], Loss: 8.3559\n",
      "Epoch [176/300], Step [52/172], Loss: 18.8111\n",
      "Epoch [176/300], Step [53/172], Loss: 22.2319\n",
      "Epoch [176/300], Step [54/172], Loss: 14.2476\n",
      "Epoch [176/300], Step [55/172], Loss: 14.1969\n",
      "Epoch [176/300], Step [56/172], Loss: 17.2057\n",
      "Epoch [176/300], Step [57/172], Loss: 17.1781\n",
      "Epoch [176/300], Step [58/172], Loss: 13.4596\n",
      "Epoch [176/300], Step [59/172], Loss: 27.6314\n",
      "Epoch [176/300], Step [60/172], Loss: 27.5534\n",
      "Epoch [176/300], Step [61/172], Loss: 6.4080\n",
      "Epoch [176/300], Step [62/172], Loss: 20.3551\n",
      "Epoch [176/300], Step [63/172], Loss: 10.2161\n",
      "Epoch [176/300], Step [64/172], Loss: 10.4591\n",
      "Epoch [176/300], Step [65/172], Loss: 20.1297\n",
      "Epoch [176/300], Step [66/172], Loss: 6.4668\n",
      "Epoch [176/300], Step [67/172], Loss: 23.5342\n",
      "Epoch [176/300], Step [68/172], Loss: 5.3116\n",
      "Epoch [176/300], Step [69/172], Loss: 36.4559\n",
      "Epoch [176/300], Step [70/172], Loss: 40.2804\n",
      "Epoch [176/300], Step [71/172], Loss: 41.0490\n",
      "Epoch [176/300], Step [72/172], Loss: 41.9997\n",
      "Epoch [176/300], Step [73/172], Loss: 50.2867\n",
      "Epoch [176/300], Step [74/172], Loss: 26.3971\n",
      "Epoch [176/300], Step [75/172], Loss: 27.9619\n",
      "Epoch [176/300], Step [76/172], Loss: 29.4197\n",
      "Epoch [176/300], Step [77/172], Loss: 50.2165\n",
      "Epoch [176/300], Step [78/172], Loss: 38.8521\n",
      "Epoch [176/300], Step [79/172], Loss: 37.8434\n",
      "Epoch [176/300], Step [80/172], Loss: 51.0849\n",
      "Epoch [176/300], Step [81/172], Loss: 34.6461\n",
      "Epoch [176/300], Step [82/172], Loss: 36.1784\n",
      "Epoch [176/300], Step [83/172], Loss: 43.9201\n",
      "Epoch [176/300], Step [84/172], Loss: 33.2189\n",
      "Epoch [176/300], Step [85/172], Loss: 38.5588\n",
      "Epoch [176/300], Step [86/172], Loss: 32.9652\n",
      "Epoch [176/300], Step [87/172], Loss: 26.3937\n",
      "Epoch [176/300], Step [88/172], Loss: 25.5336\n",
      "Epoch [176/300], Step [89/172], Loss: 26.6221\n",
      "Epoch [176/300], Step [90/172], Loss: 21.5015\n",
      "Epoch [176/300], Step [91/172], Loss: 26.7454\n",
      "Epoch [176/300], Step [92/172], Loss: 20.1459\n",
      "Epoch [176/300], Step [93/172], Loss: 20.8518\n",
      "Epoch [176/300], Step [94/172], Loss: 29.3852\n",
      "Epoch [176/300], Step [95/172], Loss: 20.9690\n",
      "Epoch [176/300], Step [96/172], Loss: 19.9149\n",
      "Epoch [176/300], Step [97/172], Loss: 27.9373\n",
      "Epoch [176/300], Step [98/172], Loss: 19.3924\n",
      "Epoch [176/300], Step [99/172], Loss: 18.9701\n",
      "Epoch [176/300], Step [100/172], Loss: 15.2715\n",
      "Epoch [176/300], Step [101/172], Loss: 18.4622\n",
      "Epoch [176/300], Step [102/172], Loss: 16.9306\n",
      "Epoch [176/300], Step [103/172], Loss: 12.4515\n",
      "Epoch [176/300], Step [104/172], Loss: 18.2173\n",
      "Epoch [176/300], Step [105/172], Loss: 18.8432\n",
      "Epoch [176/300], Step [106/172], Loss: 15.4861\n",
      "Epoch [176/300], Step [107/172], Loss: 15.3611\n",
      "Epoch [176/300], Step [108/172], Loss: 14.5677\n",
      "Epoch [176/300], Step [109/172], Loss: 14.5460\n",
      "Epoch [176/300], Step [110/172], Loss: 15.6248\n",
      "Epoch [176/300], Step [111/172], Loss: 15.0506\n",
      "Epoch [176/300], Step [112/172], Loss: 17.5683\n",
      "Epoch [176/300], Step [113/172], Loss: 11.6524\n",
      "Epoch [176/300], Step [114/172], Loss: 13.5962\n",
      "Epoch [176/300], Step [115/172], Loss: 19.1809\n",
      "Epoch [176/300], Step [116/172], Loss: 14.4078\n",
      "Epoch [176/300], Step [117/172], Loss: 11.2473\n",
      "Epoch [176/300], Step [118/172], Loss: 14.0584\n",
      "Epoch [176/300], Step [119/172], Loss: 15.6571\n",
      "Epoch [176/300], Step [120/172], Loss: 9.8397\n",
      "Epoch [176/300], Step [121/172], Loss: 9.4397\n",
      "Epoch [176/300], Step [122/172], Loss: 10.3973\n",
      "Epoch [176/300], Step [123/172], Loss: 10.1287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [176/300], Step [124/172], Loss: 7.4832\n",
      "Epoch [176/300], Step [125/172], Loss: 11.6979\n",
      "Epoch [176/300], Step [126/172], Loss: 10.8235\n",
      "Epoch [176/300], Step [127/172], Loss: 10.7639\n",
      "Epoch [176/300], Step [128/172], Loss: 10.3060\n",
      "Epoch [176/300], Step [129/172], Loss: 7.9026\n",
      "Epoch [176/300], Step [130/172], Loss: 12.0957\n",
      "Epoch [176/300], Step [131/172], Loss: 7.1938\n",
      "Epoch [176/300], Step [132/172], Loss: 8.1369\n",
      "Epoch [176/300], Step [133/172], Loss: 8.8507\n",
      "Epoch [176/300], Step [134/172], Loss: 11.3436\n",
      "Epoch [176/300], Step [135/172], Loss: 8.3549\n",
      "Epoch [176/300], Step [136/172], Loss: 8.0455\n",
      "Epoch [176/300], Step [137/172], Loss: 9.1502\n",
      "Epoch [176/300], Step [138/172], Loss: 6.7716\n",
      "Epoch [176/300], Step [139/172], Loss: 9.3807\n",
      "Epoch [176/300], Step [140/172], Loss: 9.3488\n",
      "Epoch [176/300], Step [141/172], Loss: 9.3640\n",
      "Epoch [176/300], Step [142/172], Loss: 13.8816\n",
      "Epoch [176/300], Step [143/172], Loss: 10.4335\n",
      "Epoch [176/300], Step [144/172], Loss: 8.7775\n",
      "Epoch [176/300], Step [145/172], Loss: 9.6533\n",
      "Epoch [176/300], Step [146/172], Loss: 9.1464\n",
      "Epoch [176/300], Step [147/172], Loss: 5.0158\n",
      "Epoch [176/300], Step [148/172], Loss: 5.8270\n",
      "Epoch [176/300], Step [149/172], Loss: 6.4692\n",
      "Epoch [176/300], Step [150/172], Loss: 5.9423\n",
      "Epoch [176/300], Step [151/172], Loss: 5.3536\n",
      "Epoch [176/300], Step [152/172], Loss: 7.2039\n",
      "Epoch [176/300], Step [153/172], Loss: 6.2375\n",
      "Epoch [176/300], Step [154/172], Loss: 7.2760\n",
      "Epoch [176/300], Step [155/172], Loss: 6.0189\n",
      "Epoch [176/300], Step [156/172], Loss: 12.9418\n",
      "Epoch [176/300], Step [157/172], Loss: 9.3052\n",
      "Epoch [176/300], Step [158/172], Loss: 6.9502\n",
      "Epoch [176/300], Step [159/172], Loss: 9.0005\n",
      "Epoch [176/300], Step [160/172], Loss: 9.8526\n",
      "Epoch [176/300], Step [161/172], Loss: 7.0113\n",
      "Epoch [176/300], Step [162/172], Loss: 5.1842\n",
      "Epoch [176/300], Step [163/172], Loss: 6.2538\n",
      "Epoch [176/300], Step [164/172], Loss: 8.4471\n",
      "Epoch [176/300], Step [165/172], Loss: 5.8282\n",
      "Epoch [176/300], Step [166/172], Loss: 5.3236\n",
      "Epoch [176/300], Step [167/172], Loss: 10.1579\n",
      "Epoch [176/300], Step [168/172], Loss: 6.3065\n",
      "Epoch [176/300], Step [169/172], Loss: 6.4766\n",
      "Epoch [176/300], Step [170/172], Loss: 4.5860\n",
      "Epoch [176/300], Step [171/172], Loss: 7.5740\n",
      "Epoch [176/300], Step [172/172], Loss: 5.0387\n",
      "Epoch [177/300], Step [1/172], Loss: 54.4048\n",
      "Epoch [177/300], Step [2/172], Loss: 55.4256\n",
      "Epoch [177/300], Step [3/172], Loss: 50.0978\n",
      "Epoch [177/300], Step [4/172], Loss: 28.7662\n",
      "Epoch [177/300], Step [5/172], Loss: 50.2727\n",
      "Epoch [177/300], Step [6/172], Loss: 19.0825\n",
      "Epoch [177/300], Step [7/172], Loss: 29.4330\n",
      "Epoch [177/300], Step [8/172], Loss: 4.5421\n",
      "Epoch [177/300], Step [9/172], Loss: 33.2409\n",
      "Epoch [177/300], Step [10/172], Loss: 43.0201\n",
      "Epoch [177/300], Step [11/172], Loss: 60.7395\n",
      "Epoch [177/300], Step [12/172], Loss: 68.1324\n",
      "Epoch [177/300], Step [13/172], Loss: 35.6678\n",
      "Epoch [177/300], Step [14/172], Loss: 62.8048\n",
      "Epoch [177/300], Step [15/172], Loss: 57.6088\n",
      "Epoch [177/300], Step [16/172], Loss: 10.8591\n",
      "Epoch [177/300], Step [17/172], Loss: 45.8405\n",
      "Epoch [177/300], Step [18/172], Loss: 59.8530\n",
      "Epoch [177/300], Step [19/172], Loss: 82.1781\n",
      "Epoch [177/300], Step [20/172], Loss: 38.1562\n",
      "Epoch [177/300], Step [21/172], Loss: 84.8869\n",
      "Epoch [177/300], Step [22/172], Loss: 61.3699\n",
      "Epoch [177/300], Step [23/172], Loss: 1.8151\n",
      "Epoch [177/300], Step [24/172], Loss: 56.7111\n",
      "Epoch [177/300], Step [25/172], Loss: 40.9726\n",
      "Epoch [177/300], Step [26/172], Loss: 47.7612\n",
      "Epoch [177/300], Step [27/172], Loss: 62.3522\n",
      "Epoch [177/300], Step [28/172], Loss: 24.6083\n",
      "Epoch [177/300], Step [29/172], Loss: 17.0585\n",
      "Epoch [177/300], Step [30/172], Loss: 64.8122\n",
      "Epoch [177/300], Step [31/172], Loss: 37.7730\n",
      "Epoch [177/300], Step [32/172], Loss: 42.0881\n",
      "Epoch [177/300], Step [33/172], Loss: 70.0583\n",
      "Epoch [177/300], Step [34/172], Loss: 2.8905\n",
      "Epoch [177/300], Step [35/172], Loss: 13.2546\n",
      "Epoch [177/300], Step [36/172], Loss: 17.8029\n",
      "Epoch [177/300], Step [37/172], Loss: 16.6946\n",
      "Epoch [177/300], Step [38/172], Loss: 29.3148\n",
      "Epoch [177/300], Step [39/172], Loss: 35.8495\n",
      "Epoch [177/300], Step [40/172], Loss: 20.1003\n",
      "Epoch [177/300], Step [41/172], Loss: 33.6774\n",
      "Epoch [177/300], Step [42/172], Loss: 38.1819\n",
      "Epoch [177/300], Step [43/172], Loss: 27.0143\n",
      "Epoch [177/300], Step [44/172], Loss: 20.4425\n",
      "Epoch [177/300], Step [45/172], Loss: 25.9623\n",
      "Epoch [177/300], Step [46/172], Loss: 17.7462\n",
      "Epoch [177/300], Step [47/172], Loss: 46.9201\n",
      "Epoch [177/300], Step [48/172], Loss: 60.6896\n",
      "Epoch [177/300], Step [49/172], Loss: 20.8063\n",
      "Epoch [177/300], Step [50/172], Loss: 46.9512\n",
      "Epoch [177/300], Step [51/172], Loss: 8.4402\n",
      "Epoch [177/300], Step [52/172], Loss: 18.8992\n",
      "Epoch [177/300], Step [53/172], Loss: 22.3992\n",
      "Epoch [177/300], Step [54/172], Loss: 14.2526\n",
      "Epoch [177/300], Step [55/172], Loss: 14.1628\n",
      "Epoch [177/300], Step [56/172], Loss: 17.1437\n",
      "Epoch [177/300], Step [57/172], Loss: 17.0847\n",
      "Epoch [177/300], Step [58/172], Loss: 13.4031\n",
      "Epoch [177/300], Step [59/172], Loss: 27.5212\n",
      "Epoch [177/300], Step [60/172], Loss: 27.2654\n",
      "Epoch [177/300], Step [61/172], Loss: 6.3411\n",
      "Epoch [177/300], Step [62/172], Loss: 20.4002\n",
      "Epoch [177/300], Step [63/172], Loss: 10.2684\n",
      "Epoch [177/300], Step [64/172], Loss: 10.4803\n",
      "Epoch [177/300], Step [65/172], Loss: 20.0947\n",
      "Epoch [177/300], Step [66/172], Loss: 6.4913\n",
      "Epoch [177/300], Step [67/172], Loss: 23.3813\n",
      "Epoch [177/300], Step [68/172], Loss: 5.1439\n",
      "Epoch [177/300], Step [69/172], Loss: 36.3500\n",
      "Epoch [177/300], Step [70/172], Loss: 40.0847\n",
      "Epoch [177/300], Step [71/172], Loss: 40.8672\n",
      "Epoch [177/300], Step [72/172], Loss: 41.8798\n",
      "Epoch [177/300], Step [73/172], Loss: 50.0260\n",
      "Epoch [177/300], Step [74/172], Loss: 26.3442\n",
      "Epoch [177/300], Step [75/172], Loss: 27.7192\n",
      "Epoch [177/300], Step [76/172], Loss: 29.2795\n",
      "Epoch [177/300], Step [77/172], Loss: 50.0582\n",
      "Epoch [177/300], Step [78/172], Loss: 38.7658\n",
      "Epoch [177/300], Step [79/172], Loss: 37.6457\n",
      "Epoch [177/300], Step [80/172], Loss: 50.9434\n",
      "Epoch [177/300], Step [81/172], Loss: 34.4367\n",
      "Epoch [177/300], Step [82/172], Loss: 36.1119\n",
      "Epoch [177/300], Step [83/172], Loss: 43.8527\n",
      "Epoch [177/300], Step [84/172], Loss: 33.1195\n",
      "Epoch [177/300], Step [85/172], Loss: 38.5380\n",
      "Epoch [177/300], Step [86/172], Loss: 32.8664\n",
      "Epoch [177/300], Step [87/172], Loss: 26.3191\n",
      "Epoch [177/300], Step [88/172], Loss: 25.4435\n",
      "Epoch [177/300], Step [89/172], Loss: 26.6759\n",
      "Epoch [177/300], Step [90/172], Loss: 21.3908\n",
      "Epoch [177/300], Step [91/172], Loss: 26.7286\n",
      "Epoch [177/300], Step [92/172], Loss: 20.0858\n",
      "Epoch [177/300], Step [93/172], Loss: 20.8099\n",
      "Epoch [177/300], Step [94/172], Loss: 29.3283\n",
      "Epoch [177/300], Step [95/172], Loss: 20.9051\n",
      "Epoch [177/300], Step [96/172], Loss: 19.9770\n",
      "Epoch [177/300], Step [97/172], Loss: 27.9843\n",
      "Epoch [177/300], Step [98/172], Loss: 19.4632\n",
      "Epoch [177/300], Step [99/172], Loss: 19.0277\n",
      "Epoch [177/300], Step [100/172], Loss: 15.3425\n",
      "Epoch [177/300], Step [101/172], Loss: 18.5180\n",
      "Epoch [177/300], Step [102/172], Loss: 16.9817\n",
      "Epoch [177/300], Step [103/172], Loss: 12.4948\n",
      "Epoch [177/300], Step [104/172], Loss: 18.2549\n",
      "Epoch [177/300], Step [105/172], Loss: 18.9550\n",
      "Epoch [177/300], Step [106/172], Loss: 15.4964\n",
      "Epoch [177/300], Step [107/172], Loss: 15.4075\n",
      "Epoch [177/300], Step [108/172], Loss: 14.5766\n",
      "Epoch [177/300], Step [109/172], Loss: 14.5415\n",
      "Epoch [177/300], Step [110/172], Loss: 15.6857\n",
      "Epoch [177/300], Step [111/172], Loss: 15.1450\n",
      "Epoch [177/300], Step [112/172], Loss: 17.4973\n",
      "Epoch [177/300], Step [113/172], Loss: 11.6530\n",
      "Epoch [177/300], Step [114/172], Loss: 13.6141\n",
      "Epoch [177/300], Step [115/172], Loss: 19.1678\n",
      "Epoch [177/300], Step [116/172], Loss: 14.4386\n",
      "Epoch [177/300], Step [117/172], Loss: 11.2414\n",
      "Epoch [177/300], Step [118/172], Loss: 14.0042\n",
      "Epoch [177/300], Step [119/172], Loss: 15.7208\n",
      "Epoch [177/300], Step [120/172], Loss: 9.8465\n",
      "Epoch [177/300], Step [121/172], Loss: 9.4606\n",
      "Epoch [177/300], Step [122/172], Loss: 10.4259\n",
      "Epoch [177/300], Step [123/172], Loss: 10.1504\n",
      "Epoch [177/300], Step [124/172], Loss: 7.4734\n",
      "Epoch [177/300], Step [125/172], Loss: 11.6547\n",
      "Epoch [177/300], Step [126/172], Loss: 10.8298\n",
      "Epoch [177/300], Step [127/172], Loss: 10.8137\n",
      "Epoch [177/300], Step [128/172], Loss: 10.3614\n",
      "Epoch [177/300], Step [129/172], Loss: 7.9194\n",
      "Epoch [177/300], Step [130/172], Loss: 12.0872\n",
      "Epoch [177/300], Step [131/172], Loss: 7.2104\n",
      "Epoch [177/300], Step [132/172], Loss: 8.1766\n",
      "Epoch [177/300], Step [133/172], Loss: 8.8682\n",
      "Epoch [177/300], Step [134/172], Loss: 11.3648\n",
      "Epoch [177/300], Step [135/172], Loss: 8.3553\n",
      "Epoch [177/300], Step [136/172], Loss: 8.0879\n",
      "Epoch [177/300], Step [137/172], Loss: 9.1760\n",
      "Epoch [177/300], Step [138/172], Loss: 6.8246\n",
      "Epoch [177/300], Step [139/172], Loss: 9.3378\n",
      "Epoch [177/300], Step [140/172], Loss: 9.4183\n",
      "Epoch [177/300], Step [141/172], Loss: 9.3087\n",
      "Epoch [177/300], Step [142/172], Loss: 13.8941\n",
      "Epoch [177/300], Step [143/172], Loss: 10.4321\n",
      "Epoch [177/300], Step [144/172], Loss: 8.8077\n",
      "Epoch [177/300], Step [145/172], Loss: 9.6341\n",
      "Epoch [177/300], Step [146/172], Loss: 9.2229\n",
      "Epoch [177/300], Step [147/172], Loss: 5.0002\n",
      "Epoch [177/300], Step [148/172], Loss: 5.8442\n",
      "Epoch [177/300], Step [149/172], Loss: 6.4424\n",
      "Epoch [177/300], Step [150/172], Loss: 5.9565\n",
      "Epoch [177/300], Step [151/172], Loss: 5.3461\n",
      "Epoch [177/300], Step [152/172], Loss: 7.2342\n",
      "Epoch [177/300], Step [153/172], Loss: 6.2120\n",
      "Epoch [177/300], Step [154/172], Loss: 7.3094\n",
      "Epoch [177/300], Step [155/172], Loss: 5.9570\n",
      "Epoch [177/300], Step [156/172], Loss: 12.9802\n",
      "Epoch [177/300], Step [157/172], Loss: 9.3152\n",
      "Epoch [177/300], Step [158/172], Loss: 6.9759\n",
      "Epoch [177/300], Step [159/172], Loss: 9.1334\n",
      "Epoch [177/300], Step [160/172], Loss: 9.8897\n",
      "Epoch [177/300], Step [161/172], Loss: 6.9636\n",
      "Epoch [177/300], Step [162/172], Loss: 5.1808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [177/300], Step [163/172], Loss: 6.3021\n",
      "Epoch [177/300], Step [164/172], Loss: 8.4407\n",
      "Epoch [177/300], Step [165/172], Loss: 5.8431\n",
      "Epoch [177/300], Step [166/172], Loss: 5.3835\n",
      "Epoch [177/300], Step [167/172], Loss: 10.2176\n",
      "Epoch [177/300], Step [168/172], Loss: 6.3184\n",
      "Epoch [177/300], Step [169/172], Loss: 6.4667\n",
      "Epoch [177/300], Step [170/172], Loss: 4.6029\n",
      "Epoch [177/300], Step [171/172], Loss: 7.6310\n",
      "Epoch [177/300], Step [172/172], Loss: 5.0071\n",
      "Epoch [178/300], Step [1/172], Loss: 54.2156\n",
      "Epoch [178/300], Step [2/172], Loss: 55.3603\n",
      "Epoch [178/300], Step [3/172], Loss: 48.9190\n",
      "Epoch [178/300], Step [4/172], Loss: 28.6185\n",
      "Epoch [178/300], Step [5/172], Loss: 48.8187\n",
      "Epoch [178/300], Step [6/172], Loss: 19.1202\n",
      "Epoch [178/300], Step [7/172], Loss: 29.5215\n",
      "Epoch [178/300], Step [8/172], Loss: 4.4921\n",
      "Epoch [178/300], Step [9/172], Loss: 33.0344\n",
      "Epoch [178/300], Step [10/172], Loss: 42.9345\n",
      "Epoch [178/300], Step [11/172], Loss: 60.4638\n",
      "Epoch [178/300], Step [12/172], Loss: 67.9024\n",
      "Epoch [178/300], Step [13/172], Loss: 35.3672\n",
      "Epoch [178/300], Step [14/172], Loss: 62.1805\n",
      "Epoch [178/300], Step [15/172], Loss: 57.3468\n",
      "Epoch [178/300], Step [16/172], Loss: 10.6940\n",
      "Epoch [178/300], Step [17/172], Loss: 45.7005\n",
      "Epoch [178/300], Step [18/172], Loss: 59.6801\n",
      "Epoch [178/300], Step [19/172], Loss: 82.2840\n",
      "Epoch [178/300], Step [20/172], Loss: 38.0633\n",
      "Epoch [178/300], Step [21/172], Loss: 84.5680\n",
      "Epoch [178/300], Step [22/172], Loss: 61.1368\n",
      "Epoch [178/300], Step [23/172], Loss: 1.7655\n",
      "Epoch [178/300], Step [24/172], Loss: 56.7077\n",
      "Epoch [178/300], Step [25/172], Loss: 40.9612\n",
      "Epoch [178/300], Step [26/172], Loss: 47.7063\n",
      "Epoch [178/300], Step [27/172], Loss: 62.2009\n",
      "Epoch [178/300], Step [28/172], Loss: 24.5944\n",
      "Epoch [178/300], Step [29/172], Loss: 17.0795\n",
      "Epoch [178/300], Step [30/172], Loss: 64.5384\n",
      "Epoch [178/300], Step [31/172], Loss: 37.6694\n",
      "Epoch [178/300], Step [32/172], Loss: 42.1889\n",
      "Epoch [178/300], Step [33/172], Loss: 70.2070\n",
      "Epoch [178/300], Step [34/172], Loss: 2.8914\n",
      "Epoch [178/300], Step [35/172], Loss: 13.2674\n",
      "Epoch [178/300], Step [36/172], Loss: 17.9064\n",
      "Epoch [178/300], Step [37/172], Loss: 16.6338\n",
      "Epoch [178/300], Step [38/172], Loss: 29.2369\n",
      "Epoch [178/300], Step [39/172], Loss: 35.7766\n",
      "Epoch [178/300], Step [40/172], Loss: 20.1059\n",
      "Epoch [178/300], Step [41/172], Loss: 33.7292\n",
      "Epoch [178/300], Step [42/172], Loss: 37.9467\n",
      "Epoch [178/300], Step [43/172], Loss: 27.0381\n",
      "Epoch [178/300], Step [44/172], Loss: 20.5466\n",
      "Epoch [178/300], Step [45/172], Loss: 26.0237\n",
      "Epoch [178/300], Step [46/172], Loss: 17.5566\n",
      "Epoch [178/300], Step [47/172], Loss: 46.8580\n",
      "Epoch [178/300], Step [48/172], Loss: 60.5002\n",
      "Epoch [178/300], Step [49/172], Loss: 20.7925\n",
      "Epoch [178/300], Step [50/172], Loss: 46.9623\n",
      "Epoch [178/300], Step [51/172], Loss: 8.3996\n",
      "Epoch [178/300], Step [52/172], Loss: 18.9130\n",
      "Epoch [178/300], Step [53/172], Loss: 22.3024\n",
      "Epoch [178/300], Step [54/172], Loss: 14.2810\n",
      "Epoch [178/300], Step [55/172], Loss: 14.1051\n",
      "Epoch [178/300], Step [56/172], Loss: 17.1382\n",
      "Epoch [178/300], Step [57/172], Loss: 17.0492\n",
      "Epoch [178/300], Step [58/172], Loss: 13.4114\n",
      "Epoch [178/300], Step [59/172], Loss: 27.5755\n",
      "Epoch [178/300], Step [60/172], Loss: 26.8523\n",
      "Epoch [178/300], Step [61/172], Loss: 6.2597\n",
      "Epoch [178/300], Step [62/172], Loss: 20.0176\n",
      "Epoch [178/300], Step [63/172], Loss: 10.1113\n",
      "Epoch [178/300], Step [64/172], Loss: 10.4488\n",
      "Epoch [178/300], Step [65/172], Loss: 19.9715\n",
      "Epoch [178/300], Step [66/172], Loss: 6.4393\n",
      "Epoch [178/300], Step [67/172], Loss: 23.2614\n",
      "Epoch [178/300], Step [68/172], Loss: 4.9921\n",
      "Epoch [178/300], Step [69/172], Loss: 36.1217\n",
      "Epoch [178/300], Step [70/172], Loss: 40.0045\n",
      "Epoch [178/300], Step [71/172], Loss: 40.9141\n",
      "Epoch [178/300], Step [72/172], Loss: 41.7502\n",
      "Epoch [178/300], Step [73/172], Loss: 50.2004\n",
      "Epoch [178/300], Step [74/172], Loss: 26.2309\n",
      "Epoch [178/300], Step [75/172], Loss: 27.7571\n",
      "Epoch [178/300], Step [76/172], Loss: 29.3152\n",
      "Epoch [178/300], Step [77/172], Loss: 50.1201\n",
      "Epoch [178/300], Step [78/172], Loss: 38.6504\n",
      "Epoch [178/300], Step [79/172], Loss: 37.6395\n",
      "Epoch [178/300], Step [80/172], Loss: 50.9256\n",
      "Epoch [178/300], Step [81/172], Loss: 34.4973\n",
      "Epoch [178/300], Step [82/172], Loss: 35.9818\n",
      "Epoch [178/300], Step [83/172], Loss: 43.8708\n",
      "Epoch [178/300], Step [84/172], Loss: 33.1118\n",
      "Epoch [178/300], Step [85/172], Loss: 38.6042\n",
      "Epoch [178/300], Step [86/172], Loss: 32.8832\n",
      "Epoch [178/300], Step [87/172], Loss: 26.3475\n",
      "Epoch [178/300], Step [88/172], Loss: 25.3322\n",
      "Epoch [178/300], Step [89/172], Loss: 26.6076\n",
      "Epoch [178/300], Step [90/172], Loss: 21.3153\n",
      "Epoch [178/300], Step [91/172], Loss: 26.6897\n",
      "Epoch [178/300], Step [92/172], Loss: 20.0614\n",
      "Epoch [178/300], Step [93/172], Loss: 20.7914\n",
      "Epoch [178/300], Step [94/172], Loss: 29.3200\n",
      "Epoch [178/300], Step [95/172], Loss: 20.8305\n",
      "Epoch [178/300], Step [96/172], Loss: 19.9276\n",
      "Epoch [178/300], Step [97/172], Loss: 27.9991\n",
      "Epoch [178/300], Step [98/172], Loss: 19.4488\n",
      "Epoch [178/300], Step [99/172], Loss: 18.9574\n",
      "Epoch [178/300], Step [100/172], Loss: 15.3655\n",
      "Epoch [178/300], Step [101/172], Loss: 18.5281\n",
      "Epoch [178/300], Step [102/172], Loss: 16.9631\n",
      "Epoch [178/300], Step [103/172], Loss: 12.4742\n",
      "Epoch [178/300], Step [104/172], Loss: 18.2544\n",
      "Epoch [178/300], Step [105/172], Loss: 18.9337\n",
      "Epoch [178/300], Step [106/172], Loss: 15.5489\n",
      "Epoch [178/300], Step [107/172], Loss: 15.4166\n",
      "Epoch [178/300], Step [108/172], Loss: 14.5906\n",
      "Epoch [178/300], Step [109/172], Loss: 14.5535\n",
      "Epoch [178/300], Step [110/172], Loss: 15.6572\n",
      "Epoch [178/300], Step [111/172], Loss: 15.1421\n",
      "Epoch [178/300], Step [112/172], Loss: 17.4818\n",
      "Epoch [178/300], Step [113/172], Loss: 11.6608\n",
      "Epoch [178/300], Step [114/172], Loss: 13.6350\n",
      "Epoch [178/300], Step [115/172], Loss: 19.1726\n",
      "Epoch [178/300], Step [116/172], Loss: 14.4119\n",
      "Epoch [178/300], Step [117/172], Loss: 11.2112\n",
      "Epoch [178/300], Step [118/172], Loss: 14.0156\n",
      "Epoch [178/300], Step [119/172], Loss: 15.6844\n",
      "Epoch [178/300], Step [120/172], Loss: 9.8633\n",
      "Epoch [178/300], Step [121/172], Loss: 9.4130\n",
      "Epoch [178/300], Step [122/172], Loss: 10.4608\n",
      "Epoch [178/300], Step [123/172], Loss: 10.0563\n",
      "Epoch [178/300], Step [124/172], Loss: 7.4288\n",
      "Epoch [178/300], Step [125/172], Loss: 11.6502\n",
      "Epoch [178/300], Step [126/172], Loss: 10.8210\n",
      "Epoch [178/300], Step [127/172], Loss: 10.7543\n",
      "Epoch [178/300], Step [128/172], Loss: 10.2254\n",
      "Epoch [178/300], Step [129/172], Loss: 7.8773\n",
      "Epoch [178/300], Step [130/172], Loss: 12.0548\n",
      "Epoch [178/300], Step [131/172], Loss: 7.1874\n",
      "Epoch [178/300], Step [132/172], Loss: 8.1684\n",
      "Epoch [178/300], Step [133/172], Loss: 8.8688\n",
      "Epoch [178/300], Step [134/172], Loss: 11.2880\n",
      "Epoch [178/300], Step [135/172], Loss: 8.3292\n",
      "Epoch [178/300], Step [136/172], Loss: 7.9806\n",
      "Epoch [178/300], Step [137/172], Loss: 9.0951\n",
      "Epoch [178/300], Step [138/172], Loss: 6.7400\n",
      "Epoch [178/300], Step [139/172], Loss: 9.3232\n",
      "Epoch [178/300], Step [140/172], Loss: 9.3640\n",
      "Epoch [178/300], Step [141/172], Loss: 9.3371\n",
      "Epoch [178/300], Step [142/172], Loss: 13.8602\n",
      "Epoch [178/300], Step [143/172], Loss: 10.4037\n",
      "Epoch [178/300], Step [144/172], Loss: 8.7917\n",
      "Epoch [178/300], Step [145/172], Loss: 9.6324\n",
      "Epoch [178/300], Step [146/172], Loss: 9.1518\n",
      "Epoch [178/300], Step [147/172], Loss: 5.0048\n",
      "Epoch [178/300], Step [148/172], Loss: 5.8211\n",
      "Epoch [178/300], Step [149/172], Loss: 6.4183\n",
      "Epoch [178/300], Step [150/172], Loss: 5.9004\n",
      "Epoch [178/300], Step [151/172], Loss: 5.3247\n",
      "Epoch [178/300], Step [152/172], Loss: 7.1969\n",
      "Epoch [178/300], Step [153/172], Loss: 6.2040\n",
      "Epoch [178/300], Step [154/172], Loss: 7.2665\n",
      "Epoch [178/300], Step [155/172], Loss: 5.9590\n",
      "Epoch [178/300], Step [156/172], Loss: 12.9602\n",
      "Epoch [178/300], Step [157/172], Loss: 9.2860\n",
      "Epoch [178/300], Step [158/172], Loss: 6.9580\n",
      "Epoch [178/300], Step [159/172], Loss: 9.0184\n",
      "Epoch [178/300], Step [160/172], Loss: 9.8598\n",
      "Epoch [178/300], Step [161/172], Loss: 6.8691\n",
      "Epoch [178/300], Step [162/172], Loss: 5.1526\n",
      "Epoch [178/300], Step [163/172], Loss: 6.2309\n",
      "Epoch [178/300], Step [164/172], Loss: 8.3724\n",
      "Epoch [178/300], Step [165/172], Loss: 5.8615\n",
      "Epoch [178/300], Step [166/172], Loss: 5.3521\n",
      "Epoch [178/300], Step [167/172], Loss: 10.1931\n",
      "Epoch [178/300], Step [168/172], Loss: 6.2983\n",
      "Epoch [178/300], Step [169/172], Loss: 6.5224\n",
      "Epoch [178/300], Step [170/172], Loss: 4.5843\n",
      "Epoch [178/300], Step [171/172], Loss: 7.6100\n",
      "Epoch [178/300], Step [172/172], Loss: 4.9923\n",
      "Epoch [179/300], Step [1/172], Loss: 54.0477\n",
      "Epoch [179/300], Step [2/172], Loss: 55.2736\n",
      "Epoch [179/300], Step [3/172], Loss: 49.6841\n",
      "Epoch [179/300], Step [4/172], Loss: 28.4875\n",
      "Epoch [179/300], Step [5/172], Loss: 49.0757\n",
      "Epoch [179/300], Step [6/172], Loss: 19.0959\n",
      "Epoch [179/300], Step [7/172], Loss: 29.4402\n",
      "Epoch [179/300], Step [8/172], Loss: 4.7876\n",
      "Epoch [179/300], Step [9/172], Loss: 33.0600\n",
      "Epoch [179/300], Step [10/172], Loss: 42.8699\n",
      "Epoch [179/300], Step [11/172], Loss: 60.5307\n",
      "Epoch [179/300], Step [12/172], Loss: 67.7705\n",
      "Epoch [179/300], Step [13/172], Loss: 35.1665\n",
      "Epoch [179/300], Step [14/172], Loss: 62.3823\n",
      "Epoch [179/300], Step [15/172], Loss: 57.4693\n",
      "Epoch [179/300], Step [16/172], Loss: 10.5313\n",
      "Epoch [179/300], Step [17/172], Loss: 45.9496\n",
      "Epoch [179/300], Step [18/172], Loss: 59.8765\n",
      "Epoch [179/300], Step [19/172], Loss: 82.4770\n",
      "Epoch [179/300], Step [20/172], Loss: 37.7212\n",
      "Epoch [179/300], Step [21/172], Loss: 85.2056\n",
      "Epoch [179/300], Step [22/172], Loss: 61.0044\n",
      "Epoch [179/300], Step [23/172], Loss: 1.8591\n",
      "Epoch [179/300], Step [24/172], Loss: 56.5677\n",
      "Epoch [179/300], Step [25/172], Loss: 40.9418\n",
      "Epoch [179/300], Step [26/172], Loss: 47.7283\n",
      "Epoch [179/300], Step [27/172], Loss: 62.0606\n",
      "Epoch [179/300], Step [28/172], Loss: 24.6887\n",
      "Epoch [179/300], Step [29/172], Loss: 17.0580\n",
      "Epoch [179/300], Step [30/172], Loss: 64.2593\n",
      "Epoch [179/300], Step [31/172], Loss: 37.5413\n",
      "Epoch [179/300], Step [32/172], Loss: 42.2130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [179/300], Step [33/172], Loss: 70.2035\n",
      "Epoch [179/300], Step [34/172], Loss: 2.9492\n",
      "Epoch [179/300], Step [35/172], Loss: 13.3725\n",
      "Epoch [179/300], Step [36/172], Loss: 17.9686\n",
      "Epoch [179/300], Step [37/172], Loss: 16.6312\n",
      "Epoch [179/300], Step [38/172], Loss: 29.1838\n",
      "Epoch [179/300], Step [39/172], Loss: 35.7525\n",
      "Epoch [179/300], Step [40/172], Loss: 20.2036\n",
      "Epoch [179/300], Step [41/172], Loss: 33.6635\n",
      "Epoch [179/300], Step [42/172], Loss: 37.9963\n",
      "Epoch [179/300], Step [43/172], Loss: 27.0602\n",
      "Epoch [179/300], Step [44/172], Loss: 20.6410\n",
      "Epoch [179/300], Step [45/172], Loss: 26.1645\n",
      "Epoch [179/300], Step [46/172], Loss: 17.5826\n",
      "Epoch [179/300], Step [47/172], Loss: 46.8760\n",
      "Epoch [179/300], Step [48/172], Loss: 60.5851\n",
      "Epoch [179/300], Step [49/172], Loss: 20.8616\n",
      "Epoch [179/300], Step [50/172], Loss: 46.7090\n",
      "Epoch [179/300], Step [51/172], Loss: 8.4206\n",
      "Epoch [179/300], Step [52/172], Loss: 19.0519\n",
      "Epoch [179/300], Step [53/172], Loss: 22.2782\n",
      "Epoch [179/300], Step [54/172], Loss: 14.4767\n",
      "Epoch [179/300], Step [55/172], Loss: 14.2545\n",
      "Epoch [179/300], Step [56/172], Loss: 17.1444\n",
      "Epoch [179/300], Step [57/172], Loss: 17.0376\n",
      "Epoch [179/300], Step [58/172], Loss: 13.3062\n",
      "Epoch [179/300], Step [59/172], Loss: 27.4932\n",
      "Epoch [179/300], Step [60/172], Loss: 26.6799\n",
      "Epoch [179/300], Step [61/172], Loss: 6.3126\n",
      "Epoch [179/300], Step [62/172], Loss: 19.9146\n",
      "Epoch [179/300], Step [63/172], Loss: 10.1316\n",
      "Epoch [179/300], Step [64/172], Loss: 10.4613\n",
      "Epoch [179/300], Step [65/172], Loss: 19.9865\n",
      "Epoch [179/300], Step [66/172], Loss: 6.4568\n",
      "Epoch [179/300], Step [67/172], Loss: 23.1886\n",
      "Epoch [179/300], Step [68/172], Loss: 5.2677\n",
      "Epoch [179/300], Step [69/172], Loss: 35.8035\n",
      "Epoch [179/300], Step [70/172], Loss: 39.7142\n",
      "Epoch [179/300], Step [71/172], Loss: 40.8017\n",
      "Epoch [179/300], Step [72/172], Loss: 41.4462\n",
      "Epoch [179/300], Step [73/172], Loss: 50.0775\n",
      "Epoch [179/300], Step [74/172], Loss: 26.0627\n",
      "Epoch [179/300], Step [75/172], Loss: 27.5292\n",
      "Epoch [179/300], Step [76/172], Loss: 29.2382\n",
      "Epoch [179/300], Step [77/172], Loss: 49.7208\n",
      "Epoch [179/300], Step [78/172], Loss: 38.4418\n",
      "Epoch [179/300], Step [79/172], Loss: 37.4417\n",
      "Epoch [179/300], Step [80/172], Loss: 50.7184\n",
      "Epoch [179/300], Step [81/172], Loss: 34.3091\n",
      "Epoch [179/300], Step [82/172], Loss: 35.9024\n",
      "Epoch [179/300], Step [83/172], Loss: 43.6321\n",
      "Epoch [179/300], Step [84/172], Loss: 32.9071\n",
      "Epoch [179/300], Step [85/172], Loss: 38.2740\n",
      "Epoch [179/300], Step [86/172], Loss: 32.6460\n",
      "Epoch [179/300], Step [87/172], Loss: 26.1798\n",
      "Epoch [179/300], Step [88/172], Loss: 25.1403\n",
      "Epoch [179/300], Step [89/172], Loss: 26.4779\n",
      "Epoch [179/300], Step [90/172], Loss: 21.2053\n",
      "Epoch [179/300], Step [91/172], Loss: 26.5701\n",
      "Epoch [179/300], Step [92/172], Loss: 19.9870\n",
      "Epoch [179/300], Step [93/172], Loss: 20.6989\n",
      "Epoch [179/300], Step [94/172], Loss: 29.2610\n",
      "Epoch [179/300], Step [95/172], Loss: 20.7410\n",
      "Epoch [179/300], Step [96/172], Loss: 19.9071\n",
      "Epoch [179/300], Step [97/172], Loss: 27.9389\n",
      "Epoch [179/300], Step [98/172], Loss: 19.4188\n",
      "Epoch [179/300], Step [99/172], Loss: 18.9917\n",
      "Epoch [179/300], Step [100/172], Loss: 15.3742\n",
      "Epoch [179/300], Step [101/172], Loss: 18.5638\n",
      "Epoch [179/300], Step [102/172], Loss: 16.9932\n",
      "Epoch [179/300], Step [103/172], Loss: 12.4794\n",
      "Epoch [179/300], Step [104/172], Loss: 18.3057\n",
      "Epoch [179/300], Step [105/172], Loss: 19.0561\n",
      "Epoch [179/300], Step [106/172], Loss: 15.5448\n",
      "Epoch [179/300], Step [107/172], Loss: 15.3934\n",
      "Epoch [179/300], Step [108/172], Loss: 14.5684\n",
      "Epoch [179/300], Step [109/172], Loss: 14.5857\n",
      "Epoch [179/300], Step [110/172], Loss: 15.6741\n",
      "Epoch [179/300], Step [111/172], Loss: 15.2020\n",
      "Epoch [179/300], Step [112/172], Loss: 17.4683\n",
      "Epoch [179/300], Step [113/172], Loss: 11.6581\n",
      "Epoch [179/300], Step [114/172], Loss: 13.6684\n",
      "Epoch [179/300], Step [115/172], Loss: 19.0731\n",
      "Epoch [179/300], Step [116/172], Loss: 14.4413\n",
      "Epoch [179/300], Step [117/172], Loss: 11.2550\n",
      "Epoch [179/300], Step [118/172], Loss: 13.9890\n",
      "Epoch [179/300], Step [119/172], Loss: 15.6216\n",
      "Epoch [179/300], Step [120/172], Loss: 9.8868\n",
      "Epoch [179/300], Step [121/172], Loss: 9.4172\n",
      "Epoch [179/300], Step [122/172], Loss: 10.4682\n",
      "Epoch [179/300], Step [123/172], Loss: 10.1170\n",
      "Epoch [179/300], Step [124/172], Loss: 7.4622\n",
      "Epoch [179/300], Step [125/172], Loss: 11.6358\n",
      "Epoch [179/300], Step [126/172], Loss: 10.8255\n",
      "Epoch [179/300], Step [127/172], Loss: 10.7246\n",
      "Epoch [179/300], Step [128/172], Loss: 10.1760\n",
      "Epoch [179/300], Step [129/172], Loss: 7.8656\n",
      "Epoch [179/300], Step [130/172], Loss: 12.0776\n",
      "Epoch [179/300], Step [131/172], Loss: 7.1780\n",
      "Epoch [179/300], Step [132/172], Loss: 8.1781\n",
      "Epoch [179/300], Step [133/172], Loss: 8.8790\n",
      "Epoch [179/300], Step [134/172], Loss: 11.2028\n",
      "Epoch [179/300], Step [135/172], Loss: 8.3254\n",
      "Epoch [179/300], Step [136/172], Loss: 8.0922\n",
      "Epoch [179/300], Step [137/172], Loss: 9.1223\n",
      "Epoch [179/300], Step [138/172], Loss: 6.7362\n",
      "Epoch [179/300], Step [139/172], Loss: 9.3513\n",
      "Epoch [179/300], Step [140/172], Loss: 9.3947\n",
      "Epoch [179/300], Step [141/172], Loss: 9.2791\n",
      "Epoch [179/300], Step [142/172], Loss: 13.8403\n",
      "Epoch [179/300], Step [143/172], Loss: 10.4276\n",
      "Epoch [179/300], Step [144/172], Loss: 8.8010\n",
      "Epoch [179/300], Step [145/172], Loss: 9.6563\n",
      "Epoch [179/300], Step [146/172], Loss: 9.1365\n",
      "Epoch [179/300], Step [147/172], Loss: 5.0394\n",
      "Epoch [179/300], Step [148/172], Loss: 5.8394\n",
      "Epoch [179/300], Step [149/172], Loss: 6.4529\n",
      "Epoch [179/300], Step [150/172], Loss: 5.9315\n",
      "Epoch [179/300], Step [151/172], Loss: 5.3093\n",
      "Epoch [179/300], Step [152/172], Loss: 7.1879\n",
      "Epoch [179/300], Step [153/172], Loss: 6.2424\n",
      "Epoch [179/300], Step [154/172], Loss: 7.2486\n",
      "Epoch [179/300], Step [155/172], Loss: 5.9988\n",
      "Epoch [179/300], Step [156/172], Loss: 12.9675\n",
      "Epoch [179/300], Step [157/172], Loss: 9.2947\n",
      "Epoch [179/300], Step [158/172], Loss: 6.9648\n",
      "Epoch [179/300], Step [159/172], Loss: 9.0828\n",
      "Epoch [179/300], Step [160/172], Loss: 9.8270\n",
      "Epoch [179/300], Step [161/172], Loss: 6.9186\n",
      "Epoch [179/300], Step [162/172], Loss: 5.1461\n",
      "Epoch [179/300], Step [163/172], Loss: 6.2358\n",
      "Epoch [179/300], Step [164/172], Loss: 8.4277\n",
      "Epoch [179/300], Step [165/172], Loss: 5.9025\n",
      "Epoch [179/300], Step [166/172], Loss: 5.3546\n",
      "Epoch [179/300], Step [167/172], Loss: 10.2656\n",
      "Epoch [179/300], Step [168/172], Loss: 6.2963\n",
      "Epoch [179/300], Step [169/172], Loss: 6.5286\n",
      "Epoch [179/300], Step [170/172], Loss: 4.5919\n",
      "Epoch [179/300], Step [171/172], Loss: 7.6606\n",
      "Epoch [179/300], Step [172/172], Loss: 5.0493\n",
      "Epoch [180/300], Step [1/172], Loss: 53.9049\n",
      "Epoch [180/300], Step [2/172], Loss: 55.0913\n",
      "Epoch [180/300], Step [3/172], Loss: 49.1213\n",
      "Epoch [180/300], Step [4/172], Loss: 28.4538\n",
      "Epoch [180/300], Step [5/172], Loss: 48.4211\n",
      "Epoch [180/300], Step [6/172], Loss: 19.0990\n",
      "Epoch [180/300], Step [7/172], Loss: 29.4789\n",
      "Epoch [180/300], Step [8/172], Loss: 4.4737\n",
      "Epoch [180/300], Step [9/172], Loss: 32.9248\n",
      "Epoch [180/300], Step [10/172], Loss: 42.8117\n",
      "Epoch [180/300], Step [11/172], Loss: 60.2151\n",
      "Epoch [180/300], Step [12/172], Loss: 67.2856\n",
      "Epoch [180/300], Step [13/172], Loss: 34.6093\n",
      "Epoch [180/300], Step [14/172], Loss: 61.5924\n",
      "Epoch [180/300], Step [15/172], Loss: 57.1562\n",
      "Epoch [180/300], Step [16/172], Loss: 11.2205\n",
      "Epoch [180/300], Step [17/172], Loss: 45.5345\n",
      "Epoch [180/300], Step [18/172], Loss: 59.5961\n",
      "Epoch [180/300], Step [19/172], Loss: 82.3725\n",
      "Epoch [180/300], Step [20/172], Loss: 37.5831\n",
      "Epoch [180/300], Step [21/172], Loss: 85.0106\n",
      "Epoch [180/300], Step [22/172], Loss: 61.2974\n",
      "Epoch [180/300], Step [23/172], Loss: 1.8556\n",
      "Epoch [180/300], Step [24/172], Loss: 56.7942\n",
      "Epoch [180/300], Step [25/172], Loss: 40.9761\n",
      "Epoch [180/300], Step [26/172], Loss: 47.8416\n",
      "Epoch [180/300], Step [27/172], Loss: 62.5159\n",
      "Epoch [180/300], Step [28/172], Loss: 24.8648\n",
      "Epoch [180/300], Step [29/172], Loss: 17.1077\n",
      "Epoch [180/300], Step [30/172], Loss: 64.1708\n",
      "Epoch [180/300], Step [31/172], Loss: 37.3907\n",
      "Epoch [180/300], Step [32/172], Loss: 42.2383\n",
      "Epoch [180/300], Step [33/172], Loss: 70.2674\n",
      "Epoch [180/300], Step [34/172], Loss: 2.8074\n",
      "Epoch [180/300], Step [35/172], Loss: 13.3176\n",
      "Epoch [180/300], Step [36/172], Loss: 18.1362\n",
      "Epoch [180/300], Step [37/172], Loss: 16.5715\n",
      "Epoch [180/300], Step [38/172], Loss: 29.1824\n",
      "Epoch [180/300], Step [39/172], Loss: 35.6986\n",
      "Epoch [180/300], Step [40/172], Loss: 20.0946\n",
      "Epoch [180/300], Step [41/172], Loss: 33.6203\n",
      "Epoch [180/300], Step [42/172], Loss: 37.7801\n",
      "Epoch [180/300], Step [43/172], Loss: 26.8190\n",
      "Epoch [180/300], Step [44/172], Loss: 20.4876\n",
      "Epoch [180/300], Step [45/172], Loss: 25.9557\n",
      "Epoch [180/300], Step [46/172], Loss: 17.4914\n",
      "Epoch [180/300], Step [47/172], Loss: 46.8852\n",
      "Epoch [180/300], Step [48/172], Loss: 60.7757\n",
      "Epoch [180/300], Step [49/172], Loss: 20.7687\n",
      "Epoch [180/300], Step [50/172], Loss: 46.8825\n",
      "Epoch [180/300], Step [51/172], Loss: 8.3926\n",
      "Epoch [180/300], Step [52/172], Loss: 18.9805\n",
      "Epoch [180/300], Step [53/172], Loss: 22.3125\n",
      "Epoch [180/300], Step [54/172], Loss: 14.3203\n",
      "Epoch [180/300], Step [55/172], Loss: 14.1490\n",
      "Epoch [180/300], Step [56/172], Loss: 17.4227\n",
      "Epoch [180/300], Step [57/172], Loss: 16.9648\n",
      "Epoch [180/300], Step [58/172], Loss: 13.3596\n",
      "Epoch [180/300], Step [59/172], Loss: 27.5540\n",
      "Epoch [180/300], Step [60/172], Loss: 26.4981\n",
      "Epoch [180/300], Step [61/172], Loss: 6.2470\n",
      "Epoch [180/300], Step [62/172], Loss: 19.9472\n",
      "Epoch [180/300], Step [63/172], Loss: 10.1408\n",
      "Epoch [180/300], Step [64/172], Loss: 10.4927\n",
      "Epoch [180/300], Step [65/172], Loss: 19.9724\n",
      "Epoch [180/300], Step [66/172], Loss: 6.4394\n",
      "Epoch [180/300], Step [67/172], Loss: 23.2602\n",
      "Epoch [180/300], Step [68/172], Loss: 5.0340\n",
      "Epoch [180/300], Step [69/172], Loss: 35.6393\n",
      "Epoch [180/300], Step [70/172], Loss: 39.4955\n",
      "Epoch [180/300], Step [71/172], Loss: 40.5581\n",
      "Epoch [180/300], Step [72/172], Loss: 41.2976\n",
      "Epoch [180/300], Step [73/172], Loss: 49.7882\n",
      "Epoch [180/300], Step [74/172], Loss: 25.9425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/300], Step [75/172], Loss: 27.2936\n",
      "Epoch [180/300], Step [76/172], Loss: 29.0157\n",
      "Epoch [180/300], Step [77/172], Loss: 49.6283\n",
      "Epoch [180/300], Step [78/172], Loss: 38.3244\n",
      "Epoch [180/300], Step [79/172], Loss: 37.3884\n",
      "Epoch [180/300], Step [80/172], Loss: 50.5801\n",
      "Epoch [180/300], Step [81/172], Loss: 34.1797\n",
      "Epoch [180/300], Step [82/172], Loss: 35.6556\n",
      "Epoch [180/300], Step [83/172], Loss: 43.3841\n",
      "Epoch [180/300], Step [84/172], Loss: 32.8119\n",
      "Epoch [180/300], Step [85/172], Loss: 38.1180\n",
      "Epoch [180/300], Step [86/172], Loss: 32.5435\n",
      "Epoch [180/300], Step [87/172], Loss: 26.0473\n",
      "Epoch [180/300], Step [88/172], Loss: 24.9603\n",
      "Epoch [180/300], Step [89/172], Loss: 26.4200\n",
      "Epoch [180/300], Step [90/172], Loss: 21.0066\n",
      "Epoch [180/300], Step [91/172], Loss: 26.3946\n",
      "Epoch [180/300], Step [92/172], Loss: 19.8276\n",
      "Epoch [180/300], Step [93/172], Loss: 20.5659\n",
      "Epoch [180/300], Step [94/172], Loss: 29.1243\n",
      "Epoch [180/300], Step [95/172], Loss: 20.6338\n",
      "Epoch [180/300], Step [96/172], Loss: 19.8263\n",
      "Epoch [180/300], Step [97/172], Loss: 27.8399\n",
      "Epoch [180/300], Step [98/172], Loss: 19.2530\n",
      "Epoch [180/300], Step [99/172], Loss: 18.8634\n",
      "Epoch [180/300], Step [100/172], Loss: 15.2362\n",
      "Epoch [180/300], Step [101/172], Loss: 18.4288\n",
      "Epoch [180/300], Step [102/172], Loss: 16.7945\n",
      "Epoch [180/300], Step [103/172], Loss: 12.3666\n",
      "Epoch [180/300], Step [104/172], Loss: 18.1604\n",
      "Epoch [180/300], Step [105/172], Loss: 18.8495\n",
      "Epoch [180/300], Step [106/172], Loss: 15.4115\n",
      "Epoch [180/300], Step [107/172], Loss: 15.3007\n",
      "Epoch [180/300], Step [108/172], Loss: 14.4347\n",
      "Epoch [180/300], Step [109/172], Loss: 14.3996\n",
      "Epoch [180/300], Step [110/172], Loss: 15.5697\n",
      "Epoch [180/300], Step [111/172], Loss: 15.1295\n",
      "Epoch [180/300], Step [112/172], Loss: 17.3732\n",
      "Epoch [180/300], Step [113/172], Loss: 11.5540\n",
      "Epoch [180/300], Step [114/172], Loss: 13.5858\n",
      "Epoch [180/300], Step [115/172], Loss: 18.9853\n",
      "Epoch [180/300], Step [116/172], Loss: 14.3155\n",
      "Epoch [180/300], Step [117/172], Loss: 11.1808\n",
      "Epoch [180/300], Step [118/172], Loss: 13.9349\n",
      "Epoch [180/300], Step [119/172], Loss: 15.5921\n",
      "Epoch [180/300], Step [120/172], Loss: 9.8317\n",
      "Epoch [180/300], Step [121/172], Loss: 9.3405\n",
      "Epoch [180/300], Step [122/172], Loss: 10.4414\n",
      "Epoch [180/300], Step [123/172], Loss: 10.0036\n",
      "Epoch [180/300], Step [124/172], Loss: 7.3425\n",
      "Epoch [180/300], Step [125/172], Loss: 11.4984\n",
      "Epoch [180/300], Step [126/172], Loss: 10.7298\n",
      "Epoch [180/300], Step [127/172], Loss: 10.6980\n",
      "Epoch [180/300], Step [128/172], Loss: 10.1326\n",
      "Epoch [180/300], Step [129/172], Loss: 7.8226\n",
      "Epoch [180/300], Step [130/172], Loss: 12.0514\n",
      "Epoch [180/300], Step [131/172], Loss: 7.1307\n",
      "Epoch [180/300], Step [132/172], Loss: 8.1288\n",
      "Epoch [180/300], Step [133/172], Loss: 8.8505\n",
      "Epoch [180/300], Step [134/172], Loss: 11.1695\n",
      "Epoch [180/300], Step [135/172], Loss: 8.2555\n",
      "Epoch [180/300], Step [136/172], Loss: 7.9154\n",
      "Epoch [180/300], Step [137/172], Loss: 9.0630\n",
      "Epoch [180/300], Step [138/172], Loss: 6.6833\n",
      "Epoch [180/300], Step [139/172], Loss: 9.2274\n",
      "Epoch [180/300], Step [140/172], Loss: 9.3360\n",
      "Epoch [180/300], Step [141/172], Loss: 9.1857\n",
      "Epoch [180/300], Step [142/172], Loss: 13.7499\n",
      "Epoch [180/300], Step [143/172], Loss: 10.3817\n",
      "Epoch [180/300], Step [144/172], Loss: 8.7215\n",
      "Epoch [180/300], Step [145/172], Loss: 9.5443\n",
      "Epoch [180/300], Step [146/172], Loss: 9.0829\n",
      "Epoch [180/300], Step [147/172], Loss: 4.9634\n",
      "Epoch [180/300], Step [148/172], Loss: 5.7912\n",
      "Epoch [180/300], Step [149/172], Loss: 6.3613\n",
      "Epoch [180/300], Step [150/172], Loss: 5.8175\n",
      "Epoch [180/300], Step [151/172], Loss: 5.2555\n",
      "Epoch [180/300], Step [152/172], Loss: 7.1384\n",
      "Epoch [180/300], Step [153/172], Loss: 6.1348\n",
      "Epoch [180/300], Step [154/172], Loss: 7.1974\n",
      "Epoch [180/300], Step [155/172], Loss: 5.8561\n",
      "Epoch [180/300], Step [156/172], Loss: 12.9576\n",
      "Epoch [180/300], Step [157/172], Loss: 9.2499\n",
      "Epoch [180/300], Step [158/172], Loss: 6.9145\n",
      "Epoch [180/300], Step [159/172], Loss: 8.9162\n",
      "Epoch [180/300], Step [160/172], Loss: 9.7947\n",
      "Epoch [180/300], Step [161/172], Loss: 6.8082\n",
      "Epoch [180/300], Step [162/172], Loss: 5.1027\n",
      "Epoch [180/300], Step [163/172], Loss: 6.2169\n",
      "Epoch [180/300], Step [164/172], Loss: 8.3357\n",
      "Epoch [180/300], Step [165/172], Loss: 5.8518\n",
      "Epoch [180/300], Step [166/172], Loss: 5.3506\n",
      "Epoch [180/300], Step [167/172], Loss: 10.1154\n",
      "Epoch [180/300], Step [168/172], Loss: 6.2422\n",
      "Epoch [180/300], Step [169/172], Loss: 6.4494\n",
      "Epoch [180/300], Step [170/172], Loss: 4.5531\n",
      "Epoch [180/300], Step [171/172], Loss: 7.5387\n",
      "Epoch [180/300], Step [172/172], Loss: 4.9254\n",
      "Epoch [181/300], Step [1/172], Loss: 53.8537\n",
      "Epoch [181/300], Step [2/172], Loss: 54.9775\n",
      "Epoch [181/300], Step [3/172], Loss: 50.1954\n",
      "Epoch [181/300], Step [4/172], Loss: 28.3067\n",
      "Epoch [181/300], Step [5/172], Loss: 48.6580\n",
      "Epoch [181/300], Step [6/172], Loss: 19.0428\n",
      "Epoch [181/300], Step [7/172], Loss: 29.4075\n",
      "Epoch [181/300], Step [8/172], Loss: 4.5774\n",
      "Epoch [181/300], Step [9/172], Loss: 32.8807\n",
      "Epoch [181/300], Step [10/172], Loss: 42.6564\n",
      "Epoch [181/300], Step [11/172], Loss: 60.2678\n",
      "Epoch [181/300], Step [12/172], Loss: 67.3239\n",
      "Epoch [181/300], Step [13/172], Loss: 34.5948\n",
      "Epoch [181/300], Step [14/172], Loss: 61.4481\n",
      "Epoch [181/300], Step [15/172], Loss: 56.9749\n",
      "Epoch [181/300], Step [16/172], Loss: 10.5592\n",
      "Epoch [181/300], Step [17/172], Loss: 45.5108\n",
      "Epoch [181/300], Step [18/172], Loss: 59.5276\n",
      "Epoch [181/300], Step [19/172], Loss: 82.3538\n",
      "Epoch [181/300], Step [20/172], Loss: 37.3650\n",
      "Epoch [181/300], Step [21/172], Loss: 85.0825\n",
      "Epoch [181/300], Step [22/172], Loss: 61.0457\n",
      "Epoch [181/300], Step [23/172], Loss: 1.8362\n",
      "Epoch [181/300], Step [24/172], Loss: 56.6267\n",
      "Epoch [181/300], Step [25/172], Loss: 40.8079\n",
      "Epoch [181/300], Step [26/172], Loss: 47.6795\n",
      "Epoch [181/300], Step [27/172], Loss: 62.0579\n",
      "Epoch [181/300], Step [28/172], Loss: 24.7188\n",
      "Epoch [181/300], Step [29/172], Loss: 16.8444\n",
      "Epoch [181/300], Step [30/172], Loss: 64.1853\n",
      "Epoch [181/300], Step [31/172], Loss: 37.2039\n",
      "Epoch [181/300], Step [32/172], Loss: 42.3338\n",
      "Epoch [181/300], Step [33/172], Loss: 70.2051\n",
      "Epoch [181/300], Step [34/172], Loss: 2.8537\n",
      "Epoch [181/300], Step [35/172], Loss: 13.3496\n",
      "Epoch [181/300], Step [36/172], Loss: 17.9511\n",
      "Epoch [181/300], Step [37/172], Loss: 16.6020\n",
      "Epoch [181/300], Step [38/172], Loss: 29.1071\n",
      "Epoch [181/300], Step [39/172], Loss: 35.6072\n",
      "Epoch [181/300], Step [40/172], Loss: 20.1004\n",
      "Epoch [181/300], Step [41/172], Loss: 33.3987\n",
      "Epoch [181/300], Step [42/172], Loss: 37.7265\n",
      "Epoch [181/300], Step [43/172], Loss: 26.8378\n",
      "Epoch [181/300], Step [44/172], Loss: 20.5213\n",
      "Epoch [181/300], Step [45/172], Loss: 26.0652\n",
      "Epoch [181/300], Step [46/172], Loss: 17.5423\n",
      "Epoch [181/300], Step [47/172], Loss: 46.9534\n",
      "Epoch [181/300], Step [48/172], Loss: 60.6274\n",
      "Epoch [181/300], Step [49/172], Loss: 20.7857\n",
      "Epoch [181/300], Step [50/172], Loss: 46.8514\n",
      "Epoch [181/300], Step [51/172], Loss: 8.3832\n",
      "Epoch [181/300], Step [52/172], Loss: 19.0359\n",
      "Epoch [181/300], Step [53/172], Loss: 22.3204\n",
      "Epoch [181/300], Step [54/172], Loss: 14.4073\n",
      "Epoch [181/300], Step [55/172], Loss: 14.2094\n",
      "Epoch [181/300], Step [56/172], Loss: 17.4054\n",
      "Epoch [181/300], Step [57/172], Loss: 16.9255\n",
      "Epoch [181/300], Step [58/172], Loss: 13.3586\n",
      "Epoch [181/300], Step [59/172], Loss: 27.5734\n",
      "Epoch [181/300], Step [60/172], Loss: 26.3993\n",
      "Epoch [181/300], Step [61/172], Loss: 6.2466\n",
      "Epoch [181/300], Step [62/172], Loss: 19.9944\n",
      "Epoch [181/300], Step [63/172], Loss: 10.1424\n",
      "Epoch [181/300], Step [64/172], Loss: 10.5143\n",
      "Epoch [181/300], Step [65/172], Loss: 19.9127\n",
      "Epoch [181/300], Step [66/172], Loss: 6.4285\n",
      "Epoch [181/300], Step [67/172], Loss: 23.2458\n",
      "Epoch [181/300], Step [68/172], Loss: 5.1326\n",
      "Epoch [181/300], Step [69/172], Loss: 35.6279\n",
      "Epoch [181/300], Step [70/172], Loss: 39.3932\n",
      "Epoch [181/300], Step [71/172], Loss: 40.5183\n",
      "Epoch [181/300], Step [72/172], Loss: 41.1427\n",
      "Epoch [181/300], Step [73/172], Loss: 49.8348\n",
      "Epoch [181/300], Step [74/172], Loss: 25.8573\n",
      "Epoch [181/300], Step [75/172], Loss: 27.4167\n",
      "Epoch [181/300], Step [76/172], Loss: 29.0518\n",
      "Epoch [181/300], Step [77/172], Loss: 49.4903\n",
      "Epoch [181/300], Step [78/172], Loss: 38.2127\n",
      "Epoch [181/300], Step [79/172], Loss: 37.1801\n",
      "Epoch [181/300], Step [80/172], Loss: 50.4661\n",
      "Epoch [181/300], Step [81/172], Loss: 34.0798\n",
      "Epoch [181/300], Step [82/172], Loss: 35.6110\n",
      "Epoch [181/300], Step [83/172], Loss: 43.4001\n",
      "Epoch [181/300], Step [84/172], Loss: 32.7190\n",
      "Epoch [181/300], Step [85/172], Loss: 38.0955\n",
      "Epoch [181/300], Step [86/172], Loss: 32.4530\n",
      "Epoch [181/300], Step [87/172], Loss: 26.0015\n",
      "Epoch [181/300], Step [88/172], Loss: 24.7549\n",
      "Epoch [181/300], Step [89/172], Loss: 26.3945\n",
      "Epoch [181/300], Step [90/172], Loss: 20.9570\n",
      "Epoch [181/300], Step [91/172], Loss: 26.3662\n",
      "Epoch [181/300], Step [92/172], Loss: 19.7627\n",
      "Epoch [181/300], Step [93/172], Loss: 20.5384\n",
      "Epoch [181/300], Step [94/172], Loss: 28.9801\n",
      "Epoch [181/300], Step [95/172], Loss: 20.5416\n",
      "Epoch [181/300], Step [96/172], Loss: 19.7738\n",
      "Epoch [181/300], Step [97/172], Loss: 27.8189\n",
      "Epoch [181/300], Step [98/172], Loss: 19.2115\n",
      "Epoch [181/300], Step [99/172], Loss: 18.8377\n",
      "Epoch [181/300], Step [100/172], Loss: 15.2348\n",
      "Epoch [181/300], Step [101/172], Loss: 18.4352\n",
      "Epoch [181/300], Step [102/172], Loss: 16.9010\n",
      "Epoch [181/300], Step [103/172], Loss: 12.3333\n",
      "Epoch [181/300], Step [104/172], Loss: 18.1712\n",
      "Epoch [181/300], Step [105/172], Loss: 18.9073\n",
      "Epoch [181/300], Step [106/172], Loss: 15.3996\n",
      "Epoch [181/300], Step [107/172], Loss: 15.3162\n",
      "Epoch [181/300], Step [108/172], Loss: 14.4153\n",
      "Epoch [181/300], Step [109/172], Loss: 14.4337\n",
      "Epoch [181/300], Step [110/172], Loss: 15.5229\n",
      "Epoch [181/300], Step [111/172], Loss: 15.1490\n",
      "Epoch [181/300], Step [112/172], Loss: 17.3355\n",
      "Epoch [181/300], Step [113/172], Loss: 11.5823\n",
      "Epoch [181/300], Step [114/172], Loss: 13.5720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [181/300], Step [115/172], Loss: 19.0029\n",
      "Epoch [181/300], Step [116/172], Loss: 14.2703\n",
      "Epoch [181/300], Step [117/172], Loss: 11.2050\n",
      "Epoch [181/300], Step [118/172], Loss: 13.9028\n",
      "Epoch [181/300], Step [119/172], Loss: 15.5916\n",
      "Epoch [181/300], Step [120/172], Loss: 9.8435\n",
      "Epoch [181/300], Step [121/172], Loss: 9.3055\n",
      "Epoch [181/300], Step [122/172], Loss: 10.4465\n",
      "Epoch [181/300], Step [123/172], Loss: 9.9949\n",
      "Epoch [181/300], Step [124/172], Loss: 7.3545\n",
      "Epoch [181/300], Step [125/172], Loss: 11.5120\n",
      "Epoch [181/300], Step [126/172], Loss: 10.7419\n",
      "Epoch [181/300], Step [127/172], Loss: 10.6288\n",
      "Epoch [181/300], Step [128/172], Loss: 10.0831\n",
      "Epoch [181/300], Step [129/172], Loss: 7.8002\n",
      "Epoch [181/300], Step [130/172], Loss: 12.0609\n",
      "Epoch [181/300], Step [131/172], Loss: 7.1210\n",
      "Epoch [181/300], Step [132/172], Loss: 8.0808\n",
      "Epoch [181/300], Step [133/172], Loss: 8.8424\n",
      "Epoch [181/300], Step [134/172], Loss: 11.2057\n",
      "Epoch [181/300], Step [135/172], Loss: 8.2864\n",
      "Epoch [181/300], Step [136/172], Loss: 8.0008\n",
      "Epoch [181/300], Step [137/172], Loss: 9.0772\n",
      "Epoch [181/300], Step [138/172], Loss: 6.6772\n",
      "Epoch [181/300], Step [139/172], Loss: 9.2903\n",
      "Epoch [181/300], Step [140/172], Loss: 9.3355\n",
      "Epoch [181/300], Step [141/172], Loss: 9.2201\n",
      "Epoch [181/300], Step [142/172], Loss: 13.7374\n",
      "Epoch [181/300], Step [143/172], Loss: 10.3920\n",
      "Epoch [181/300], Step [144/172], Loss: 8.7142\n",
      "Epoch [181/300], Step [145/172], Loss: 9.5701\n",
      "Epoch [181/300], Step [146/172], Loss: 9.0678\n",
      "Epoch [181/300], Step [147/172], Loss: 5.0126\n",
      "Epoch [181/300], Step [148/172], Loss: 5.8049\n",
      "Epoch [181/300], Step [149/172], Loss: 6.4061\n",
      "Epoch [181/300], Step [150/172], Loss: 5.8361\n",
      "Epoch [181/300], Step [151/172], Loss: 5.2827\n",
      "Epoch [181/300], Step [152/172], Loss: 7.1574\n",
      "Epoch [181/300], Step [153/172], Loss: 6.1696\n",
      "Epoch [181/300], Step [154/172], Loss: 7.2585\n",
      "Epoch [181/300], Step [155/172], Loss: 5.9111\n",
      "Epoch [181/300], Step [156/172], Loss: 12.9892\n",
      "Epoch [181/300], Step [157/172], Loss: 9.2570\n",
      "Epoch [181/300], Step [158/172], Loss: 6.9192\n",
      "Epoch [181/300], Step [159/172], Loss: 8.9324\n",
      "Epoch [181/300], Step [160/172], Loss: 9.7770\n",
      "Epoch [181/300], Step [161/172], Loss: 6.8757\n",
      "Epoch [181/300], Step [162/172], Loss: 5.0931\n",
      "Epoch [181/300], Step [163/172], Loss: 6.2382\n",
      "Epoch [181/300], Step [164/172], Loss: 8.4118\n",
      "Epoch [181/300], Step [165/172], Loss: 5.8850\n",
      "Epoch [181/300], Step [166/172], Loss: 5.3563\n",
      "Epoch [181/300], Step [167/172], Loss: 10.2238\n",
      "Epoch [181/300], Step [168/172], Loss: 6.2949\n",
      "Epoch [181/300], Step [169/172], Loss: 6.5119\n",
      "Epoch [181/300], Step [170/172], Loss: 4.5663\n",
      "Epoch [181/300], Step [171/172], Loss: 7.6207\n",
      "Epoch [181/300], Step [172/172], Loss: 5.0157\n",
      "Epoch [182/300], Step [1/172], Loss: 53.6345\n",
      "Epoch [182/300], Step [2/172], Loss: 54.9195\n",
      "Epoch [182/300], Step [3/172], Loss: 49.9641\n",
      "Epoch [182/300], Step [4/172], Loss: 28.1685\n",
      "Epoch [182/300], Step [5/172], Loss: 48.4721\n",
      "Epoch [182/300], Step [6/172], Loss: 18.9936\n",
      "Epoch [182/300], Step [7/172], Loss: 29.4999\n",
      "Epoch [182/300], Step [8/172], Loss: 4.4424\n",
      "Epoch [182/300], Step [9/172], Loss: 32.6539\n",
      "Epoch [182/300], Step [10/172], Loss: 42.6404\n",
      "Epoch [182/300], Step [11/172], Loss: 59.9779\n",
      "Epoch [182/300], Step [12/172], Loss: 66.8731\n",
      "Epoch [182/300], Step [13/172], Loss: 34.5144\n",
      "Epoch [182/300], Step [14/172], Loss: 61.8289\n",
      "Epoch [182/300], Step [15/172], Loss: 56.8881\n",
      "Epoch [182/300], Step [16/172], Loss: 10.7320\n",
      "Epoch [182/300], Step [17/172], Loss: 45.4188\n",
      "Epoch [182/300], Step [18/172], Loss: 59.3447\n",
      "Epoch [182/300], Step [19/172], Loss: 82.2915\n",
      "Epoch [182/300], Step [20/172], Loss: 37.3415\n",
      "Epoch [182/300], Step [21/172], Loss: 85.2650\n",
      "Epoch [182/300], Step [22/172], Loss: 61.0401\n",
      "Epoch [182/300], Step [23/172], Loss: 1.9927\n",
      "Epoch [182/300], Step [24/172], Loss: 56.5558\n",
      "Epoch [182/300], Step [25/172], Loss: 40.7677\n",
      "Epoch [182/300], Step [26/172], Loss: 47.5387\n",
      "Epoch [182/300], Step [27/172], Loss: 62.1627\n",
      "Epoch [182/300], Step [28/172], Loss: 24.7626\n",
      "Epoch [182/300], Step [29/172], Loss: 16.9801\n",
      "Epoch [182/300], Step [30/172], Loss: 64.2424\n",
      "Epoch [182/300], Step [31/172], Loss: 37.2134\n",
      "Epoch [182/300], Step [32/172], Loss: 42.2948\n",
      "Epoch [182/300], Step [33/172], Loss: 70.0662\n",
      "Epoch [182/300], Step [34/172], Loss: 2.8314\n",
      "Epoch [182/300], Step [35/172], Loss: 13.4170\n",
      "Epoch [182/300], Step [36/172], Loss: 17.9232\n",
      "Epoch [182/300], Step [37/172], Loss: 16.6372\n",
      "Epoch [182/300], Step [38/172], Loss: 29.2726\n",
      "Epoch [182/300], Step [39/172], Loss: 35.5201\n",
      "Epoch [182/300], Step [40/172], Loss: 20.1236\n",
      "Epoch [182/300], Step [41/172], Loss: 33.3287\n",
      "Epoch [182/300], Step [42/172], Loss: 37.7828\n",
      "Epoch [182/300], Step [43/172], Loss: 26.7475\n",
      "Epoch [182/300], Step [44/172], Loss: 20.5173\n",
      "Epoch [182/300], Step [45/172], Loss: 25.9501\n",
      "Epoch [182/300], Step [46/172], Loss: 17.4916\n",
      "Epoch [182/300], Step [47/172], Loss: 47.0808\n",
      "Epoch [182/300], Step [48/172], Loss: 60.6739\n",
      "Epoch [182/300], Step [49/172], Loss: 20.6947\n",
      "Epoch [182/300], Step [50/172], Loss: 46.9162\n",
      "Epoch [182/300], Step [51/172], Loss: 8.3447\n",
      "Epoch [182/300], Step [52/172], Loss: 19.0620\n",
      "Epoch [182/300], Step [53/172], Loss: 22.2853\n",
      "Epoch [182/300], Step [54/172], Loss: 14.5099\n",
      "Epoch [182/300], Step [55/172], Loss: 14.2935\n",
      "Epoch [182/300], Step [56/172], Loss: 17.5323\n",
      "Epoch [182/300], Step [57/172], Loss: 16.8789\n",
      "Epoch [182/300], Step [58/172], Loss: 13.4227\n",
      "Epoch [182/300], Step [59/172], Loss: 27.6177\n",
      "Epoch [182/300], Step [60/172], Loss: 26.2218\n",
      "Epoch [182/300], Step [61/172], Loss: 6.2990\n",
      "Epoch [182/300], Step [62/172], Loss: 19.9401\n",
      "Epoch [182/300], Step [63/172], Loss: 10.1677\n",
      "Epoch [182/300], Step [64/172], Loss: 10.5069\n",
      "Epoch [182/300], Step [65/172], Loss: 19.9109\n",
      "Epoch [182/300], Step [66/172], Loss: 6.4089\n",
      "Epoch [182/300], Step [67/172], Loss: 23.1763\n",
      "Epoch [182/300], Step [68/172], Loss: 5.2600\n",
      "Epoch [182/300], Step [69/172], Loss: 35.3603\n",
      "Epoch [182/300], Step [70/172], Loss: 39.1371\n",
      "Epoch [182/300], Step [71/172], Loss: 40.4157\n",
      "Epoch [182/300], Step [72/172], Loss: 40.9172\n",
      "Epoch [182/300], Step [73/172], Loss: 49.6650\n",
      "Epoch [182/300], Step [74/172], Loss: 25.8134\n",
      "Epoch [182/300], Step [75/172], Loss: 27.3485\n",
      "Epoch [182/300], Step [76/172], Loss: 29.0182\n",
      "Epoch [182/300], Step [77/172], Loss: 49.3010\n",
      "Epoch [182/300], Step [78/172], Loss: 38.0782\n",
      "Epoch [182/300], Step [79/172], Loss: 37.1968\n",
      "Epoch [182/300], Step [80/172], Loss: 50.4724\n",
      "Epoch [182/300], Step [81/172], Loss: 33.9941\n",
      "Epoch [182/300], Step [82/172], Loss: 35.5789\n",
      "Epoch [182/300], Step [83/172], Loss: 43.3999\n",
      "Epoch [182/300], Step [84/172], Loss: 32.7741\n",
      "Epoch [182/300], Step [85/172], Loss: 38.1770\n",
      "Epoch [182/300], Step [86/172], Loss: 32.5247\n",
      "Epoch [182/300], Step [87/172], Loss: 26.0054\n",
      "Epoch [182/300], Step [88/172], Loss: 24.7968\n",
      "Epoch [182/300], Step [89/172], Loss: 26.5402\n",
      "Epoch [182/300], Step [90/172], Loss: 21.0306\n",
      "Epoch [182/300], Step [91/172], Loss: 26.4074\n",
      "Epoch [182/300], Step [92/172], Loss: 19.8843\n",
      "Epoch [182/300], Step [93/172], Loss: 20.5687\n",
      "Epoch [182/300], Step [94/172], Loss: 28.9399\n",
      "Epoch [182/300], Step [95/172], Loss: 20.5801\n",
      "Epoch [182/300], Step [96/172], Loss: 19.8190\n",
      "Epoch [182/300], Step [97/172], Loss: 27.8696\n",
      "Epoch [182/300], Step [98/172], Loss: 19.2590\n",
      "Epoch [182/300], Step [99/172], Loss: 18.9061\n",
      "Epoch [182/300], Step [100/172], Loss: 15.3115\n",
      "Epoch [182/300], Step [101/172], Loss: 18.5487\n",
      "Epoch [182/300], Step [102/172], Loss: 16.9217\n",
      "Epoch [182/300], Step [103/172], Loss: 12.3747\n",
      "Epoch [182/300], Step [104/172], Loss: 18.2570\n",
      "Epoch [182/300], Step [105/172], Loss: 18.9173\n",
      "Epoch [182/300], Step [106/172], Loss: 15.4210\n",
      "Epoch [182/300], Step [107/172], Loss: 15.3895\n",
      "Epoch [182/300], Step [108/172], Loss: 14.4484\n",
      "Epoch [182/300], Step [109/172], Loss: 14.4634\n",
      "Epoch [182/300], Step [110/172], Loss: 15.6129\n",
      "Epoch [182/300], Step [111/172], Loss: 15.1858\n",
      "Epoch [182/300], Step [112/172], Loss: 17.2969\n",
      "Epoch [182/300], Step [113/172], Loss: 11.5820\n",
      "Epoch [182/300], Step [114/172], Loss: 13.5815\n",
      "Epoch [182/300], Step [115/172], Loss: 19.0720\n",
      "Epoch [182/300], Step [116/172], Loss: 14.3194\n",
      "Epoch [182/300], Step [117/172], Loss: 11.2826\n",
      "Epoch [182/300], Step [118/172], Loss: 13.9133\n",
      "Epoch [182/300], Step [119/172], Loss: 15.6674\n",
      "Epoch [182/300], Step [120/172], Loss: 9.8539\n",
      "Epoch [182/300], Step [121/172], Loss: 9.3371\n",
      "Epoch [182/300], Step [122/172], Loss: 10.4616\n",
      "Epoch [182/300], Step [123/172], Loss: 10.1015\n",
      "Epoch [182/300], Step [124/172], Loss: 7.4014\n",
      "Epoch [182/300], Step [125/172], Loss: 11.5507\n",
      "Epoch [182/300], Step [126/172], Loss: 10.8451\n",
      "Epoch [182/300], Step [127/172], Loss: 10.6168\n",
      "Epoch [182/300], Step [128/172], Loss: 10.1259\n",
      "Epoch [182/300], Step [129/172], Loss: 7.8168\n",
      "Epoch [182/300], Step [130/172], Loss: 12.0991\n",
      "Epoch [182/300], Step [131/172], Loss: 7.1231\n",
      "Epoch [182/300], Step [132/172], Loss: 8.1142\n",
      "Epoch [182/300], Step [133/172], Loss: 8.8568\n",
      "Epoch [182/300], Step [134/172], Loss: 11.2381\n",
      "Epoch [182/300], Step [135/172], Loss: 8.3381\n",
      "Epoch [182/300], Step [136/172], Loss: 7.9812\n",
      "Epoch [182/300], Step [137/172], Loss: 9.0723\n",
      "Epoch [182/300], Step [138/172], Loss: 6.7469\n",
      "Epoch [182/300], Step [139/172], Loss: 9.3670\n",
      "Epoch [182/300], Step [140/172], Loss: 9.3539\n",
      "Epoch [182/300], Step [141/172], Loss: 9.2473\n",
      "Epoch [182/300], Step [142/172], Loss: 13.8572\n",
      "Epoch [182/300], Step [143/172], Loss: 10.3868\n",
      "Epoch [182/300], Step [144/172], Loss: 8.7902\n",
      "Epoch [182/300], Step [145/172], Loss: 9.6660\n",
      "Epoch [182/300], Step [146/172], Loss: 9.1225\n",
      "Epoch [182/300], Step [147/172], Loss: 5.0372\n",
      "Epoch [182/300], Step [148/172], Loss: 5.8200\n",
      "Epoch [182/300], Step [149/172], Loss: 6.4348\n",
      "Epoch [182/300], Step [150/172], Loss: 5.8478\n",
      "Epoch [182/300], Step [151/172], Loss: 5.2686\n",
      "Epoch [182/300], Step [152/172], Loss: 7.1850\n",
      "Epoch [182/300], Step [153/172], Loss: 6.2029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [182/300], Step [154/172], Loss: 7.2262\n",
      "Epoch [182/300], Step [155/172], Loss: 5.9426\n",
      "Epoch [182/300], Step [156/172], Loss: 12.9835\n",
      "Epoch [182/300], Step [157/172], Loss: 9.2197\n",
      "Epoch [182/300], Step [158/172], Loss: 6.8984\n",
      "Epoch [182/300], Step [159/172], Loss: 9.0242\n",
      "Epoch [182/300], Step [160/172], Loss: 9.8131\n",
      "Epoch [182/300], Step [161/172], Loss: 6.8812\n",
      "Epoch [182/300], Step [162/172], Loss: 5.0886\n",
      "Epoch [182/300], Step [163/172], Loss: 6.2925\n",
      "Epoch [182/300], Step [164/172], Loss: 8.3749\n",
      "Epoch [182/300], Step [165/172], Loss: 5.9291\n",
      "Epoch [182/300], Step [166/172], Loss: 5.3687\n",
      "Epoch [182/300], Step [167/172], Loss: 10.2485\n",
      "Epoch [182/300], Step [168/172], Loss: 6.2649\n",
      "Epoch [182/300], Step [169/172], Loss: 6.5083\n",
      "Epoch [182/300], Step [170/172], Loss: 4.5596\n",
      "Epoch [182/300], Step [171/172], Loss: 7.7125\n",
      "Epoch [182/300], Step [172/172], Loss: 5.0421\n",
      "Epoch [183/300], Step [1/172], Loss: 53.3789\n",
      "Epoch [183/300], Step [2/172], Loss: 54.7112\n",
      "Epoch [183/300], Step [3/172], Loss: 48.7674\n",
      "Epoch [183/300], Step [4/172], Loss: 27.9764\n",
      "Epoch [183/300], Step [5/172], Loss: 47.9762\n",
      "Epoch [183/300], Step [6/172], Loss: 19.0003\n",
      "Epoch [183/300], Step [7/172], Loss: 29.5511\n",
      "Epoch [183/300], Step [8/172], Loss: 4.5323\n",
      "Epoch [183/300], Step [9/172], Loss: 32.7158\n",
      "Epoch [183/300], Step [10/172], Loss: 42.7526\n",
      "Epoch [183/300], Step [11/172], Loss: 59.8786\n",
      "Epoch [183/300], Step [12/172], Loss: 66.6555\n",
      "Epoch [183/300], Step [13/172], Loss: 34.6137\n",
      "Epoch [183/300], Step [14/172], Loss: 61.4350\n",
      "Epoch [183/300], Step [15/172], Loss: 56.6595\n",
      "Epoch [183/300], Step [16/172], Loss: 10.3803\n",
      "Epoch [183/300], Step [17/172], Loss: 45.2349\n",
      "Epoch [183/300], Step [18/172], Loss: 59.3675\n",
      "Epoch [183/300], Step [19/172], Loss: 82.1464\n",
      "Epoch [183/300], Step [20/172], Loss: 37.0802\n",
      "Epoch [183/300], Step [21/172], Loss: 84.5741\n",
      "Epoch [183/300], Step [22/172], Loss: 60.8195\n",
      "Epoch [183/300], Step [23/172], Loss: 1.8551\n",
      "Epoch [183/300], Step [24/172], Loss: 56.4322\n",
      "Epoch [183/300], Step [25/172], Loss: 40.8042\n",
      "Epoch [183/300], Step [26/172], Loss: 47.6116\n",
      "Epoch [183/300], Step [27/172], Loss: 62.2369\n",
      "Epoch [183/300], Step [28/172], Loss: 24.5651\n",
      "Epoch [183/300], Step [29/172], Loss: 16.9571\n",
      "Epoch [183/300], Step [30/172], Loss: 64.0722\n",
      "Epoch [183/300], Step [31/172], Loss: 37.1797\n",
      "Epoch [183/300], Step [32/172], Loss: 42.3346\n",
      "Epoch [183/300], Step [33/172], Loss: 70.1047\n",
      "Epoch [183/300], Step [34/172], Loss: 2.7770\n",
      "Epoch [183/300], Step [35/172], Loss: 13.5574\n",
      "Epoch [183/300], Step [36/172], Loss: 17.8421\n",
      "Epoch [183/300], Step [37/172], Loss: 16.7112\n",
      "Epoch [183/300], Step [38/172], Loss: 29.4189\n",
      "Epoch [183/300], Step [39/172], Loss: 35.6961\n",
      "Epoch [183/300], Step [40/172], Loss: 20.2077\n",
      "Epoch [183/300], Step [41/172], Loss: 33.3860\n",
      "Epoch [183/300], Step [42/172], Loss: 37.9243\n",
      "Epoch [183/300], Step [43/172], Loss: 26.8162\n",
      "Epoch [183/300], Step [44/172], Loss: 20.6462\n",
      "Epoch [183/300], Step [45/172], Loss: 26.1331\n",
      "Epoch [183/300], Step [46/172], Loss: 17.7400\n",
      "Epoch [183/300], Step [47/172], Loss: 47.2986\n",
      "Epoch [183/300], Step [48/172], Loss: 61.2477\n",
      "Epoch [183/300], Step [49/172], Loss: 20.7357\n",
      "Epoch [183/300], Step [50/172], Loss: 47.1881\n",
      "Epoch [183/300], Step [51/172], Loss: 8.3790\n",
      "Epoch [183/300], Step [52/172], Loss: 19.1537\n",
      "Epoch [183/300], Step [53/172], Loss: 22.3418\n",
      "Epoch [183/300], Step [54/172], Loss: 14.4941\n",
      "Epoch [183/300], Step [55/172], Loss: 14.2864\n",
      "Epoch [183/300], Step [56/172], Loss: 17.7112\n",
      "Epoch [183/300], Step [57/172], Loss: 16.6491\n",
      "Epoch [183/300], Step [58/172], Loss: 13.2107\n",
      "Epoch [183/300], Step [59/172], Loss: 27.3956\n",
      "Epoch [183/300], Step [60/172], Loss: 25.6123\n",
      "Epoch [183/300], Step [61/172], Loss: 6.2593\n",
      "Epoch [183/300], Step [62/172], Loss: 19.8732\n",
      "Epoch [183/300], Step [63/172], Loss: 10.1674\n",
      "Epoch [183/300], Step [64/172], Loss: 10.5034\n",
      "Epoch [183/300], Step [65/172], Loss: 19.8426\n",
      "Epoch [183/300], Step [66/172], Loss: 6.4517\n",
      "Epoch [183/300], Step [67/172], Loss: 23.0810\n",
      "Epoch [183/300], Step [68/172], Loss: 4.9768\n",
      "Epoch [183/300], Step [69/172], Loss: 35.2880\n",
      "Epoch [183/300], Step [70/172], Loss: 39.0121\n",
      "Epoch [183/300], Step [71/172], Loss: 40.2676\n",
      "Epoch [183/300], Step [72/172], Loss: 40.8367\n",
      "Epoch [183/300], Step [73/172], Loss: 49.4388\n",
      "Epoch [183/300], Step [74/172], Loss: 25.8902\n",
      "Epoch [183/300], Step [75/172], Loss: 27.1237\n",
      "Epoch [183/300], Step [76/172], Loss: 28.8198\n",
      "Epoch [183/300], Step [77/172], Loss: 49.2042\n",
      "Epoch [183/300], Step [78/172], Loss: 38.0343\n",
      "Epoch [183/300], Step [79/172], Loss: 37.0261\n",
      "Epoch [183/300], Step [80/172], Loss: 50.3403\n",
      "Epoch [183/300], Step [81/172], Loss: 33.9444\n",
      "Epoch [183/300], Step [82/172], Loss: 35.6454\n",
      "Epoch [183/300], Step [83/172], Loss: 43.4924\n",
      "Epoch [183/300], Step [84/172], Loss: 32.6896\n",
      "Epoch [183/300], Step [85/172], Loss: 38.2819\n",
      "Epoch [183/300], Step [86/172], Loss: 32.6077\n",
      "Epoch [183/300], Step [87/172], Loss: 25.9800\n",
      "Epoch [183/300], Step [88/172], Loss: 24.7528\n",
      "Epoch [183/300], Step [89/172], Loss: 26.7023\n",
      "Epoch [183/300], Step [90/172], Loss: 21.0561\n",
      "Epoch [183/300], Step [91/172], Loss: 26.3771\n",
      "Epoch [183/300], Step [92/172], Loss: 19.9387\n",
      "Epoch [183/300], Step [93/172], Loss: 20.7087\n",
      "Epoch [183/300], Step [94/172], Loss: 28.9998\n",
      "Epoch [183/300], Step [95/172], Loss: 20.6666\n",
      "Epoch [183/300], Step [96/172], Loss: 19.9081\n",
      "Epoch [183/300], Step [97/172], Loss: 27.9232\n",
      "Epoch [183/300], Step [98/172], Loss: 19.3564\n",
      "Epoch [183/300], Step [99/172], Loss: 19.0155\n",
      "Epoch [183/300], Step [100/172], Loss: 15.4061\n",
      "Epoch [183/300], Step [101/172], Loss: 18.6180\n",
      "Epoch [183/300], Step [102/172], Loss: 16.9086\n",
      "Epoch [183/300], Step [103/172], Loss: 12.4721\n",
      "Epoch [183/300], Step [104/172], Loss: 18.3313\n",
      "Epoch [183/300], Step [105/172], Loss: 19.0840\n",
      "Epoch [183/300], Step [106/172], Loss: 15.5362\n",
      "Epoch [183/300], Step [107/172], Loss: 15.4573\n",
      "Epoch [183/300], Step [108/172], Loss: 14.5187\n",
      "Epoch [183/300], Step [109/172], Loss: 14.4667\n",
      "Epoch [183/300], Step [110/172], Loss: 15.7863\n",
      "Epoch [183/300], Step [111/172], Loss: 15.3148\n",
      "Epoch [183/300], Step [112/172], Loss: 17.2852\n",
      "Epoch [183/300], Step [113/172], Loss: 11.6595\n",
      "Epoch [183/300], Step [114/172], Loss: 13.6526\n",
      "Epoch [183/300], Step [115/172], Loss: 19.0828\n",
      "Epoch [183/300], Step [116/172], Loss: 14.4676\n",
      "Epoch [183/300], Step [117/172], Loss: 11.3794\n",
      "Epoch [183/300], Step [118/172], Loss: 13.9597\n",
      "Epoch [183/300], Step [119/172], Loss: 15.7199\n",
      "Epoch [183/300], Step [120/172], Loss: 9.8905\n",
      "Epoch [183/300], Step [121/172], Loss: 9.3600\n",
      "Epoch [183/300], Step [122/172], Loss: 10.4645\n",
      "Epoch [183/300], Step [123/172], Loss: 10.1401\n",
      "Epoch [183/300], Step [124/172], Loss: 7.3739\n",
      "Epoch [183/300], Step [125/172], Loss: 11.4939\n",
      "Epoch [183/300], Step [126/172], Loss: 10.8696\n",
      "Epoch [183/300], Step [127/172], Loss: 10.6841\n",
      "Epoch [183/300], Step [128/172], Loss: 10.2487\n",
      "Epoch [183/300], Step [129/172], Loss: 7.8417\n",
      "Epoch [183/300], Step [130/172], Loss: 12.1531\n",
      "Epoch [183/300], Step [131/172], Loss: 7.1523\n",
      "Epoch [183/300], Step [132/172], Loss: 8.1887\n",
      "Epoch [183/300], Step [133/172], Loss: 8.8711\n",
      "Epoch [183/300], Step [134/172], Loss: 11.2130\n",
      "Epoch [183/300], Step [135/172], Loss: 8.3357\n",
      "Epoch [183/300], Step [136/172], Loss: 7.9251\n",
      "Epoch [183/300], Step [137/172], Loss: 9.0353\n",
      "Epoch [183/300], Step [138/172], Loss: 6.7580\n",
      "Epoch [183/300], Step [139/172], Loss: 9.2847\n",
      "Epoch [183/300], Step [140/172], Loss: 9.4471\n",
      "Epoch [183/300], Step [141/172], Loss: 9.1988\n",
      "Epoch [183/300], Step [142/172], Loss: 13.8913\n",
      "Epoch [183/300], Step [143/172], Loss: 10.4522\n",
      "Epoch [183/300], Step [144/172], Loss: 8.8019\n",
      "Epoch [183/300], Step [145/172], Loss: 9.6803\n",
      "Epoch [183/300], Step [146/172], Loss: 9.2315\n",
      "Epoch [183/300], Step [147/172], Loss: 5.0112\n",
      "Epoch [183/300], Step [148/172], Loss: 5.8385\n",
      "Epoch [183/300], Step [149/172], Loss: 6.4040\n",
      "Epoch [183/300], Step [150/172], Loss: 5.8193\n",
      "Epoch [183/300], Step [151/172], Loss: 5.2641\n",
      "Epoch [183/300], Step [152/172], Loss: 7.1969\n",
      "Epoch [183/300], Step [153/172], Loss: 6.1609\n",
      "Epoch [183/300], Step [154/172], Loss: 7.2425\n",
      "Epoch [183/300], Step [155/172], Loss: 5.8630\n",
      "Epoch [183/300], Step [156/172], Loss: 12.9638\n",
      "Epoch [183/300], Step [157/172], Loss: 9.1706\n",
      "Epoch [183/300], Step [158/172], Loss: 6.8802\n",
      "Epoch [183/300], Step [159/172], Loss: 9.2033\n",
      "Epoch [183/300], Step [160/172], Loss: 9.7342\n",
      "Epoch [183/300], Step [161/172], Loss: 6.8240\n",
      "Epoch [183/300], Step [162/172], Loss: 5.0839\n",
      "Epoch [183/300], Step [163/172], Loss: 6.3357\n",
      "Epoch [183/300], Step [164/172], Loss: 8.3375\n",
      "Epoch [183/300], Step [165/172], Loss: 5.9658\n",
      "Epoch [183/300], Step [166/172], Loss: 5.4052\n",
      "Epoch [183/300], Step [167/172], Loss: 10.1606\n",
      "Epoch [183/300], Step [168/172], Loss: 6.2501\n",
      "Epoch [183/300], Step [169/172], Loss: 6.4614\n",
      "Epoch [183/300], Step [170/172], Loss: 4.5893\n",
      "Epoch [183/300], Step [171/172], Loss: 7.6347\n",
      "Epoch [183/300], Step [172/172], Loss: 5.0028\n",
      "Epoch [184/300], Step [1/172], Loss: 53.0244\n",
      "Epoch [184/300], Step [2/172], Loss: 54.5154\n",
      "Epoch [184/300], Step [3/172], Loss: 49.1513\n",
      "Epoch [184/300], Step [4/172], Loss: 27.8342\n",
      "Epoch [184/300], Step [5/172], Loss: 47.1788\n",
      "Epoch [184/300], Step [6/172], Loss: 19.2030\n",
      "Epoch [184/300], Step [7/172], Loss: 30.5652\n",
      "Epoch [184/300], Step [8/172], Loss: 4.6602\n",
      "Epoch [184/300], Step [9/172], Loss: 32.5838\n",
      "Epoch [184/300], Step [10/172], Loss: 42.5366\n",
      "Epoch [184/300], Step [11/172], Loss: 59.7529\n",
      "Epoch [184/300], Step [12/172], Loss: 66.3767\n",
      "Epoch [184/300], Step [13/172], Loss: 34.4245\n",
      "Epoch [184/300], Step [14/172], Loss: 61.1998\n",
      "Epoch [184/300], Step [15/172], Loss: 56.4125\n",
      "Epoch [184/300], Step [16/172], Loss: 10.3769\n",
      "Epoch [184/300], Step [17/172], Loss: 45.3694\n",
      "Epoch [184/300], Step [18/172], Loss: 59.2639\n",
      "Epoch [184/300], Step [19/172], Loss: 82.5779\n",
      "Epoch [184/300], Step [20/172], Loss: 36.9501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [184/300], Step [21/172], Loss: 84.8915\n",
      "Epoch [184/300], Step [22/172], Loss: 60.5459\n",
      "Epoch [184/300], Step [23/172], Loss: 1.8699\n",
      "Epoch [184/300], Step [24/172], Loss: 56.5589\n",
      "Epoch [184/300], Step [25/172], Loss: 40.8909\n",
      "Epoch [184/300], Step [26/172], Loss: 47.7284\n",
      "Epoch [184/300], Step [27/172], Loss: 61.8594\n",
      "Epoch [184/300], Step [28/172], Loss: 24.6644\n",
      "Epoch [184/300], Step [29/172], Loss: 17.0474\n",
      "Epoch [184/300], Step [30/172], Loss: 63.3412\n",
      "Epoch [184/300], Step [31/172], Loss: 36.7816\n",
      "Epoch [184/300], Step [32/172], Loss: 42.4476\n",
      "Epoch [184/300], Step [33/172], Loss: 70.1463\n",
      "Epoch [184/300], Step [34/172], Loss: 2.7904\n",
      "Epoch [184/300], Step [35/172], Loss: 13.6660\n",
      "Epoch [184/300], Step [36/172], Loss: 17.9835\n",
      "Epoch [184/300], Step [37/172], Loss: 16.5725\n",
      "Epoch [184/300], Step [38/172], Loss: 29.2364\n",
      "Epoch [184/300], Step [39/172], Loss: 35.6310\n",
      "Epoch [184/300], Step [40/172], Loss: 20.2129\n",
      "Epoch [184/300], Step [41/172], Loss: 33.3513\n",
      "Epoch [184/300], Step [42/172], Loss: 37.9400\n",
      "Epoch [184/300], Step [43/172], Loss: 26.7748\n",
      "Epoch [184/300], Step [44/172], Loss: 20.6441\n",
      "Epoch [184/300], Step [45/172], Loss: 26.3382\n",
      "Epoch [184/300], Step [46/172], Loss: 17.3822\n",
      "Epoch [184/300], Step [47/172], Loss: 46.9848\n",
      "Epoch [184/300], Step [48/172], Loss: 60.9002\n",
      "Epoch [184/300], Step [49/172], Loss: 20.8564\n",
      "Epoch [184/300], Step [50/172], Loss: 47.2181\n",
      "Epoch [184/300], Step [51/172], Loss: 8.3553\n",
      "Epoch [184/300], Step [52/172], Loss: 19.1106\n",
      "Epoch [184/300], Step [53/172], Loss: 22.1722\n",
      "Epoch [184/300], Step [54/172], Loss: 14.4303\n",
      "Epoch [184/300], Step [55/172], Loss: 14.1321\n",
      "Epoch [184/300], Step [56/172], Loss: 17.4386\n",
      "Epoch [184/300], Step [57/172], Loss: 16.6192\n",
      "Epoch [184/300], Step [58/172], Loss: 13.1132\n",
      "Epoch [184/300], Step [59/172], Loss: 27.4777\n",
      "Epoch [184/300], Step [60/172], Loss: 25.8791\n",
      "Epoch [184/300], Step [61/172], Loss: 6.1590\n",
      "Epoch [184/300], Step [62/172], Loss: 19.6995\n",
      "Epoch [184/300], Step [63/172], Loss: 10.0886\n",
      "Epoch [184/300], Step [64/172], Loss: 10.5170\n",
      "Epoch [184/300], Step [65/172], Loss: 19.8402\n",
      "Epoch [184/300], Step [66/172], Loss: 6.4375\n",
      "Epoch [184/300], Step [67/172], Loss: 23.0784\n",
      "Epoch [184/300], Step [68/172], Loss: 5.1701\n",
      "Epoch [184/300], Step [69/172], Loss: 34.9877\n",
      "Epoch [184/300], Step [70/172], Loss: 38.8443\n",
      "Epoch [184/300], Step [71/172], Loss: 40.1957\n",
      "Epoch [184/300], Step [72/172], Loss: 40.5825\n",
      "Epoch [184/300], Step [73/172], Loss: 49.4133\n",
      "Epoch [184/300], Step [74/172], Loss: 25.7211\n",
      "Epoch [184/300], Step [75/172], Loss: 27.0197\n",
      "Epoch [184/300], Step [76/172], Loss: 28.8122\n",
      "Epoch [184/300], Step [77/172], Loss: 49.0670\n",
      "Epoch [184/300], Step [78/172], Loss: 37.8389\n",
      "Epoch [184/300], Step [79/172], Loss: 36.8551\n",
      "Epoch [184/300], Step [80/172], Loss: 50.3547\n",
      "Epoch [184/300], Step [81/172], Loss: 33.8952\n",
      "Epoch [184/300], Step [82/172], Loss: 35.6532\n",
      "Epoch [184/300], Step [83/172], Loss: 43.3482\n",
      "Epoch [184/300], Step [84/172], Loss: 32.6529\n",
      "Epoch [184/300], Step [85/172], Loss: 38.1143\n",
      "Epoch [184/300], Step [86/172], Loss: 32.4619\n",
      "Epoch [184/300], Step [87/172], Loss: 25.8851\n",
      "Epoch [184/300], Step [88/172], Loss: 24.5470\n",
      "Epoch [184/300], Step [89/172], Loss: 26.5655\n",
      "Epoch [184/300], Step [90/172], Loss: 20.8486\n",
      "Epoch [184/300], Step [91/172], Loss: 26.2682\n",
      "Epoch [184/300], Step [92/172], Loss: 19.7647\n",
      "Epoch [184/300], Step [93/172], Loss: 20.4932\n",
      "Epoch [184/300], Step [94/172], Loss: 28.8449\n",
      "Epoch [184/300], Step [95/172], Loss: 20.5979\n",
      "Epoch [184/300], Step [96/172], Loss: 19.7978\n",
      "Epoch [184/300], Step [97/172], Loss: 27.8790\n",
      "Epoch [184/300], Step [98/172], Loss: 19.2177\n",
      "Epoch [184/300], Step [99/172], Loss: 18.8762\n",
      "Epoch [184/300], Step [100/172], Loss: 15.2885\n",
      "Epoch [184/300], Step [101/172], Loss: 18.5145\n",
      "Epoch [184/300], Step [102/172], Loss: 16.9523\n",
      "Epoch [184/300], Step [103/172], Loss: 12.3360\n",
      "Epoch [184/300], Step [104/172], Loss: 18.2704\n",
      "Epoch [184/300], Step [105/172], Loss: 19.0779\n",
      "Epoch [184/300], Step [106/172], Loss: 15.4483\n",
      "Epoch [184/300], Step [107/172], Loss: 15.4299\n",
      "Epoch [184/300], Step [108/172], Loss: 14.4842\n",
      "Epoch [184/300], Step [109/172], Loss: 14.4485\n",
      "Epoch [184/300], Step [110/172], Loss: 15.7190\n",
      "Epoch [184/300], Step [111/172], Loss: 15.2496\n",
      "Epoch [184/300], Step [112/172], Loss: 17.2136\n",
      "Epoch [184/300], Step [113/172], Loss: 11.6459\n",
      "Epoch [184/300], Step [114/172], Loss: 13.6037\n",
      "Epoch [184/300], Step [115/172], Loss: 19.0228\n",
      "Epoch [184/300], Step [116/172], Loss: 14.3617\n",
      "Epoch [184/300], Step [117/172], Loss: 11.3584\n",
      "Epoch [184/300], Step [118/172], Loss: 13.9249\n",
      "Epoch [184/300], Step [119/172], Loss: 15.6764\n",
      "Epoch [184/300], Step [120/172], Loss: 9.8814\n",
      "Epoch [184/300], Step [121/172], Loss: 9.2794\n",
      "Epoch [184/300], Step [122/172], Loss: 10.5392\n",
      "Epoch [184/300], Step [123/172], Loss: 10.0988\n",
      "Epoch [184/300], Step [124/172], Loss: 7.3146\n",
      "Epoch [184/300], Step [125/172], Loss: 11.3956\n",
      "Epoch [184/300], Step [126/172], Loss: 10.8115\n",
      "Epoch [184/300], Step [127/172], Loss: 10.6514\n",
      "Epoch [184/300], Step [128/172], Loss: 10.1693\n",
      "Epoch [184/300], Step [129/172], Loss: 7.8154\n",
      "Epoch [184/300], Step [130/172], Loss: 12.1332\n",
      "Epoch [184/300], Step [131/172], Loss: 7.1294\n",
      "Epoch [184/300], Step [132/172], Loss: 8.1382\n",
      "Epoch [184/300], Step [133/172], Loss: 8.8873\n",
      "Epoch [184/300], Step [134/172], Loss: 11.1962\n",
      "Epoch [184/300], Step [135/172], Loss: 8.3053\n",
      "Epoch [184/300], Step [136/172], Loss: 7.9435\n",
      "Epoch [184/300], Step [137/172], Loss: 8.9657\n",
      "Epoch [184/300], Step [138/172], Loss: 6.7111\n",
      "Epoch [184/300], Step [139/172], Loss: 9.2172\n",
      "Epoch [184/300], Step [140/172], Loss: 9.4159\n",
      "Epoch [184/300], Step [141/172], Loss: 9.1317\n",
      "Epoch [184/300], Step [142/172], Loss: 13.9455\n",
      "Epoch [184/300], Step [143/172], Loss: 10.4379\n",
      "Epoch [184/300], Step [144/172], Loss: 8.7591\n",
      "Epoch [184/300], Step [145/172], Loss: 9.6762\n",
      "Epoch [184/300], Step [146/172], Loss: 9.1740\n",
      "Epoch [184/300], Step [147/172], Loss: 4.9777\n",
      "Epoch [184/300], Step [148/172], Loss: 5.7964\n",
      "Epoch [184/300], Step [149/172], Loss: 6.3719\n",
      "Epoch [184/300], Step [150/172], Loss: 5.7830\n",
      "Epoch [184/300], Step [151/172], Loss: 5.2395\n",
      "Epoch [184/300], Step [152/172], Loss: 7.1604\n",
      "Epoch [184/300], Step [153/172], Loss: 6.1124\n",
      "Epoch [184/300], Step [154/172], Loss: 7.2010\n",
      "Epoch [184/300], Step [155/172], Loss: 5.8195\n",
      "Epoch [184/300], Step [156/172], Loss: 13.0188\n",
      "Epoch [184/300], Step [157/172], Loss: 9.1817\n",
      "Epoch [184/300], Step [158/172], Loss: 6.8924\n",
      "Epoch [184/300], Step [159/172], Loss: 9.1006\n",
      "Epoch [184/300], Step [160/172], Loss: 9.8058\n",
      "Epoch [184/300], Step [161/172], Loss: 6.7970\n",
      "Epoch [184/300], Step [162/172], Loss: 5.0439\n",
      "Epoch [184/300], Step [163/172], Loss: 6.2812\n",
      "Epoch [184/300], Step [164/172], Loss: 8.4018\n",
      "Epoch [184/300], Step [165/172], Loss: 5.9221\n",
      "Epoch [184/300], Step [166/172], Loss: 5.3468\n",
      "Epoch [184/300], Step [167/172], Loss: 10.1538\n",
      "Epoch [184/300], Step [168/172], Loss: 6.2147\n",
      "Epoch [184/300], Step [169/172], Loss: 6.4378\n",
      "Epoch [184/300], Step [170/172], Loss: 4.5398\n",
      "Epoch [184/300], Step [171/172], Loss: 7.6671\n",
      "Epoch [184/300], Step [172/172], Loss: 4.9735\n",
      "Epoch [185/300], Step [1/172], Loss: 52.9702\n",
      "Epoch [185/300], Step [2/172], Loss: 54.5074\n",
      "Epoch [185/300], Step [3/172], Loss: 48.5560\n",
      "Epoch [185/300], Step [4/172], Loss: 27.6881\n",
      "Epoch [185/300], Step [5/172], Loss: 47.6364\n",
      "Epoch [185/300], Step [6/172], Loss: 18.9122\n",
      "Epoch [185/300], Step [7/172], Loss: 29.2768\n",
      "Epoch [185/300], Step [8/172], Loss: 4.3267\n",
      "Epoch [185/300], Step [9/172], Loss: 32.5384\n",
      "Epoch [185/300], Step [10/172], Loss: 42.5070\n",
      "Epoch [185/300], Step [11/172], Loss: 59.3731\n",
      "Epoch [185/300], Step [12/172], Loss: 66.0113\n",
      "Epoch [185/300], Step [13/172], Loss: 34.1864\n",
      "Epoch [185/300], Step [14/172], Loss: 61.1224\n",
      "Epoch [185/300], Step [15/172], Loss: 56.2274\n",
      "Epoch [185/300], Step [16/172], Loss: 10.6689\n",
      "Epoch [185/300], Step [17/172], Loss: 45.1477\n",
      "Epoch [185/300], Step [18/172], Loss: 59.2448\n",
      "Epoch [185/300], Step [19/172], Loss: 82.5951\n",
      "Epoch [185/300], Step [20/172], Loss: 36.5952\n",
      "Epoch [185/300], Step [21/172], Loss: 85.0193\n",
      "Epoch [185/300], Step [22/172], Loss: 60.6065\n",
      "Epoch [185/300], Step [23/172], Loss: 1.8780\n",
      "Epoch [185/300], Step [24/172], Loss: 56.4282\n",
      "Epoch [185/300], Step [25/172], Loss: 41.0129\n",
      "Epoch [185/300], Step [26/172], Loss: 47.7581\n",
      "Epoch [185/300], Step [27/172], Loss: 61.9737\n",
      "Epoch [185/300], Step [28/172], Loss: 24.6805\n",
      "Epoch [185/300], Step [29/172], Loss: 16.9158\n",
      "Epoch [185/300], Step [30/172], Loss: 63.7023\n",
      "Epoch [185/300], Step [31/172], Loss: 37.1968\n",
      "Epoch [185/300], Step [32/172], Loss: 42.4943\n",
      "Epoch [185/300], Step [33/172], Loss: 70.1821\n",
      "Epoch [185/300], Step [34/172], Loss: 2.7716\n",
      "Epoch [185/300], Step [35/172], Loss: 13.5993\n",
      "Epoch [185/300], Step [36/172], Loss: 17.7499\n",
      "Epoch [185/300], Step [37/172], Loss: 16.6329\n",
      "Epoch [185/300], Step [38/172], Loss: 29.4233\n",
      "Epoch [185/300], Step [39/172], Loss: 35.6760\n",
      "Epoch [185/300], Step [40/172], Loss: 20.2261\n",
      "Epoch [185/300], Step [41/172], Loss: 33.3829\n",
      "Epoch [185/300], Step [42/172], Loss: 37.9336\n",
      "Epoch [185/300], Step [43/172], Loss: 26.7566\n",
      "Epoch [185/300], Step [44/172], Loss: 20.7880\n",
      "Epoch [185/300], Step [45/172], Loss: 26.3838\n",
      "Epoch [185/300], Step [46/172], Loss: 17.3922\n",
      "Epoch [185/300], Step [47/172], Loss: 47.2020\n",
      "Epoch [185/300], Step [48/172], Loss: 60.9011\n",
      "Epoch [185/300], Step [49/172], Loss: 20.6454\n",
      "Epoch [185/300], Step [50/172], Loss: 47.2405\n",
      "Epoch [185/300], Step [51/172], Loss: 8.3291\n",
      "Epoch [185/300], Step [52/172], Loss: 19.0915\n",
      "Epoch [185/300], Step [53/172], Loss: 22.1926\n",
      "Epoch [185/300], Step [54/172], Loss: 14.4897\n",
      "Epoch [185/300], Step [55/172], Loss: 14.1974\n",
      "Epoch [185/300], Step [56/172], Loss: 17.7084\n",
      "Epoch [185/300], Step [57/172], Loss: 16.7285\n",
      "Epoch [185/300], Step [58/172], Loss: 13.2829\n",
      "Epoch [185/300], Step [59/172], Loss: 27.7250\n",
      "Epoch [185/300], Step [60/172], Loss: 25.6930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [185/300], Step [61/172], Loss: 6.2103\n",
      "Epoch [185/300], Step [62/172], Loss: 19.8261\n",
      "Epoch [185/300], Step [63/172], Loss: 10.0656\n",
      "Epoch [185/300], Step [64/172], Loss: 10.5504\n",
      "Epoch [185/300], Step [65/172], Loss: 19.8306\n",
      "Epoch [185/300], Step [66/172], Loss: 6.4289\n",
      "Epoch [185/300], Step [67/172], Loss: 23.0328\n",
      "Epoch [185/300], Step [68/172], Loss: 5.2146\n",
      "Epoch [185/300], Step [69/172], Loss: 34.7857\n",
      "Epoch [185/300], Step [70/172], Loss: 38.6033\n",
      "Epoch [185/300], Step [71/172], Loss: 40.1705\n",
      "Epoch [185/300], Step [72/172], Loss: 40.4217\n",
      "Epoch [185/300], Step [73/172], Loss: 49.2891\n",
      "Epoch [185/300], Step [74/172], Loss: 25.5283\n",
      "Epoch [185/300], Step [75/172], Loss: 27.0812\n",
      "Epoch [185/300], Step [76/172], Loss: 28.7737\n",
      "Epoch [185/300], Step [77/172], Loss: 48.8672\n",
      "Epoch [185/300], Step [78/172], Loss: 37.7231\n",
      "Epoch [185/300], Step [79/172], Loss: 36.8193\n",
      "Epoch [185/300], Step [80/172], Loss: 50.1331\n",
      "Epoch [185/300], Step [81/172], Loss: 33.7936\n",
      "Epoch [185/300], Step [82/172], Loss: 35.3436\n",
      "Epoch [185/300], Step [83/172], Loss: 43.2580\n",
      "Epoch [185/300], Step [84/172], Loss: 32.6050\n",
      "Epoch [185/300], Step [85/172], Loss: 38.0919\n",
      "Epoch [185/300], Step [86/172], Loss: 32.4191\n",
      "Epoch [185/300], Step [87/172], Loss: 25.8741\n",
      "Epoch [185/300], Step [88/172], Loss: 24.4334\n",
      "Epoch [185/300], Step [89/172], Loss: 26.5745\n",
      "Epoch [185/300], Step [90/172], Loss: 20.7471\n",
      "Epoch [185/300], Step [91/172], Loss: 26.2780\n",
      "Epoch [185/300], Step [92/172], Loss: 19.7805\n",
      "Epoch [185/300], Step [93/172], Loss: 20.4622\n",
      "Epoch [185/300], Step [94/172], Loss: 28.7490\n",
      "Epoch [185/300], Step [95/172], Loss: 20.5233\n",
      "Epoch [185/300], Step [96/172], Loss: 19.8169\n",
      "Epoch [185/300], Step [97/172], Loss: 27.8744\n",
      "Epoch [185/300], Step [98/172], Loss: 19.1890\n",
      "Epoch [185/300], Step [99/172], Loss: 18.8459\n",
      "Epoch [185/300], Step [100/172], Loss: 15.3519\n",
      "Epoch [185/300], Step [101/172], Loss: 18.5332\n",
      "Epoch [185/300], Step [102/172], Loss: 16.8551\n",
      "Epoch [185/300], Step [103/172], Loss: 12.3584\n",
      "Epoch [185/300], Step [104/172], Loss: 18.3117\n",
      "Epoch [185/300], Step [105/172], Loss: 19.0222\n",
      "Epoch [185/300], Step [106/172], Loss: 15.4378\n",
      "Epoch [185/300], Step [107/172], Loss: 15.4380\n",
      "Epoch [185/300], Step [108/172], Loss: 14.4923\n",
      "Epoch [185/300], Step [109/172], Loss: 14.4407\n",
      "Epoch [185/300], Step [110/172], Loss: 15.6831\n",
      "Epoch [185/300], Step [111/172], Loss: 15.3236\n",
      "Epoch [185/300], Step [112/172], Loss: 17.2735\n",
      "Epoch [185/300], Step [113/172], Loss: 11.6659\n",
      "Epoch [185/300], Step [114/172], Loss: 13.6236\n",
      "Epoch [185/300], Step [115/172], Loss: 18.9786\n",
      "Epoch [185/300], Step [116/172], Loss: 14.3068\n",
      "Epoch [185/300], Step [117/172], Loss: 11.4232\n",
      "Epoch [185/300], Step [118/172], Loss: 13.9687\n",
      "Epoch [185/300], Step [119/172], Loss: 15.6633\n",
      "Epoch [185/300], Step [120/172], Loss: 9.9426\n",
      "Epoch [185/300], Step [121/172], Loss: 9.2866\n",
      "Epoch [185/300], Step [122/172], Loss: 10.5669\n",
      "Epoch [185/300], Step [123/172], Loss: 10.1283\n",
      "Epoch [185/300], Step [124/172], Loss: 7.3371\n",
      "Epoch [185/300], Step [125/172], Loss: 11.4845\n",
      "Epoch [185/300], Step [126/172], Loss: 10.8509\n",
      "Epoch [185/300], Step [127/172], Loss: 10.5970\n",
      "Epoch [185/300], Step [128/172], Loss: 10.0785\n",
      "Epoch [185/300], Step [129/172], Loss: 7.7847\n",
      "Epoch [185/300], Step [130/172], Loss: 12.1086\n",
      "Epoch [185/300], Step [131/172], Loss: 7.1071\n",
      "Epoch [185/300], Step [132/172], Loss: 8.1348\n",
      "Epoch [185/300], Step [133/172], Loss: 8.8808\n",
      "Epoch [185/300], Step [134/172], Loss: 11.1596\n",
      "Epoch [185/300], Step [135/172], Loss: 8.2870\n",
      "Epoch [185/300], Step [136/172], Loss: 7.9360\n",
      "Epoch [185/300], Step [137/172], Loss: 8.9476\n",
      "Epoch [185/300], Step [138/172], Loss: 6.6968\n",
      "Epoch [185/300], Step [139/172], Loss: 9.2312\n",
      "Epoch [185/300], Step [140/172], Loss: 9.3993\n",
      "Epoch [185/300], Step [141/172], Loss: 9.1684\n",
      "Epoch [185/300], Step [142/172], Loss: 13.8858\n",
      "Epoch [185/300], Step [143/172], Loss: 10.4191\n",
      "Epoch [185/300], Step [144/172], Loss: 8.7775\n",
      "Epoch [185/300], Step [145/172], Loss: 9.7014\n",
      "Epoch [185/300], Step [146/172], Loss: 9.1052\n",
      "Epoch [185/300], Step [147/172], Loss: 5.0150\n",
      "Epoch [185/300], Step [148/172], Loss: 5.7877\n",
      "Epoch [185/300], Step [149/172], Loss: 6.3671\n",
      "Epoch [185/300], Step [150/172], Loss: 5.7724\n",
      "Epoch [185/300], Step [151/172], Loss: 5.2221\n",
      "Epoch [185/300], Step [152/172], Loss: 7.1328\n",
      "Epoch [185/300], Step [153/172], Loss: 6.1364\n",
      "Epoch [185/300], Step [154/172], Loss: 7.1307\n",
      "Epoch [185/300], Step [155/172], Loss: 5.8444\n",
      "Epoch [185/300], Step [156/172], Loss: 12.9825\n",
      "Epoch [185/300], Step [157/172], Loss: 9.1413\n",
      "Epoch [185/300], Step [158/172], Loss: 6.8757\n",
      "Epoch [185/300], Step [159/172], Loss: 9.1676\n",
      "Epoch [185/300], Step [160/172], Loss: 9.7928\n",
      "Epoch [185/300], Step [161/172], Loss: 6.7675\n",
      "Epoch [185/300], Step [162/172], Loss: 5.0249\n",
      "Epoch [185/300], Step [163/172], Loss: 6.2555\n",
      "Epoch [185/300], Step [164/172], Loss: 8.2744\n",
      "Epoch [185/300], Step [165/172], Loss: 5.9343\n",
      "Epoch [185/300], Step [166/172], Loss: 5.3058\n",
      "Epoch [185/300], Step [167/172], Loss: 10.2368\n",
      "Epoch [185/300], Step [168/172], Loss: 6.1887\n",
      "Epoch [185/300], Step [169/172], Loss: 6.4371\n",
      "Epoch [185/300], Step [170/172], Loss: 4.5123\n",
      "Epoch [185/300], Step [171/172], Loss: 7.7269\n",
      "Epoch [185/300], Step [172/172], Loss: 5.0134\n",
      "Epoch [186/300], Step [1/172], Loss: 52.9295\n",
      "Epoch [186/300], Step [2/172], Loss: 54.4366\n",
      "Epoch [186/300], Step [3/172], Loss: 48.0272\n",
      "Epoch [186/300], Step [4/172], Loss: 27.6966\n",
      "Epoch [186/300], Step [5/172], Loss: 47.0382\n",
      "Epoch [186/300], Step [6/172], Loss: 18.9626\n",
      "Epoch [186/300], Step [7/172], Loss: 29.6131\n",
      "Epoch [186/300], Step [8/172], Loss: 4.8646\n",
      "Epoch [186/300], Step [9/172], Loss: 32.5553\n",
      "Epoch [186/300], Step [10/172], Loss: 42.5589\n",
      "Epoch [186/300], Step [11/172], Loss: 59.1514\n",
      "Epoch [186/300], Step [12/172], Loss: 65.7845\n",
      "Epoch [186/300], Step [13/172], Loss: 34.4255\n",
      "Epoch [186/300], Step [14/172], Loss: 61.2359\n",
      "Epoch [186/300], Step [15/172], Loss: 56.0023\n",
      "Epoch [186/300], Step [16/172], Loss: 10.2320\n",
      "Epoch [186/300], Step [17/172], Loss: 44.9403\n",
      "Epoch [186/300], Step [18/172], Loss: 58.9233\n",
      "Epoch [186/300], Step [19/172], Loss: 82.0724\n",
      "Epoch [186/300], Step [20/172], Loss: 36.4155\n",
      "Epoch [186/300], Step [21/172], Loss: 84.9360\n",
      "Epoch [186/300], Step [22/172], Loss: 60.4570\n",
      "Epoch [186/300], Step [23/172], Loss: 1.9543\n",
      "Epoch [186/300], Step [24/172], Loss: 56.1012\n",
      "Epoch [186/300], Step [25/172], Loss: 40.9214\n",
      "Epoch [186/300], Step [26/172], Loss: 47.8206\n",
      "Epoch [186/300], Step [27/172], Loss: 62.0322\n",
      "Epoch [186/300], Step [28/172], Loss: 24.5376\n",
      "Epoch [186/300], Step [29/172], Loss: 16.7827\n",
      "Epoch [186/300], Step [30/172], Loss: 63.7284\n",
      "Epoch [186/300], Step [31/172], Loss: 37.1315\n",
      "Epoch [186/300], Step [32/172], Loss: 42.7143\n",
      "Epoch [186/300], Step [33/172], Loss: 70.3681\n",
      "Epoch [186/300], Step [34/172], Loss: 2.7333\n",
      "Epoch [186/300], Step [35/172], Loss: 13.7996\n",
      "Epoch [186/300], Step [36/172], Loss: 17.9273\n",
      "Epoch [186/300], Step [37/172], Loss: 16.7229\n",
      "Epoch [186/300], Step [38/172], Loss: 29.6509\n",
      "Epoch [186/300], Step [39/172], Loss: 35.8120\n",
      "Epoch [186/300], Step [40/172], Loss: 20.3129\n",
      "Epoch [186/300], Step [41/172], Loss: 33.4766\n",
      "Epoch [186/300], Step [42/172], Loss: 38.0740\n",
      "Epoch [186/300], Step [43/172], Loss: 26.8068\n",
      "Epoch [186/300], Step [44/172], Loss: 20.7783\n",
      "Epoch [186/300], Step [45/172], Loss: 26.4726\n",
      "Epoch [186/300], Step [46/172], Loss: 17.3704\n",
      "Epoch [186/300], Step [47/172], Loss: 47.2381\n",
      "Epoch [186/300], Step [48/172], Loss: 60.8548\n",
      "Epoch [186/300], Step [49/172], Loss: 20.7571\n",
      "Epoch [186/300], Step [50/172], Loss: 47.1310\n",
      "Epoch [186/300], Step [51/172], Loss: 8.3456\n",
      "Epoch [186/300], Step [52/172], Loss: 19.0365\n",
      "Epoch [186/300], Step [53/172], Loss: 22.0381\n",
      "Epoch [186/300], Step [54/172], Loss: 14.4068\n",
      "Epoch [186/300], Step [55/172], Loss: 14.1491\n",
      "Epoch [186/300], Step [56/172], Loss: 17.4578\n",
      "Epoch [186/300], Step [57/172], Loss: 16.7459\n",
      "Epoch [186/300], Step [58/172], Loss: 13.1156\n",
      "Epoch [186/300], Step [59/172], Loss: 27.4350\n",
      "Epoch [186/300], Step [60/172], Loss: 25.7102\n",
      "Epoch [186/300], Step [61/172], Loss: 6.1891\n",
      "Epoch [186/300], Step [62/172], Loss: 19.5798\n",
      "Epoch [186/300], Step [63/172], Loss: 9.9927\n",
      "Epoch [186/300], Step [64/172], Loss: 10.4059\n",
      "Epoch [186/300], Step [65/172], Loss: 19.7108\n",
      "Epoch [186/300], Step [66/172], Loss: 6.3639\n",
      "Epoch [186/300], Step [67/172], Loss: 22.9697\n",
      "Epoch [186/300], Step [68/172], Loss: 5.1691\n",
      "Epoch [186/300], Step [69/172], Loss: 34.7278\n",
      "Epoch [186/300], Step [70/172], Loss: 38.5248\n",
      "Epoch [186/300], Step [71/172], Loss: 40.0973\n",
      "Epoch [186/300], Step [72/172], Loss: 40.3678\n",
      "Epoch [186/300], Step [73/172], Loss: 49.1914\n",
      "Epoch [186/300], Step [74/172], Loss: 25.4831\n",
      "Epoch [186/300], Step [75/172], Loss: 27.0070\n",
      "Epoch [186/300], Step [76/172], Loss: 28.5243\n",
      "Epoch [186/300], Step [77/172], Loss: 48.6586\n",
      "Epoch [186/300], Step [78/172], Loss: 37.5767\n",
      "Epoch [186/300], Step [79/172], Loss: 36.7529\n",
      "Epoch [186/300], Step [80/172], Loss: 50.1642\n",
      "Epoch [186/300], Step [81/172], Loss: 33.6664\n",
      "Epoch [186/300], Step [82/172], Loss: 35.6747\n",
      "Epoch [186/300], Step [83/172], Loss: 43.1033\n",
      "Epoch [186/300], Step [84/172], Loss: 32.4621\n",
      "Epoch [186/300], Step [85/172], Loss: 37.9553\n",
      "Epoch [186/300], Step [86/172], Loss: 32.3809\n",
      "Epoch [186/300], Step [87/172], Loss: 25.7615\n",
      "Epoch [186/300], Step [88/172], Loss: 24.3155\n",
      "Epoch [186/300], Step [89/172], Loss: 26.5097\n",
      "Epoch [186/300], Step [90/172], Loss: 20.6670\n",
      "Epoch [186/300], Step [91/172], Loss: 26.1549\n",
      "Epoch [186/300], Step [92/172], Loss: 19.6654\n",
      "Epoch [186/300], Step [93/172], Loss: 20.4038\n",
      "Epoch [186/300], Step [94/172], Loss: 28.6663\n",
      "Epoch [186/300], Step [95/172], Loss: 20.4532\n",
      "Epoch [186/300], Step [96/172], Loss: 19.7518\n",
      "Epoch [186/300], Step [97/172], Loss: 27.8645\n",
      "Epoch [186/300], Step [98/172], Loss: 19.1412\n",
      "Epoch [186/300], Step [99/172], Loss: 18.8235\n",
      "Epoch [186/300], Step [100/172], Loss: 15.3115\n",
      "Epoch [186/300], Step [101/172], Loss: 18.5453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [186/300], Step [102/172], Loss: 16.9253\n",
      "Epoch [186/300], Step [103/172], Loss: 12.3230\n",
      "Epoch [186/300], Step [104/172], Loss: 18.3290\n",
      "Epoch [186/300], Step [105/172], Loss: 19.1335\n",
      "Epoch [186/300], Step [106/172], Loss: 15.3844\n",
      "Epoch [186/300], Step [107/172], Loss: 15.4216\n",
      "Epoch [186/300], Step [108/172], Loss: 14.3994\n",
      "Epoch [186/300], Step [109/172], Loss: 14.3806\n",
      "Epoch [186/300], Step [110/172], Loss: 15.6355\n",
      "Epoch [186/300], Step [111/172], Loss: 15.2636\n",
      "Epoch [186/300], Step [112/172], Loss: 17.2120\n",
      "Epoch [186/300], Step [113/172], Loss: 11.6196\n",
      "Epoch [186/300], Step [114/172], Loss: 13.5839\n",
      "Epoch [186/300], Step [115/172], Loss: 18.9291\n",
      "Epoch [186/300], Step [116/172], Loss: 14.2725\n",
      "Epoch [186/300], Step [117/172], Loss: 11.3929\n",
      "Epoch [186/300], Step [118/172], Loss: 13.9305\n",
      "Epoch [186/300], Step [119/172], Loss: 15.6640\n",
      "Epoch [186/300], Step [120/172], Loss: 9.9272\n",
      "Epoch [186/300], Step [121/172], Loss: 9.2879\n",
      "Epoch [186/300], Step [122/172], Loss: 10.4925\n",
      "Epoch [186/300], Step [123/172], Loss: 10.0753\n",
      "Epoch [186/300], Step [124/172], Loss: 7.3240\n",
      "Epoch [186/300], Step [125/172], Loss: 11.4419\n",
      "Epoch [186/300], Step [126/172], Loss: 10.8516\n",
      "Epoch [186/300], Step [127/172], Loss: 10.5739\n",
      "Epoch [186/300], Step [128/172], Loss: 10.0657\n",
      "Epoch [186/300], Step [129/172], Loss: 7.7934\n",
      "Epoch [186/300], Step [130/172], Loss: 12.1763\n",
      "Epoch [186/300], Step [131/172], Loss: 7.0916\n",
      "Epoch [186/300], Step [132/172], Loss: 8.1515\n",
      "Epoch [186/300], Step [133/172], Loss: 8.8773\n",
      "Epoch [186/300], Step [134/172], Loss: 11.1407\n",
      "Epoch [186/300], Step [135/172], Loss: 8.2930\n",
      "Epoch [186/300], Step [136/172], Loss: 7.8812\n",
      "Epoch [186/300], Step [137/172], Loss: 8.9801\n",
      "Epoch [186/300], Step [138/172], Loss: 6.6667\n",
      "Epoch [186/300], Step [139/172], Loss: 9.2822\n",
      "Epoch [186/300], Step [140/172], Loss: 9.3821\n",
      "Epoch [186/300], Step [141/172], Loss: 9.1116\n",
      "Epoch [186/300], Step [142/172], Loss: 13.8659\n",
      "Epoch [186/300], Step [143/172], Loss: 10.4136\n",
      "Epoch [186/300], Step [144/172], Loss: 8.7964\n",
      "Epoch [186/300], Step [145/172], Loss: 9.7100\n",
      "Epoch [186/300], Step [146/172], Loss: 9.1102\n",
      "Epoch [186/300], Step [147/172], Loss: 5.0298\n",
      "Epoch [186/300], Step [148/172], Loss: 5.7907\n",
      "Epoch [186/300], Step [149/172], Loss: 6.3690\n",
      "Epoch [186/300], Step [150/172], Loss: 5.7366\n",
      "Epoch [186/300], Step [151/172], Loss: 5.2054\n",
      "Epoch [186/300], Step [152/172], Loss: 7.1055\n",
      "Epoch [186/300], Step [153/172], Loss: 6.1480\n",
      "Epoch [186/300], Step [154/172], Loss: 7.1436\n",
      "Epoch [186/300], Step [155/172], Loss: 5.8350\n",
      "Epoch [186/300], Step [156/172], Loss: 13.0142\n",
      "Epoch [186/300], Step [157/172], Loss: 9.1189\n",
      "Epoch [186/300], Step [158/172], Loss: 6.8674\n",
      "Epoch [186/300], Step [159/172], Loss: 9.0546\n",
      "Epoch [186/300], Step [160/172], Loss: 9.7895\n",
      "Epoch [186/300], Step [161/172], Loss: 6.7977\n",
      "Epoch [186/300], Step [162/172], Loss: 4.9979\n",
      "Epoch [186/300], Step [163/172], Loss: 6.2639\n",
      "Epoch [186/300], Step [164/172], Loss: 8.4058\n",
      "Epoch [186/300], Step [165/172], Loss: 5.9471\n",
      "Epoch [186/300], Step [166/172], Loss: 5.3280\n",
      "Epoch [186/300], Step [167/172], Loss: 10.1439\n",
      "Epoch [186/300], Step [168/172], Loss: 6.1748\n",
      "Epoch [186/300], Step [169/172], Loss: 6.3914\n",
      "Epoch [186/300], Step [170/172], Loss: 4.5194\n",
      "Epoch [186/300], Step [171/172], Loss: 7.6653\n",
      "Epoch [186/300], Step [172/172], Loss: 5.0201\n",
      "Epoch [187/300], Step [1/172], Loss: 52.6991\n",
      "Epoch [187/300], Step [2/172], Loss: 54.3666\n",
      "Epoch [187/300], Step [3/172], Loss: 48.5751\n",
      "Epoch [187/300], Step [4/172], Loss: 27.5392\n",
      "Epoch [187/300], Step [5/172], Loss: 47.2794\n",
      "Epoch [187/300], Step [6/172], Loss: 18.9244\n",
      "Epoch [187/300], Step [7/172], Loss: 29.1092\n",
      "Epoch [187/300], Step [8/172], Loss: 4.1774\n",
      "Epoch [187/300], Step [9/172], Loss: 32.4067\n",
      "Epoch [187/300], Step [10/172], Loss: 42.5753\n",
      "Epoch [187/300], Step [11/172], Loss: 58.9646\n",
      "Epoch [187/300], Step [12/172], Loss: 65.7804\n",
      "Epoch [187/300], Step [13/172], Loss: 34.2355\n",
      "Epoch [187/300], Step [14/172], Loss: 61.3563\n",
      "Epoch [187/300], Step [15/172], Loss: 56.1191\n",
      "Epoch [187/300], Step [16/172], Loss: 10.9840\n",
      "Epoch [187/300], Step [17/172], Loss: 44.6783\n",
      "Epoch [187/300], Step [18/172], Loss: 58.8568\n",
      "Epoch [187/300], Step [19/172], Loss: 81.7783\n",
      "Epoch [187/300], Step [20/172], Loss: 36.3138\n",
      "Epoch [187/300], Step [21/172], Loss: 84.4786\n",
      "Epoch [187/300], Step [22/172], Loss: 60.2003\n",
      "Epoch [187/300], Step [23/172], Loss: 1.8715\n",
      "Epoch [187/300], Step [24/172], Loss: 55.7708\n",
      "Epoch [187/300], Step [25/172], Loss: 40.7095\n",
      "Epoch [187/300], Step [26/172], Loss: 47.4415\n",
      "Epoch [187/300], Step [27/172], Loss: 61.4897\n",
      "Epoch [187/300], Step [28/172], Loss: 24.5706\n",
      "Epoch [187/300], Step [29/172], Loss: 17.0043\n",
      "Epoch [187/300], Step [30/172], Loss: 63.7106\n",
      "Epoch [187/300], Step [31/172], Loss: 37.5993\n",
      "Epoch [187/300], Step [32/172], Loss: 42.7349\n",
      "Epoch [187/300], Step [33/172], Loss: 70.2127\n",
      "Epoch [187/300], Step [34/172], Loss: 2.7745\n",
      "Epoch [187/300], Step [35/172], Loss: 13.7031\n",
      "Epoch [187/300], Step [36/172], Loss: 17.8967\n",
      "Epoch [187/300], Step [37/172], Loss: 16.8180\n",
      "Epoch [187/300], Step [38/172], Loss: 29.7441\n",
      "Epoch [187/300], Step [39/172], Loss: 36.0249\n",
      "Epoch [187/300], Step [40/172], Loss: 20.5467\n",
      "Epoch [187/300], Step [41/172], Loss: 33.6887\n",
      "Epoch [187/300], Step [42/172], Loss: 38.4081\n",
      "Epoch [187/300], Step [43/172], Loss: 27.2101\n",
      "Epoch [187/300], Step [44/172], Loss: 21.0632\n",
      "Epoch [187/300], Step [45/172], Loss: 26.8448\n",
      "Epoch [187/300], Step [46/172], Loss: 17.4190\n",
      "Epoch [187/300], Step [47/172], Loss: 47.5938\n",
      "Epoch [187/300], Step [48/172], Loss: 61.1655\n",
      "Epoch [187/300], Step [49/172], Loss: 20.9730\n",
      "Epoch [187/300], Step [50/172], Loss: 47.3081\n",
      "Epoch [187/300], Step [51/172], Loss: 8.3958\n",
      "Epoch [187/300], Step [52/172], Loss: 19.3680\n",
      "Epoch [187/300], Step [53/172], Loss: 22.2402\n",
      "Epoch [187/300], Step [54/172], Loss: 14.5870\n",
      "Epoch [187/300], Step [55/172], Loss: 14.2651\n",
      "Epoch [187/300], Step [56/172], Loss: 17.7833\n",
      "Epoch [187/300], Step [57/172], Loss: 16.8058\n",
      "Epoch [187/300], Step [58/172], Loss: 12.9964\n",
      "Epoch [187/300], Step [59/172], Loss: 27.0932\n",
      "Epoch [187/300], Step [60/172], Loss: 25.4342\n",
      "Epoch [187/300], Step [61/172], Loss: 6.1727\n",
      "Epoch [187/300], Step [62/172], Loss: 19.3849\n",
      "Epoch [187/300], Step [63/172], Loss: 9.8837\n",
      "Epoch [187/300], Step [64/172], Loss: 10.3786\n",
      "Epoch [187/300], Step [65/172], Loss: 19.6333\n",
      "Epoch [187/300], Step [66/172], Loss: 6.3909\n",
      "Epoch [187/300], Step [67/172], Loss: 22.7273\n",
      "Epoch [187/300], Step [68/172], Loss: 5.0944\n",
      "Epoch [187/300], Step [69/172], Loss: 34.7126\n",
      "Epoch [187/300], Step [70/172], Loss: 38.4608\n",
      "Epoch [187/300], Step [71/172], Loss: 40.2013\n",
      "Epoch [187/300], Step [72/172], Loss: 40.1961\n",
      "Epoch [187/300], Step [73/172], Loss: 49.0953\n",
      "Epoch [187/300], Step [74/172], Loss: 25.4361\n",
      "Epoch [187/300], Step [75/172], Loss: 26.8630\n",
      "Epoch [187/300], Step [76/172], Loss: 28.6463\n",
      "Epoch [187/300], Step [77/172], Loss: 48.5052\n",
      "Epoch [187/300], Step [78/172], Loss: 37.4063\n",
      "Epoch [187/300], Step [79/172], Loss: 36.6984\n",
      "Epoch [187/300], Step [80/172], Loss: 49.6810\n",
      "Epoch [187/300], Step [81/172], Loss: 33.6337\n",
      "Epoch [187/300], Step [82/172], Loss: 35.1466\n",
      "Epoch [187/300], Step [83/172], Loss: 43.1762\n",
      "Epoch [187/300], Step [84/172], Loss: 32.5528\n",
      "Epoch [187/300], Step [85/172], Loss: 37.9691\n",
      "Epoch [187/300], Step [86/172], Loss: 32.3049\n",
      "Epoch [187/300], Step [87/172], Loss: 25.7903\n",
      "Epoch [187/300], Step [88/172], Loss: 24.2775\n",
      "Epoch [187/300], Step [89/172], Loss: 26.4276\n",
      "Epoch [187/300], Step [90/172], Loss: 20.6371\n",
      "Epoch [187/300], Step [91/172], Loss: 26.1692\n",
      "Epoch [187/300], Step [92/172], Loss: 19.7659\n",
      "Epoch [187/300], Step [93/172], Loss: 20.4248\n",
      "Epoch [187/300], Step [94/172], Loss: 28.5784\n",
      "Epoch [187/300], Step [95/172], Loss: 20.5417\n",
      "Epoch [187/300], Step [96/172], Loss: 19.8336\n",
      "Epoch [187/300], Step [97/172], Loss: 27.9222\n",
      "Epoch [187/300], Step [98/172], Loss: 19.1826\n",
      "Epoch [187/300], Step [99/172], Loss: 18.8746\n",
      "Epoch [187/300], Step [100/172], Loss: 15.3651\n",
      "Epoch [187/300], Step [101/172], Loss: 18.6642\n",
      "Epoch [187/300], Step [102/172], Loss: 16.7929\n",
      "Epoch [187/300], Step [103/172], Loss: 12.3844\n",
      "Epoch [187/300], Step [104/172], Loss: 18.4028\n",
      "Epoch [187/300], Step [105/172], Loss: 19.0132\n",
      "Epoch [187/300], Step [106/172], Loss: 15.4546\n",
      "Epoch [187/300], Step [107/172], Loss: 15.4925\n",
      "Epoch [187/300], Step [108/172], Loss: 14.4832\n",
      "Epoch [187/300], Step [109/172], Loss: 14.4475\n",
      "Epoch [187/300], Step [110/172], Loss: 15.6980\n",
      "Epoch [187/300], Step [111/172], Loss: 15.3653\n",
      "Epoch [187/300], Step [112/172], Loss: 17.2902\n",
      "Epoch [187/300], Step [113/172], Loss: 11.7398\n",
      "Epoch [187/300], Step [114/172], Loss: 13.6630\n",
      "Epoch [187/300], Step [115/172], Loss: 19.0221\n",
      "Epoch [187/300], Step [116/172], Loss: 14.3438\n",
      "Epoch [187/300], Step [117/172], Loss: 11.4576\n",
      "Epoch [187/300], Step [118/172], Loss: 13.9880\n",
      "Epoch [187/300], Step [119/172], Loss: 15.7444\n",
      "Epoch [187/300], Step [120/172], Loss: 9.9804\n",
      "Epoch [187/300], Step [121/172], Loss: 9.3343\n",
      "Epoch [187/300], Step [122/172], Loss: 10.6010\n",
      "Epoch [187/300], Step [123/172], Loss: 10.1107\n",
      "Epoch [187/300], Step [124/172], Loss: 7.3833\n",
      "Epoch [187/300], Step [125/172], Loss: 11.5061\n",
      "Epoch [187/300], Step [126/172], Loss: 10.9299\n",
      "Epoch [187/300], Step [127/172], Loss: 10.6205\n",
      "Epoch [187/300], Step [128/172], Loss: 10.1209\n",
      "Epoch [187/300], Step [129/172], Loss: 7.8352\n",
      "Epoch [187/300], Step [130/172], Loss: 12.2410\n",
      "Epoch [187/300], Step [131/172], Loss: 7.1301\n",
      "Epoch [187/300], Step [132/172], Loss: 8.2184\n",
      "Epoch [187/300], Step [133/172], Loss: 8.9271\n",
      "Epoch [187/300], Step [134/172], Loss: 11.1910\n",
      "Epoch [187/300], Step [135/172], Loss: 8.3584\n",
      "Epoch [187/300], Step [136/172], Loss: 8.0181\n",
      "Epoch [187/300], Step [137/172], Loss: 9.0599\n",
      "Epoch [187/300], Step [138/172], Loss: 6.7059\n",
      "Epoch [187/300], Step [139/172], Loss: 9.3814\n",
      "Epoch [187/300], Step [140/172], Loss: 9.4620\n",
      "Epoch [187/300], Step [141/172], Loss: 9.1540\n",
      "Epoch [187/300], Step [142/172], Loss: 13.9000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [187/300], Step [143/172], Loss: 10.4824\n",
      "Epoch [187/300], Step [144/172], Loss: 8.8541\n",
      "Epoch [187/300], Step [145/172], Loss: 9.7668\n",
      "Epoch [187/300], Step [146/172], Loss: 9.1574\n",
      "Epoch [187/300], Step [147/172], Loss: 5.0855\n",
      "Epoch [187/300], Step [148/172], Loss: 5.8270\n",
      "Epoch [187/300], Step [149/172], Loss: 6.4140\n",
      "Epoch [187/300], Step [150/172], Loss: 5.7778\n",
      "Epoch [187/300], Step [151/172], Loss: 5.2377\n",
      "Epoch [187/300], Step [152/172], Loss: 7.1621\n",
      "Epoch [187/300], Step [153/172], Loss: 6.1917\n",
      "Epoch [187/300], Step [154/172], Loss: 7.1987\n",
      "Epoch [187/300], Step [155/172], Loss: 5.9096\n",
      "Epoch [187/300], Step [156/172], Loss: 13.0926\n",
      "Epoch [187/300], Step [157/172], Loss: 9.1978\n",
      "Epoch [187/300], Step [158/172], Loss: 6.9020\n",
      "Epoch [187/300], Step [159/172], Loss: 9.2019\n",
      "Epoch [187/300], Step [160/172], Loss: 9.8513\n",
      "Epoch [187/300], Step [161/172], Loss: 6.8320\n",
      "Epoch [187/300], Step [162/172], Loss: 5.0100\n",
      "Epoch [187/300], Step [163/172], Loss: 6.2794\n",
      "Epoch [187/300], Step [164/172], Loss: 8.2954\n",
      "Epoch [187/300], Step [165/172], Loss: 5.9902\n",
      "Epoch [187/300], Step [166/172], Loss: 5.3687\n",
      "Epoch [187/300], Step [167/172], Loss: 10.3156\n",
      "Epoch [187/300], Step [168/172], Loss: 6.1977\n",
      "Epoch [187/300], Step [169/172], Loss: 6.4751\n",
      "Epoch [187/300], Step [170/172], Loss: 4.5564\n",
      "Epoch [187/300], Step [171/172], Loss: 7.8154\n",
      "Epoch [187/300], Step [172/172], Loss: 5.1089\n",
      "Epoch [188/300], Step [1/172], Loss: 52.4298\n",
      "Epoch [188/300], Step [2/172], Loss: 54.1128\n",
      "Epoch [188/300], Step [3/172], Loss: 47.4869\n",
      "Epoch [188/300], Step [4/172], Loss: 27.4712\n",
      "Epoch [188/300], Step [5/172], Loss: 46.5343\n",
      "Epoch [188/300], Step [6/172], Loss: 18.7618\n",
      "Epoch [188/300], Step [7/172], Loss: 28.9918\n",
      "Epoch [188/300], Step [8/172], Loss: 5.0994\n",
      "Epoch [188/300], Step [9/172], Loss: 32.4551\n",
      "Epoch [188/300], Step [10/172], Loss: 42.2793\n",
      "Epoch [188/300], Step [11/172], Loss: 58.6926\n",
      "Epoch [188/300], Step [12/172], Loss: 65.3313\n",
      "Epoch [188/300], Step [13/172], Loss: 34.3432\n",
      "Epoch [188/300], Step [14/172], Loss: 61.2132\n",
      "Epoch [188/300], Step [15/172], Loss: 55.7705\n",
      "Epoch [188/300], Step [16/172], Loss: 9.8857\n",
      "Epoch [188/300], Step [17/172], Loss: 44.5697\n",
      "Epoch [188/300], Step [18/172], Loss: 58.7183\n",
      "Epoch [188/300], Step [19/172], Loss: 81.8848\n",
      "Epoch [188/300], Step [20/172], Loss: 36.0216\n",
      "Epoch [188/300], Step [21/172], Loss: 84.2125\n",
      "Epoch [188/300], Step [22/172], Loss: 59.8876\n",
      "Epoch [188/300], Step [23/172], Loss: 1.9188\n",
      "Epoch [188/300], Step [24/172], Loss: 55.7750\n",
      "Epoch [188/300], Step [25/172], Loss: 40.7398\n",
      "Epoch [188/300], Step [26/172], Loss: 47.4692\n",
      "Epoch [188/300], Step [27/172], Loss: 61.4835\n",
      "Epoch [188/300], Step [28/172], Loss: 24.2471\n",
      "Epoch [188/300], Step [29/172], Loss: 16.9617\n",
      "Epoch [188/300], Step [30/172], Loss: 62.8679\n",
      "Epoch [188/300], Step [31/172], Loss: 36.6188\n",
      "Epoch [188/300], Step [32/172], Loss: 42.5711\n",
      "Epoch [188/300], Step [33/172], Loss: 70.0400\n",
      "Epoch [188/300], Step [34/172], Loss: 2.7845\n",
      "Epoch [188/300], Step [35/172], Loss: 13.8943\n",
      "Epoch [188/300], Step [36/172], Loss: 18.0885\n",
      "Epoch [188/300], Step [37/172], Loss: 16.6696\n",
      "Epoch [188/300], Step [38/172], Loss: 29.6820\n",
      "Epoch [188/300], Step [39/172], Loss: 35.8850\n",
      "Epoch [188/300], Step [40/172], Loss: 20.3271\n",
      "Epoch [188/300], Step [41/172], Loss: 33.4917\n",
      "Epoch [188/300], Step [42/172], Loss: 38.1266\n",
      "Epoch [188/300], Step [43/172], Loss: 27.0584\n",
      "Epoch [188/300], Step [44/172], Loss: 20.9601\n",
      "Epoch [188/300], Step [45/172], Loss: 26.7649\n",
      "Epoch [188/300], Step [46/172], Loss: 17.6447\n",
      "Epoch [188/300], Step [47/172], Loss: 47.5864\n",
      "Epoch [188/300], Step [48/172], Loss: 61.5685\n",
      "Epoch [188/300], Step [49/172], Loss: 21.0597\n",
      "Epoch [188/300], Step [50/172], Loss: 47.4312\n",
      "Epoch [188/300], Step [51/172], Loss: 8.5438\n",
      "Epoch [188/300], Step [52/172], Loss: 19.3062\n",
      "Epoch [188/300], Step [53/172], Loss: 22.1327\n",
      "Epoch [188/300], Step [54/172], Loss: 14.5055\n",
      "Epoch [188/300], Step [55/172], Loss: 14.2324\n",
      "Epoch [188/300], Step [56/172], Loss: 17.5397\n",
      "Epoch [188/300], Step [57/172], Loss: 16.8639\n",
      "Epoch [188/300], Step [58/172], Loss: 13.0252\n",
      "Epoch [188/300], Step [59/172], Loss: 27.1502\n",
      "Epoch [188/300], Step [60/172], Loss: 25.3316\n",
      "Epoch [188/300], Step [61/172], Loss: 6.1322\n",
      "Epoch [188/300], Step [62/172], Loss: 19.3444\n",
      "Epoch [188/300], Step [63/172], Loss: 10.1420\n",
      "Epoch [188/300], Step [64/172], Loss: 10.4591\n",
      "Epoch [188/300], Step [65/172], Loss: 19.5791\n",
      "Epoch [188/300], Step [66/172], Loss: 6.3386\n",
      "Epoch [188/300], Step [67/172], Loss: 22.7134\n",
      "Epoch [188/300], Step [68/172], Loss: 4.7433\n",
      "Epoch [188/300], Step [69/172], Loss: 34.6633\n",
      "Epoch [188/300], Step [70/172], Loss: 38.5329\n",
      "Epoch [188/300], Step [71/172], Loss: 39.9538\n",
      "Epoch [188/300], Step [72/172], Loss: 40.4750\n",
      "Epoch [188/300], Step [73/172], Loss: 49.1853\n",
      "Epoch [188/300], Step [74/172], Loss: 25.5363\n",
      "Epoch [188/300], Step [75/172], Loss: 26.7349\n",
      "Epoch [188/300], Step [76/172], Loss: 28.3479\n",
      "Epoch [188/300], Step [77/172], Loss: 48.4830\n",
      "Epoch [188/300], Step [78/172], Loss: 37.5784\n",
      "Epoch [188/300], Step [79/172], Loss: 36.7790\n",
      "Epoch [188/300], Step [80/172], Loss: 50.2751\n",
      "Epoch [188/300], Step [81/172], Loss: 33.6498\n",
      "Epoch [188/300], Step [82/172], Loss: 35.8327\n",
      "Epoch [188/300], Step [83/172], Loss: 43.3171\n",
      "Epoch [188/300], Step [84/172], Loss: 32.4172\n",
      "Epoch [188/300], Step [85/172], Loss: 37.8841\n",
      "Epoch [188/300], Step [86/172], Loss: 32.3634\n",
      "Epoch [188/300], Step [87/172], Loss: 25.7161\n",
      "Epoch [188/300], Step [88/172], Loss: 24.2370\n",
      "Epoch [188/300], Step [89/172], Loss: 26.4300\n",
      "Epoch [188/300], Step [90/172], Loss: 20.5183\n",
      "Epoch [188/300], Step [91/172], Loss: 26.0408\n",
      "Epoch [188/300], Step [92/172], Loss: 19.7047\n",
      "Epoch [188/300], Step [93/172], Loss: 20.4249\n",
      "Epoch [188/300], Step [94/172], Loss: 28.6444\n",
      "Epoch [188/300], Step [95/172], Loss: 20.4153\n",
      "Epoch [188/300], Step [96/172], Loss: 19.7911\n",
      "Epoch [188/300], Step [97/172], Loss: 27.9100\n",
      "Epoch [188/300], Step [98/172], Loss: 19.1404\n",
      "Epoch [188/300], Step [99/172], Loss: 18.8505\n",
      "Epoch [188/300], Step [100/172], Loss: 15.3096\n",
      "Epoch [188/300], Step [101/172], Loss: 18.6412\n",
      "Epoch [188/300], Step [102/172], Loss: 16.8586\n",
      "Epoch [188/300], Step [103/172], Loss: 12.3858\n",
      "Epoch [188/300], Step [104/172], Loss: 18.3376\n",
      "Epoch [188/300], Step [105/172], Loss: 19.1628\n",
      "Epoch [188/300], Step [106/172], Loss: 15.4222\n",
      "Epoch [188/300], Step [107/172], Loss: 15.4381\n",
      "Epoch [188/300], Step [108/172], Loss: 14.3705\n",
      "Epoch [188/300], Step [109/172], Loss: 14.3855\n",
      "Epoch [188/300], Step [110/172], Loss: 15.6899\n",
      "Epoch [188/300], Step [111/172], Loss: 15.3774\n",
      "Epoch [188/300], Step [112/172], Loss: 17.2695\n",
      "Epoch [188/300], Step [113/172], Loss: 11.6414\n",
      "Epoch [188/300], Step [114/172], Loss: 13.6951\n",
      "Epoch [188/300], Step [115/172], Loss: 18.9815\n",
      "Epoch [188/300], Step [116/172], Loss: 14.3140\n",
      "Epoch [188/300], Step [117/172], Loss: 11.4847\n",
      "Epoch [188/300], Step [118/172], Loss: 14.0317\n",
      "Epoch [188/300], Step [119/172], Loss: 15.6664\n",
      "Epoch [188/300], Step [120/172], Loss: 9.9911\n",
      "Epoch [188/300], Step [121/172], Loss: 9.2955\n",
      "Epoch [188/300], Step [122/172], Loss: 10.5307\n",
      "Epoch [188/300], Step [123/172], Loss: 10.0445\n",
      "Epoch [188/300], Step [124/172], Loss: 7.3240\n",
      "Epoch [188/300], Step [125/172], Loss: 11.3807\n",
      "Epoch [188/300], Step [126/172], Loss: 10.8667\n",
      "Epoch [188/300], Step [127/172], Loss: 10.6268\n",
      "Epoch [188/300], Step [128/172], Loss: 10.1461\n",
      "Epoch [188/300], Step [129/172], Loss: 7.8392\n",
      "Epoch [188/300], Step [130/172], Loss: 12.2334\n",
      "Epoch [188/300], Step [131/172], Loss: 7.1160\n",
      "Epoch [188/300], Step [132/172], Loss: 8.2112\n",
      "Epoch [188/300], Step [133/172], Loss: 8.9195\n",
      "Epoch [188/300], Step [134/172], Loss: 11.0805\n",
      "Epoch [188/300], Step [135/172], Loss: 8.2726\n",
      "Epoch [188/300], Step [136/172], Loss: 7.8902\n",
      "Epoch [188/300], Step [137/172], Loss: 8.9616\n",
      "Epoch [188/300], Step [138/172], Loss: 6.6440\n",
      "Epoch [188/300], Step [139/172], Loss: 9.2390\n",
      "Epoch [188/300], Step [140/172], Loss: 9.4589\n",
      "Epoch [188/300], Step [141/172], Loss: 9.0759\n",
      "Epoch [188/300], Step [142/172], Loss: 13.8653\n",
      "Epoch [188/300], Step [143/172], Loss: 10.4659\n",
      "Epoch [188/300], Step [144/172], Loss: 8.7920\n",
      "Epoch [188/300], Step [145/172], Loss: 9.7485\n",
      "Epoch [188/300], Step [146/172], Loss: 9.1506\n",
      "Epoch [188/300], Step [147/172], Loss: 5.0411\n",
      "Epoch [188/300], Step [148/172], Loss: 5.8262\n",
      "Epoch [188/300], Step [149/172], Loss: 6.3573\n",
      "Epoch [188/300], Step [150/172], Loss: 5.6995\n",
      "Epoch [188/300], Step [151/172], Loss: 5.2360\n",
      "Epoch [188/300], Step [152/172], Loss: 7.1061\n",
      "Epoch [188/300], Step [153/172], Loss: 6.1121\n",
      "Epoch [188/300], Step [154/172], Loss: 7.1419\n",
      "Epoch [188/300], Step [155/172], Loss: 5.7699\n",
      "Epoch [188/300], Step [156/172], Loss: 13.0792\n",
      "Epoch [188/300], Step [157/172], Loss: 9.1289\n",
      "Epoch [188/300], Step [158/172], Loss: 6.8829\n",
      "Epoch [188/300], Step [159/172], Loss: 9.0795\n",
      "Epoch [188/300], Step [160/172], Loss: 9.7424\n",
      "Epoch [188/300], Step [161/172], Loss: 6.7739\n",
      "Epoch [188/300], Step [162/172], Loss: 4.9632\n",
      "Epoch [188/300], Step [163/172], Loss: 6.2434\n",
      "Epoch [188/300], Step [164/172], Loss: 8.4663\n",
      "Epoch [188/300], Step [165/172], Loss: 5.9688\n",
      "Epoch [188/300], Step [166/172], Loss: 5.3652\n",
      "Epoch [188/300], Step [167/172], Loss: 10.2149\n",
      "Epoch [188/300], Step [168/172], Loss: 6.1309\n",
      "Epoch [188/300], Step [169/172], Loss: 6.4866\n",
      "Epoch [188/300], Step [170/172], Loss: 4.5529\n",
      "Epoch [188/300], Step [171/172], Loss: 7.7573\n",
      "Epoch [188/300], Step [172/172], Loss: 4.9859\n",
      "Epoch [189/300], Step [1/172], Loss: 52.1826\n",
      "Epoch [189/300], Step [2/172], Loss: 54.0901\n",
      "Epoch [189/300], Step [3/172], Loss: 48.0889\n",
      "Epoch [189/300], Step [4/172], Loss: 27.3608\n",
      "Epoch [189/300], Step [5/172], Loss: 46.4756\n",
      "Epoch [189/300], Step [6/172], Loss: 18.8390\n",
      "Epoch [189/300], Step [7/172], Loss: 28.8394\n",
      "Epoch [189/300], Step [8/172], Loss: 4.2477\n",
      "Epoch [189/300], Step [9/172], Loss: 32.1593\n",
      "Epoch [189/300], Step [10/172], Loss: 42.2789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [189/300], Step [11/172], Loss: 58.6534\n",
      "Epoch [189/300], Step [12/172], Loss: 65.1699\n",
      "Epoch [189/300], Step [13/172], Loss: 34.2357\n",
      "Epoch [189/300], Step [14/172], Loss: 61.0392\n",
      "Epoch [189/300], Step [15/172], Loss: 55.7785\n",
      "Epoch [189/300], Step [16/172], Loss: 10.6760\n",
      "Epoch [189/300], Step [17/172], Loss: 44.6033\n",
      "Epoch [189/300], Step [18/172], Loss: 58.6543\n",
      "Epoch [189/300], Step [19/172], Loss: 82.2731\n",
      "Epoch [189/300], Step [20/172], Loss: 35.6951\n",
      "Epoch [189/300], Step [21/172], Loss: 84.6581\n",
      "Epoch [189/300], Step [22/172], Loss: 60.0951\n",
      "Epoch [189/300], Step [23/172], Loss: 1.8957\n",
      "Epoch [189/300], Step [24/172], Loss: 55.7078\n",
      "Epoch [189/300], Step [25/172], Loss: 40.6120\n",
      "Epoch [189/300], Step [26/172], Loss: 47.3781\n",
      "Epoch [189/300], Step [27/172], Loss: 61.1356\n",
      "Epoch [189/300], Step [28/172], Loss: 24.1860\n",
      "Epoch [189/300], Step [29/172], Loss: 16.6953\n",
      "Epoch [189/300], Step [30/172], Loss: 62.8427\n",
      "Epoch [189/300], Step [31/172], Loss: 36.8459\n",
      "Epoch [189/300], Step [32/172], Loss: 42.7837\n",
      "Epoch [189/300], Step [33/172], Loss: 70.1893\n",
      "Epoch [189/300], Step [34/172], Loss: 2.6909\n",
      "Epoch [189/300], Step [35/172], Loss: 13.8615\n",
      "Epoch [189/300], Step [36/172], Loss: 17.7513\n",
      "Epoch [189/300], Step [37/172], Loss: 16.6206\n",
      "Epoch [189/300], Step [38/172], Loss: 29.5143\n",
      "Epoch [189/300], Step [39/172], Loss: 35.8584\n",
      "Epoch [189/300], Step [40/172], Loss: 20.4348\n",
      "Epoch [189/300], Step [41/172], Loss: 33.5159\n",
      "Epoch [189/300], Step [42/172], Loss: 38.0745\n",
      "Epoch [189/300], Step [43/172], Loss: 27.1243\n",
      "Epoch [189/300], Step [44/172], Loss: 20.8089\n",
      "Epoch [189/300], Step [45/172], Loss: 26.7591\n",
      "Epoch [189/300], Step [46/172], Loss: 17.4460\n",
      "Epoch [189/300], Step [47/172], Loss: 47.4171\n",
      "Epoch [189/300], Step [48/172], Loss: 61.0007\n",
      "Epoch [189/300], Step [49/172], Loss: 20.9751\n",
      "Epoch [189/300], Step [50/172], Loss: 47.3182\n",
      "Epoch [189/300], Step [51/172], Loss: 8.4643\n",
      "Epoch [189/300], Step [52/172], Loss: 19.3040\n",
      "Epoch [189/300], Step [53/172], Loss: 22.1344\n",
      "Epoch [189/300], Step [54/172], Loss: 14.4693\n",
      "Epoch [189/300], Step [55/172], Loss: 14.2139\n",
      "Epoch [189/300], Step [56/172], Loss: 18.0372\n",
      "Epoch [189/300], Step [57/172], Loss: 16.8126\n",
      "Epoch [189/300], Step [58/172], Loss: 13.0218\n",
      "Epoch [189/300], Step [59/172], Loss: 27.0516\n",
      "Epoch [189/300], Step [60/172], Loss: 25.1210\n",
      "Epoch [189/300], Step [61/172], Loss: 6.0948\n",
      "Epoch [189/300], Step [62/172], Loss: 19.4003\n",
      "Epoch [189/300], Step [63/172], Loss: 10.0719\n",
      "Epoch [189/300], Step [64/172], Loss: 10.6060\n",
      "Epoch [189/300], Step [65/172], Loss: 19.5874\n",
      "Epoch [189/300], Step [66/172], Loss: 6.3575\n",
      "Epoch [189/300], Step [67/172], Loss: 22.7195\n",
      "Epoch [189/300], Step [68/172], Loss: 4.9978\n",
      "Epoch [189/300], Step [69/172], Loss: 34.3339\n",
      "Epoch [189/300], Step [70/172], Loss: 38.0759\n",
      "Epoch [189/300], Step [71/172], Loss: 39.7876\n",
      "Epoch [189/300], Step [72/172], Loss: 39.9513\n",
      "Epoch [189/300], Step [73/172], Loss: 48.8834\n",
      "Epoch [189/300], Step [74/172], Loss: 25.2467\n",
      "Epoch [189/300], Step [75/172], Loss: 26.7300\n",
      "Epoch [189/300], Step [76/172], Loss: 28.3290\n",
      "Epoch [189/300], Step [77/172], Loss: 48.2071\n",
      "Epoch [189/300], Step [78/172], Loss: 37.1808\n",
      "Epoch [189/300], Step [79/172], Loss: 36.6013\n",
      "Epoch [189/300], Step [80/172], Loss: 49.7231\n",
      "Epoch [189/300], Step [81/172], Loss: 33.5588\n",
      "Epoch [189/300], Step [82/172], Loss: 35.2347\n",
      "Epoch [189/300], Step [83/172], Loss: 43.0965\n",
      "Epoch [189/300], Step [84/172], Loss: 32.5439\n",
      "Epoch [189/300], Step [85/172], Loss: 37.9915\n",
      "Epoch [189/300], Step [86/172], Loss: 32.3209\n",
      "Epoch [189/300], Step [87/172], Loss: 25.6898\n",
      "Epoch [189/300], Step [88/172], Loss: 24.0595\n",
      "Epoch [189/300], Step [89/172], Loss: 26.5317\n",
      "Epoch [189/300], Step [90/172], Loss: 20.5261\n",
      "Epoch [189/300], Step [91/172], Loss: 26.0668\n",
      "Epoch [189/300], Step [92/172], Loss: 19.6876\n",
      "Epoch [189/300], Step [93/172], Loss: 20.3440\n",
      "Epoch [189/300], Step [94/172], Loss: 28.4770\n",
      "Epoch [189/300], Step [95/172], Loss: 20.4400\n",
      "Epoch [189/300], Step [96/172], Loss: 19.7920\n",
      "Epoch [189/300], Step [97/172], Loss: 27.8948\n",
      "Epoch [189/300], Step [98/172], Loss: 19.1167\n",
      "Epoch [189/300], Step [99/172], Loss: 18.8430\n",
      "Epoch [189/300], Step [100/172], Loss: 15.3208\n",
      "Epoch [189/300], Step [101/172], Loss: 18.6799\n",
      "Epoch [189/300], Step [102/172], Loss: 16.7650\n",
      "Epoch [189/300], Step [103/172], Loss: 12.3611\n",
      "Epoch [189/300], Step [104/172], Loss: 18.3526\n",
      "Epoch [189/300], Step [105/172], Loss: 19.1055\n",
      "Epoch [189/300], Step [106/172], Loss: 15.4047\n",
      "Epoch [189/300], Step [107/172], Loss: 15.4518\n",
      "Epoch [189/300], Step [108/172], Loss: 14.3971\n",
      "Epoch [189/300], Step [109/172], Loss: 14.3742\n",
      "Epoch [189/300], Step [110/172], Loss: 15.6687\n",
      "Epoch [189/300], Step [111/172], Loss: 15.3968\n",
      "Epoch [189/300], Step [112/172], Loss: 17.2522\n",
      "Epoch [189/300], Step [113/172], Loss: 11.7040\n",
      "Epoch [189/300], Step [114/172], Loss: 13.6966\n",
      "Epoch [189/300], Step [115/172], Loss: 18.9414\n",
      "Epoch [189/300], Step [116/172], Loss: 14.2814\n",
      "Epoch [189/300], Step [117/172], Loss: 11.5155\n",
      "Epoch [189/300], Step [118/172], Loss: 13.9292\n",
      "Epoch [189/300], Step [119/172], Loss: 15.7131\n",
      "Epoch [189/300], Step [120/172], Loss: 9.9949\n",
      "Epoch [189/300], Step [121/172], Loss: 9.2792\n",
      "Epoch [189/300], Step [122/172], Loss: 10.5821\n",
      "Epoch [189/300], Step [123/172], Loss: 10.0889\n",
      "Epoch [189/300], Step [124/172], Loss: 7.3165\n",
      "Epoch [189/300], Step [125/172], Loss: 11.3595\n",
      "Epoch [189/300], Step [126/172], Loss: 10.8995\n",
      "Epoch [189/300], Step [127/172], Loss: 10.5724\n",
      "Epoch [189/300], Step [128/172], Loss: 10.1022\n",
      "Epoch [189/300], Step [129/172], Loss: 7.8476\n",
      "Epoch [189/300], Step [130/172], Loss: 12.2511\n",
      "Epoch [189/300], Step [131/172], Loss: 7.1142\n",
      "Epoch [189/300], Step [132/172], Loss: 8.2419\n",
      "Epoch [189/300], Step [133/172], Loss: 8.9063\n",
      "Epoch [189/300], Step [134/172], Loss: 11.1163\n",
      "Epoch [189/300], Step [135/172], Loss: 8.3125\n",
      "Epoch [189/300], Step [136/172], Loss: 7.9938\n",
      "Epoch [189/300], Step [137/172], Loss: 8.9836\n",
      "Epoch [189/300], Step [138/172], Loss: 6.6805\n",
      "Epoch [189/300], Step [139/172], Loss: 9.2616\n",
      "Epoch [189/300], Step [140/172], Loss: 9.4854\n",
      "Epoch [189/300], Step [141/172], Loss: 9.0520\n",
      "Epoch [189/300], Step [142/172], Loss: 13.8500\n",
      "Epoch [189/300], Step [143/172], Loss: 10.4778\n",
      "Epoch [189/300], Step [144/172], Loss: 8.7996\n",
      "Epoch [189/300], Step [145/172], Loss: 9.7910\n",
      "Epoch [189/300], Step [146/172], Loss: 9.1301\n",
      "Epoch [189/300], Step [147/172], Loss: 5.0552\n",
      "Epoch [189/300], Step [148/172], Loss: 5.8232\n",
      "Epoch [189/300], Step [149/172], Loss: 6.3700\n",
      "Epoch [189/300], Step [150/172], Loss: 5.7415\n",
      "Epoch [189/300], Step [151/172], Loss: 5.2250\n",
      "Epoch [189/300], Step [152/172], Loss: 7.1502\n",
      "Epoch [189/300], Step [153/172], Loss: 6.1507\n",
      "Epoch [189/300], Step [154/172], Loss: 7.1698\n",
      "Epoch [189/300], Step [155/172], Loss: 5.8050\n",
      "Epoch [189/300], Step [156/172], Loss: 13.0498\n",
      "Epoch [189/300], Step [157/172], Loss: 9.1161\n",
      "Epoch [189/300], Step [158/172], Loss: 6.8632\n",
      "Epoch [189/300], Step [159/172], Loss: 9.1903\n",
      "Epoch [189/300], Step [160/172], Loss: 9.7590\n",
      "Epoch [189/300], Step [161/172], Loss: 6.7846\n",
      "Epoch [189/300], Step [162/172], Loss: 4.9643\n",
      "Epoch [189/300], Step [163/172], Loss: 6.3033\n",
      "Epoch [189/300], Step [164/172], Loss: 8.3784\n",
      "Epoch [189/300], Step [165/172], Loss: 5.9855\n",
      "Epoch [189/300], Step [166/172], Loss: 5.3756\n",
      "Epoch [189/300], Step [167/172], Loss: 10.2734\n",
      "Epoch [189/300], Step [168/172], Loss: 6.1567\n",
      "Epoch [189/300], Step [169/172], Loss: 6.4422\n",
      "Epoch [189/300], Step [170/172], Loss: 4.5496\n",
      "Epoch [189/300], Step [171/172], Loss: 7.8262\n",
      "Epoch [189/300], Step [172/172], Loss: 5.0805\n",
      "Epoch [190/300], Step [1/172], Loss: 52.0379\n",
      "Epoch [190/300], Step [2/172], Loss: 53.6983\n",
      "Epoch [190/300], Step [3/172], Loss: 47.2353\n",
      "Epoch [190/300], Step [4/172], Loss: 27.3142\n",
      "Epoch [190/300], Step [5/172], Loss: 45.9987\n",
      "Epoch [190/300], Step [6/172], Loss: 19.0455\n",
      "Epoch [190/300], Step [7/172], Loss: 29.3070\n",
      "Epoch [190/300], Step [8/172], Loss: 4.5571\n",
      "Epoch [190/300], Step [9/172], Loss: 32.1275\n",
      "Epoch [190/300], Step [10/172], Loss: 42.1623\n",
      "Epoch [190/300], Step [11/172], Loss: 58.3828\n",
      "Epoch [190/300], Step [12/172], Loss: 64.7416\n",
      "Epoch [190/300], Step [13/172], Loss: 34.4671\n",
      "Epoch [190/300], Step [14/172], Loss: 61.0211\n",
      "Epoch [190/300], Step [15/172], Loss: 55.4504\n",
      "Epoch [190/300], Step [16/172], Loss: 10.1176\n",
      "Epoch [190/300], Step [17/172], Loss: 44.4495\n",
      "Epoch [190/300], Step [18/172], Loss: 58.3006\n",
      "Epoch [190/300], Step [19/172], Loss: 81.9094\n",
      "Epoch [190/300], Step [20/172], Loss: 35.5014\n",
      "Epoch [190/300], Step [21/172], Loss: 84.5301\n",
      "Epoch [190/300], Step [22/172], Loss: 59.7465\n",
      "Epoch [190/300], Step [23/172], Loss: 1.9125\n",
      "Epoch [190/300], Step [24/172], Loss: 55.6489\n",
      "Epoch [190/300], Step [25/172], Loss: 40.5749\n",
      "Epoch [190/300], Step [26/172], Loss: 47.4437\n",
      "Epoch [190/300], Step [27/172], Loss: 60.9904\n",
      "Epoch [190/300], Step [28/172], Loss: 24.2384\n",
      "Epoch [190/300], Step [29/172], Loss: 16.8975\n",
      "Epoch [190/300], Step [30/172], Loss: 62.8136\n",
      "Epoch [190/300], Step [31/172], Loss: 36.8560\n",
      "Epoch [190/300], Step [32/172], Loss: 42.9247\n",
      "Epoch [190/300], Step [33/172], Loss: 70.2922\n",
      "Epoch [190/300], Step [34/172], Loss: 2.6993\n",
      "Epoch [190/300], Step [35/172], Loss: 13.8783\n",
      "Epoch [190/300], Step [36/172], Loss: 17.9511\n",
      "Epoch [190/300], Step [37/172], Loss: 16.5924\n",
      "Epoch [190/300], Step [38/172], Loss: 29.4658\n",
      "Epoch [190/300], Step [39/172], Loss: 35.8932\n",
      "Epoch [190/300], Step [40/172], Loss: 20.5317\n",
      "Epoch [190/300], Step [41/172], Loss: 33.5952\n",
      "Epoch [190/300], Step [42/172], Loss: 38.0088\n",
      "Epoch [190/300], Step [43/172], Loss: 27.0777\n",
      "Epoch [190/300], Step [44/172], Loss: 20.8624\n",
      "Epoch [190/300], Step [45/172], Loss: 26.8477\n",
      "Epoch [190/300], Step [46/172], Loss: 17.4743\n",
      "Epoch [190/300], Step [47/172], Loss: 47.4647\n",
      "Epoch [190/300], Step [48/172], Loss: 61.5161\n",
      "Epoch [190/300], Step [49/172], Loss: 21.0517\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/300], Step [50/172], Loss: 47.2264\n",
      "Epoch [190/300], Step [51/172], Loss: 8.4253\n",
      "Epoch [190/300], Step [52/172], Loss: 19.2534\n",
      "Epoch [190/300], Step [53/172], Loss: 22.0523\n",
      "Epoch [190/300], Step [54/172], Loss: 14.4028\n",
      "Epoch [190/300], Step [55/172], Loss: 14.1310\n",
      "Epoch [190/300], Step [56/172], Loss: 17.6050\n",
      "Epoch [190/300], Step [57/172], Loss: 16.9226\n",
      "Epoch [190/300], Step [58/172], Loss: 13.0293\n",
      "Epoch [190/300], Step [59/172], Loss: 27.3394\n",
      "Epoch [190/300], Step [60/172], Loss: 25.0975\n",
      "Epoch [190/300], Step [61/172], Loss: 6.1246\n",
      "Epoch [190/300], Step [62/172], Loss: 19.1982\n",
      "Epoch [190/300], Step [63/172], Loss: 9.8939\n",
      "Epoch [190/300], Step [64/172], Loss: 10.4715\n",
      "Epoch [190/300], Step [65/172], Loss: 19.4600\n",
      "Epoch [190/300], Step [66/172], Loss: 6.3411\n",
      "Epoch [190/300], Step [67/172], Loss: 22.5550\n",
      "Epoch [190/300], Step [68/172], Loss: 4.9606\n",
      "Epoch [190/300], Step [69/172], Loss: 34.2841\n",
      "Epoch [190/300], Step [70/172], Loss: 38.0765\n",
      "Epoch [190/300], Step [71/172], Loss: 39.7867\n",
      "Epoch [190/300], Step [72/172], Loss: 39.8675\n",
      "Epoch [190/300], Step [73/172], Loss: 49.0072\n",
      "Epoch [190/300], Step [74/172], Loss: 25.2606\n",
      "Epoch [190/300], Step [75/172], Loss: 26.7944\n",
      "Epoch [190/300], Step [76/172], Loss: 28.2615\n",
      "Epoch [190/300], Step [77/172], Loss: 48.0925\n",
      "Epoch [190/300], Step [78/172], Loss: 37.1310\n",
      "Epoch [190/300], Step [79/172], Loss: 36.3302\n",
      "Epoch [190/300], Step [80/172], Loss: 49.6363\n",
      "Epoch [190/300], Step [81/172], Loss: 33.3774\n",
      "Epoch [190/300], Step [82/172], Loss: 35.2541\n",
      "Epoch [190/300], Step [83/172], Loss: 42.9444\n",
      "Epoch [190/300], Step [84/172], Loss: 32.3048\n",
      "Epoch [190/300], Step [85/172], Loss: 37.8438\n",
      "Epoch [190/300], Step [86/172], Loss: 32.1830\n",
      "Epoch [190/300], Step [87/172], Loss: 25.6356\n",
      "Epoch [190/300], Step [88/172], Loss: 23.9068\n",
      "Epoch [190/300], Step [89/172], Loss: 26.3489\n",
      "Epoch [190/300], Step [90/172], Loss: 20.3813\n",
      "Epoch [190/300], Step [91/172], Loss: 25.9392\n",
      "Epoch [190/300], Step [92/172], Loss: 19.5068\n",
      "Epoch [190/300], Step [93/172], Loss: 20.2679\n",
      "Epoch [190/300], Step [94/172], Loss: 28.4136\n",
      "Epoch [190/300], Step [95/172], Loss: 20.4223\n",
      "Epoch [190/300], Step [96/172], Loss: 19.7817\n",
      "Epoch [190/300], Step [97/172], Loss: 27.8757\n",
      "Epoch [190/300], Step [98/172], Loss: 19.0137\n",
      "Epoch [190/300], Step [99/172], Loss: 18.7929\n",
      "Epoch [190/300], Step [100/172], Loss: 15.2375\n",
      "Epoch [190/300], Step [101/172], Loss: 18.6689\n",
      "Epoch [190/300], Step [102/172], Loss: 16.7518\n",
      "Epoch [190/300], Step [103/172], Loss: 12.3182\n",
      "Epoch [190/300], Step [104/172], Loss: 18.3655\n",
      "Epoch [190/300], Step [105/172], Loss: 19.1484\n",
      "Epoch [190/300], Step [106/172], Loss: 15.3343\n",
      "Epoch [190/300], Step [107/172], Loss: 15.4436\n",
      "Epoch [190/300], Step [108/172], Loss: 14.3309\n",
      "Epoch [190/300], Step [109/172], Loss: 14.3501\n",
      "Epoch [190/300], Step [110/172], Loss: 15.6316\n",
      "Epoch [190/300], Step [111/172], Loss: 15.4113\n",
      "Epoch [190/300], Step [112/172], Loss: 17.2263\n",
      "Epoch [190/300], Step [113/172], Loss: 11.7198\n",
      "Epoch [190/300], Step [114/172], Loss: 13.6645\n",
      "Epoch [190/300], Step [115/172], Loss: 18.9886\n",
      "Epoch [190/300], Step [116/172], Loss: 14.2998\n",
      "Epoch [190/300], Step [117/172], Loss: 11.5040\n",
      "Epoch [190/300], Step [118/172], Loss: 13.9414\n",
      "Epoch [190/300], Step [119/172], Loss: 15.7071\n",
      "Epoch [190/300], Step [120/172], Loss: 9.9702\n",
      "Epoch [190/300], Step [121/172], Loss: 9.2712\n",
      "Epoch [190/300], Step [122/172], Loss: 10.5822\n",
      "Epoch [190/300], Step [123/172], Loss: 10.0368\n",
      "Epoch [190/300], Step [124/172], Loss: 7.3615\n",
      "Epoch [190/300], Step [125/172], Loss: 11.4711\n",
      "Epoch [190/300], Step [126/172], Loss: 10.9675\n",
      "Epoch [190/300], Step [127/172], Loss: 10.5686\n",
      "Epoch [190/300], Step [128/172], Loss: 10.1009\n",
      "Epoch [190/300], Step [129/172], Loss: 7.8555\n",
      "Epoch [190/300], Step [130/172], Loss: 12.2894\n",
      "Epoch [190/300], Step [131/172], Loss: 7.1104\n",
      "Epoch [190/300], Step [132/172], Loss: 8.2236\n",
      "Epoch [190/300], Step [133/172], Loss: 8.9167\n",
      "Epoch [190/300], Step [134/172], Loss: 11.1025\n",
      "Epoch [190/300], Step [135/172], Loss: 8.3213\n",
      "Epoch [190/300], Step [136/172], Loss: 7.9108\n",
      "Epoch [190/300], Step [137/172], Loss: 8.9802\n",
      "Epoch [190/300], Step [138/172], Loss: 6.6870\n",
      "Epoch [190/300], Step [139/172], Loss: 9.3509\n",
      "Epoch [190/300], Step [140/172], Loss: 9.4785\n",
      "Epoch [190/300], Step [141/172], Loss: 9.0658\n",
      "Epoch [190/300], Step [142/172], Loss: 13.9056\n",
      "Epoch [190/300], Step [143/172], Loss: 10.4911\n",
      "Epoch [190/300], Step [144/172], Loss: 8.8134\n",
      "Epoch [190/300], Step [145/172], Loss: 9.8121\n",
      "Epoch [190/300], Step [146/172], Loss: 9.1624\n",
      "Epoch [190/300], Step [147/172], Loss: 5.1058\n",
      "Epoch [190/300], Step [148/172], Loss: 5.8524\n",
      "Epoch [190/300], Step [149/172], Loss: 6.4259\n",
      "Epoch [190/300], Step [150/172], Loss: 5.7181\n",
      "Epoch [190/300], Step [151/172], Loss: 5.2131\n",
      "Epoch [190/300], Step [152/172], Loss: 7.1579\n",
      "Epoch [190/300], Step [153/172], Loss: 6.2103\n",
      "Epoch [190/300], Step [154/172], Loss: 7.1513\n",
      "Epoch [190/300], Step [155/172], Loss: 5.8919\n",
      "Epoch [190/300], Step [156/172], Loss: 13.1384\n",
      "Epoch [190/300], Step [157/172], Loss: 9.1075\n",
      "Epoch [190/300], Step [158/172], Loss: 6.8848\n",
      "Epoch [190/300], Step [159/172], Loss: 9.1189\n",
      "Epoch [190/300], Step [160/172], Loss: 9.8006\n",
      "Epoch [190/300], Step [161/172], Loss: 6.8961\n",
      "Epoch [190/300], Step [162/172], Loss: 4.9747\n",
      "Epoch [190/300], Step [163/172], Loss: 6.2971\n",
      "Epoch [190/300], Step [164/172], Loss: 8.4298\n",
      "Epoch [190/300], Step [165/172], Loss: 6.0089\n",
      "Epoch [190/300], Step [166/172], Loss: 5.3503\n",
      "Epoch [190/300], Step [167/172], Loss: 10.2875\n",
      "Epoch [190/300], Step [168/172], Loss: 6.1974\n",
      "Epoch [190/300], Step [169/172], Loss: 6.4763\n",
      "Epoch [190/300], Step [170/172], Loss: 4.5713\n",
      "Epoch [190/300], Step [171/172], Loss: 7.8366\n",
      "Epoch [190/300], Step [172/172], Loss: 5.1683\n",
      "Epoch [191/300], Step [1/172], Loss: 51.8734\n",
      "Epoch [191/300], Step [2/172], Loss: 53.6415\n",
      "Epoch [191/300], Step [3/172], Loss: 47.9502\n",
      "Epoch [191/300], Step [4/172], Loss: 27.1278\n",
      "Epoch [191/300], Step [5/172], Loss: 46.5324\n",
      "Epoch [191/300], Step [6/172], Loss: 18.8321\n",
      "Epoch [191/300], Step [7/172], Loss: 28.9641\n",
      "Epoch [191/300], Step [8/172], Loss: 4.3338\n",
      "Epoch [191/300], Step [9/172], Loss: 32.0558\n",
      "Epoch [191/300], Step [10/172], Loss: 42.2533\n",
      "Epoch [191/300], Step [11/172], Loss: 58.2219\n",
      "Epoch [191/300], Step [12/172], Loss: 64.8502\n",
      "Epoch [191/300], Step [13/172], Loss: 34.2062\n",
      "Epoch [191/300], Step [14/172], Loss: 61.3432\n",
      "Epoch [191/300], Step [15/172], Loss: 55.5837\n",
      "Epoch [191/300], Step [16/172], Loss: 10.1834\n",
      "Epoch [191/300], Step [17/172], Loss: 44.4307\n",
      "Epoch [191/300], Step [18/172], Loss: 58.4264\n",
      "Epoch [191/300], Step [19/172], Loss: 81.6408\n",
      "Epoch [191/300], Step [20/172], Loss: 35.5665\n",
      "Epoch [191/300], Step [21/172], Loss: 84.0708\n",
      "Epoch [191/300], Step [22/172], Loss: 59.6841\n",
      "Epoch [191/300], Step [23/172], Loss: 1.8192\n",
      "Epoch [191/300], Step [24/172], Loss: 55.2363\n",
      "Epoch [191/300], Step [25/172], Loss: 40.3789\n",
      "Epoch [191/300], Step [26/172], Loss: 47.3132\n",
      "Epoch [191/300], Step [27/172], Loss: 61.0746\n",
      "Epoch [191/300], Step [28/172], Loss: 24.0426\n",
      "Epoch [191/300], Step [29/172], Loss: 16.7677\n",
      "Epoch [191/300], Step [30/172], Loss: 62.7031\n",
      "Epoch [191/300], Step [31/172], Loss: 36.8141\n",
      "Epoch [191/300], Step [32/172], Loss: 42.7299\n",
      "Epoch [191/300], Step [33/172], Loss: 69.8969\n",
      "Epoch [191/300], Step [34/172], Loss: 2.6827\n",
      "Epoch [191/300], Step [35/172], Loss: 13.9481\n",
      "Epoch [191/300], Step [36/172], Loss: 17.8248\n",
      "Epoch [191/300], Step [37/172], Loss: 16.5660\n",
      "Epoch [191/300], Step [38/172], Loss: 29.4235\n",
      "Epoch [191/300], Step [39/172], Loss: 35.9845\n",
      "Epoch [191/300], Step [40/172], Loss: 20.5781\n",
      "Epoch [191/300], Step [41/172], Loss: 33.5073\n",
      "Epoch [191/300], Step [42/172], Loss: 37.9649\n",
      "Epoch [191/300], Step [43/172], Loss: 27.2505\n",
      "Epoch [191/300], Step [44/172], Loss: 21.0014\n",
      "Epoch [191/300], Step [45/172], Loss: 27.0350\n",
      "Epoch [191/300], Step [46/172], Loss: 17.5776\n",
      "Epoch [191/300], Step [47/172], Loss: 47.6406\n",
      "Epoch [191/300], Step [48/172], Loss: 61.7109\n",
      "Epoch [191/300], Step [49/172], Loss: 21.0143\n",
      "Epoch [191/300], Step [50/172], Loss: 47.6250\n",
      "Epoch [191/300], Step [51/172], Loss: 8.5203\n",
      "Epoch [191/300], Step [52/172], Loss: 19.4954\n",
      "Epoch [191/300], Step [53/172], Loss: 22.3127\n",
      "Epoch [191/300], Step [54/172], Loss: 14.5855\n",
      "Epoch [191/300], Step [55/172], Loss: 14.3266\n",
      "Epoch [191/300], Step [56/172], Loss: 18.1607\n",
      "Epoch [191/300], Step [57/172], Loss: 17.1820\n",
      "Epoch [191/300], Step [58/172], Loss: 13.1114\n",
      "Epoch [191/300], Step [59/172], Loss: 27.2821\n",
      "Epoch [191/300], Step [60/172], Loss: 24.7865\n",
      "Epoch [191/300], Step [61/172], Loss: 6.1420\n",
      "Epoch [191/300], Step [62/172], Loss: 19.2512\n",
      "Epoch [191/300], Step [63/172], Loss: 10.1413\n",
      "Epoch [191/300], Step [64/172], Loss: 10.6161\n",
      "Epoch [191/300], Step [65/172], Loss: 19.4780\n",
      "Epoch [191/300], Step [66/172], Loss: 6.3965\n",
      "Epoch [191/300], Step [67/172], Loss: 22.5915\n",
      "Epoch [191/300], Step [68/172], Loss: 4.8482\n",
      "Epoch [191/300], Step [69/172], Loss: 34.1929\n",
      "Epoch [191/300], Step [70/172], Loss: 37.7647\n",
      "Epoch [191/300], Step [71/172], Loss: 39.4609\n",
      "Epoch [191/300], Step [72/172], Loss: 39.4907\n",
      "Epoch [191/300], Step [73/172], Loss: 48.6123\n",
      "Epoch [191/300], Step [74/172], Loss: 25.0799\n",
      "Epoch [191/300], Step [75/172], Loss: 26.5724\n",
      "Epoch [191/300], Step [76/172], Loss: 28.1279\n",
      "Epoch [191/300], Step [77/172], Loss: 47.7921\n",
      "Epoch [191/300], Step [78/172], Loss: 36.8905\n",
      "Epoch [191/300], Step [79/172], Loss: 36.1278\n",
      "Epoch [191/300], Step [80/172], Loss: 49.2687\n",
      "Epoch [191/300], Step [81/172], Loss: 33.2736\n",
      "Epoch [191/300], Step [82/172], Loss: 34.9969\n",
      "Epoch [191/300], Step [83/172], Loss: 42.8853\n",
      "Epoch [191/300], Step [84/172], Loss: 32.2980\n",
      "Epoch [191/300], Step [85/172], Loss: 37.7703\n",
      "Epoch [191/300], Step [86/172], Loss: 32.0673\n",
      "Epoch [191/300], Step [87/172], Loss: 25.6188\n",
      "Epoch [191/300], Step [88/172], Loss: 23.8038\n",
      "Epoch [191/300], Step [89/172], Loss: 26.2365\n",
      "Epoch [191/300], Step [90/172], Loss: 20.4160\n",
      "Epoch [191/300], Step [91/172], Loss: 25.9545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [191/300], Step [92/172], Loss: 19.5290\n",
      "Epoch [191/300], Step [93/172], Loss: 20.2280\n",
      "Epoch [191/300], Step [94/172], Loss: 28.3727\n",
      "Epoch [191/300], Step [95/172], Loss: 20.4744\n",
      "Epoch [191/300], Step [96/172], Loss: 19.8326\n",
      "Epoch [191/300], Step [97/172], Loss: 27.9092\n",
      "Epoch [191/300], Step [98/172], Loss: 19.1271\n",
      "Epoch [191/300], Step [99/172], Loss: 18.8889\n",
      "Epoch [191/300], Step [100/172], Loss: 15.3484\n",
      "Epoch [191/300], Step [101/172], Loss: 18.8131\n",
      "Epoch [191/300], Step [102/172], Loss: 16.7540\n",
      "Epoch [191/300], Step [103/172], Loss: 12.4180\n",
      "Epoch [191/300], Step [104/172], Loss: 18.4752\n",
      "Epoch [191/300], Step [105/172], Loss: 19.3372\n",
      "Epoch [191/300], Step [106/172], Loss: 15.5132\n",
      "Epoch [191/300], Step [107/172], Loss: 15.5489\n",
      "Epoch [191/300], Step [108/172], Loss: 14.3989\n",
      "Epoch [191/300], Step [109/172], Loss: 14.4190\n",
      "Epoch [191/300], Step [110/172], Loss: 15.6896\n",
      "Epoch [191/300], Step [111/172], Loss: 15.5546\n",
      "Epoch [191/300], Step [112/172], Loss: 17.2223\n",
      "Epoch [191/300], Step [113/172], Loss: 11.7917\n",
      "Epoch [191/300], Step [114/172], Loss: 13.7342\n",
      "Epoch [191/300], Step [115/172], Loss: 18.9897\n",
      "Epoch [191/300], Step [116/172], Loss: 14.3309\n",
      "Epoch [191/300], Step [117/172], Loss: 11.6123\n",
      "Epoch [191/300], Step [118/172], Loss: 14.0308\n",
      "Epoch [191/300], Step [119/172], Loss: 15.7507\n",
      "Epoch [191/300], Step [120/172], Loss: 10.0647\n",
      "Epoch [191/300], Step [121/172], Loss: 9.3206\n",
      "Epoch [191/300], Step [122/172], Loss: 10.6259\n",
      "Epoch [191/300], Step [123/172], Loss: 10.1283\n",
      "Epoch [191/300], Step [124/172], Loss: 7.3895\n",
      "Epoch [191/300], Step [125/172], Loss: 11.4757\n",
      "Epoch [191/300], Step [126/172], Loss: 11.0030\n",
      "Epoch [191/300], Step [127/172], Loss: 10.5568\n",
      "Epoch [191/300], Step [128/172], Loss: 10.0579\n",
      "Epoch [191/300], Step [129/172], Loss: 7.8918\n",
      "Epoch [191/300], Step [130/172], Loss: 12.3186\n",
      "Epoch [191/300], Step [131/172], Loss: 7.1259\n",
      "Epoch [191/300], Step [132/172], Loss: 8.2909\n",
      "Epoch [191/300], Step [133/172], Loss: 8.9162\n",
      "Epoch [191/300], Step [134/172], Loss: 11.1039\n",
      "Epoch [191/300], Step [135/172], Loss: 8.3715\n",
      "Epoch [191/300], Step [136/172], Loss: 7.9829\n",
      "Epoch [191/300], Step [137/172], Loss: 9.0563\n",
      "Epoch [191/300], Step [138/172], Loss: 6.7381\n",
      "Epoch [191/300], Step [139/172], Loss: 9.4121\n",
      "Epoch [191/300], Step [140/172], Loss: 9.5520\n",
      "Epoch [191/300], Step [141/172], Loss: 9.1041\n",
      "Epoch [191/300], Step [142/172], Loss: 13.9340\n",
      "Epoch [191/300], Step [143/172], Loss: 10.5592\n",
      "Epoch [191/300], Step [144/172], Loss: 8.8443\n",
      "Epoch [191/300], Step [145/172], Loss: 9.9387\n",
      "Epoch [191/300], Step [146/172], Loss: 9.1635\n",
      "Epoch [191/300], Step [147/172], Loss: 5.1546\n",
      "Epoch [191/300], Step [148/172], Loss: 5.8714\n",
      "Epoch [191/300], Step [149/172], Loss: 6.4443\n",
      "Epoch [191/300], Step [150/172], Loss: 5.7649\n",
      "Epoch [191/300], Step [151/172], Loss: 5.2418\n",
      "Epoch [191/300], Step [152/172], Loss: 7.1656\n",
      "Epoch [191/300], Step [153/172], Loss: 6.2420\n",
      "Epoch [191/300], Step [154/172], Loss: 7.2189\n",
      "Epoch [191/300], Step [155/172], Loss: 5.9347\n",
      "Epoch [191/300], Step [156/172], Loss: 13.1386\n",
      "Epoch [191/300], Step [157/172], Loss: 9.1657\n",
      "Epoch [191/300], Step [158/172], Loss: 6.8718\n",
      "Epoch [191/300], Step [159/172], Loss: 9.3166\n",
      "Epoch [191/300], Step [160/172], Loss: 9.7389\n",
      "Epoch [191/300], Step [161/172], Loss: 6.9114\n",
      "Epoch [191/300], Step [162/172], Loss: 4.9685\n",
      "Epoch [191/300], Step [163/172], Loss: 6.3805\n",
      "Epoch [191/300], Step [164/172], Loss: 8.4072\n",
      "Epoch [191/300], Step [165/172], Loss: 6.0533\n",
      "Epoch [191/300], Step [166/172], Loss: 5.4231\n",
      "Epoch [191/300], Step [167/172], Loss: 10.3967\n",
      "Epoch [191/300], Step [168/172], Loss: 6.2072\n",
      "Epoch [191/300], Step [169/172], Loss: 6.5148\n",
      "Epoch [191/300], Step [170/172], Loss: 4.5856\n",
      "Epoch [191/300], Step [171/172], Loss: 7.8949\n",
      "Epoch [191/300], Step [172/172], Loss: 5.2332\n",
      "Epoch [192/300], Step [1/172], Loss: 51.6840\n",
      "Epoch [192/300], Step [2/172], Loss: 53.4458\n",
      "Epoch [192/300], Step [3/172], Loss: 46.9653\n",
      "Epoch [192/300], Step [4/172], Loss: 27.0717\n",
      "Epoch [192/300], Step [5/172], Loss: 45.6046\n",
      "Epoch [192/300], Step [6/172], Loss: 19.0202\n",
      "Epoch [192/300], Step [7/172], Loss: 29.3111\n",
      "Epoch [192/300], Step [8/172], Loss: 4.5195\n",
      "Epoch [192/300], Step [9/172], Loss: 31.9643\n",
      "Epoch [192/300], Step [10/172], Loss: 42.0424\n",
      "Epoch [192/300], Step [11/172], Loss: 57.9705\n",
      "Epoch [192/300], Step [12/172], Loss: 64.4710\n",
      "Epoch [192/300], Step [13/172], Loss: 34.4711\n",
      "Epoch [192/300], Step [14/172], Loss: 60.7218\n",
      "Epoch [192/300], Step [15/172], Loss: 55.3679\n",
      "Epoch [192/300], Step [16/172], Loss: 10.1470\n",
      "Epoch [192/300], Step [17/172], Loss: 44.4159\n",
      "Epoch [192/300], Step [18/172], Loss: 58.3074\n",
      "Epoch [192/300], Step [19/172], Loss: 81.8857\n",
      "Epoch [192/300], Step [20/172], Loss: 35.0638\n",
      "Epoch [192/300], Step [21/172], Loss: 84.2270\n",
      "Epoch [192/300], Step [22/172], Loss: 59.6529\n",
      "Epoch [192/300], Step [23/172], Loss: 1.9061\n",
      "Epoch [192/300], Step [24/172], Loss: 55.1619\n",
      "Epoch [192/300], Step [25/172], Loss: 40.4220\n",
      "Epoch [192/300], Step [26/172], Loss: 47.2642\n",
      "Epoch [192/300], Step [27/172], Loss: 61.0700\n",
      "Epoch [192/300], Step [28/172], Loss: 24.0103\n",
      "Epoch [192/300], Step [29/172], Loss: 16.6309\n",
      "Epoch [192/300], Step [30/172], Loss: 62.2799\n",
      "Epoch [192/300], Step [31/172], Loss: 36.4778\n",
      "Epoch [192/300], Step [32/172], Loss: 42.7363\n",
      "Epoch [192/300], Step [33/172], Loss: 69.9440\n",
      "Epoch [192/300], Step [34/172], Loss: 2.6720\n",
      "Epoch [192/300], Step [35/172], Loss: 14.0215\n",
      "Epoch [192/300], Step [36/172], Loss: 17.9604\n",
      "Epoch [192/300], Step [37/172], Loss: 16.6587\n",
      "Epoch [192/300], Step [38/172], Loss: 29.5939\n",
      "Epoch [192/300], Step [39/172], Loss: 35.9947\n",
      "Epoch [192/300], Step [40/172], Loss: 20.6850\n",
      "Epoch [192/300], Step [41/172], Loss: 33.6507\n",
      "Epoch [192/300], Step [42/172], Loss: 38.2079\n",
      "Epoch [192/300], Step [43/172], Loss: 27.3598\n",
      "Epoch [192/300], Step [44/172], Loss: 21.1691\n",
      "Epoch [192/300], Step [45/172], Loss: 27.2485\n",
      "Epoch [192/300], Step [46/172], Loss: 17.5701\n",
      "Epoch [192/300], Step [47/172], Loss: 47.6281\n",
      "Epoch [192/300], Step [48/172], Loss: 61.7004\n",
      "Epoch [192/300], Step [49/172], Loss: 21.1280\n",
      "Epoch [192/300], Step [50/172], Loss: 47.5814\n",
      "Epoch [192/300], Step [51/172], Loss: 8.5470\n",
      "Epoch [192/300], Step [52/172], Loss: 19.4687\n",
      "Epoch [192/300], Step [53/172], Loss: 22.2268\n",
      "Epoch [192/300], Step [54/172], Loss: 14.5422\n",
      "Epoch [192/300], Step [55/172], Loss: 14.2604\n",
      "Epoch [192/300], Step [56/172], Loss: 17.8165\n",
      "Epoch [192/300], Step [57/172], Loss: 16.8186\n",
      "Epoch [192/300], Step [58/172], Loss: 12.9401\n",
      "Epoch [192/300], Step [59/172], Loss: 27.1884\n",
      "Epoch [192/300], Step [60/172], Loss: 24.6593\n",
      "Epoch [192/300], Step [61/172], Loss: 6.0780\n",
      "Epoch [192/300], Step [62/172], Loss: 19.0599\n",
      "Epoch [192/300], Step [63/172], Loss: 10.0124\n",
      "Epoch [192/300], Step [64/172], Loss: 10.5158\n",
      "Epoch [192/300], Step [65/172], Loss: 19.4447\n",
      "Epoch [192/300], Step [66/172], Loss: 6.4218\n",
      "Epoch [192/300], Step [67/172], Loss: 22.4559\n",
      "Epoch [192/300], Step [68/172], Loss: 4.8645\n",
      "Epoch [192/300], Step [69/172], Loss: 34.1290\n",
      "Epoch [192/300], Step [70/172], Loss: 37.6852\n",
      "Epoch [192/300], Step [71/172], Loss: 39.4261\n",
      "Epoch [192/300], Step [72/172], Loss: 39.5044\n",
      "Epoch [192/300], Step [73/172], Loss: 48.5576\n",
      "Epoch [192/300], Step [74/172], Loss: 25.0297\n",
      "Epoch [192/300], Step [75/172], Loss: 26.3768\n",
      "Epoch [192/300], Step [76/172], Loss: 27.9220\n",
      "Epoch [192/300], Step [77/172], Loss: 47.5016\n",
      "Epoch [192/300], Step [78/172], Loss: 36.7989\n",
      "Epoch [192/300], Step [79/172], Loss: 35.9349\n",
      "Epoch [192/300], Step [80/172], Loss: 49.2128\n",
      "Epoch [192/300], Step [81/172], Loss: 33.1876\n",
      "Epoch [192/300], Step [82/172], Loss: 35.1516\n",
      "Epoch [192/300], Step [83/172], Loss: 42.7811\n",
      "Epoch [192/300], Step [84/172], Loss: 32.2867\n",
      "Epoch [192/300], Step [85/172], Loss: 37.7686\n",
      "Epoch [192/300], Step [86/172], Loss: 32.1899\n",
      "Epoch [192/300], Step [87/172], Loss: 25.5006\n",
      "Epoch [192/300], Step [88/172], Loss: 23.7257\n",
      "Epoch [192/300], Step [89/172], Loss: 26.3400\n",
      "Epoch [192/300], Step [90/172], Loss: 20.2763\n",
      "Epoch [192/300], Step [91/172], Loss: 25.8760\n",
      "Epoch [192/300], Step [92/172], Loss: 19.3959\n",
      "Epoch [192/300], Step [93/172], Loss: 20.1784\n",
      "Epoch [192/300], Step [94/172], Loss: 28.3674\n",
      "Epoch [192/300], Step [95/172], Loss: 20.4310\n",
      "Epoch [192/300], Step [96/172], Loss: 19.7957\n",
      "Epoch [192/300], Step [97/172], Loss: 27.9335\n",
      "Epoch [192/300], Step [98/172], Loss: 19.1109\n",
      "Epoch [192/300], Step [99/172], Loss: 18.8684\n",
      "Epoch [192/300], Step [100/172], Loss: 15.3553\n",
      "Epoch [192/300], Step [101/172], Loss: 18.8084\n",
      "Epoch [192/300], Step [102/172], Loss: 16.6704\n",
      "Epoch [192/300], Step [103/172], Loss: 12.4171\n",
      "Epoch [192/300], Step [104/172], Loss: 18.4353\n",
      "Epoch [192/300], Step [105/172], Loss: 19.3684\n",
      "Epoch [192/300], Step [106/172], Loss: 15.4665\n",
      "Epoch [192/300], Step [107/172], Loss: 15.5390\n",
      "Epoch [192/300], Step [108/172], Loss: 14.3081\n",
      "Epoch [192/300], Step [109/172], Loss: 14.3156\n",
      "Epoch [192/300], Step [110/172], Loss: 15.6538\n",
      "Epoch [192/300], Step [111/172], Loss: 15.5429\n",
      "Epoch [192/300], Step [112/172], Loss: 17.2227\n",
      "Epoch [192/300], Step [113/172], Loss: 11.7680\n",
      "Epoch [192/300], Step [114/172], Loss: 13.7135\n",
      "Epoch [192/300], Step [115/172], Loss: 18.8991\n",
      "Epoch [192/300], Step [116/172], Loss: 14.2909\n",
      "Epoch [192/300], Step [117/172], Loss: 11.6184\n",
      "Epoch [192/300], Step [118/172], Loss: 13.9844\n",
      "Epoch [192/300], Step [119/172], Loss: 15.6824\n",
      "Epoch [192/300], Step [120/172], Loss: 10.0736\n",
      "Epoch [192/300], Step [121/172], Loss: 9.2939\n",
      "Epoch [192/300], Step [122/172], Loss: 10.6521\n",
      "Epoch [192/300], Step [123/172], Loss: 10.0815\n",
      "Epoch [192/300], Step [124/172], Loss: 7.3062\n",
      "Epoch [192/300], Step [125/172], Loss: 11.3471\n",
      "Epoch [192/300], Step [126/172], Loss: 10.9333\n",
      "Epoch [192/300], Step [127/172], Loss: 10.5574\n",
      "Epoch [192/300], Step [128/172], Loss: 10.0319\n",
      "Epoch [192/300], Step [129/172], Loss: 7.8818\n",
      "Epoch [192/300], Step [130/172], Loss: 12.3094\n",
      "Epoch [192/300], Step [131/172], Loss: 7.1290\n",
      "Epoch [192/300], Step [132/172], Loss: 8.3126\n",
      "Epoch [192/300], Step [133/172], Loss: 8.9595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [192/300], Step [134/172], Loss: 11.0190\n",
      "Epoch [192/300], Step [135/172], Loss: 8.3154\n",
      "Epoch [192/300], Step [136/172], Loss: 7.9032\n",
      "Epoch [192/300], Step [137/172], Loss: 8.9929\n",
      "Epoch [192/300], Step [138/172], Loss: 6.6574\n",
      "Epoch [192/300], Step [139/172], Loss: 9.2352\n",
      "Epoch [192/300], Step [140/172], Loss: 9.5763\n",
      "Epoch [192/300], Step [141/172], Loss: 9.0403\n",
      "Epoch [192/300], Step [142/172], Loss: 13.9164\n",
      "Epoch [192/300], Step [143/172], Loss: 10.5586\n",
      "Epoch [192/300], Step [144/172], Loss: 8.8147\n",
      "Epoch [192/300], Step [145/172], Loss: 9.9366\n",
      "Epoch [192/300], Step [146/172], Loss: 9.1786\n",
      "Epoch [192/300], Step [147/172], Loss: 5.1124\n",
      "Epoch [192/300], Step [148/172], Loss: 5.8932\n",
      "Epoch [192/300], Step [149/172], Loss: 6.4179\n",
      "Epoch [192/300], Step [150/172], Loss: 5.7027\n",
      "Epoch [192/300], Step [151/172], Loss: 5.2262\n",
      "Epoch [192/300], Step [152/172], Loss: 7.1549\n",
      "Epoch [192/300], Step [153/172], Loss: 6.1796\n",
      "Epoch [192/300], Step [154/172], Loss: 7.2289\n",
      "Epoch [192/300], Step [155/172], Loss: 5.7935\n",
      "Epoch [192/300], Step [156/172], Loss: 13.1409\n",
      "Epoch [192/300], Step [157/172], Loss: 9.0409\n",
      "Epoch [192/300], Step [158/172], Loss: 6.8576\n",
      "Epoch [192/300], Step [159/172], Loss: 9.1773\n",
      "Epoch [192/300], Step [160/172], Loss: 9.6280\n",
      "Epoch [192/300], Step [161/172], Loss: 6.8629\n",
      "Epoch [192/300], Step [162/172], Loss: 4.9744\n",
      "Epoch [192/300], Step [163/172], Loss: 6.3731\n",
      "Epoch [192/300], Step [164/172], Loss: 8.4167\n",
      "Epoch [192/300], Step [165/172], Loss: 6.0626\n",
      "Epoch [192/300], Step [166/172], Loss: 5.4308\n",
      "Epoch [192/300], Step [167/172], Loss: 10.2257\n",
      "Epoch [192/300], Step [168/172], Loss: 6.1719\n",
      "Epoch [192/300], Step [169/172], Loss: 6.5083\n",
      "Epoch [192/300], Step [170/172], Loss: 4.6258\n",
      "Epoch [192/300], Step [171/172], Loss: 7.7382\n",
      "Epoch [192/300], Step [172/172], Loss: 5.1040\n",
      "Epoch [193/300], Step [1/172], Loss: 51.2900\n",
      "Epoch [193/300], Step [2/172], Loss: 53.2457\n",
      "Epoch [193/300], Step [3/172], Loss: 49.3836\n",
      "Epoch [193/300], Step [4/172], Loss: 26.9378\n",
      "Epoch [193/300], Step [5/172], Loss: 45.2196\n",
      "Epoch [193/300], Step [6/172], Loss: 18.9610\n",
      "Epoch [193/300], Step [7/172], Loss: 29.7027\n",
      "Epoch [193/300], Step [8/172], Loss: 4.4893\n",
      "Epoch [193/300], Step [9/172], Loss: 31.7549\n",
      "Epoch [193/300], Step [10/172], Loss: 41.7581\n",
      "Epoch [193/300], Step [11/172], Loss: 57.8104\n",
      "Epoch [193/300], Step [12/172], Loss: 64.1079\n",
      "Epoch [193/300], Step [13/172], Loss: 34.1990\n",
      "Epoch [193/300], Step [14/172], Loss: 60.8995\n",
      "Epoch [193/300], Step [15/172], Loss: 55.1891\n",
      "Epoch [193/300], Step [16/172], Loss: 9.9284\n",
      "Epoch [193/300], Step [17/172], Loss: 44.4135\n",
      "Epoch [193/300], Step [18/172], Loss: 58.0955\n",
      "Epoch [193/300], Step [19/172], Loss: 82.0180\n",
      "Epoch [193/300], Step [20/172], Loss: 34.9116\n",
      "Epoch [193/300], Step [21/172], Loss: 84.6023\n",
      "Epoch [193/300], Step [22/172], Loss: 59.3101\n",
      "Epoch [193/300], Step [23/172], Loss: 1.8998\n",
      "Epoch [193/300], Step [24/172], Loss: 55.1941\n",
      "Epoch [193/300], Step [25/172], Loss: 40.2373\n",
      "Epoch [193/300], Step [26/172], Loss: 47.0245\n",
      "Epoch [193/300], Step [27/172], Loss: 60.6764\n",
      "Epoch [193/300], Step [28/172], Loss: 23.9626\n",
      "Epoch [193/300], Step [29/172], Loss: 16.4938\n",
      "Epoch [193/300], Step [30/172], Loss: 62.0319\n",
      "Epoch [193/300], Step [31/172], Loss: 36.4444\n",
      "Epoch [193/300], Step [32/172], Loss: 42.9116\n",
      "Epoch [193/300], Step [33/172], Loss: 69.9999\n",
      "Epoch [193/300], Step [34/172], Loss: 2.7451\n",
      "Epoch [193/300], Step [35/172], Loss: 13.8967\n",
      "Epoch [193/300], Step [36/172], Loss: 17.7715\n",
      "Epoch [193/300], Step [37/172], Loss: 16.5521\n",
      "Epoch [193/300], Step [38/172], Loss: 29.4719\n",
      "Epoch [193/300], Step [39/172], Loss: 35.8205\n",
      "Epoch [193/300], Step [40/172], Loss: 20.6272\n",
      "Epoch [193/300], Step [41/172], Loss: 33.3038\n",
      "Epoch [193/300], Step [42/172], Loss: 38.0269\n",
      "Epoch [193/300], Step [43/172], Loss: 27.2374\n",
      "Epoch [193/300], Step [44/172], Loss: 20.9487\n",
      "Epoch [193/300], Step [45/172], Loss: 27.1562\n",
      "Epoch [193/300], Step [46/172], Loss: 17.3975\n",
      "Epoch [193/300], Step [47/172], Loss: 47.4977\n",
      "Epoch [193/300], Step [48/172], Loss: 62.0018\n",
      "Epoch [193/300], Step [49/172], Loss: 21.1155\n",
      "Epoch [193/300], Step [50/172], Loss: 47.2170\n",
      "Epoch [193/300], Step [51/172], Loss: 8.5751\n",
      "Epoch [193/300], Step [52/172], Loss: 19.5799\n",
      "Epoch [193/300], Step [53/172], Loss: 22.2547\n",
      "Epoch [193/300], Step [54/172], Loss: 14.6662\n",
      "Epoch [193/300], Step [55/172], Loss: 14.4345\n",
      "Epoch [193/300], Step [56/172], Loss: 17.9001\n",
      "Epoch [193/300], Step [57/172], Loss: 16.7256\n",
      "Epoch [193/300], Step [58/172], Loss: 13.1112\n",
      "Epoch [193/300], Step [59/172], Loss: 27.0792\n",
      "Epoch [193/300], Step [60/172], Loss: 24.0274\n",
      "Epoch [193/300], Step [61/172], Loss: 6.1456\n",
      "Epoch [193/300], Step [62/172], Loss: 19.0627\n",
      "Epoch [193/300], Step [63/172], Loss: 10.1521\n",
      "Epoch [193/300], Step [64/172], Loss: 10.6983\n",
      "Epoch [193/300], Step [65/172], Loss: 19.4677\n",
      "Epoch [193/300], Step [66/172], Loss: 6.4561\n",
      "Epoch [193/300], Step [67/172], Loss: 22.5176\n",
      "Epoch [193/300], Step [68/172], Loss: 5.3363\n",
      "Epoch [193/300], Step [69/172], Loss: 33.9554\n",
      "Epoch [193/300], Step [70/172], Loss: 37.2808\n",
      "Epoch [193/300], Step [71/172], Loss: 39.1719\n",
      "Epoch [193/300], Step [72/172], Loss: 39.2746\n",
      "Epoch [193/300], Step [73/172], Loss: 48.3999\n",
      "Epoch [193/300], Step [74/172], Loss: 24.9986\n",
      "Epoch [193/300], Step [75/172], Loss: 26.3695\n",
      "Epoch [193/300], Step [76/172], Loss: 27.9236\n",
      "Epoch [193/300], Step [77/172], Loss: 47.2985\n",
      "Epoch [193/300], Step [78/172], Loss: 36.7683\n",
      "Epoch [193/300], Step [79/172], Loss: 35.8837\n",
      "Epoch [193/300], Step [80/172], Loss: 49.1754\n",
      "Epoch [193/300], Step [81/172], Loss: 33.0767\n",
      "Epoch [193/300], Step [82/172], Loss: 35.0595\n",
      "Epoch [193/300], Step [83/172], Loss: 42.7458\n",
      "Epoch [193/300], Step [84/172], Loss: 32.2320\n",
      "Epoch [193/300], Step [85/172], Loss: 37.6506\n",
      "Epoch [193/300], Step [86/172], Loss: 32.0257\n",
      "Epoch [193/300], Step [87/172], Loss: 25.4084\n",
      "Epoch [193/300], Step [88/172], Loss: 23.4722\n",
      "Epoch [193/300], Step [89/172], Loss: 26.2012\n",
      "Epoch [193/300], Step [90/172], Loss: 20.1320\n",
      "Epoch [193/300], Step [91/172], Loss: 25.7017\n",
      "Epoch [193/300], Step [92/172], Loss: 19.4234\n",
      "Epoch [193/300], Step [93/172], Loss: 20.1613\n",
      "Epoch [193/300], Step [94/172], Loss: 28.1928\n",
      "Epoch [193/300], Step [95/172], Loss: 20.4161\n",
      "Epoch [193/300], Step [96/172], Loss: 19.7709\n",
      "Epoch [193/300], Step [97/172], Loss: 27.8090\n",
      "Epoch [193/300], Step [98/172], Loss: 18.9353\n",
      "Epoch [193/300], Step [99/172], Loss: 18.7788\n",
      "Epoch [193/300], Step [100/172], Loss: 15.2611\n",
      "Epoch [193/300], Step [101/172], Loss: 18.7845\n",
      "Epoch [193/300], Step [102/172], Loss: 16.5975\n",
      "Epoch [193/300], Step [103/172], Loss: 12.3635\n",
      "Epoch [193/300], Step [104/172], Loss: 18.4666\n",
      "Epoch [193/300], Step [105/172], Loss: 19.2958\n",
      "Epoch [193/300], Step [106/172], Loss: 15.4760\n",
      "Epoch [193/300], Step [107/172], Loss: 15.5200\n",
      "Epoch [193/300], Step [108/172], Loss: 14.2576\n",
      "Epoch [193/300], Step [109/172], Loss: 14.2890\n",
      "Epoch [193/300], Step [110/172], Loss: 15.7486\n",
      "Epoch [193/300], Step [111/172], Loss: 15.6209\n",
      "Epoch [193/300], Step [112/172], Loss: 17.2482\n",
      "Epoch [193/300], Step [113/172], Loss: 11.8298\n",
      "Epoch [193/300], Step [114/172], Loss: 13.7271\n",
      "Epoch [193/300], Step [115/172], Loss: 18.9258\n",
      "Epoch [193/300], Step [116/172], Loss: 14.3976\n",
      "Epoch [193/300], Step [117/172], Loss: 11.6327\n",
      "Epoch [193/300], Step [118/172], Loss: 13.8924\n",
      "Epoch [193/300], Step [119/172], Loss: 15.6928\n",
      "Epoch [193/300], Step [120/172], Loss: 10.0598\n",
      "Epoch [193/300], Step [121/172], Loss: 9.3453\n",
      "Epoch [193/300], Step [122/172], Loss: 10.6578\n",
      "Epoch [193/300], Step [123/172], Loss: 10.1070\n",
      "Epoch [193/300], Step [124/172], Loss: 7.4157\n",
      "Epoch [193/300], Step [125/172], Loss: 11.5223\n",
      "Epoch [193/300], Step [126/172], Loss: 11.0208\n",
      "Epoch [193/300], Step [127/172], Loss: 10.5816\n",
      "Epoch [193/300], Step [128/172], Loss: 10.1543\n",
      "Epoch [193/300], Step [129/172], Loss: 7.9510\n",
      "Epoch [193/300], Step [130/172], Loss: 12.3657\n",
      "Epoch [193/300], Step [131/172], Loss: 7.1610\n",
      "Epoch [193/300], Step [132/172], Loss: 8.3555\n",
      "Epoch [193/300], Step [133/172], Loss: 8.9810\n",
      "Epoch [193/300], Step [134/172], Loss: 11.0248\n",
      "Epoch [193/300], Step [135/172], Loss: 8.3568\n",
      "Epoch [193/300], Step [136/172], Loss: 7.9687\n",
      "Epoch [193/300], Step [137/172], Loss: 9.0999\n",
      "Epoch [193/300], Step [138/172], Loss: 6.7249\n",
      "Epoch [193/300], Step [139/172], Loss: 9.3748\n",
      "Epoch [193/300], Step [140/172], Loss: 9.6409\n",
      "Epoch [193/300], Step [141/172], Loss: 9.0579\n",
      "Epoch [193/300], Step [142/172], Loss: 13.9610\n",
      "Epoch [193/300], Step [143/172], Loss: 10.6041\n",
      "Epoch [193/300], Step [144/172], Loss: 8.8661\n",
      "Epoch [193/300], Step [145/172], Loss: 9.9733\n",
      "Epoch [193/300], Step [146/172], Loss: 9.2169\n",
      "Epoch [193/300], Step [147/172], Loss: 5.1946\n",
      "Epoch [193/300], Step [148/172], Loss: 5.9367\n",
      "Epoch [193/300], Step [149/172], Loss: 6.4880\n",
      "Epoch [193/300], Step [150/172], Loss: 5.7317\n",
      "Epoch [193/300], Step [151/172], Loss: 5.2321\n",
      "Epoch [193/300], Step [152/172], Loss: 7.1747\n",
      "Epoch [193/300], Step [153/172], Loss: 6.2471\n",
      "Epoch [193/300], Step [154/172], Loss: 7.2996\n",
      "Epoch [193/300], Step [155/172], Loss: 5.8974\n",
      "Epoch [193/300], Step [156/172], Loss: 13.1681\n",
      "Epoch [193/300], Step [157/172], Loss: 9.0740\n",
      "Epoch [193/300], Step [158/172], Loss: 6.8706\n",
      "Epoch [193/300], Step [159/172], Loss: 9.3489\n",
      "Epoch [193/300], Step [160/172], Loss: 9.6110\n",
      "Epoch [193/300], Step [161/172], Loss: 6.9839\n",
      "Epoch [193/300], Step [162/172], Loss: 4.9644\n",
      "Epoch [193/300], Step [163/172], Loss: 6.3700\n",
      "Epoch [193/300], Step [164/172], Loss: 8.4419\n",
      "Epoch [193/300], Step [165/172], Loss: 6.1050\n",
      "Epoch [193/300], Step [166/172], Loss: 5.4407\n",
      "Epoch [193/300], Step [167/172], Loss: 10.3303\n",
      "Epoch [193/300], Step [168/172], Loss: 6.2446\n",
      "Epoch [193/300], Step [169/172], Loss: 6.5258\n",
      "Epoch [193/300], Step [170/172], Loss: 4.6501\n",
      "Epoch [193/300], Step [171/172], Loss: 7.8791\n",
      "Epoch [193/300], Step [172/172], Loss: 5.2166\n",
      "Epoch [194/300], Step [1/172], Loss: 51.0957\n",
      "Epoch [194/300], Step [2/172], Loss: 53.0000\n",
      "Epoch [194/300], Step [3/172], Loss: 48.9100\n",
      "Epoch [194/300], Step [4/172], Loss: 26.9296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [194/300], Step [5/172], Loss: 45.1825\n",
      "Epoch [194/300], Step [6/172], Loss: 18.9487\n",
      "Epoch [194/300], Step [7/172], Loss: 29.6310\n",
      "Epoch [194/300], Step [8/172], Loss: 4.5115\n",
      "Epoch [194/300], Step [9/172], Loss: 31.5942\n",
      "Epoch [194/300], Step [10/172], Loss: 41.7175\n",
      "Epoch [194/300], Step [11/172], Loss: 57.4596\n",
      "Epoch [194/300], Step [12/172], Loss: 63.8681\n",
      "Epoch [194/300], Step [13/172], Loss: 34.2868\n",
      "Epoch [194/300], Step [14/172], Loss: 61.0252\n",
      "Epoch [194/300], Step [15/172], Loss: 55.1547\n",
      "Epoch [194/300], Step [16/172], Loss: 9.9968\n",
      "Epoch [194/300], Step [17/172], Loss: 44.2413\n",
      "Epoch [194/300], Step [18/172], Loss: 57.8746\n",
      "Epoch [194/300], Step [19/172], Loss: 81.7261\n",
      "Epoch [194/300], Step [20/172], Loss: 34.6195\n",
      "Epoch [194/300], Step [21/172], Loss: 84.5294\n",
      "Epoch [194/300], Step [22/172], Loss: 59.2485\n",
      "Epoch [194/300], Step [23/172], Loss: 1.8686\n",
      "Epoch [194/300], Step [24/172], Loss: 55.0864\n",
      "Epoch [194/300], Step [25/172], Loss: 40.3154\n",
      "Epoch [194/300], Step [26/172], Loss: 47.1377\n",
      "Epoch [194/300], Step [27/172], Loss: 60.8270\n",
      "Epoch [194/300], Step [28/172], Loss: 24.0234\n",
      "Epoch [194/300], Step [29/172], Loss: 16.5816\n",
      "Epoch [194/300], Step [30/172], Loss: 62.4032\n",
      "Epoch [194/300], Step [31/172], Loss: 36.5902\n",
      "Epoch [194/300], Step [32/172], Loss: 42.8776\n",
      "Epoch [194/300], Step [33/172], Loss: 69.9333\n",
      "Epoch [194/300], Step [34/172], Loss: 2.6045\n",
      "Epoch [194/300], Step [35/172], Loss: 13.9975\n",
      "Epoch [194/300], Step [36/172], Loss: 18.1127\n",
      "Epoch [194/300], Step [37/172], Loss: 16.5565\n",
      "Epoch [194/300], Step [38/172], Loss: 29.7322\n",
      "Epoch [194/300], Step [39/172], Loss: 35.8544\n",
      "Epoch [194/300], Step [40/172], Loss: 20.6008\n",
      "Epoch [194/300], Step [41/172], Loss: 33.2773\n",
      "Epoch [194/300], Step [42/172], Loss: 37.9342\n",
      "Epoch [194/300], Step [43/172], Loss: 27.2172\n",
      "Epoch [194/300], Step [44/172], Loss: 20.9675\n",
      "Epoch [194/300], Step [45/172], Loss: 27.0321\n",
      "Epoch [194/300], Step [46/172], Loss: 17.2105\n",
      "Epoch [194/300], Step [47/172], Loss: 47.5210\n",
      "Epoch [194/300], Step [48/172], Loss: 61.1241\n",
      "Epoch [194/300], Step [49/172], Loss: 21.1305\n",
      "Epoch [194/300], Step [50/172], Loss: 47.3209\n",
      "Epoch [194/300], Step [51/172], Loss: 8.5797\n",
      "Epoch [194/300], Step [52/172], Loss: 19.5532\n",
      "Epoch [194/300], Step [53/172], Loss: 22.2761\n",
      "Epoch [194/300], Step [54/172], Loss: 14.5555\n",
      "Epoch [194/300], Step [55/172], Loss: 14.2697\n",
      "Epoch [194/300], Step [56/172], Loss: 18.1485\n",
      "Epoch [194/300], Step [57/172], Loss: 16.9376\n",
      "Epoch [194/300], Step [58/172], Loss: 13.0584\n",
      "Epoch [194/300], Step [59/172], Loss: 27.0649\n",
      "Epoch [194/300], Step [60/172], Loss: 23.9529\n",
      "Epoch [194/300], Step [61/172], Loss: 6.0670\n",
      "Epoch [194/300], Step [62/172], Loss: 18.9619\n",
      "Epoch [194/300], Step [63/172], Loss: 10.1212\n",
      "Epoch [194/300], Step [64/172], Loss: 10.6704\n",
      "Epoch [194/300], Step [65/172], Loss: 19.3932\n",
      "Epoch [194/300], Step [66/172], Loss: 6.3687\n",
      "Epoch [194/300], Step [67/172], Loss: 22.3270\n",
      "Epoch [194/300], Step [68/172], Loss: 4.8982\n",
      "Epoch [194/300], Step [69/172], Loss: 33.8992\n",
      "Epoch [194/300], Step [70/172], Loss: 37.2730\n",
      "Epoch [194/300], Step [71/172], Loss: 39.1660\n",
      "Epoch [194/300], Step [72/172], Loss: 39.2749\n",
      "Epoch [194/300], Step [73/172], Loss: 48.3731\n",
      "Epoch [194/300], Step [74/172], Loss: 24.9772\n",
      "Epoch [194/300], Step [75/172], Loss: 26.1859\n",
      "Epoch [194/300], Step [76/172], Loss: 27.9189\n",
      "Epoch [194/300], Step [77/172], Loss: 47.3829\n",
      "Epoch [194/300], Step [78/172], Loss: 36.7089\n",
      "Epoch [194/300], Step [79/172], Loss: 35.9251\n",
      "Epoch [194/300], Step [80/172], Loss: 49.3911\n",
      "Epoch [194/300], Step [81/172], Loss: 33.1265\n",
      "Epoch [194/300], Step [82/172], Loss: 35.0850\n",
      "Epoch [194/300], Step [83/172], Loss: 42.7246\n",
      "Epoch [194/300], Step [84/172], Loss: 32.3470\n",
      "Epoch [194/300], Step [85/172], Loss: 37.7410\n",
      "Epoch [194/300], Step [86/172], Loss: 32.0709\n",
      "Epoch [194/300], Step [87/172], Loss: 25.4288\n",
      "Epoch [194/300], Step [88/172], Loss: 23.4420\n",
      "Epoch [194/300], Step [89/172], Loss: 26.2521\n",
      "Epoch [194/300], Step [90/172], Loss: 20.1456\n",
      "Epoch [194/300], Step [91/172], Loss: 25.7600\n",
      "Epoch [194/300], Step [92/172], Loss: 19.4109\n",
      "Epoch [194/300], Step [93/172], Loss: 20.0866\n",
      "Epoch [194/300], Step [94/172], Loss: 28.0313\n",
      "Epoch [194/300], Step [95/172], Loss: 20.3849\n",
      "Epoch [194/300], Step [96/172], Loss: 19.7825\n",
      "Epoch [194/300], Step [97/172], Loss: 27.9145\n",
      "Epoch [194/300], Step [98/172], Loss: 18.9480\n",
      "Epoch [194/300], Step [99/172], Loss: 18.7856\n",
      "Epoch [194/300], Step [100/172], Loss: 15.2955\n",
      "Epoch [194/300], Step [101/172], Loss: 18.8048\n",
      "Epoch [194/300], Step [102/172], Loss: 16.7062\n",
      "Epoch [194/300], Step [103/172], Loss: 12.3154\n",
      "Epoch [194/300], Step [104/172], Loss: 18.4506\n",
      "Epoch [194/300], Step [105/172], Loss: 19.3485\n",
      "Epoch [194/300], Step [106/172], Loss: 15.4127\n",
      "Epoch [194/300], Step [107/172], Loss: 15.5395\n",
      "Epoch [194/300], Step [108/172], Loss: 14.2538\n",
      "Epoch [194/300], Step [109/172], Loss: 14.2635\n",
      "Epoch [194/300], Step [110/172], Loss: 15.6998\n",
      "Epoch [194/300], Step [111/172], Loss: 15.5911\n",
      "Epoch [194/300], Step [112/172], Loss: 17.1255\n",
      "Epoch [194/300], Step [113/172], Loss: 11.7658\n",
      "Epoch [194/300], Step [114/172], Loss: 13.6465\n",
      "Epoch [194/300], Step [115/172], Loss: 18.8752\n",
      "Epoch [194/300], Step [116/172], Loss: 14.2526\n",
      "Epoch [194/300], Step [117/172], Loss: 11.5634\n",
      "Epoch [194/300], Step [118/172], Loss: 13.7696\n",
      "Epoch [194/300], Step [119/172], Loss: 15.7180\n",
      "Epoch [194/300], Step [120/172], Loss: 10.0116\n",
      "Epoch [194/300], Step [121/172], Loss: 9.2972\n",
      "Epoch [194/300], Step [122/172], Loss: 10.6858\n",
      "Epoch [194/300], Step [123/172], Loss: 10.0277\n",
      "Epoch [194/300], Step [124/172], Loss: 7.3760\n",
      "Epoch [194/300], Step [125/172], Loss: 11.4443\n",
      "Epoch [194/300], Step [126/172], Loss: 10.9973\n",
      "Epoch [194/300], Step [127/172], Loss: 10.5266\n",
      "Epoch [194/300], Step [128/172], Loss: 10.0760\n",
      "Epoch [194/300], Step [129/172], Loss: 7.9323\n",
      "Epoch [194/300], Step [130/172], Loss: 12.3720\n",
      "Epoch [194/300], Step [131/172], Loss: 7.1514\n",
      "Epoch [194/300], Step [132/172], Loss: 8.3563\n",
      "Epoch [194/300], Step [133/172], Loss: 9.0167\n",
      "Epoch [194/300], Step [134/172], Loss: 11.0452\n",
      "Epoch [194/300], Step [135/172], Loss: 8.3549\n",
      "Epoch [194/300], Step [136/172], Loss: 7.9431\n",
      "Epoch [194/300], Step [137/172], Loss: 9.0572\n",
      "Epoch [194/300], Step [138/172], Loss: 6.7092\n",
      "Epoch [194/300], Step [139/172], Loss: 9.4008\n",
      "Epoch [194/300], Step [140/172], Loss: 9.6176\n",
      "Epoch [194/300], Step [141/172], Loss: 9.0339\n",
      "Epoch [194/300], Step [142/172], Loss: 14.0157\n",
      "Epoch [194/300], Step [143/172], Loss: 10.5590\n",
      "Epoch [194/300], Step [144/172], Loss: 8.8352\n",
      "Epoch [194/300], Step [145/172], Loss: 9.9797\n",
      "Epoch [194/300], Step [146/172], Loss: 9.1973\n",
      "Epoch [194/300], Step [147/172], Loss: 5.1786\n",
      "Epoch [194/300], Step [148/172], Loss: 5.9092\n",
      "Epoch [194/300], Step [149/172], Loss: 6.4660\n",
      "Epoch [194/300], Step [150/172], Loss: 5.7082\n",
      "Epoch [194/300], Step [151/172], Loss: 5.2263\n",
      "Epoch [194/300], Step [152/172], Loss: 7.1885\n",
      "Epoch [194/300], Step [153/172], Loss: 6.2170\n",
      "Epoch [194/300], Step [154/172], Loss: 7.2262\n",
      "Epoch [194/300], Step [155/172], Loss: 5.8599\n",
      "Epoch [194/300], Step [156/172], Loss: 13.2827\n",
      "Epoch [194/300], Step [157/172], Loss: 9.0827\n",
      "Epoch [194/300], Step [158/172], Loss: 6.8851\n",
      "Epoch [194/300], Step [159/172], Loss: 9.2226\n",
      "Epoch [194/300], Step [160/172], Loss: 9.6843\n",
      "Epoch [194/300], Step [161/172], Loss: 6.9637\n",
      "Epoch [194/300], Step [162/172], Loss: 4.9363\n",
      "Epoch [194/300], Step [163/172], Loss: 6.3694\n",
      "Epoch [194/300], Step [164/172], Loss: 8.5159\n",
      "Epoch [194/300], Step [165/172], Loss: 6.0928\n",
      "Epoch [194/300], Step [166/172], Loss: 5.4586\n",
      "Epoch [194/300], Step [167/172], Loss: 10.3443\n",
      "Epoch [194/300], Step [168/172], Loss: 6.2016\n",
      "Epoch [194/300], Step [169/172], Loss: 6.5380\n",
      "Epoch [194/300], Step [170/172], Loss: 4.6199\n",
      "Epoch [194/300], Step [171/172], Loss: 7.8789\n",
      "Epoch [194/300], Step [172/172], Loss: 5.1549\n",
      "Epoch [195/300], Step [1/172], Loss: 50.9891\n",
      "Epoch [195/300], Step [2/172], Loss: 53.1959\n",
      "Epoch [195/300], Step [3/172], Loss: 48.1187\n",
      "Epoch [195/300], Step [4/172], Loss: 26.7822\n",
      "Epoch [195/300], Step [5/172], Loss: 44.9806\n",
      "Epoch [195/300], Step [6/172], Loss: 18.6099\n",
      "Epoch [195/300], Step [7/172], Loss: 28.3163\n",
      "Epoch [195/300], Step [8/172], Loss: 4.2395\n",
      "Epoch [195/300], Step [9/172], Loss: 31.4441\n",
      "Epoch [195/300], Step [10/172], Loss: 41.8357\n",
      "Epoch [195/300], Step [11/172], Loss: 57.2551\n",
      "Epoch [195/300], Step [12/172], Loss: 63.6515\n",
      "Epoch [195/300], Step [13/172], Loss: 34.6217\n",
      "Epoch [195/300], Step [14/172], Loss: 60.6595\n",
      "Epoch [195/300], Step [15/172], Loss: 54.9412\n",
      "Epoch [195/300], Step [16/172], Loss: 10.3058\n",
      "Epoch [195/300], Step [17/172], Loss: 43.8574\n",
      "Epoch [195/300], Step [18/172], Loss: 57.8356\n",
      "Epoch [195/300], Step [19/172], Loss: 81.4685\n",
      "Epoch [195/300], Step [20/172], Loss: 34.6664\n",
      "Epoch [195/300], Step [21/172], Loss: 84.1407\n",
      "Epoch [195/300], Step [22/172], Loss: 59.1147\n",
      "Epoch [195/300], Step [23/172], Loss: 1.8508\n",
      "Epoch [195/300], Step [24/172], Loss: 54.8173\n",
      "Epoch [195/300], Step [25/172], Loss: 40.3414\n",
      "Epoch [195/300], Step [26/172], Loss: 46.9282\n",
      "Epoch [195/300], Step [27/172], Loss: 60.3862\n",
      "Epoch [195/300], Step [28/172], Loss: 23.8845\n",
      "Epoch [195/300], Step [29/172], Loss: 16.5557\n",
      "Epoch [195/300], Step [30/172], Loss: 62.1263\n",
      "Epoch [195/300], Step [31/172], Loss: 36.6635\n",
      "Epoch [195/300], Step [32/172], Loss: 43.0485\n",
      "Epoch [195/300], Step [33/172], Loss: 70.1293\n",
      "Epoch [195/300], Step [34/172], Loss: 2.6237\n",
      "Epoch [195/300], Step [35/172], Loss: 13.9827\n",
      "Epoch [195/300], Step [36/172], Loss: 17.6650\n",
      "Epoch [195/300], Step [37/172], Loss: 16.6228\n",
      "Epoch [195/300], Step [38/172], Loss: 29.8427\n",
      "Epoch [195/300], Step [39/172], Loss: 36.0873\n",
      "Epoch [195/300], Step [40/172], Loss: 20.8325\n",
      "Epoch [195/300], Step [41/172], Loss: 33.5995\n",
      "Epoch [195/300], Step [42/172], Loss: 38.2649\n",
      "Epoch [195/300], Step [43/172], Loss: 27.5112\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [195/300], Step [44/172], Loss: 21.2332\n",
      "Epoch [195/300], Step [45/172], Loss: 27.3252\n",
      "Epoch [195/300], Step [46/172], Loss: 17.4198\n",
      "Epoch [195/300], Step [47/172], Loss: 47.8880\n",
      "Epoch [195/300], Step [48/172], Loss: 61.9374\n",
      "Epoch [195/300], Step [49/172], Loss: 21.3981\n",
      "Epoch [195/300], Step [50/172], Loss: 47.4843\n",
      "Epoch [195/300], Step [51/172], Loss: 8.6336\n",
      "Epoch [195/300], Step [52/172], Loss: 19.7589\n",
      "Epoch [195/300], Step [53/172], Loss: 22.4635\n",
      "Epoch [195/300], Step [54/172], Loss: 14.7272\n",
      "Epoch [195/300], Step [55/172], Loss: 14.4305\n",
      "Epoch [195/300], Step [56/172], Loss: 18.2214\n",
      "Epoch [195/300], Step [57/172], Loss: 16.7230\n",
      "Epoch [195/300], Step [58/172], Loss: 13.0701\n",
      "Epoch [195/300], Step [59/172], Loss: 27.0547\n",
      "Epoch [195/300], Step [60/172], Loss: 24.2950\n",
      "Epoch [195/300], Step [61/172], Loss: 6.0900\n",
      "Epoch [195/300], Step [62/172], Loss: 19.0205\n",
      "Epoch [195/300], Step [63/172], Loss: 10.1404\n",
      "Epoch [195/300], Step [64/172], Loss: 10.6987\n",
      "Epoch [195/300], Step [65/172], Loss: 19.3792\n",
      "Epoch [195/300], Step [66/172], Loss: 6.4656\n",
      "Epoch [195/300], Step [67/172], Loss: 22.4246\n",
      "Epoch [195/300], Step [68/172], Loss: 5.2764\n",
      "Epoch [195/300], Step [69/172], Loss: 33.7044\n",
      "Epoch [195/300], Step [70/172], Loss: 36.9363\n",
      "Epoch [195/300], Step [71/172], Loss: 38.9977\n",
      "Epoch [195/300], Step [72/172], Loss: 38.8446\n",
      "Epoch [195/300], Step [73/172], Loss: 47.9307\n",
      "Epoch [195/300], Step [74/172], Loss: 24.8368\n",
      "Epoch [195/300], Step [75/172], Loss: 26.0780\n",
      "Epoch [195/300], Step [76/172], Loss: 27.7552\n",
      "Epoch [195/300], Step [77/172], Loss: 46.9658\n",
      "Epoch [195/300], Step [78/172], Loss: 36.4616\n",
      "Epoch [195/300], Step [79/172], Loss: 35.6532\n",
      "Epoch [195/300], Step [80/172], Loss: 49.0012\n",
      "Epoch [195/300], Step [81/172], Loss: 32.9474\n",
      "Epoch [195/300], Step [82/172], Loss: 34.7449\n",
      "Epoch [195/300], Step [83/172], Loss: 42.5980\n",
      "Epoch [195/300], Step [84/172], Loss: 32.3190\n",
      "Epoch [195/300], Step [85/172], Loss: 37.6103\n",
      "Epoch [195/300], Step [86/172], Loss: 31.9660\n",
      "Epoch [195/300], Step [87/172], Loss: 25.3250\n",
      "Epoch [195/300], Step [88/172], Loss: 23.2655\n",
      "Epoch [195/300], Step [89/172], Loss: 26.0993\n",
      "Epoch [195/300], Step [90/172], Loss: 19.9706\n",
      "Epoch [195/300], Step [91/172], Loss: 25.6709\n",
      "Epoch [195/300], Step [92/172], Loss: 19.3819\n",
      "Epoch [195/300], Step [93/172], Loss: 20.0553\n",
      "Epoch [195/300], Step [94/172], Loss: 27.9441\n",
      "Epoch [195/300], Step [95/172], Loss: 20.4370\n",
      "Epoch [195/300], Step [96/172], Loss: 19.7628\n",
      "Epoch [195/300], Step [97/172], Loss: 27.8616\n",
      "Epoch [195/300], Step [98/172], Loss: 18.8445\n",
      "Epoch [195/300], Step [99/172], Loss: 18.7598\n",
      "Epoch [195/300], Step [100/172], Loss: 15.2259\n",
      "Epoch [195/300], Step [101/172], Loss: 18.8455\n",
      "Epoch [195/300], Step [102/172], Loss: 16.5335\n",
      "Epoch [195/300], Step [103/172], Loss: 12.2903\n",
      "Epoch [195/300], Step [104/172], Loss: 18.4737\n",
      "Epoch [195/300], Step [105/172], Loss: 19.3053\n",
      "Epoch [195/300], Step [106/172], Loss: 15.4209\n",
      "Epoch [195/300], Step [107/172], Loss: 15.5609\n",
      "Epoch [195/300], Step [108/172], Loss: 14.1706\n",
      "Epoch [195/300], Step [109/172], Loss: 14.1686\n",
      "Epoch [195/300], Step [110/172], Loss: 15.6322\n",
      "Epoch [195/300], Step [111/172], Loss: 15.6608\n",
      "Epoch [195/300], Step [112/172], Loss: 17.1566\n",
      "Epoch [195/300], Step [113/172], Loss: 11.8085\n",
      "Epoch [195/300], Step [114/172], Loss: 13.6962\n",
      "Epoch [195/300], Step [115/172], Loss: 18.8448\n",
      "Epoch [195/300], Step [116/172], Loss: 14.2386\n",
      "Epoch [195/300], Step [117/172], Loss: 11.6265\n",
      "Epoch [195/300], Step [118/172], Loss: 13.7653\n",
      "Epoch [195/300], Step [119/172], Loss: 15.7244\n",
      "Epoch [195/300], Step [120/172], Loss: 10.0325\n",
      "Epoch [195/300], Step [121/172], Loss: 9.2832\n",
      "Epoch [195/300], Step [122/172], Loss: 10.7237\n",
      "Epoch [195/300], Step [123/172], Loss: 10.1216\n",
      "Epoch [195/300], Step [124/172], Loss: 7.3879\n",
      "Epoch [195/300], Step [125/172], Loss: 11.4572\n",
      "Epoch [195/300], Step [126/172], Loss: 11.0132\n",
      "Epoch [195/300], Step [127/172], Loss: 10.5373\n",
      "Epoch [195/300], Step [128/172], Loss: 10.0853\n",
      "Epoch [195/300], Step [129/172], Loss: 7.9651\n",
      "Epoch [195/300], Step [130/172], Loss: 12.4036\n",
      "Epoch [195/300], Step [131/172], Loss: 7.1858\n",
      "Epoch [195/300], Step [132/172], Loss: 8.3584\n",
      "Epoch [195/300], Step [133/172], Loss: 9.0792\n",
      "Epoch [195/300], Step [134/172], Loss: 11.0801\n",
      "Epoch [195/300], Step [135/172], Loss: 8.4072\n",
      "Epoch [195/300], Step [136/172], Loss: 8.0386\n",
      "Epoch [195/300], Step [137/172], Loss: 9.0776\n",
      "Epoch [195/300], Step [138/172], Loss: 6.7320\n",
      "Epoch [195/300], Step [139/172], Loss: 9.4353\n",
      "Epoch [195/300], Step [140/172], Loss: 9.6660\n",
      "Epoch [195/300], Step [141/172], Loss: 9.0455\n",
      "Epoch [195/300], Step [142/172], Loss: 14.0114\n",
      "Epoch [195/300], Step [143/172], Loss: 10.6034\n",
      "Epoch [195/300], Step [144/172], Loss: 8.8473\n",
      "Epoch [195/300], Step [145/172], Loss: 9.9954\n",
      "Epoch [195/300], Step [146/172], Loss: 9.1948\n",
      "Epoch [195/300], Step [147/172], Loss: 5.2134\n",
      "Epoch [195/300], Step [148/172], Loss: 5.9411\n",
      "Epoch [195/300], Step [149/172], Loss: 6.4885\n",
      "Epoch [195/300], Step [150/172], Loss: 5.7182\n",
      "Epoch [195/300], Step [151/172], Loss: 5.2096\n",
      "Epoch [195/300], Step [152/172], Loss: 7.2065\n",
      "Epoch [195/300], Step [153/172], Loss: 6.2241\n",
      "Epoch [195/300], Step [154/172], Loss: 7.2791\n",
      "Epoch [195/300], Step [155/172], Loss: 5.8617\n",
      "Epoch [195/300], Step [156/172], Loss: 13.2658\n",
      "Epoch [195/300], Step [157/172], Loss: 9.1567\n",
      "Epoch [195/300], Step [158/172], Loss: 6.8858\n",
      "Epoch [195/300], Step [159/172], Loss: 9.3097\n",
      "Epoch [195/300], Step [160/172], Loss: 9.6702\n",
      "Epoch [195/300], Step [161/172], Loss: 7.0139\n",
      "Epoch [195/300], Step [162/172], Loss: 4.9416\n",
      "Epoch [195/300], Step [163/172], Loss: 6.3644\n",
      "Epoch [195/300], Step [164/172], Loss: 8.4330\n",
      "Epoch [195/300], Step [165/172], Loss: 6.1320\n",
      "Epoch [195/300], Step [166/172], Loss: 5.4731\n",
      "Epoch [195/300], Step [167/172], Loss: 10.4147\n",
      "Epoch [195/300], Step [168/172], Loss: 6.2228\n",
      "Epoch [195/300], Step [169/172], Loss: 6.5555\n",
      "Epoch [195/300], Step [170/172], Loss: 4.6423\n",
      "Epoch [195/300], Step [171/172], Loss: 7.9163\n",
      "Epoch [195/300], Step [172/172], Loss: 5.2030\n",
      "Epoch [196/300], Step [1/172], Loss: 50.7444\n",
      "Epoch [196/300], Step [2/172], Loss: 52.9091\n",
      "Epoch [196/300], Step [3/172], Loss: 48.2704\n",
      "Epoch [196/300], Step [4/172], Loss: 26.8110\n",
      "Epoch [196/300], Step [5/172], Loss: 44.8964\n",
      "Epoch [196/300], Step [6/172], Loss: 18.5509\n",
      "Epoch [196/300], Step [7/172], Loss: 28.3272\n",
      "Epoch [196/300], Step [8/172], Loss: 4.7410\n",
      "Epoch [196/300], Step [9/172], Loss: 31.3158\n",
      "Epoch [196/300], Step [10/172], Loss: 41.5125\n",
      "Epoch [196/300], Step [11/172], Loss: 57.0511\n",
      "Epoch [196/300], Step [12/172], Loss: 63.3550\n",
      "Epoch [196/300], Step [13/172], Loss: 34.3443\n",
      "Epoch [196/300], Step [14/172], Loss: 60.6146\n",
      "Epoch [196/300], Step [15/172], Loss: 54.8022\n",
      "Epoch [196/300], Step [16/172], Loss: 9.5400\n",
      "Epoch [196/300], Step [17/172], Loss: 43.8086\n",
      "Epoch [196/300], Step [18/172], Loss: 57.4710\n",
      "Epoch [196/300], Step [19/172], Loss: 81.2208\n",
      "Epoch [196/300], Step [20/172], Loss: 34.3161\n",
      "Epoch [196/300], Step [21/172], Loss: 83.7481\n",
      "Epoch [196/300], Step [22/172], Loss: 58.9875\n",
      "Epoch [196/300], Step [23/172], Loss: 1.8142\n",
      "Epoch [196/300], Step [24/172], Loss: 54.6290\n",
      "Epoch [196/300], Step [25/172], Loss: 40.0872\n",
      "Epoch [196/300], Step [26/172], Loss: 46.6377\n",
      "Epoch [196/300], Step [27/172], Loss: 60.0854\n",
      "Epoch [196/300], Step [28/172], Loss: 23.6585\n",
      "Epoch [196/300], Step [29/172], Loss: 16.3326\n",
      "Epoch [196/300], Step [30/172], Loss: 61.9391\n",
      "Epoch [196/300], Step [31/172], Loss: 36.4638\n",
      "Epoch [196/300], Step [32/172], Loss: 42.8587\n",
      "Epoch [196/300], Step [33/172], Loss: 69.7816\n",
      "Epoch [196/300], Step [34/172], Loss: 2.6302\n",
      "Epoch [196/300], Step [35/172], Loss: 14.1073\n",
      "Epoch [196/300], Step [36/172], Loss: 17.9247\n",
      "Epoch [196/300], Step [37/172], Loss: 16.6601\n",
      "Epoch [196/300], Step [38/172], Loss: 29.9754\n",
      "Epoch [196/300], Step [39/172], Loss: 36.0299\n",
      "Epoch [196/300], Step [40/172], Loss: 20.7818\n",
      "Epoch [196/300], Step [41/172], Loss: 33.5755\n",
      "Epoch [196/300], Step [42/172], Loss: 38.4923\n",
      "Epoch [196/300], Step [43/172], Loss: 27.5918\n",
      "Epoch [196/300], Step [44/172], Loss: 21.2332\n",
      "Epoch [196/300], Step [45/172], Loss: 27.2445\n",
      "Epoch [196/300], Step [46/172], Loss: 17.3186\n",
      "Epoch [196/300], Step [47/172], Loss: 47.8230\n",
      "Epoch [196/300], Step [48/172], Loss: 61.5051\n",
      "Epoch [196/300], Step [49/172], Loss: 21.3441\n",
      "Epoch [196/300], Step [50/172], Loss: 47.4775\n",
      "Epoch [196/300], Step [51/172], Loss: 8.6523\n",
      "Epoch [196/300], Step [52/172], Loss: 19.6936\n",
      "Epoch [196/300], Step [53/172], Loss: 22.1944\n",
      "Epoch [196/300], Step [54/172], Loss: 14.5070\n",
      "Epoch [196/300], Step [55/172], Loss: 14.2140\n",
      "Epoch [196/300], Step [56/172], Loss: 17.9776\n",
      "Epoch [196/300], Step [57/172], Loss: 16.7927\n",
      "Epoch [196/300], Step [58/172], Loss: 12.9660\n",
      "Epoch [196/300], Step [59/172], Loss: 26.8416\n",
      "Epoch [196/300], Step [60/172], Loss: 24.1523\n",
      "Epoch [196/300], Step [61/172], Loss: 5.9658\n",
      "Epoch [196/300], Step [62/172], Loss: 18.7283\n",
      "Epoch [196/300], Step [63/172], Loss: 10.0082\n",
      "Epoch [196/300], Step [64/172], Loss: 10.5448\n",
      "Epoch [196/300], Step [65/172], Loss: 19.1496\n",
      "Epoch [196/300], Step [66/172], Loss: 6.2838\n",
      "Epoch [196/300], Step [67/172], Loss: 22.3518\n",
      "Epoch [196/300], Step [68/172], Loss: 4.7336\n",
      "Epoch [196/300], Step [69/172], Loss: 34.0000\n",
      "Epoch [196/300], Step [70/172], Loss: 37.2641\n",
      "Epoch [196/300], Step [71/172], Loss: 39.1949\n",
      "Epoch [196/300], Step [72/172], Loss: 39.0869\n",
      "Epoch [196/300], Step [73/172], Loss: 48.3052\n",
      "Epoch [196/300], Step [74/172], Loss: 25.0020\n",
      "Epoch [196/300], Step [75/172], Loss: 26.1576\n",
      "Epoch [196/300], Step [76/172], Loss: 27.7879\n",
      "Epoch [196/300], Step [77/172], Loss: 47.0773\n",
      "Epoch [196/300], Step [78/172], Loss: 36.5924\n",
      "Epoch [196/300], Step [79/172], Loss: 35.6748\n",
      "Epoch [196/300], Step [80/172], Loss: 49.1540\n",
      "Epoch [196/300], Step [81/172], Loss: 32.8982\n",
      "Epoch [196/300], Step [82/172], Loss: 35.1444\n",
      "Epoch [196/300], Step [83/172], Loss: 42.6369\n",
      "Epoch [196/300], Step [84/172], Loss: 32.3095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [196/300], Step [85/172], Loss: 37.6624\n",
      "Epoch [196/300], Step [86/172], Loss: 32.1207\n",
      "Epoch [196/300], Step [87/172], Loss: 25.2872\n",
      "Epoch [196/300], Step [88/172], Loss: 23.3316\n",
      "Epoch [196/300], Step [89/172], Loss: 26.3713\n",
      "Epoch [196/300], Step [90/172], Loss: 20.0890\n",
      "Epoch [196/300], Step [91/172], Loss: 25.7040\n",
      "Epoch [196/300], Step [92/172], Loss: 19.4020\n",
      "Epoch [196/300], Step [93/172], Loss: 20.0545\n",
      "Epoch [196/300], Step [94/172], Loss: 27.8960\n",
      "Epoch [196/300], Step [95/172], Loss: 20.3067\n",
      "Epoch [196/300], Step [96/172], Loss: 19.7952\n",
      "Epoch [196/300], Step [97/172], Loss: 28.0234\n",
      "Epoch [196/300], Step [98/172], Loss: 18.9762\n",
      "Epoch [196/300], Step [99/172], Loss: 18.7605\n",
      "Epoch [196/300], Step [100/172], Loss: 15.3440\n",
      "Epoch [196/300], Step [101/172], Loss: 18.8392\n",
      "Epoch [196/300], Step [102/172], Loss: 16.7098\n",
      "Epoch [196/300], Step [103/172], Loss: 12.2982\n",
      "Epoch [196/300], Step [104/172], Loss: 18.4695\n",
      "Epoch [196/300], Step [105/172], Loss: 19.4687\n",
      "Epoch [196/300], Step [106/172], Loss: 15.4022\n",
      "Epoch [196/300], Step [107/172], Loss: 15.5998\n",
      "Epoch [196/300], Step [108/172], Loss: 14.2415\n",
      "Epoch [196/300], Step [109/172], Loss: 14.2589\n",
      "Epoch [196/300], Step [110/172], Loss: 15.7078\n",
      "Epoch [196/300], Step [111/172], Loss: 15.7393\n",
      "Epoch [196/300], Step [112/172], Loss: 17.0718\n",
      "Epoch [196/300], Step [113/172], Loss: 11.8673\n",
      "Epoch [196/300], Step [114/172], Loss: 13.7100\n",
      "Epoch [196/300], Step [115/172], Loss: 18.9278\n",
      "Epoch [196/300], Step [116/172], Loss: 14.2298\n",
      "Epoch [196/300], Step [117/172], Loss: 11.7002\n",
      "Epoch [196/300], Step [118/172], Loss: 13.7954\n",
      "Epoch [196/300], Step [119/172], Loss: 15.7399\n",
      "Epoch [196/300], Step [120/172], Loss: 10.0284\n",
      "Epoch [196/300], Step [121/172], Loss: 9.2886\n",
      "Epoch [196/300], Step [122/172], Loss: 10.6318\n",
      "Epoch [196/300], Step [123/172], Loss: 10.0721\n",
      "Epoch [196/300], Step [124/172], Loss: 7.3452\n",
      "Epoch [196/300], Step [125/172], Loss: 11.3377\n",
      "Epoch [196/300], Step [126/172], Loss: 10.9539\n",
      "Epoch [196/300], Step [127/172], Loss: 10.4765\n",
      "Epoch [196/300], Step [128/172], Loss: 10.0709\n",
      "Epoch [196/300], Step [129/172], Loss: 7.9621\n",
      "Epoch [196/300], Step [130/172], Loss: 12.3901\n",
      "Epoch [196/300], Step [131/172], Loss: 7.1332\n",
      "Epoch [196/300], Step [132/172], Loss: 8.3676\n",
      "Epoch [196/300], Step [133/172], Loss: 9.0085\n",
      "Epoch [196/300], Step [134/172], Loss: 11.0198\n",
      "Epoch [196/300], Step [135/172], Loss: 8.3772\n",
      "Epoch [196/300], Step [136/172], Loss: 7.9690\n",
      "Epoch [196/300], Step [137/172], Loss: 9.0671\n",
      "Epoch [196/300], Step [138/172], Loss: 6.7180\n",
      "Epoch [196/300], Step [139/172], Loss: 9.3283\n",
      "Epoch [196/300], Step [140/172], Loss: 9.6536\n",
      "Epoch [196/300], Step [141/172], Loss: 8.9637\n",
      "Epoch [196/300], Step [142/172], Loss: 13.9453\n",
      "Epoch [196/300], Step [143/172], Loss: 10.5690\n",
      "Epoch [196/300], Step [144/172], Loss: 8.7999\n",
      "Epoch [196/300], Step [145/172], Loss: 9.9931\n",
      "Epoch [196/300], Step [146/172], Loss: 9.2122\n",
      "Epoch [196/300], Step [147/172], Loss: 5.1372\n",
      "Epoch [196/300], Step [148/172], Loss: 5.9273\n",
      "Epoch [196/300], Step [149/172], Loss: 6.4174\n",
      "Epoch [196/300], Step [150/172], Loss: 5.6997\n",
      "Epoch [196/300], Step [151/172], Loss: 5.2102\n",
      "Epoch [196/300], Step [152/172], Loss: 7.1898\n",
      "Epoch [196/300], Step [153/172], Loss: 6.1587\n",
      "Epoch [196/300], Step [154/172], Loss: 7.2939\n",
      "Epoch [196/300], Step [155/172], Loss: 5.7965\n",
      "Epoch [196/300], Step [156/172], Loss: 13.1577\n",
      "Epoch [196/300], Step [157/172], Loss: 9.0655\n",
      "Epoch [196/300], Step [158/172], Loss: 6.8220\n",
      "Epoch [196/300], Step [159/172], Loss: 9.2689\n",
      "Epoch [196/300], Step [160/172], Loss: 9.5782\n",
      "Epoch [196/300], Step [161/172], Loss: 6.9713\n",
      "Epoch [196/300], Step [162/172], Loss: 4.9300\n",
      "Epoch [196/300], Step [163/172], Loss: 6.3436\n",
      "Epoch [196/300], Step [164/172], Loss: 8.5640\n",
      "Epoch [196/300], Step [165/172], Loss: 6.1152\n",
      "Epoch [196/300], Step [166/172], Loss: 5.4952\n",
      "Epoch [196/300], Step [167/172], Loss: 10.3720\n",
      "Epoch [196/300], Step [168/172], Loss: 6.2317\n",
      "Epoch [196/300], Step [169/172], Loss: 6.5293\n",
      "Epoch [196/300], Step [170/172], Loss: 4.6556\n",
      "Epoch [196/300], Step [171/172], Loss: 7.9302\n",
      "Epoch [196/300], Step [172/172], Loss: 5.1860\n",
      "Epoch [197/300], Step [1/172], Loss: 50.8125\n",
      "Epoch [197/300], Step [2/172], Loss: 53.0621\n",
      "Epoch [197/300], Step [3/172], Loss: 47.7743\n",
      "Epoch [197/300], Step [4/172], Loss: 26.8106\n",
      "Epoch [197/300], Step [5/172], Loss: 45.4203\n",
      "Epoch [197/300], Step [6/172], Loss: 19.0135\n",
      "Epoch [197/300], Step [7/172], Loss: 29.3347\n",
      "Epoch [197/300], Step [8/172], Loss: 4.2627\n",
      "Epoch [197/300], Step [9/172], Loss: 31.3125\n",
      "Epoch [197/300], Step [10/172], Loss: 41.7405\n",
      "Epoch [197/300], Step [11/172], Loss: 56.5632\n",
      "Epoch [197/300], Step [12/172], Loss: 63.0086\n",
      "Epoch [197/300], Step [13/172], Loss: 34.5115\n",
      "Epoch [197/300], Step [14/172], Loss: 60.3435\n",
      "Epoch [197/300], Step [15/172], Loss: 54.6565\n",
      "Epoch [197/300], Step [16/172], Loss: 10.6303\n",
      "Epoch [197/300], Step [17/172], Loss: 43.6096\n",
      "Epoch [197/300], Step [18/172], Loss: 57.2559\n",
      "Epoch [197/300], Step [19/172], Loss: 81.2375\n",
      "Epoch [197/300], Step [20/172], Loss: 33.9723\n",
      "Epoch [197/300], Step [21/172], Loss: 83.6656\n",
      "Epoch [197/300], Step [22/172], Loss: 59.1330\n",
      "Epoch [197/300], Step [23/172], Loss: 1.8694\n",
      "Epoch [197/300], Step [24/172], Loss: 54.5649\n",
      "Epoch [197/300], Step [25/172], Loss: 40.0874\n",
      "Epoch [197/300], Step [26/172], Loss: 46.5776\n",
      "Epoch [197/300], Step [27/172], Loss: 60.0235\n",
      "Epoch [197/300], Step [28/172], Loss: 23.6782\n",
      "Epoch [197/300], Step [29/172], Loss: 16.3270\n",
      "Epoch [197/300], Step [30/172], Loss: 61.7553\n",
      "Epoch [197/300], Step [31/172], Loss: 36.5437\n",
      "Epoch [197/300], Step [32/172], Loss: 42.9429\n",
      "Epoch [197/300], Step [33/172], Loss: 69.7264\n",
      "Epoch [197/300], Step [34/172], Loss: 2.5418\n",
      "Epoch [197/300], Step [35/172], Loss: 14.0265\n",
      "Epoch [197/300], Step [36/172], Loss: 17.6262\n",
      "Epoch [197/300], Step [37/172], Loss: 16.6895\n",
      "Epoch [197/300], Step [38/172], Loss: 30.0425\n",
      "Epoch [197/300], Step [39/172], Loss: 36.0868\n",
      "Epoch [197/300], Step [40/172], Loss: 20.8524\n",
      "Epoch [197/300], Step [41/172], Loss: 33.5690\n",
      "Epoch [197/300], Step [42/172], Loss: 38.4163\n",
      "Epoch [197/300], Step [43/172], Loss: 27.4472\n",
      "Epoch [197/300], Step [44/172], Loss: 21.2189\n",
      "Epoch [197/300], Step [45/172], Loss: 27.4420\n",
      "Epoch [197/300], Step [46/172], Loss: 17.2662\n",
      "Epoch [197/300], Step [47/172], Loss: 47.8662\n",
      "Epoch [197/300], Step [48/172], Loss: 61.3407\n",
      "Epoch [197/300], Step [49/172], Loss: 21.3978\n",
      "Epoch [197/300], Step [50/172], Loss: 47.5810\n",
      "Epoch [197/300], Step [51/172], Loss: 8.6470\n",
      "Epoch [197/300], Step [52/172], Loss: 19.8646\n",
      "Epoch [197/300], Step [53/172], Loss: 22.4570\n",
      "Epoch [197/300], Step [54/172], Loss: 14.8078\n",
      "Epoch [197/300], Step [55/172], Loss: 14.4100\n",
      "Epoch [197/300], Step [56/172], Loss: 18.0207\n",
      "Epoch [197/300], Step [57/172], Loss: 16.6936\n",
      "Epoch [197/300], Step [58/172], Loss: 12.9316\n",
      "Epoch [197/300], Step [59/172], Loss: 27.0562\n",
      "Epoch [197/300], Step [60/172], Loss: 23.3388\n",
      "Epoch [197/300], Step [61/172], Loss: 5.9438\n",
      "Epoch [197/300], Step [62/172], Loss: 18.9310\n",
      "Epoch [197/300], Step [63/172], Loss: 9.9222\n",
      "Epoch [197/300], Step [64/172], Loss: 10.5263\n",
      "Epoch [197/300], Step [65/172], Loss: 19.0128\n",
      "Epoch [197/300], Step [66/172], Loss: 6.3567\n",
      "Epoch [197/300], Step [67/172], Loss: 22.1138\n",
      "Epoch [197/300], Step [68/172], Loss: 4.6754\n",
      "Epoch [197/300], Step [69/172], Loss: 33.7068\n",
      "Epoch [197/300], Step [70/172], Loss: 37.0671\n",
      "Epoch [197/300], Step [71/172], Loss: 39.1697\n",
      "Epoch [197/300], Step [72/172], Loss: 38.9647\n",
      "Epoch [197/300], Step [73/172], Loss: 48.0407\n",
      "Epoch [197/300], Step [74/172], Loss: 24.9576\n",
      "Epoch [197/300], Step [75/172], Loss: 26.0697\n",
      "Epoch [197/300], Step [76/172], Loss: 27.7541\n",
      "Epoch [197/300], Step [77/172], Loss: 46.9050\n",
      "Epoch [197/300], Step [78/172], Loss: 36.5137\n",
      "Epoch [197/300], Step [79/172], Loss: 35.6214\n",
      "Epoch [197/300], Step [80/172], Loss: 48.9222\n",
      "Epoch [197/300], Step [81/172], Loss: 32.7610\n",
      "Epoch [197/300], Step [82/172], Loss: 34.7853\n",
      "Epoch [197/300], Step [83/172], Loss: 42.6149\n",
      "Epoch [197/300], Step [84/172], Loss: 32.2041\n",
      "Epoch [197/300], Step [85/172], Loss: 37.6651\n",
      "Epoch [197/300], Step [86/172], Loss: 32.1007\n",
      "Epoch [197/300], Step [87/172], Loss: 25.2590\n",
      "Epoch [197/300], Step [88/172], Loss: 23.2247\n",
      "Epoch [197/300], Step [89/172], Loss: 26.3222\n",
      "Epoch [197/300], Step [90/172], Loss: 20.0550\n",
      "Epoch [197/300], Step [91/172], Loss: 25.6592\n",
      "Epoch [197/300], Step [92/172], Loss: 19.3497\n",
      "Epoch [197/300], Step [93/172], Loss: 20.0272\n",
      "Epoch [197/300], Step [94/172], Loss: 27.8298\n",
      "Epoch [197/300], Step [95/172], Loss: 20.3354\n",
      "Epoch [197/300], Step [96/172], Loss: 19.7375\n",
      "Epoch [197/300], Step [97/172], Loss: 27.9762\n",
      "Epoch [197/300], Step [98/172], Loss: 18.9080\n",
      "Epoch [197/300], Step [99/172], Loss: 18.7905\n",
      "Epoch [197/300], Step [100/172], Loss: 15.3145\n",
      "Epoch [197/300], Step [101/172], Loss: 18.8986\n",
      "Epoch [197/300], Step [102/172], Loss: 16.5194\n",
      "Epoch [197/300], Step [103/172], Loss: 12.3481\n",
      "Epoch [197/300], Step [104/172], Loss: 18.5225\n",
      "Epoch [197/300], Step [105/172], Loss: 19.4329\n",
      "Epoch [197/300], Step [106/172], Loss: 15.4114\n",
      "Epoch [197/300], Step [107/172], Loss: 15.5882\n",
      "Epoch [197/300], Step [108/172], Loss: 14.2249\n",
      "Epoch [197/300], Step [109/172], Loss: 14.1568\n",
      "Epoch [197/300], Step [110/172], Loss: 15.7539\n",
      "Epoch [197/300], Step [111/172], Loss: 15.8019\n",
      "Epoch [197/300], Step [112/172], Loss: 17.1116\n",
      "Epoch [197/300], Step [113/172], Loss: 11.8065\n",
      "Epoch [197/300], Step [114/172], Loss: 13.7231\n",
      "Epoch [197/300], Step [115/172], Loss: 18.7688\n",
      "Epoch [197/300], Step [116/172], Loss: 14.1833\n",
      "Epoch [197/300], Step [117/172], Loss: 11.5946\n",
      "Epoch [197/300], Step [118/172], Loss: 13.6581\n",
      "Epoch [197/300], Step [119/172], Loss: 15.7624\n",
      "Epoch [197/300], Step [120/172], Loss: 10.0315\n",
      "Epoch [197/300], Step [121/172], Loss: 9.3054\n",
      "Epoch [197/300], Step [122/172], Loss: 10.7026\n",
      "Epoch [197/300], Step [123/172], Loss: 10.0102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [197/300], Step [124/172], Loss: 7.3199\n",
      "Epoch [197/300], Step [125/172], Loss: 11.4008\n",
      "Epoch [197/300], Step [126/172], Loss: 11.0181\n",
      "Epoch [197/300], Step [127/172], Loss: 10.5034\n",
      "Epoch [197/300], Step [128/172], Loss: 10.0624\n",
      "Epoch [197/300], Step [129/172], Loss: 7.9732\n",
      "Epoch [197/300], Step [130/172], Loss: 12.4079\n",
      "Epoch [197/300], Step [131/172], Loss: 7.1633\n",
      "Epoch [197/300], Step [132/172], Loss: 8.4348\n",
      "Epoch [197/300], Step [133/172], Loss: 9.0656\n",
      "Epoch [197/300], Step [134/172], Loss: 11.0078\n",
      "Epoch [197/300], Step [135/172], Loss: 8.3942\n",
      "Epoch [197/300], Step [136/172], Loss: 7.8140\n",
      "Epoch [197/300], Step [137/172], Loss: 8.9850\n",
      "Epoch [197/300], Step [138/172], Loss: 6.7507\n",
      "Epoch [197/300], Step [139/172], Loss: 9.3805\n",
      "Epoch [197/300], Step [140/172], Loss: 9.6629\n",
      "Epoch [197/300], Step [141/172], Loss: 9.0275\n",
      "Epoch [197/300], Step [142/172], Loss: 13.9489\n",
      "Epoch [197/300], Step [143/172], Loss: 10.5919\n",
      "Epoch [197/300], Step [144/172], Loss: 8.8688\n",
      "Epoch [197/300], Step [145/172], Loss: 10.0036\n",
      "Epoch [197/300], Step [146/172], Loss: 9.2203\n",
      "Epoch [197/300], Step [147/172], Loss: 5.2154\n",
      "Epoch [197/300], Step [148/172], Loss: 5.9408\n",
      "Epoch [197/300], Step [149/172], Loss: 6.4223\n",
      "Epoch [197/300], Step [150/172], Loss: 5.6583\n",
      "Epoch [197/300], Step [151/172], Loss: 5.2039\n",
      "Epoch [197/300], Step [152/172], Loss: 7.2156\n",
      "Epoch [197/300], Step [153/172], Loss: 6.1837\n",
      "Epoch [197/300], Step [154/172], Loss: 7.1977\n",
      "Epoch [197/300], Step [155/172], Loss: 5.8417\n",
      "Epoch [197/300], Step [156/172], Loss: 13.2625\n",
      "Epoch [197/300], Step [157/172], Loss: 9.0447\n",
      "Epoch [197/300], Step [158/172], Loss: 6.8370\n",
      "Epoch [197/300], Step [159/172], Loss: 9.1879\n",
      "Epoch [197/300], Step [160/172], Loss: 9.6847\n",
      "Epoch [197/300], Step [161/172], Loss: 6.8270\n",
      "Epoch [197/300], Step [162/172], Loss: 4.9175\n",
      "Epoch [197/300], Step [163/172], Loss: 6.4118\n",
      "Epoch [197/300], Step [164/172], Loss: 8.4358\n",
      "Epoch [197/300], Step [165/172], Loss: 6.1348\n",
      "Epoch [197/300], Step [166/172], Loss: 5.4837\n",
      "Epoch [197/300], Step [167/172], Loss: 10.3073\n",
      "Epoch [197/300], Step [168/172], Loss: 6.2271\n",
      "Epoch [197/300], Step [169/172], Loss: 6.5250\n",
      "Epoch [197/300], Step [170/172], Loss: 4.6399\n",
      "Epoch [197/300], Step [171/172], Loss: 7.9128\n",
      "Epoch [197/300], Step [172/172], Loss: 5.1400\n",
      "Epoch [198/300], Step [1/172], Loss: 50.6182\n",
      "Epoch [198/300], Step [2/172], Loss: 52.9632\n",
      "Epoch [198/300], Step [3/172], Loss: 48.0880\n",
      "Epoch [198/300], Step [4/172], Loss: 26.5327\n",
      "Epoch [198/300], Step [5/172], Loss: 44.9393\n",
      "Epoch [198/300], Step [6/172], Loss: 18.7353\n",
      "Epoch [198/300], Step [7/172], Loss: 28.7351\n",
      "Epoch [198/300], Step [8/172], Loss: 4.8419\n",
      "Epoch [198/300], Step [9/172], Loss: 31.3698\n",
      "Epoch [198/300], Step [10/172], Loss: 41.4952\n",
      "Epoch [198/300], Step [11/172], Loss: 56.6782\n",
      "Epoch [198/300], Step [12/172], Loss: 63.1312\n",
      "Epoch [198/300], Step [13/172], Loss: 34.8703\n",
      "Epoch [198/300], Step [14/172], Loss: 60.4584\n",
      "Epoch [198/300], Step [15/172], Loss: 54.6562\n",
      "Epoch [198/300], Step [16/172], Loss: 9.2715\n",
      "Epoch [198/300], Step [17/172], Loss: 43.7588\n",
      "Epoch [198/300], Step [18/172], Loss: 57.4224\n",
      "Epoch [198/300], Step [19/172], Loss: 81.2524\n",
      "Epoch [198/300], Step [20/172], Loss: 34.0238\n",
      "Epoch [198/300], Step [21/172], Loss: 83.8770\n",
      "Epoch [198/300], Step [22/172], Loss: 59.0782\n",
      "Epoch [198/300], Step [23/172], Loss: 1.8603\n",
      "Epoch [198/300], Step [24/172], Loss: 54.4387\n",
      "Epoch [198/300], Step [25/172], Loss: 39.8565\n",
      "Epoch [198/300], Step [26/172], Loss: 46.3720\n",
      "Epoch [198/300], Step [27/172], Loss: 59.7411\n",
      "Epoch [198/300], Step [28/172], Loss: 23.6537\n",
      "Epoch [198/300], Step [29/172], Loss: 16.4243\n",
      "Epoch [198/300], Step [30/172], Loss: 61.5103\n",
      "Epoch [198/300], Step [31/172], Loss: 36.4925\n",
      "Epoch [198/300], Step [32/172], Loss: 43.1282\n",
      "Epoch [198/300], Step [33/172], Loss: 69.8028\n",
      "Epoch [198/300], Step [34/172], Loss: 2.6628\n",
      "Epoch [198/300], Step [35/172], Loss: 14.0691\n",
      "Epoch [198/300], Step [36/172], Loss: 17.8551\n",
      "Epoch [198/300], Step [37/172], Loss: 16.7931\n",
      "Epoch [198/300], Step [38/172], Loss: 30.2039\n",
      "Epoch [198/300], Step [39/172], Loss: 36.2076\n",
      "Epoch [198/300], Step [40/172], Loss: 21.0156\n",
      "Epoch [198/300], Step [41/172], Loss: 33.3829\n",
      "Epoch [198/300], Step [42/172], Loss: 38.5101\n",
      "Epoch [198/300], Step [43/172], Loss: 27.6225\n",
      "Epoch [198/300], Step [44/172], Loss: 21.2457\n",
      "Epoch [198/300], Step [45/172], Loss: 27.7373\n",
      "Epoch [198/300], Step [46/172], Loss: 17.3760\n",
      "Epoch [198/300], Step [47/172], Loss: 48.1636\n",
      "Epoch [198/300], Step [48/172], Loss: 61.3351\n",
      "Epoch [198/300], Step [49/172], Loss: 21.6354\n",
      "Epoch [198/300], Step [50/172], Loss: 47.6865\n",
      "Epoch [198/300], Step [51/172], Loss: 8.7852\n",
      "Epoch [198/300], Step [52/172], Loss: 20.0710\n",
      "Epoch [198/300], Step [53/172], Loss: 22.6977\n",
      "Epoch [198/300], Step [54/172], Loss: 14.7545\n",
      "Epoch [198/300], Step [55/172], Loss: 14.5028\n",
      "Epoch [198/300], Step [56/172], Loss: 18.2053\n",
      "Epoch [198/300], Step [57/172], Loss: 17.0739\n",
      "Epoch [198/300], Step [58/172], Loss: 12.6947\n",
      "Epoch [198/300], Step [59/172], Loss: 26.4869\n",
      "Epoch [198/300], Step [60/172], Loss: 23.6082\n",
      "Epoch [198/300], Step [61/172], Loss: 5.9151\n",
      "Epoch [198/300], Step [62/172], Loss: 18.9284\n",
      "Epoch [198/300], Step [63/172], Loss: 10.0672\n",
      "Epoch [198/300], Step [64/172], Loss: 10.7479\n",
      "Epoch [198/300], Step [65/172], Loss: 19.0243\n",
      "Epoch [198/300], Step [66/172], Loss: 6.3629\n",
      "Epoch [198/300], Step [67/172], Loss: 22.0141\n",
      "Epoch [198/300], Step [68/172], Loss: 5.0244\n",
      "Epoch [198/300], Step [69/172], Loss: 33.4670\n",
      "Epoch [198/300], Step [70/172], Loss: 36.6823\n",
      "Epoch [198/300], Step [71/172], Loss: 38.9400\n",
      "Epoch [198/300], Step [72/172], Loss: 38.6143\n",
      "Epoch [198/300], Step [73/172], Loss: 47.8586\n",
      "Epoch [198/300], Step [74/172], Loss: 24.7654\n",
      "Epoch [198/300], Step [75/172], Loss: 25.9196\n",
      "Epoch [198/300], Step [76/172], Loss: 27.7371\n",
      "Epoch [198/300], Step [77/172], Loss: 46.6696\n",
      "Epoch [198/300], Step [78/172], Loss: 36.3800\n",
      "Epoch [198/300], Step [79/172], Loss: 35.5845\n",
      "Epoch [198/300], Step [80/172], Loss: 48.8871\n",
      "Epoch [198/300], Step [81/172], Loss: 32.8036\n",
      "Epoch [198/300], Step [82/172], Loss: 34.8802\n",
      "Epoch [198/300], Step [83/172], Loss: 42.6694\n",
      "Epoch [198/300], Step [84/172], Loss: 32.3923\n",
      "Epoch [198/300], Step [85/172], Loss: 37.7267\n",
      "Epoch [198/300], Step [86/172], Loss: 32.1736\n",
      "Epoch [198/300], Step [87/172], Loss: 25.3411\n",
      "Epoch [198/300], Step [88/172], Loss: 23.2794\n",
      "Epoch [198/300], Step [89/172], Loss: 26.5112\n",
      "Epoch [198/300], Step [90/172], Loss: 20.1455\n",
      "Epoch [198/300], Step [91/172], Loss: 25.7350\n",
      "Epoch [198/300], Step [92/172], Loss: 19.4229\n",
      "Epoch [198/300], Step [93/172], Loss: 20.0607\n",
      "Epoch [198/300], Step [94/172], Loss: 27.9339\n",
      "Epoch [198/300], Step [95/172], Loss: 20.3943\n",
      "Epoch [198/300], Step [96/172], Loss: 19.8477\n",
      "Epoch [198/300], Step [97/172], Loss: 28.0228\n",
      "Epoch [198/300], Step [98/172], Loss: 19.0606\n",
      "Epoch [198/300], Step [99/172], Loss: 18.8926\n",
      "Epoch [198/300], Step [100/172], Loss: 15.4778\n",
      "Epoch [198/300], Step [101/172], Loss: 19.0145\n",
      "Epoch [198/300], Step [102/172], Loss: 16.6803\n",
      "Epoch [198/300], Step [103/172], Loss: 12.4238\n",
      "Epoch [198/300], Step [104/172], Loss: 18.6112\n",
      "Epoch [198/300], Step [105/172], Loss: 19.6949\n",
      "Epoch [198/300], Step [106/172], Loss: 15.5190\n",
      "Epoch [198/300], Step [107/172], Loss: 15.6607\n",
      "Epoch [198/300], Step [108/172], Loss: 14.3336\n",
      "Epoch [198/300], Step [109/172], Loss: 14.3478\n",
      "Epoch [198/300], Step [110/172], Loss: 15.8992\n",
      "Epoch [198/300], Step [111/172], Loss: 15.9035\n",
      "Epoch [198/300], Step [112/172], Loss: 17.1566\n",
      "Epoch [198/300], Step [113/172], Loss: 11.8799\n",
      "Epoch [198/300], Step [114/172], Loss: 13.8237\n",
      "Epoch [198/300], Step [115/172], Loss: 18.9157\n",
      "Epoch [198/300], Step [116/172], Loss: 14.2500\n",
      "Epoch [198/300], Step [117/172], Loss: 11.6990\n",
      "Epoch [198/300], Step [118/172], Loss: 13.7329\n",
      "Epoch [198/300], Step [119/172], Loss: 15.7500\n",
      "Epoch [198/300], Step [120/172], Loss: 10.1156\n",
      "Epoch [198/300], Step [121/172], Loss: 9.3510\n",
      "Epoch [198/300], Step [122/172], Loss: 10.7577\n",
      "Epoch [198/300], Step [123/172], Loss: 10.0178\n",
      "Epoch [198/300], Step [124/172], Loss: 7.3720\n",
      "Epoch [198/300], Step [125/172], Loss: 11.4646\n",
      "Epoch [198/300], Step [126/172], Loss: 11.0900\n",
      "Epoch [198/300], Step [127/172], Loss: 10.5362\n",
      "Epoch [198/300], Step [128/172], Loss: 10.1038\n",
      "Epoch [198/300], Step [129/172], Loss: 8.0245\n",
      "Epoch [198/300], Step [130/172], Loss: 12.4320\n",
      "Epoch [198/300], Step [131/172], Loss: 7.2014\n",
      "Epoch [198/300], Step [132/172], Loss: 8.5087\n",
      "Epoch [198/300], Step [133/172], Loss: 9.0985\n",
      "Epoch [198/300], Step [134/172], Loss: 10.9566\n",
      "Epoch [198/300], Step [135/172], Loss: 8.4097\n",
      "Epoch [198/300], Step [136/172], Loss: 7.8878\n",
      "Epoch [198/300], Step [137/172], Loss: 9.0718\n",
      "Epoch [198/300], Step [138/172], Loss: 6.7664\n",
      "Epoch [198/300], Step [139/172], Loss: 9.4018\n",
      "Epoch [198/300], Step [140/172], Loss: 9.7508\n",
      "Epoch [198/300], Step [141/172], Loss: 9.0308\n",
      "Epoch [198/300], Step [142/172], Loss: 14.0146\n",
      "Epoch [198/300], Step [143/172], Loss: 10.6449\n",
      "Epoch [198/300], Step [144/172], Loss: 8.9085\n",
      "Epoch [198/300], Step [145/172], Loss: 10.0990\n",
      "Epoch [198/300], Step [146/172], Loss: 9.2854\n",
      "Epoch [198/300], Step [147/172], Loss: 5.2401\n",
      "Epoch [198/300], Step [148/172], Loss: 5.9863\n",
      "Epoch [198/300], Step [149/172], Loss: 6.4314\n",
      "Epoch [198/300], Step [150/172], Loss: 5.6903\n",
      "Epoch [198/300], Step [151/172], Loss: 5.2243\n",
      "Epoch [198/300], Step [152/172], Loss: 7.1761\n",
      "Epoch [198/300], Step [153/172], Loss: 6.1995\n",
      "Epoch [198/300], Step [154/172], Loss: 7.2793\n",
      "Epoch [198/300], Step [155/172], Loss: 5.8598\n",
      "Epoch [198/300], Step [156/172], Loss: 13.2459\n",
      "Epoch [198/300], Step [157/172], Loss: 9.0625\n",
      "Epoch [198/300], Step [158/172], Loss: 6.8382\n",
      "Epoch [198/300], Step [159/172], Loss: 9.3626\n",
      "Epoch [198/300], Step [160/172], Loss: 9.6463\n",
      "Epoch [198/300], Step [161/172], Loss: 6.8254\n",
      "Epoch [198/300], Step [162/172], Loss: 4.9213\n",
      "Epoch [198/300], Step [163/172], Loss: 6.4426\n",
      "Epoch [198/300], Step [164/172], Loss: 8.5378\n",
      "Epoch [198/300], Step [165/172], Loss: 6.1857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [198/300], Step [166/172], Loss: 5.5494\n",
      "Epoch [198/300], Step [167/172], Loss: 10.3716\n",
      "Epoch [198/300], Step [168/172], Loss: 6.2439\n",
      "Epoch [198/300], Step [169/172], Loss: 6.5276\n",
      "Epoch [198/300], Step [170/172], Loss: 4.6754\n",
      "Epoch [198/300], Step [171/172], Loss: 8.0292\n",
      "Epoch [198/300], Step [172/172], Loss: 5.1886\n",
      "Epoch [199/300], Step [1/172], Loss: 50.3611\n",
      "Epoch [199/300], Step [2/172], Loss: 52.7482\n",
      "Epoch [199/300], Step [3/172], Loss: 47.7317\n",
      "Epoch [199/300], Step [4/172], Loss: 26.5462\n",
      "Epoch [199/300], Step [5/172], Loss: 44.7093\n",
      "Epoch [199/300], Step [6/172], Loss: 18.9374\n",
      "Epoch [199/300], Step [7/172], Loss: 28.7896\n",
      "Epoch [199/300], Step [8/172], Loss: 4.2234\n",
      "Epoch [199/300], Step [9/172], Loss: 31.0786\n",
      "Epoch [199/300], Step [10/172], Loss: 41.3883\n",
      "Epoch [199/300], Step [11/172], Loss: 56.3787\n",
      "Epoch [199/300], Step [12/172], Loss: 62.4328\n",
      "Epoch [199/300], Step [13/172], Loss: 34.3318\n",
      "Epoch [199/300], Step [14/172], Loss: 60.1948\n",
      "Epoch [199/300], Step [15/172], Loss: 54.4463\n",
      "Epoch [199/300], Step [16/172], Loss: 10.1175\n",
      "Epoch [199/300], Step [17/172], Loss: 43.4039\n",
      "Epoch [199/300], Step [18/172], Loss: 56.7778\n",
      "Epoch [199/300], Step [19/172], Loss: 80.9542\n",
      "Epoch [199/300], Step [20/172], Loss: 33.6754\n",
      "Epoch [199/300], Step [21/172], Loss: 83.9818\n",
      "Epoch [199/300], Step [22/172], Loss: 58.9277\n",
      "Epoch [199/300], Step [23/172], Loss: 1.8792\n",
      "Epoch [199/300], Step [24/172], Loss: 54.3311\n",
      "Epoch [199/300], Step [25/172], Loss: 39.7198\n",
      "Epoch [199/300], Step [26/172], Loss: 46.3597\n",
      "Epoch [199/300], Step [27/172], Loss: 59.8533\n",
      "Epoch [199/300], Step [28/172], Loss: 23.5055\n",
      "Epoch [199/300], Step [29/172], Loss: 16.1634\n",
      "Epoch [199/300], Step [30/172], Loss: 61.5616\n",
      "Epoch [199/300], Step [31/172], Loss: 36.3467\n",
      "Epoch [199/300], Step [32/172], Loss: 43.1829\n",
      "Epoch [199/300], Step [33/172], Loss: 69.9972\n",
      "Epoch [199/300], Step [34/172], Loss: 2.5568\n",
      "Epoch [199/300], Step [35/172], Loss: 14.0442\n",
      "Epoch [199/300], Step [36/172], Loss: 18.0226\n",
      "Epoch [199/300], Step [37/172], Loss: 16.6671\n",
      "Epoch [199/300], Step [38/172], Loss: 30.0623\n",
      "Epoch [199/300], Step [39/172], Loss: 36.1369\n",
      "Epoch [199/300], Step [40/172], Loss: 20.9228\n",
      "Epoch [199/300], Step [41/172], Loss: 33.3609\n",
      "Epoch [199/300], Step [42/172], Loss: 38.4914\n",
      "Epoch [199/300], Step [43/172], Loss: 27.4875\n",
      "Epoch [199/300], Step [44/172], Loss: 21.2490\n",
      "Epoch [199/300], Step [45/172], Loss: 27.5080\n",
      "Epoch [199/300], Step [46/172], Loss: 17.0083\n",
      "Epoch [199/300], Step [47/172], Loss: 47.9669\n",
      "Epoch [199/300], Step [48/172], Loss: 61.1143\n",
      "Epoch [199/300], Step [49/172], Loss: 21.2944\n",
      "Epoch [199/300], Step [50/172], Loss: 47.4975\n",
      "Epoch [199/300], Step [51/172], Loss: 8.7703\n",
      "Epoch [199/300], Step [52/172], Loss: 20.0621\n",
      "Epoch [199/300], Step [53/172], Loss: 22.4541\n",
      "Epoch [199/300], Step [54/172], Loss: 14.6580\n",
      "Epoch [199/300], Step [55/172], Loss: 14.4306\n",
      "Epoch [199/300], Step [56/172], Loss: 18.5739\n",
      "Epoch [199/300], Step [57/172], Loss: 16.9942\n",
      "Epoch [199/300], Step [58/172], Loss: 12.9707\n",
      "Epoch [199/300], Step [59/172], Loss: 26.8056\n",
      "Epoch [199/300], Step [60/172], Loss: 23.3363\n",
      "Epoch [199/300], Step [61/172], Loss: 5.9441\n",
      "Epoch [199/300], Step [62/172], Loss: 18.7454\n",
      "Epoch [199/300], Step [63/172], Loss: 10.0854\n",
      "Epoch [199/300], Step [64/172], Loss: 10.6912\n",
      "Epoch [199/300], Step [65/172], Loss: 18.9433\n",
      "Epoch [199/300], Step [66/172], Loss: 6.4659\n",
      "Epoch [199/300], Step [67/172], Loss: 22.3856\n",
      "Epoch [199/300], Step [68/172], Loss: 5.0331\n",
      "Epoch [199/300], Step [69/172], Loss: 33.7680\n",
      "Epoch [199/300], Step [70/172], Loss: 36.5816\n",
      "Epoch [199/300], Step [71/172], Loss: 38.8610\n",
      "Epoch [199/300], Step [72/172], Loss: 38.5802\n",
      "Epoch [199/300], Step [73/172], Loss: 47.9469\n",
      "Epoch [199/300], Step [74/172], Loss: 24.8011\n",
      "Epoch [199/300], Step [75/172], Loss: 25.7300\n",
      "Epoch [199/300], Step [76/172], Loss: 27.6177\n",
      "Epoch [199/300], Step [77/172], Loss: 46.5548\n",
      "Epoch [199/300], Step [78/172], Loss: 36.3338\n",
      "Epoch [199/300], Step [79/172], Loss: 35.3651\n",
      "Epoch [199/300], Step [80/172], Loss: 48.7035\n",
      "Epoch [199/300], Step [81/172], Loss: 32.5337\n",
      "Epoch [199/300], Step [82/172], Loss: 34.5871\n",
      "Epoch [199/300], Step [83/172], Loss: 42.4900\n",
      "Epoch [199/300], Step [84/172], Loss: 32.1883\n",
      "Epoch [199/300], Step [85/172], Loss: 37.6053\n",
      "Epoch [199/300], Step [86/172], Loss: 32.0791\n",
      "Epoch [199/300], Step [87/172], Loss: 25.1509\n",
      "Epoch [199/300], Step [88/172], Loss: 22.9946\n",
      "Epoch [199/300], Step [89/172], Loss: 26.4268\n",
      "Epoch [199/300], Step [90/172], Loss: 20.0213\n",
      "Epoch [199/300], Step [91/172], Loss: 25.5193\n",
      "Epoch [199/300], Step [92/172], Loss: 19.1864\n",
      "Epoch [199/300], Step [93/172], Loss: 19.8598\n",
      "Epoch [199/300], Step [94/172], Loss: 27.6650\n",
      "Epoch [199/300], Step [95/172], Loss: 20.2672\n",
      "Epoch [199/300], Step [96/172], Loss: 19.7152\n",
      "Epoch [199/300], Step [97/172], Loss: 28.0083\n",
      "Epoch [199/300], Step [98/172], Loss: 18.8642\n",
      "Epoch [199/300], Step [99/172], Loss: 18.7267\n",
      "Epoch [199/300], Step [100/172], Loss: 15.3615\n",
      "Epoch [199/300], Step [101/172], Loss: 18.9120\n",
      "Epoch [199/300], Step [102/172], Loss: 16.4811\n",
      "Epoch [199/300], Step [103/172], Loss: 12.3573\n",
      "Epoch [199/300], Step [104/172], Loss: 18.5594\n",
      "Epoch [199/300], Step [105/172], Loss: 19.5215\n",
      "Epoch [199/300], Step [106/172], Loss: 15.4235\n",
      "Epoch [199/300], Step [107/172], Loss: 15.6001\n",
      "Epoch [199/300], Step [108/172], Loss: 14.2762\n",
      "Epoch [199/300], Step [109/172], Loss: 14.2294\n",
      "Epoch [199/300], Step [110/172], Loss: 15.8357\n",
      "Epoch [199/300], Step [111/172], Loss: 15.9344\n",
      "Epoch [199/300], Step [112/172], Loss: 17.1052\n",
      "Epoch [199/300], Step [113/172], Loss: 11.8236\n",
      "Epoch [199/300], Step [114/172], Loss: 13.7353\n",
      "Epoch [199/300], Step [115/172], Loss: 18.8239\n",
      "Epoch [199/300], Step [116/172], Loss: 14.0826\n",
      "Epoch [199/300], Step [117/172], Loss: 11.6473\n",
      "Epoch [199/300], Step [118/172], Loss: 13.6773\n",
      "Epoch [199/300], Step [119/172], Loss: 15.7078\n",
      "Epoch [199/300], Step [120/172], Loss: 10.0808\n",
      "Epoch [199/300], Step [121/172], Loss: 9.2978\n",
      "Epoch [199/300], Step [122/172], Loss: 10.8029\n",
      "Epoch [199/300], Step [123/172], Loss: 10.0147\n",
      "Epoch [199/300], Step [124/172], Loss: 7.2778\n",
      "Epoch [199/300], Step [125/172], Loss: 11.3754\n",
      "Epoch [199/300], Step [126/172], Loss: 11.0093\n",
      "Epoch [199/300], Step [127/172], Loss: 10.4636\n",
      "Epoch [199/300], Step [128/172], Loss: 9.9701\n",
      "Epoch [199/300], Step [129/172], Loss: 7.9434\n",
      "Epoch [199/300], Step [130/172], Loss: 12.4291\n",
      "Epoch [199/300], Step [131/172], Loss: 7.1295\n",
      "Epoch [199/300], Step [132/172], Loss: 8.4614\n",
      "Epoch [199/300], Step [133/172], Loss: 9.0518\n",
      "Epoch [199/300], Step [134/172], Loss: 10.9371\n",
      "Epoch [199/300], Step [135/172], Loss: 8.3316\n",
      "Epoch [199/300], Step [136/172], Loss: 7.7647\n",
      "Epoch [199/300], Step [137/172], Loss: 8.9719\n",
      "Epoch [199/300], Step [138/172], Loss: 6.7034\n",
      "Epoch [199/300], Step [139/172], Loss: 9.3019\n",
      "Epoch [199/300], Step [140/172], Loss: 9.6435\n",
      "Epoch [199/300], Step [141/172], Loss: 8.9692\n",
      "Epoch [199/300], Step [142/172], Loss: 13.9461\n",
      "Epoch [199/300], Step [143/172], Loss: 10.6285\n",
      "Epoch [199/300], Step [144/172], Loss: 8.8204\n",
      "Epoch [199/300], Step [145/172], Loss: 10.0533\n",
      "Epoch [199/300], Step [146/172], Loss: 9.1941\n",
      "Epoch [199/300], Step [147/172], Loss: 5.1960\n",
      "Epoch [199/300], Step [148/172], Loss: 5.9327\n",
      "Epoch [199/300], Step [149/172], Loss: 6.3937\n",
      "Epoch [199/300], Step [150/172], Loss: 5.6258\n",
      "Epoch [199/300], Step [151/172], Loss: 5.1993\n",
      "Epoch [199/300], Step [152/172], Loss: 7.1507\n",
      "Epoch [199/300], Step [153/172], Loss: 6.1472\n",
      "Epoch [199/300], Step [154/172], Loss: 7.2320\n",
      "Epoch [199/300], Step [155/172], Loss: 5.7840\n",
      "Epoch [199/300], Step [156/172], Loss: 13.2427\n",
      "Epoch [199/300], Step [157/172], Loss: 9.0334\n",
      "Epoch [199/300], Step [158/172], Loss: 6.8045\n",
      "Epoch [199/300], Step [159/172], Loss: 9.1845\n",
      "Epoch [199/300], Step [160/172], Loss: 9.6443\n",
      "Epoch [199/300], Step [161/172], Loss: 6.7960\n",
      "Epoch [199/300], Step [162/172], Loss: 4.9071\n",
      "Epoch [199/300], Step [163/172], Loss: 6.4120\n",
      "Epoch [199/300], Step [164/172], Loss: 8.4151\n",
      "Epoch [199/300], Step [165/172], Loss: 6.1273\n",
      "Epoch [199/300], Step [166/172], Loss: 5.4557\n",
      "Epoch [199/300], Step [167/172], Loss: 10.3371\n",
      "Epoch [199/300], Step [168/172], Loss: 6.1848\n",
      "Epoch [199/300], Step [169/172], Loss: 6.5482\n",
      "Epoch [199/300], Step [170/172], Loss: 4.6350\n",
      "Epoch [199/300], Step [171/172], Loss: 7.9956\n",
      "Epoch [199/300], Step [172/172], Loss: 5.1386\n",
      "Epoch [200/300], Step [1/172], Loss: 50.5098\n",
      "Epoch [200/300], Step [2/172], Loss: 52.8383\n",
      "Epoch [200/300], Step [3/172], Loss: 47.7550\n",
      "Epoch [200/300], Step [4/172], Loss: 26.4746\n",
      "Epoch [200/300], Step [5/172], Loss: 44.4670\n",
      "Epoch [200/300], Step [6/172], Loss: 18.7558\n",
      "Epoch [200/300], Step [7/172], Loss: 28.6087\n",
      "Epoch [200/300], Step [8/172], Loss: 4.7237\n",
      "Epoch [200/300], Step [9/172], Loss: 31.1280\n",
      "Epoch [200/300], Step [10/172], Loss: 41.4904\n",
      "Epoch [200/300], Step [11/172], Loss: 56.2959\n",
      "Epoch [200/300], Step [12/172], Loss: 62.7064\n",
      "Epoch [200/300], Step [13/172], Loss: 34.7052\n",
      "Epoch [200/300], Step [14/172], Loss: 60.5826\n",
      "Epoch [200/300], Step [15/172], Loss: 54.3986\n",
      "Epoch [200/300], Step [16/172], Loss: 9.5755\n",
      "Epoch [200/300], Step [17/172], Loss: 43.6119\n",
      "Epoch [200/300], Step [18/172], Loss: 57.1480\n",
      "Epoch [200/300], Step [19/172], Loss: 81.2051\n",
      "Epoch [200/300], Step [20/172], Loss: 33.8419\n",
      "Epoch [200/300], Step [21/172], Loss: 84.2219\n",
      "Epoch [200/300], Step [22/172], Loss: 58.9575\n",
      "Epoch [200/300], Step [23/172], Loss: 1.8949\n",
      "Epoch [200/300], Step [24/172], Loss: 54.4360\n",
      "Epoch [200/300], Step [25/172], Loss: 39.9635\n",
      "Epoch [200/300], Step [26/172], Loss: 46.4625\n",
      "Epoch [200/300], Step [27/172], Loss: 59.8950\n",
      "Epoch [200/300], Step [28/172], Loss: 23.7339\n",
      "Epoch [200/300], Step [29/172], Loss: 16.4316\n",
      "Epoch [200/300], Step [30/172], Loss: 61.5684\n",
      "Epoch [200/300], Step [31/172], Loss: 36.6301\n",
      "Epoch [200/300], Step [32/172], Loss: 43.4794\n",
      "Epoch [200/300], Step [33/172], Loss: 70.2724\n",
      "Epoch [200/300], Step [34/172], Loss: 2.5656\n",
      "Epoch [200/300], Step [35/172], Loss: 14.2458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/300], Step [36/172], Loss: 17.6864\n",
      "Epoch [200/300], Step [37/172], Loss: 16.7693\n",
      "Epoch [200/300], Step [38/172], Loss: 30.2442\n",
      "Epoch [200/300], Step [39/172], Loss: 36.4879\n",
      "Epoch [200/300], Step [40/172], Loss: 21.1482\n",
      "Epoch [200/300], Step [41/172], Loss: 33.5826\n",
      "Epoch [200/300], Step [42/172], Loss: 38.6459\n",
      "Epoch [200/300], Step [43/172], Loss: 27.6625\n",
      "Epoch [200/300], Step [44/172], Loss: 21.4237\n",
      "Epoch [200/300], Step [45/172], Loss: 27.9450\n",
      "Epoch [200/300], Step [46/172], Loss: 17.4711\n",
      "Epoch [200/300], Step [47/172], Loss: 48.5158\n",
      "Epoch [200/300], Step [48/172], Loss: 61.4212\n",
      "Epoch [200/300], Step [49/172], Loss: 21.5421\n",
      "Epoch [200/300], Step [50/172], Loss: 47.2008\n",
      "Epoch [200/300], Step [51/172], Loss: 8.8652\n",
      "Epoch [200/300], Step [52/172], Loss: 20.1807\n",
      "Epoch [200/300], Step [53/172], Loss: 22.6289\n",
      "Epoch [200/300], Step [54/172], Loss: 14.7748\n",
      "Epoch [200/300], Step [55/172], Loss: 14.5056\n",
      "Epoch [200/300], Step [56/172], Loss: 18.6947\n",
      "Epoch [200/300], Step [57/172], Loss: 16.7973\n",
      "Epoch [200/300], Step [58/172], Loss: 12.7590\n",
      "Epoch [200/300], Step [59/172], Loss: 26.6432\n",
      "Epoch [200/300], Step [60/172], Loss: 22.8592\n",
      "Epoch [200/300], Step [61/172], Loss: 5.8811\n",
      "Epoch [200/300], Step [62/172], Loss: 18.6277\n",
      "Epoch [200/300], Step [63/172], Loss: 10.2352\n",
      "Epoch [200/300], Step [64/172], Loss: 10.7454\n",
      "Epoch [200/300], Step [65/172], Loss: 18.7965\n",
      "Epoch [200/300], Step [66/172], Loss: 6.3068\n",
      "Epoch [200/300], Step [67/172], Loss: 21.9514\n",
      "Epoch [200/300], Step [68/172], Loss: 4.6238\n",
      "Epoch [200/300], Step [69/172], Loss: 33.2207\n",
      "Epoch [200/300], Step [70/172], Loss: 36.5163\n",
      "Epoch [200/300], Step [71/172], Loss: 38.7699\n",
      "Epoch [200/300], Step [72/172], Loss: 38.3238\n",
      "Epoch [200/300], Step [73/172], Loss: 47.7496\n",
      "Epoch [200/300], Step [74/172], Loss: 24.6991\n",
      "Epoch [200/300], Step [75/172], Loss: 25.9062\n",
      "Epoch [200/300], Step [76/172], Loss: 27.5215\n",
      "Epoch [200/300], Step [77/172], Loss: 46.3505\n",
      "Epoch [200/300], Step [78/172], Loss: 36.1458\n",
      "Epoch [200/300], Step [79/172], Loss: 35.3347\n",
      "Epoch [200/300], Step [80/172], Loss: 48.6997\n",
      "Epoch [200/300], Step [81/172], Loss: 32.5115\n",
      "Epoch [200/300], Step [82/172], Loss: 34.7298\n",
      "Epoch [200/300], Step [83/172], Loss: 42.4532\n",
      "Epoch [200/300], Step [84/172], Loss: 32.1839\n",
      "Epoch [200/300], Step [85/172], Loss: 37.7770\n",
      "Epoch [200/300], Step [86/172], Loss: 32.1782\n",
      "Epoch [200/300], Step [87/172], Loss: 25.2240\n",
      "Epoch [200/300], Step [88/172], Loss: 23.0847\n",
      "Epoch [200/300], Step [89/172], Loss: 26.6012\n",
      "Epoch [200/300], Step [90/172], Loss: 20.0117\n",
      "Epoch [200/300], Step [91/172], Loss: 25.6885\n",
      "Epoch [200/300], Step [92/172], Loss: 19.3512\n",
      "Epoch [200/300], Step [93/172], Loss: 19.9822\n",
      "Epoch [200/300], Step [94/172], Loss: 27.7252\n",
      "Epoch [200/300], Step [95/172], Loss: 20.3232\n",
      "Epoch [200/300], Step [96/172], Loss: 19.8526\n",
      "Epoch [200/300], Step [97/172], Loss: 28.1692\n",
      "Epoch [200/300], Step [98/172], Loss: 19.1250\n",
      "Epoch [200/300], Step [99/172], Loss: 18.9178\n",
      "Epoch [200/300], Step [100/172], Loss: 15.6080\n",
      "Epoch [200/300], Step [101/172], Loss: 19.1239\n",
      "Epoch [200/300], Step [102/172], Loss: 16.6954\n",
      "Epoch [200/300], Step [103/172], Loss: 12.5274\n",
      "Epoch [200/300], Step [104/172], Loss: 18.7158\n",
      "Epoch [200/300], Step [105/172], Loss: 19.8786\n",
      "Epoch [200/300], Step [106/172], Loss: 15.5233\n",
      "Epoch [200/300], Step [107/172], Loss: 15.7742\n",
      "Epoch [200/300], Step [108/172], Loss: 14.3873\n",
      "Epoch [200/300], Step [109/172], Loss: 14.3291\n",
      "Epoch [200/300], Step [110/172], Loss: 16.0610\n",
      "Epoch [200/300], Step [111/172], Loss: 16.1721\n",
      "Epoch [200/300], Step [112/172], Loss: 17.1681\n",
      "Epoch [200/300], Step [113/172], Loss: 11.9917\n",
      "Epoch [200/300], Step [114/172], Loss: 13.8972\n",
      "Epoch [200/300], Step [115/172], Loss: 18.9138\n",
      "Epoch [200/300], Step [116/172], Loss: 14.2979\n",
      "Epoch [200/300], Step [117/172], Loss: 11.8693\n",
      "Epoch [200/300], Step [118/172], Loss: 13.6987\n",
      "Epoch [200/300], Step [119/172], Loss: 15.8230\n",
      "Epoch [200/300], Step [120/172], Loss: 10.1983\n",
      "Epoch [200/300], Step [121/172], Loss: 9.4145\n",
      "Epoch [200/300], Step [122/172], Loss: 10.8601\n",
      "Epoch [200/300], Step [123/172], Loss: 10.1293\n",
      "Epoch [200/300], Step [124/172], Loss: 7.4096\n",
      "Epoch [200/300], Step [125/172], Loss: 11.5250\n",
      "Epoch [200/300], Step [126/172], Loss: 11.2040\n",
      "Epoch [200/300], Step [127/172], Loss: 10.5406\n",
      "Epoch [200/300], Step [128/172], Loss: 10.1328\n",
      "Epoch [200/300], Step [129/172], Loss: 8.0870\n",
      "Epoch [200/300], Step [130/172], Loss: 12.5136\n",
      "Epoch [200/300], Step [131/172], Loss: 7.2183\n",
      "Epoch [200/300], Step [132/172], Loss: 8.5891\n",
      "Epoch [200/300], Step [133/172], Loss: 9.1182\n",
      "Epoch [200/300], Step [134/172], Loss: 10.9816\n",
      "Epoch [200/300], Step [135/172], Loss: 8.4674\n",
      "Epoch [200/300], Step [136/172], Loss: 7.9187\n",
      "Epoch [200/300], Step [137/172], Loss: 9.0836\n",
      "Epoch [200/300], Step [138/172], Loss: 6.8389\n",
      "Epoch [200/300], Step [139/172], Loss: 9.4502\n",
      "Epoch [200/300], Step [140/172], Loss: 9.8138\n",
      "Epoch [200/300], Step [141/172], Loss: 9.0607\n",
      "Epoch [200/300], Step [142/172], Loss: 14.1727\n",
      "Epoch [200/300], Step [143/172], Loss: 10.7123\n",
      "Epoch [200/300], Step [144/172], Loss: 8.9666\n",
      "Epoch [200/300], Step [145/172], Loss: 10.2432\n",
      "Epoch [200/300], Step [146/172], Loss: 9.3782\n",
      "Epoch [200/300], Step [147/172], Loss: 5.2806\n",
      "Epoch [200/300], Step [148/172], Loss: 6.0068\n",
      "Epoch [200/300], Step [149/172], Loss: 6.4651\n",
      "Epoch [200/300], Step [150/172], Loss: 5.7215\n",
      "Epoch [200/300], Step [151/172], Loss: 5.2368\n",
      "Epoch [200/300], Step [152/172], Loss: 7.2213\n",
      "Epoch [200/300], Step [153/172], Loss: 6.2309\n",
      "Epoch [200/300], Step [154/172], Loss: 7.3375\n",
      "Epoch [200/300], Step [155/172], Loss: 5.8867\n",
      "Epoch [200/300], Step [156/172], Loss: 13.3276\n",
      "Epoch [200/300], Step [157/172], Loss: 9.0623\n",
      "Epoch [200/300], Step [158/172], Loss: 6.8456\n",
      "Epoch [200/300], Step [159/172], Loss: 9.5416\n",
      "Epoch [200/300], Step [160/172], Loss: 9.6855\n",
      "Epoch [200/300], Step [161/172], Loss: 6.9018\n",
      "Epoch [200/300], Step [162/172], Loss: 4.9193\n",
      "Epoch [200/300], Step [163/172], Loss: 6.4558\n",
      "Epoch [200/300], Step [164/172], Loss: 8.5423\n",
      "Epoch [200/300], Step [165/172], Loss: 6.2070\n",
      "Epoch [200/300], Step [166/172], Loss: 5.5155\n",
      "Epoch [200/300], Step [167/172], Loss: 10.5568\n",
      "Epoch [200/300], Step [168/172], Loss: 6.2075\n",
      "Epoch [200/300], Step [169/172], Loss: 6.6021\n",
      "Epoch [200/300], Step [170/172], Loss: 4.7124\n",
      "Epoch [200/300], Step [171/172], Loss: 8.2559\n",
      "Epoch [200/300], Step [172/172], Loss: 5.2798\n",
      "Epoch [201/300], Step [1/172], Loss: 50.3306\n",
      "Epoch [201/300], Step [2/172], Loss: 52.6988\n",
      "Epoch [201/300], Step [3/172], Loss: 45.7817\n",
      "Epoch [201/300], Step [4/172], Loss: 26.4370\n",
      "Epoch [201/300], Step [5/172], Loss: 43.6364\n",
      "Epoch [201/300], Step [6/172], Loss: 18.7879\n",
      "Epoch [201/300], Step [7/172], Loss: 28.8670\n",
      "Epoch [201/300], Step [8/172], Loss: 4.3577\n",
      "Epoch [201/300], Step [9/172], Loss: 30.7776\n",
      "Epoch [201/300], Step [10/172], Loss: 41.2938\n",
      "Epoch [201/300], Step [11/172], Loss: 55.6648\n",
      "Epoch [201/300], Step [12/172], Loss: 61.8509\n",
      "Epoch [201/300], Step [13/172], Loss: 34.3805\n",
      "Epoch [201/300], Step [14/172], Loss: 60.5199\n",
      "Epoch [201/300], Step [15/172], Loss: 54.0723\n",
      "Epoch [201/300], Step [16/172], Loss: 10.0326\n",
      "Epoch [201/300], Step [17/172], Loss: 43.0399\n",
      "Epoch [201/300], Step [18/172], Loss: 56.6319\n",
      "Epoch [201/300], Step [19/172], Loss: 80.4014\n",
      "Epoch [201/300], Step [20/172], Loss: 33.5728\n",
      "Epoch [201/300], Step [21/172], Loss: 83.2845\n",
      "Epoch [201/300], Step [22/172], Loss: 58.7968\n",
      "Epoch [201/300], Step [23/172], Loss: 1.8709\n",
      "Epoch [201/300], Step [24/172], Loss: 54.2303\n",
      "Epoch [201/300], Step [25/172], Loss: 39.8126\n",
      "Epoch [201/300], Step [26/172], Loss: 46.5926\n",
      "Epoch [201/300], Step [27/172], Loss: 59.9793\n",
      "Epoch [201/300], Step [28/172], Loss: 23.4654\n",
      "Epoch [201/300], Step [29/172], Loss: 16.1871\n",
      "Epoch [201/300], Step [30/172], Loss: 61.0502\n",
      "Epoch [201/300], Step [31/172], Loss: 36.4527\n",
      "Epoch [201/300], Step [32/172], Loss: 43.4224\n",
      "Epoch [201/300], Step [33/172], Loss: 70.4499\n",
      "Epoch [201/300], Step [34/172], Loss: 2.5583\n",
      "Epoch [201/300], Step [35/172], Loss: 14.2627\n",
      "Epoch [201/300], Step [36/172], Loss: 17.7491\n",
      "Epoch [201/300], Step [37/172], Loss: 16.7291\n",
      "Epoch [201/300], Step [38/172], Loss: 30.4545\n",
      "Epoch [201/300], Step [39/172], Loss: 36.4893\n",
      "Epoch [201/300], Step [40/172], Loss: 21.2030\n",
      "Epoch [201/300], Step [41/172], Loss: 33.8358\n",
      "Epoch [201/300], Step [42/172], Loss: 38.7450\n",
      "Epoch [201/300], Step [43/172], Loss: 27.7419\n",
      "Epoch [201/300], Step [44/172], Loss: 21.5249\n",
      "Epoch [201/300], Step [45/172], Loss: 28.1711\n",
      "Epoch [201/300], Step [46/172], Loss: 17.2459\n",
      "Epoch [201/300], Step [47/172], Loss: 48.4116\n",
      "Epoch [201/300], Step [48/172], Loss: 60.9028\n",
      "Epoch [201/300], Step [49/172], Loss: 21.4184\n",
      "Epoch [201/300], Step [50/172], Loss: 47.4614\n",
      "Epoch [201/300], Step [51/172], Loss: 8.8223\n",
      "Epoch [201/300], Step [52/172], Loss: 20.0520\n",
      "Epoch [201/300], Step [53/172], Loss: 22.5369\n",
      "Epoch [201/300], Step [54/172], Loss: 14.7124\n",
      "Epoch [201/300], Step [55/172], Loss: 14.4837\n",
      "Epoch [201/300], Step [56/172], Loss: 18.7892\n",
      "Epoch [201/300], Step [57/172], Loss: 16.9386\n",
      "Epoch [201/300], Step [58/172], Loss: 12.8712\n",
      "Epoch [201/300], Step [59/172], Loss: 27.0075\n",
      "Epoch [201/300], Step [60/172], Loss: 23.1433\n",
      "Epoch [201/300], Step [61/172], Loss: 5.9475\n",
      "Epoch [201/300], Step [62/172], Loss: 18.6654\n",
      "Epoch [201/300], Step [63/172], Loss: 10.1190\n",
      "Epoch [201/300], Step [64/172], Loss: 10.7123\n",
      "Epoch [201/300], Step [65/172], Loss: 18.6364\n",
      "Epoch [201/300], Step [66/172], Loss: 6.3379\n",
      "Epoch [201/300], Step [67/172], Loss: 22.0421\n",
      "Epoch [201/300], Step [68/172], Loss: 4.5520\n",
      "Epoch [201/300], Step [69/172], Loss: 33.3177\n",
      "Epoch [201/300], Step [70/172], Loss: 36.2706\n",
      "Epoch [201/300], Step [71/172], Loss: 38.5980\n",
      "Epoch [201/300], Step [72/172], Loss: 38.1393\n",
      "Epoch [201/300], Step [73/172], Loss: 47.5703\n",
      "Epoch [201/300], Step [74/172], Loss: 24.6484\n",
      "Epoch [201/300], Step [75/172], Loss: 25.6231\n",
      "Epoch [201/300], Step [76/172], Loss: 27.4052\n",
      "Epoch [201/300], Step [77/172], Loss: 46.1675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [201/300], Step [78/172], Loss: 36.0761\n",
      "Epoch [201/300], Step [79/172], Loss: 35.2143\n",
      "Epoch [201/300], Step [80/172], Loss: 48.5729\n",
      "Epoch [201/300], Step [81/172], Loss: 32.3199\n",
      "Epoch [201/300], Step [82/172], Loss: 34.5984\n",
      "Epoch [201/300], Step [83/172], Loss: 42.3374\n",
      "Epoch [201/300], Step [84/172], Loss: 32.0975\n",
      "Epoch [201/300], Step [85/172], Loss: 37.7292\n",
      "Epoch [201/300], Step [86/172], Loss: 32.1495\n",
      "Epoch [201/300], Step [87/172], Loss: 25.1472\n",
      "Epoch [201/300], Step [88/172], Loss: 23.0006\n",
      "Epoch [201/300], Step [89/172], Loss: 26.6212\n",
      "Epoch [201/300], Step [90/172], Loss: 19.9925\n",
      "Epoch [201/300], Step [91/172], Loss: 25.6025\n",
      "Epoch [201/300], Step [92/172], Loss: 19.3581\n",
      "Epoch [201/300], Step [93/172], Loss: 19.9520\n",
      "Epoch [201/300], Step [94/172], Loss: 27.6337\n",
      "Epoch [201/300], Step [95/172], Loss: 20.4239\n",
      "Epoch [201/300], Step [96/172], Loss: 19.8352\n",
      "Epoch [201/300], Step [97/172], Loss: 28.2513\n",
      "Epoch [201/300], Step [98/172], Loss: 19.1004\n",
      "Epoch [201/300], Step [99/172], Loss: 18.9021\n",
      "Epoch [201/300], Step [100/172], Loss: 15.6544\n",
      "Epoch [201/300], Step [101/172], Loss: 19.0631\n",
      "Epoch [201/300], Step [102/172], Loss: 16.6516\n",
      "Epoch [201/300], Step [103/172], Loss: 12.5919\n",
      "Epoch [201/300], Step [104/172], Loss: 18.7909\n",
      "Epoch [201/300], Step [105/172], Loss: 19.9057\n",
      "Epoch [201/300], Step [106/172], Loss: 15.5062\n",
      "Epoch [201/300], Step [107/172], Loss: 15.8077\n",
      "Epoch [201/300], Step [108/172], Loss: 14.4257\n",
      "Epoch [201/300], Step [109/172], Loss: 14.3109\n",
      "Epoch [201/300], Step [110/172], Loss: 16.1405\n",
      "Epoch [201/300], Step [111/172], Loss: 16.2048\n",
      "Epoch [201/300], Step [112/172], Loss: 17.1569\n",
      "Epoch [201/300], Step [113/172], Loss: 11.9946\n",
      "Epoch [201/300], Step [114/172], Loss: 13.8895\n",
      "Epoch [201/300], Step [115/172], Loss: 18.9053\n",
      "Epoch [201/300], Step [116/172], Loss: 14.2931\n",
      "Epoch [201/300], Step [117/172], Loss: 11.8451\n",
      "Epoch [201/300], Step [118/172], Loss: 13.7117\n",
      "Epoch [201/300], Step [119/172], Loss: 15.8857\n",
      "Epoch [201/300], Step [120/172], Loss: 10.2569\n",
      "Epoch [201/300], Step [121/172], Loss: 9.4665\n",
      "Epoch [201/300], Step [122/172], Loss: 10.9520\n",
      "Epoch [201/300], Step [123/172], Loss: 10.0932\n",
      "Epoch [201/300], Step [124/172], Loss: 7.3916\n",
      "Epoch [201/300], Step [125/172], Loss: 11.5395\n",
      "Epoch [201/300], Step [126/172], Loss: 11.2042\n",
      "Epoch [201/300], Step [127/172], Loss: 10.5581\n",
      "Epoch [201/300], Step [128/172], Loss: 10.1170\n",
      "Epoch [201/300], Step [129/172], Loss: 8.0980\n",
      "Epoch [201/300], Step [130/172], Loss: 12.4929\n",
      "Epoch [201/300], Step [131/172], Loss: 7.2485\n",
      "Epoch [201/300], Step [132/172], Loss: 8.6404\n",
      "Epoch [201/300], Step [133/172], Loss: 9.1944\n",
      "Epoch [201/300], Step [134/172], Loss: 11.0413\n",
      "Epoch [201/300], Step [135/172], Loss: 8.4920\n",
      "Epoch [201/300], Step [136/172], Loss: 7.7416\n",
      "Epoch [201/300], Step [137/172], Loss: 9.0326\n",
      "Epoch [201/300], Step [138/172], Loss: 6.8143\n",
      "Epoch [201/300], Step [139/172], Loss: 9.4698\n",
      "Epoch [201/300], Step [140/172], Loss: 9.8153\n",
      "Epoch [201/300], Step [141/172], Loss: 9.0027\n",
      "Epoch [201/300], Step [142/172], Loss: 14.1522\n",
      "Epoch [201/300], Step [143/172], Loss: 10.6992\n",
      "Epoch [201/300], Step [144/172], Loss: 9.0215\n",
      "Epoch [201/300], Step [145/172], Loss: 10.2925\n",
      "Epoch [201/300], Step [146/172], Loss: 9.4022\n",
      "Epoch [201/300], Step [147/172], Loss: 5.3342\n",
      "Epoch [201/300], Step [148/172], Loss: 6.0566\n",
      "Epoch [201/300], Step [149/172], Loss: 6.4496\n",
      "Epoch [201/300], Step [150/172], Loss: 5.6633\n",
      "Epoch [201/300], Step [151/172], Loss: 5.2457\n",
      "Epoch [201/300], Step [152/172], Loss: 7.2736\n",
      "Epoch [201/300], Step [153/172], Loss: 6.2537\n",
      "Epoch [201/300], Step [154/172], Loss: 7.2912\n",
      "Epoch [201/300], Step [155/172], Loss: 5.9260\n",
      "Epoch [201/300], Step [156/172], Loss: 13.3835\n",
      "Epoch [201/300], Step [157/172], Loss: 8.9734\n",
      "Epoch [201/300], Step [158/172], Loss: 6.8530\n",
      "Epoch [201/300], Step [159/172], Loss: 9.4568\n",
      "Epoch [201/300], Step [160/172], Loss: 9.6577\n",
      "Epoch [201/300], Step [161/172], Loss: 6.8856\n",
      "Epoch [201/300], Step [162/172], Loss: 4.9433\n",
      "Epoch [201/300], Step [163/172], Loss: 6.4350\n",
      "Epoch [201/300], Step [164/172], Loss: 8.5714\n",
      "Epoch [201/300], Step [165/172], Loss: 6.2626\n",
      "Epoch [201/300], Step [166/172], Loss: 5.5347\n",
      "Epoch [201/300], Step [167/172], Loss: 10.4585\n",
      "Epoch [201/300], Step [168/172], Loss: 6.2393\n",
      "Epoch [201/300], Step [169/172], Loss: 6.6210\n",
      "Epoch [201/300], Step [170/172], Loss: 4.7795\n",
      "Epoch [201/300], Step [171/172], Loss: 8.2482\n",
      "Epoch [201/300], Step [172/172], Loss: 5.2724\n",
      "Epoch [202/300], Step [1/172], Loss: 49.9569\n",
      "Epoch [202/300], Step [2/172], Loss: 52.6599\n",
      "Epoch [202/300], Step [3/172], Loss: 47.3533\n",
      "Epoch [202/300], Step [4/172], Loss: 26.3418\n",
      "Epoch [202/300], Step [5/172], Loss: 43.5943\n",
      "Epoch [202/300], Step [6/172], Loss: 18.7616\n",
      "Epoch [202/300], Step [7/172], Loss: 28.5822\n",
      "Epoch [202/300], Step [8/172], Loss: 4.3463\n",
      "Epoch [202/300], Step [9/172], Loss: 30.6328\n",
      "Epoch [202/300], Step [10/172], Loss: 41.2200\n",
      "Epoch [202/300], Step [11/172], Loss: 55.5371\n",
      "Epoch [202/300], Step [12/172], Loss: 61.6970\n",
      "Epoch [202/300], Step [13/172], Loss: 34.3936\n",
      "Epoch [202/300], Step [14/172], Loss: 60.9679\n",
      "Epoch [202/300], Step [15/172], Loss: 54.0146\n",
      "Epoch [202/300], Step [16/172], Loss: 9.5706\n",
      "Epoch [202/300], Step [17/172], Loss: 43.1295\n",
      "Epoch [202/300], Step [18/172], Loss: 56.7831\n",
      "Epoch [202/300], Step [19/172], Loss: 80.6593\n",
      "Epoch [202/300], Step [20/172], Loss: 33.0598\n",
      "Epoch [202/300], Step [21/172], Loss: 83.8381\n",
      "Epoch [202/300], Step [22/172], Loss: 58.8144\n",
      "Epoch [202/300], Step [23/172], Loss: 1.8215\n",
      "Epoch [202/300], Step [24/172], Loss: 54.2136\n",
      "Epoch [202/300], Step [25/172], Loss: 39.9156\n",
      "Epoch [202/300], Step [26/172], Loss: 46.2469\n",
      "Epoch [202/300], Step [27/172], Loss: 59.8284\n",
      "Epoch [202/300], Step [28/172], Loss: 23.6176\n",
      "Epoch [202/300], Step [29/172], Loss: 16.1708\n",
      "Epoch [202/300], Step [30/172], Loss: 61.2226\n",
      "Epoch [202/300], Step [31/172], Loss: 36.5875\n",
      "Epoch [202/300], Step [32/172], Loss: 43.5829\n",
      "Epoch [202/300], Step [33/172], Loss: 70.3840\n",
      "Epoch [202/300], Step [34/172], Loss: 2.5807\n",
      "Epoch [202/300], Step [35/172], Loss: 14.3082\n",
      "Epoch [202/300], Step [36/172], Loss: 17.7074\n",
      "Epoch [202/300], Step [37/172], Loss: 16.8985\n",
      "Epoch [202/300], Step [38/172], Loss: 30.6605\n",
      "Epoch [202/300], Step [39/172], Loss: 36.8274\n",
      "Epoch [202/300], Step [40/172], Loss: 21.4512\n",
      "Epoch [202/300], Step [41/172], Loss: 34.0102\n",
      "Epoch [202/300], Step [42/172], Loss: 39.1554\n",
      "Epoch [202/300], Step [43/172], Loss: 28.0346\n",
      "Epoch [202/300], Step [44/172], Loss: 21.7710\n",
      "Epoch [202/300], Step [45/172], Loss: 28.5451\n",
      "Epoch [202/300], Step [46/172], Loss: 17.2296\n",
      "Epoch [202/300], Step [47/172], Loss: 48.7271\n",
      "Epoch [202/300], Step [48/172], Loss: 61.1857\n",
      "Epoch [202/300], Step [49/172], Loss: 21.7073\n",
      "Epoch [202/300], Step [50/172], Loss: 47.7349\n",
      "Epoch [202/300], Step [51/172], Loss: 8.9716\n",
      "Epoch [202/300], Step [52/172], Loss: 20.1778\n",
      "Epoch [202/300], Step [53/172], Loss: 22.5780\n",
      "Epoch [202/300], Step [54/172], Loss: 14.6471\n",
      "Epoch [202/300], Step [55/172], Loss: 14.4302\n",
      "Epoch [202/300], Step [56/172], Loss: 19.0447\n",
      "Epoch [202/300], Step [57/172], Loss: 17.0541\n",
      "Epoch [202/300], Step [58/172], Loss: 12.7040\n",
      "Epoch [202/300], Step [59/172], Loss: 26.5727\n",
      "Epoch [202/300], Step [60/172], Loss: 22.9927\n",
      "Epoch [202/300], Step [61/172], Loss: 5.8650\n",
      "Epoch [202/300], Step [62/172], Loss: 18.3328\n",
      "Epoch [202/300], Step [63/172], Loss: 10.1310\n",
      "Epoch [202/300], Step [64/172], Loss: 10.7268\n",
      "Epoch [202/300], Step [65/172], Loss: 18.5286\n",
      "Epoch [202/300], Step [66/172], Loss: 6.3214\n",
      "Epoch [202/300], Step [67/172], Loss: 22.0713\n",
      "Epoch [202/300], Step [68/172], Loss: 4.5536\n",
      "Epoch [202/300], Step [69/172], Loss: 33.3738\n",
      "Epoch [202/300], Step [70/172], Loss: 36.2182\n",
      "Epoch [202/300], Step [71/172], Loss: 38.5497\n",
      "Epoch [202/300], Step [72/172], Loss: 38.0082\n",
      "Epoch [202/300], Step [73/172], Loss: 47.5028\n",
      "Epoch [202/300], Step [74/172], Loss: 24.5024\n",
      "Epoch [202/300], Step [75/172], Loss: 25.2244\n",
      "Epoch [202/300], Step [76/172], Loss: 27.2455\n",
      "Epoch [202/300], Step [77/172], Loss: 45.7916\n",
      "Epoch [202/300], Step [78/172], Loss: 35.7485\n",
      "Epoch [202/300], Step [79/172], Loss: 34.9448\n",
      "Epoch [202/300], Step [80/172], Loss: 48.1901\n",
      "Epoch [202/300], Step [81/172], Loss: 32.1035\n",
      "Epoch [202/300], Step [82/172], Loss: 34.2398\n",
      "Epoch [202/300], Step [83/172], Loss: 42.1511\n",
      "Epoch [202/300], Step [84/172], Loss: 31.9795\n",
      "Epoch [202/300], Step [85/172], Loss: 37.7502\n",
      "Epoch [202/300], Step [86/172], Loss: 32.1017\n",
      "Epoch [202/300], Step [87/172], Loss: 25.0124\n",
      "Epoch [202/300], Step [88/172], Loss: 22.8246\n",
      "Epoch [202/300], Step [89/172], Loss: 26.7480\n",
      "Epoch [202/300], Step [90/172], Loss: 19.9097\n",
      "Epoch [202/300], Step [91/172], Loss: 25.5804\n",
      "Epoch [202/300], Step [92/172], Loss: 19.3236\n",
      "Epoch [202/300], Step [93/172], Loss: 19.8696\n",
      "Epoch [202/300], Step [94/172], Loss: 27.4806\n",
      "Epoch [202/300], Step [95/172], Loss: 20.2876\n",
      "Epoch [202/300], Step [96/172], Loss: 19.8289\n",
      "Epoch [202/300], Step [97/172], Loss: 28.1988\n",
      "Epoch [202/300], Step [98/172], Loss: 19.1373\n",
      "Epoch [202/300], Step [99/172], Loss: 18.9058\n",
      "Epoch [202/300], Step [100/172], Loss: 15.7862\n",
      "Epoch [202/300], Step [101/172], Loss: 19.1053\n",
      "Epoch [202/300], Step [102/172], Loss: 16.6110\n",
      "Epoch [202/300], Step [103/172], Loss: 12.6672\n",
      "Epoch [202/300], Step [104/172], Loss: 18.7573\n",
      "Epoch [202/300], Step [105/172], Loss: 20.0014\n",
      "Epoch [202/300], Step [106/172], Loss: 15.5754\n",
      "Epoch [202/300], Step [107/172], Loss: 15.8108\n",
      "Epoch [202/300], Step [108/172], Loss: 14.4667\n",
      "Epoch [202/300], Step [109/172], Loss: 14.3781\n",
      "Epoch [202/300], Step [110/172], Loss: 16.2398\n",
      "Epoch [202/300], Step [111/172], Loss: 16.3664\n",
      "Epoch [202/300], Step [112/172], Loss: 17.0998\n",
      "Epoch [202/300], Step [113/172], Loss: 12.0430\n",
      "Epoch [202/300], Step [114/172], Loss: 13.9375\n",
      "Epoch [202/300], Step [115/172], Loss: 18.8522\n",
      "Epoch [202/300], Step [116/172], Loss: 14.3459\n",
      "Epoch [202/300], Step [117/172], Loss: 11.9388\n",
      "Epoch [202/300], Step [118/172], Loss: 13.6318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [202/300], Step [119/172], Loss: 15.9151\n",
      "Epoch [202/300], Step [120/172], Loss: 10.3051\n",
      "Epoch [202/300], Step [121/172], Loss: 9.5442\n",
      "Epoch [202/300], Step [122/172], Loss: 11.0527\n",
      "Epoch [202/300], Step [123/172], Loss: 10.1352\n",
      "Epoch [202/300], Step [124/172], Loss: 7.4006\n",
      "Epoch [202/300], Step [125/172], Loss: 11.5340\n",
      "Epoch [202/300], Step [126/172], Loss: 11.1601\n",
      "Epoch [202/300], Step [127/172], Loss: 10.5818\n",
      "Epoch [202/300], Step [128/172], Loss: 10.1950\n",
      "Epoch [202/300], Step [129/172], Loss: 8.1488\n",
      "Epoch [202/300], Step [130/172], Loss: 12.5346\n",
      "Epoch [202/300], Step [131/172], Loss: 7.2700\n",
      "Epoch [202/300], Step [132/172], Loss: 8.7401\n",
      "Epoch [202/300], Step [133/172], Loss: 9.2012\n",
      "Epoch [202/300], Step [134/172], Loss: 10.9992\n",
      "Epoch [202/300], Step [135/172], Loss: 8.5155\n",
      "Epoch [202/300], Step [136/172], Loss: 7.8230\n",
      "Epoch [202/300], Step [137/172], Loss: 9.0745\n",
      "Epoch [202/300], Step [138/172], Loss: 6.8227\n",
      "Epoch [202/300], Step [139/172], Loss: 9.4104\n",
      "Epoch [202/300], Step [140/172], Loss: 9.9024\n",
      "Epoch [202/300], Step [141/172], Loss: 8.9527\n",
      "Epoch [202/300], Step [142/172], Loss: 14.1107\n",
      "Epoch [202/300], Step [143/172], Loss: 10.6927\n",
      "Epoch [202/300], Step [144/172], Loss: 8.9368\n",
      "Epoch [202/300], Step [145/172], Loss: 10.3716\n",
      "Epoch [202/300], Step [146/172], Loss: 9.4461\n",
      "Epoch [202/300], Step [147/172], Loss: 5.3073\n",
      "Epoch [202/300], Step [148/172], Loss: 6.0758\n",
      "Epoch [202/300], Step [149/172], Loss: 6.4338\n",
      "Epoch [202/300], Step [150/172], Loss: 5.7027\n",
      "Epoch [202/300], Step [151/172], Loss: 5.2489\n",
      "Epoch [202/300], Step [152/172], Loss: 7.2920\n",
      "Epoch [202/300], Step [153/172], Loss: 6.2058\n",
      "Epoch [202/300], Step [154/172], Loss: 7.2942\n",
      "Epoch [202/300], Step [155/172], Loss: 5.8596\n",
      "Epoch [202/300], Step [156/172], Loss: 13.3637\n",
      "Epoch [202/300], Step [157/172], Loss: 8.9589\n",
      "Epoch [202/300], Step [158/172], Loss: 6.8243\n",
      "Epoch [202/300], Step [159/172], Loss: 9.5808\n",
      "Epoch [202/300], Step [160/172], Loss: 9.5598\n",
      "Epoch [202/300], Step [161/172], Loss: 6.8696\n",
      "Epoch [202/300], Step [162/172], Loss: 4.9384\n",
      "Epoch [202/300], Step [163/172], Loss: 6.4573\n",
      "Epoch [202/300], Step [164/172], Loss: 8.5390\n",
      "Epoch [202/300], Step [165/172], Loss: 6.2793\n",
      "Epoch [202/300], Step [166/172], Loss: 5.6072\n",
      "Epoch [202/300], Step [167/172], Loss: 10.4803\n",
      "Epoch [202/300], Step [168/172], Loss: 6.2349\n",
      "Epoch [202/300], Step [169/172], Loss: 6.6217\n",
      "Epoch [202/300], Step [170/172], Loss: 4.8062\n",
      "Epoch [202/300], Step [171/172], Loss: 8.2898\n",
      "Epoch [202/300], Step [172/172], Loss: 5.2921\n",
      "Epoch [203/300], Step [1/172], Loss: 49.8808\n",
      "Epoch [203/300], Step [2/172], Loss: 52.7554\n",
      "Epoch [203/300], Step [3/172], Loss: 46.6909\n",
      "Epoch [203/300], Step [4/172], Loss: 26.4329\n",
      "Epoch [203/300], Step [5/172], Loss: 42.9614\n",
      "Epoch [203/300], Step [6/172], Loss: 18.9918\n",
      "Epoch [203/300], Step [7/172], Loss: 29.0834\n",
      "Epoch [203/300], Step [8/172], Loss: 4.6959\n",
      "Epoch [203/300], Step [9/172], Loss: 30.6990\n",
      "Epoch [203/300], Step [10/172], Loss: 41.3283\n",
      "Epoch [203/300], Step [11/172], Loss: 55.5435\n",
      "Epoch [203/300], Step [12/172], Loss: 61.5074\n",
      "Epoch [203/300], Step [13/172], Loss: 34.6064\n",
      "Epoch [203/300], Step [14/172], Loss: 60.4642\n",
      "Epoch [203/300], Step [15/172], Loss: 53.6647\n",
      "Epoch [203/300], Step [16/172], Loss: 9.5642\n",
      "Epoch [203/300], Step [17/172], Loss: 42.9158\n",
      "Epoch [203/300], Step [18/172], Loss: 56.2714\n",
      "Epoch [203/300], Step [19/172], Loss: 80.0342\n",
      "Epoch [203/300], Step [20/172], Loss: 32.9722\n",
      "Epoch [203/300], Step [21/172], Loss: 83.5166\n",
      "Epoch [203/300], Step [22/172], Loss: 58.2857\n",
      "Epoch [203/300], Step [23/172], Loss: 1.7972\n",
      "Epoch [203/300], Step [24/172], Loss: 54.1657\n",
      "Epoch [203/300], Step [25/172], Loss: 39.7659\n",
      "Epoch [203/300], Step [26/172], Loss: 46.3377\n",
      "Epoch [203/300], Step [27/172], Loss: 59.6286\n",
      "Epoch [203/300], Step [28/172], Loss: 23.5313\n",
      "Epoch [203/300], Step [29/172], Loss: 16.1010\n",
      "Epoch [203/300], Step [30/172], Loss: 60.8556\n",
      "Epoch [203/300], Step [31/172], Loss: 36.3833\n",
      "Epoch [203/300], Step [32/172], Loss: 43.6926\n",
      "Epoch [203/300], Step [33/172], Loss: 70.7230\n",
      "Epoch [203/300], Step [34/172], Loss: 2.5506\n",
      "Epoch [203/300], Step [35/172], Loss: 14.3987\n",
      "Epoch [203/300], Step [36/172], Loss: 17.7506\n",
      "Epoch [203/300], Step [37/172], Loss: 16.8154\n",
      "Epoch [203/300], Step [38/172], Loss: 30.9094\n",
      "Epoch [203/300], Step [39/172], Loss: 36.8840\n",
      "Epoch [203/300], Step [40/172], Loss: 21.4579\n",
      "Epoch [203/300], Step [41/172], Loss: 34.3579\n",
      "Epoch [203/300], Step [42/172], Loss: 39.3668\n",
      "Epoch [203/300], Step [43/172], Loss: 28.1243\n",
      "Epoch [203/300], Step [44/172], Loss: 21.5709\n",
      "Epoch [203/300], Step [45/172], Loss: 28.6873\n",
      "Epoch [203/300], Step [46/172], Loss: 17.2564\n",
      "Epoch [203/300], Step [47/172], Loss: 48.8671\n",
      "Epoch [203/300], Step [48/172], Loss: 60.8767\n",
      "Epoch [203/300], Step [49/172], Loss: 21.6735\n",
      "Epoch [203/300], Step [50/172], Loss: 47.1758\n",
      "Epoch [203/300], Step [51/172], Loss: 8.9665\n",
      "Epoch [203/300], Step [52/172], Loss: 20.1851\n",
      "Epoch [203/300], Step [53/172], Loss: 22.6266\n",
      "Epoch [203/300], Step [54/172], Loss: 14.7357\n",
      "Epoch [203/300], Step [55/172], Loss: 14.4046\n",
      "Epoch [203/300], Step [56/172], Loss: 19.0721\n",
      "Epoch [203/300], Step [57/172], Loss: 16.6191\n",
      "Epoch [203/300], Step [58/172], Loss: 12.7143\n",
      "Epoch [203/300], Step [59/172], Loss: 26.7393\n",
      "Epoch [203/300], Step [60/172], Loss: 22.6164\n",
      "Epoch [203/300], Step [61/172], Loss: 5.8621\n",
      "Epoch [203/300], Step [62/172], Loss: 18.3714\n",
      "Epoch [203/300], Step [63/172], Loss: 10.1106\n",
      "Epoch [203/300], Step [64/172], Loss: 10.7137\n",
      "Epoch [203/300], Step [65/172], Loss: 18.3410\n",
      "Epoch [203/300], Step [66/172], Loss: 6.3845\n",
      "Epoch [203/300], Step [67/172], Loss: 21.9080\n",
      "Epoch [203/300], Step [68/172], Loss: 4.3456\n",
      "Epoch [203/300], Step [69/172], Loss: 33.0271\n",
      "Epoch [203/300], Step [70/172], Loss: 35.9980\n",
      "Epoch [203/300], Step [71/172], Loss: 38.4232\n",
      "Epoch [203/300], Step [72/172], Loss: 37.7626\n",
      "Epoch [203/300], Step [73/172], Loss: 47.2741\n",
      "Epoch [203/300], Step [74/172], Loss: 24.4282\n",
      "Epoch [203/300], Step [75/172], Loss: 25.0948\n",
      "Epoch [203/300], Step [76/172], Loss: 27.1777\n",
      "Epoch [203/300], Step [77/172], Loss: 45.7255\n",
      "Epoch [203/300], Step [78/172], Loss: 35.6525\n",
      "Epoch [203/300], Step [79/172], Loss: 34.7050\n",
      "Epoch [203/300], Step [80/172], Loss: 48.0349\n",
      "Epoch [203/300], Step [81/172], Loss: 32.0910\n",
      "Epoch [203/300], Step [82/172], Loss: 34.3120\n",
      "Epoch [203/300], Step [83/172], Loss: 41.9291\n",
      "Epoch [203/300], Step [84/172], Loss: 31.8233\n",
      "Epoch [203/300], Step [85/172], Loss: 37.5105\n",
      "Epoch [203/300], Step [86/172], Loss: 31.9331\n",
      "Epoch [203/300], Step [87/172], Loss: 24.9197\n",
      "Epoch [203/300], Step [88/172], Loss: 22.6455\n",
      "Epoch [203/300], Step [89/172], Loss: 26.6043\n",
      "Epoch [203/300], Step [90/172], Loss: 19.7344\n",
      "Epoch [203/300], Step [91/172], Loss: 25.3793\n",
      "Epoch [203/300], Step [92/172], Loss: 19.1621\n",
      "Epoch [203/300], Step [93/172], Loss: 19.8433\n",
      "Epoch [203/300], Step [94/172], Loss: 27.4230\n",
      "Epoch [203/300], Step [95/172], Loss: 20.2426\n",
      "Epoch [203/300], Step [96/172], Loss: 19.7989\n",
      "Epoch [203/300], Step [97/172], Loss: 28.2112\n",
      "Epoch [203/300], Step [98/172], Loss: 19.0810\n",
      "Epoch [203/300], Step [99/172], Loss: 18.8949\n",
      "Epoch [203/300], Step [100/172], Loss: 15.7676\n",
      "Epoch [203/300], Step [101/172], Loss: 19.1591\n",
      "Epoch [203/300], Step [102/172], Loss: 16.5758\n",
      "Epoch [203/300], Step [103/172], Loss: 12.6780\n",
      "Epoch [203/300], Step [104/172], Loss: 18.8024\n",
      "Epoch [203/300], Step [105/172], Loss: 20.1862\n",
      "Epoch [203/300], Step [106/172], Loss: 15.5983\n",
      "Epoch [203/300], Step [107/172], Loss: 15.8290\n",
      "Epoch [203/300], Step [108/172], Loss: 14.4822\n",
      "Epoch [203/300], Step [109/172], Loss: 14.3556\n",
      "Epoch [203/300], Step [110/172], Loss: 16.3034\n",
      "Epoch [203/300], Step [111/172], Loss: 16.4171\n",
      "Epoch [203/300], Step [112/172], Loss: 17.0618\n",
      "Epoch [203/300], Step [113/172], Loss: 12.0536\n",
      "Epoch [203/300], Step [114/172], Loss: 13.9572\n",
      "Epoch [203/300], Step [115/172], Loss: 18.8353\n",
      "Epoch [203/300], Step [116/172], Loss: 14.3543\n",
      "Epoch [203/300], Step [117/172], Loss: 11.9995\n",
      "Epoch [203/300], Step [118/172], Loss: 13.7013\n",
      "Epoch [203/300], Step [119/172], Loss: 15.8432\n",
      "Epoch [203/300], Step [120/172], Loss: 10.3056\n",
      "Epoch [203/300], Step [121/172], Loss: 9.5831\n",
      "Epoch [203/300], Step [122/172], Loss: 11.1247\n",
      "Epoch [203/300], Step [123/172], Loss: 10.0979\n",
      "Epoch [203/300], Step [124/172], Loss: 7.3823\n",
      "Epoch [203/300], Step [125/172], Loss: 11.5321\n",
      "Epoch [203/300], Step [126/172], Loss: 11.1590\n",
      "Epoch [203/300], Step [127/172], Loss: 10.5632\n",
      "Epoch [203/300], Step [128/172], Loss: 10.2113\n",
      "Epoch [203/300], Step [129/172], Loss: 8.1905\n",
      "Epoch [203/300], Step [130/172], Loss: 12.6029\n",
      "Epoch [203/300], Step [131/172], Loss: 7.2794\n",
      "Epoch [203/300], Step [132/172], Loss: 8.8080\n",
      "Epoch [203/300], Step [133/172], Loss: 9.2281\n",
      "Epoch [203/300], Step [134/172], Loss: 10.9443\n",
      "Epoch [203/300], Step [135/172], Loss: 8.4963\n",
      "Epoch [203/300], Step [136/172], Loss: 7.7369\n",
      "Epoch [203/300], Step [137/172], Loss: 9.0794\n",
      "Epoch [203/300], Step [138/172], Loss: 6.8341\n",
      "Epoch [203/300], Step [139/172], Loss: 9.3668\n",
      "Epoch [203/300], Step [140/172], Loss: 9.9241\n",
      "Epoch [203/300], Step [141/172], Loss: 8.9434\n",
      "Epoch [203/300], Step [142/172], Loss: 14.1296\n",
      "Epoch [203/300], Step [143/172], Loss: 10.7213\n",
      "Epoch [203/300], Step [144/172], Loss: 8.9585\n",
      "Epoch [203/300], Step [145/172], Loss: 10.4532\n",
      "Epoch [203/300], Step [146/172], Loss: 9.4970\n",
      "Epoch [203/300], Step [147/172], Loss: 5.3214\n",
      "Epoch [203/300], Step [148/172], Loss: 6.1277\n",
      "Epoch [203/300], Step [149/172], Loss: 6.4744\n",
      "Epoch [203/300], Step [150/172], Loss: 5.6590\n",
      "Epoch [203/300], Step [151/172], Loss: 5.2306\n",
      "Epoch [203/300], Step [152/172], Loss: 7.2996\n",
      "Epoch [203/300], Step [153/172], Loss: 6.2067\n",
      "Epoch [203/300], Step [154/172], Loss: 7.3077\n",
      "Epoch [203/300], Step [155/172], Loss: 5.8609\n",
      "Epoch [203/300], Step [156/172], Loss: 13.3159\n",
      "Epoch [203/300], Step [157/172], Loss: 8.8621\n",
      "Epoch [203/300], Step [158/172], Loss: 6.7937\n",
      "Epoch [203/300], Step [159/172], Loss: 9.4823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [203/300], Step [160/172], Loss: 9.4106\n",
      "Epoch [203/300], Step [161/172], Loss: 6.8743\n",
      "Epoch [203/300], Step [162/172], Loss: 4.9368\n",
      "Epoch [203/300], Step [163/172], Loss: 6.3760\n",
      "Epoch [203/300], Step [164/172], Loss: 8.5524\n",
      "Epoch [203/300], Step [165/172], Loss: 6.3534\n",
      "Epoch [203/300], Step [166/172], Loss: 5.6415\n",
      "Epoch [203/300], Step [167/172], Loss: 10.3140\n",
      "Epoch [203/300], Step [168/172], Loss: 6.3122\n",
      "Epoch [203/300], Step [169/172], Loss: 6.6193\n",
      "Epoch [203/300], Step [170/172], Loss: 4.8565\n",
      "Epoch [203/300], Step [171/172], Loss: 8.1545\n",
      "Epoch [203/300], Step [172/172], Loss: 5.2823\n",
      "Epoch [204/300], Step [1/172], Loss: 49.5310\n",
      "Epoch [204/300], Step [2/172], Loss: 52.7022\n",
      "Epoch [204/300], Step [3/172], Loss: 48.7554\n",
      "Epoch [204/300], Step [4/172], Loss: 26.3883\n",
      "Epoch [204/300], Step [5/172], Loss: 42.9464\n",
      "Epoch [204/300], Step [6/172], Loss: 19.0395\n",
      "Epoch [204/300], Step [7/172], Loss: 29.1907\n",
      "Epoch [204/300], Step [8/172], Loss: 4.5417\n",
      "Epoch [204/300], Step [9/172], Loss: 30.5843\n",
      "Epoch [204/300], Step [10/172], Loss: 41.0044\n",
      "Epoch [204/300], Step [11/172], Loss: 55.3731\n",
      "Epoch [204/300], Step [12/172], Loss: 61.3876\n",
      "Epoch [204/300], Step [13/172], Loss: 34.6372\n",
      "Epoch [204/300], Step [14/172], Loss: 61.1370\n",
      "Epoch [204/300], Step [15/172], Loss: 53.8104\n",
      "Epoch [204/300], Step [16/172], Loss: 9.7149\n",
      "Epoch [204/300], Step [17/172], Loss: 43.0911\n",
      "Epoch [204/300], Step [18/172], Loss: 56.1753\n",
      "Epoch [204/300], Step [19/172], Loss: 80.1419\n",
      "Epoch [204/300], Step [20/172], Loss: 32.5578\n",
      "Epoch [204/300], Step [21/172], Loss: 84.2645\n",
      "Epoch [204/300], Step [22/172], Loss: 58.3481\n",
      "Epoch [204/300], Step [23/172], Loss: 1.6905\n",
      "Epoch [204/300], Step [24/172], Loss: 54.3320\n",
      "Epoch [204/300], Step [25/172], Loss: 39.6818\n",
      "Epoch [204/300], Step [26/172], Loss: 46.4003\n",
      "Epoch [204/300], Step [27/172], Loss: 59.3516\n",
      "Epoch [204/300], Step [28/172], Loss: 23.6671\n",
      "Epoch [204/300], Step [29/172], Loss: 16.2060\n",
      "Epoch [204/300], Step [30/172], Loss: 60.9916\n",
      "Epoch [204/300], Step [31/172], Loss: 36.5438\n",
      "Epoch [204/300], Step [32/172], Loss: 43.8596\n",
      "Epoch [204/300], Step [33/172], Loss: 70.8072\n",
      "Epoch [204/300], Step [34/172], Loss: 2.5816\n",
      "Epoch [204/300], Step [35/172], Loss: 14.3816\n",
      "Epoch [204/300], Step [36/172], Loss: 17.5618\n",
      "Epoch [204/300], Step [37/172], Loss: 16.8670\n",
      "Epoch [204/300], Step [38/172], Loss: 30.9790\n",
      "Epoch [204/300], Step [39/172], Loss: 36.8823\n",
      "Epoch [204/300], Step [40/172], Loss: 21.5828\n",
      "Epoch [204/300], Step [41/172], Loss: 34.1956\n",
      "Epoch [204/300], Step [42/172], Loss: 39.2552\n",
      "Epoch [204/300], Step [43/172], Loss: 28.1173\n",
      "Epoch [204/300], Step [44/172], Loss: 21.6577\n",
      "Epoch [204/300], Step [45/172], Loss: 28.9992\n",
      "Epoch [204/300], Step [46/172], Loss: 17.1766\n",
      "Epoch [204/300], Step [47/172], Loss: 49.0827\n",
      "Epoch [204/300], Step [48/172], Loss: 60.0667\n",
      "Epoch [204/300], Step [49/172], Loss: 21.9515\n",
      "Epoch [204/300], Step [50/172], Loss: 47.0859\n",
      "Epoch [204/300], Step [51/172], Loss: 9.1682\n",
      "Epoch [204/300], Step [52/172], Loss: 20.5913\n",
      "Epoch [204/300], Step [53/172], Loss: 22.9381\n",
      "Epoch [204/300], Step [54/172], Loss: 14.9908\n",
      "Epoch [204/300], Step [55/172], Loss: 14.6587\n",
      "Epoch [204/300], Step [56/172], Loss: 19.5069\n",
      "Epoch [204/300], Step [57/172], Loss: 16.8222\n",
      "Epoch [204/300], Step [58/172], Loss: 12.9132\n",
      "Epoch [204/300], Step [59/172], Loss: 26.5832\n",
      "Epoch [204/300], Step [60/172], Loss: 22.3423\n",
      "Epoch [204/300], Step [61/172], Loss: 5.8779\n",
      "Epoch [204/300], Step [62/172], Loss: 18.2147\n",
      "Epoch [204/300], Step [63/172], Loss: 10.2801\n",
      "Epoch [204/300], Step [64/172], Loss: 10.8874\n",
      "Epoch [204/300], Step [65/172], Loss: 18.3231\n",
      "Epoch [204/300], Step [66/172], Loss: 6.3743\n",
      "Epoch [204/300], Step [67/172], Loss: 21.8393\n",
      "Epoch [204/300], Step [68/172], Loss: 4.4721\n",
      "Epoch [204/300], Step [69/172], Loss: 32.8314\n",
      "Epoch [204/300], Step [70/172], Loss: 35.7170\n",
      "Epoch [204/300], Step [71/172], Loss: 38.2230\n",
      "Epoch [204/300], Step [72/172], Loss: 37.4911\n",
      "Epoch [204/300], Step [73/172], Loss: 47.1739\n",
      "Epoch [204/300], Step [74/172], Loss: 24.1845\n",
      "Epoch [204/300], Step [75/172], Loss: 24.8975\n",
      "Epoch [204/300], Step [76/172], Loss: 27.0152\n",
      "Epoch [204/300], Step [77/172], Loss: 45.4239\n",
      "Epoch [204/300], Step [78/172], Loss: 35.4254\n",
      "Epoch [204/300], Step [79/172], Loss: 34.6232\n",
      "Epoch [204/300], Step [80/172], Loss: 47.8822\n",
      "Epoch [204/300], Step [81/172], Loss: 31.9034\n",
      "Epoch [204/300], Step [82/172], Loss: 34.0510\n",
      "Epoch [204/300], Step [83/172], Loss: 41.7415\n",
      "Epoch [204/300], Step [84/172], Loss: 31.7783\n",
      "Epoch [204/300], Step [85/172], Loss: 37.4706\n",
      "Epoch [204/300], Step [86/172], Loss: 31.8623\n",
      "Epoch [204/300], Step [87/172], Loss: 24.7782\n",
      "Epoch [204/300], Step [88/172], Loss: 22.4489\n",
      "Epoch [204/300], Step [89/172], Loss: 26.5986\n",
      "Epoch [204/300], Step [90/172], Loss: 19.7253\n",
      "Epoch [204/300], Step [91/172], Loss: 25.3769\n",
      "Epoch [204/300], Step [92/172], Loss: 19.1111\n",
      "Epoch [204/300], Step [93/172], Loss: 19.7180\n",
      "Epoch [204/300], Step [94/172], Loss: 27.2900\n",
      "Epoch [204/300], Step [95/172], Loss: 20.1069\n",
      "Epoch [204/300], Step [96/172], Loss: 19.8268\n",
      "Epoch [204/300], Step [97/172], Loss: 28.1759\n",
      "Epoch [204/300], Step [98/172], Loss: 19.1249\n",
      "Epoch [204/300], Step [99/172], Loss: 18.9110\n",
      "Epoch [204/300], Step [100/172], Loss: 15.9076\n",
      "Epoch [204/300], Step [101/172], Loss: 19.1684\n",
      "Epoch [204/300], Step [102/172], Loss: 16.5950\n",
      "Epoch [204/300], Step [103/172], Loss: 12.7268\n",
      "Epoch [204/300], Step [104/172], Loss: 18.8530\n",
      "Epoch [204/300], Step [105/172], Loss: 20.2693\n",
      "Epoch [204/300], Step [106/172], Loss: 15.6568\n",
      "Epoch [204/300], Step [107/172], Loss: 15.8409\n",
      "Epoch [204/300], Step [108/172], Loss: 14.5434\n",
      "Epoch [204/300], Step [109/172], Loss: 14.3416\n",
      "Epoch [204/300], Step [110/172], Loss: 16.3645\n",
      "Epoch [204/300], Step [111/172], Loss: 16.5789\n",
      "Epoch [204/300], Step [112/172], Loss: 17.0583\n",
      "Epoch [204/300], Step [113/172], Loss: 12.1294\n",
      "Epoch [204/300], Step [114/172], Loss: 13.9767\n",
      "Epoch [204/300], Step [115/172], Loss: 18.7915\n",
      "Epoch [204/300], Step [116/172], Loss: 14.4426\n",
      "Epoch [204/300], Step [117/172], Loss: 12.0169\n",
      "Epoch [204/300], Step [118/172], Loss: 13.6428\n",
      "Epoch [204/300], Step [119/172], Loss: 15.8976\n",
      "Epoch [204/300], Step [120/172], Loss: 10.3852\n",
      "Epoch [204/300], Step [121/172], Loss: 9.6422\n",
      "Epoch [204/300], Step [122/172], Loss: 11.1873\n",
      "Epoch [204/300], Step [123/172], Loss: 10.1425\n",
      "Epoch [204/300], Step [124/172], Loss: 7.4549\n",
      "Epoch [204/300], Step [125/172], Loss: 11.7134\n",
      "Epoch [204/300], Step [126/172], Loss: 11.3028\n",
      "Epoch [204/300], Step [127/172], Loss: 10.5367\n",
      "Epoch [204/300], Step [128/172], Loss: 10.2023\n",
      "Epoch [204/300], Step [129/172], Loss: 8.2209\n",
      "Epoch [204/300], Step [130/172], Loss: 12.6763\n",
      "Epoch [204/300], Step [131/172], Loss: 7.3143\n",
      "Epoch [204/300], Step [132/172], Loss: 8.8703\n",
      "Epoch [204/300], Step [133/172], Loss: 9.2636\n",
      "Epoch [204/300], Step [134/172], Loss: 10.9843\n",
      "Epoch [204/300], Step [135/172], Loss: 8.5696\n",
      "Epoch [204/300], Step [136/172], Loss: 7.8091\n",
      "Epoch [204/300], Step [137/172], Loss: 9.1885\n",
      "Epoch [204/300], Step [138/172], Loss: 6.9364\n",
      "Epoch [204/300], Step [139/172], Loss: 9.5395\n",
      "Epoch [204/300], Step [140/172], Loss: 9.9702\n",
      "Epoch [204/300], Step [141/172], Loss: 8.9793\n",
      "Epoch [204/300], Step [142/172], Loss: 14.1768\n",
      "Epoch [204/300], Step [143/172], Loss: 10.7515\n",
      "Epoch [204/300], Step [144/172], Loss: 8.9978\n",
      "Epoch [204/300], Step [145/172], Loss: 10.5219\n",
      "Epoch [204/300], Step [146/172], Loss: 9.5231\n",
      "Epoch [204/300], Step [147/172], Loss: 5.3977\n",
      "Epoch [204/300], Step [148/172], Loss: 6.1401\n",
      "Epoch [204/300], Step [149/172], Loss: 6.5289\n",
      "Epoch [204/300], Step [150/172], Loss: 5.7150\n",
      "Epoch [204/300], Step [151/172], Loss: 5.2605\n",
      "Epoch [204/300], Step [152/172], Loss: 7.3238\n",
      "Epoch [204/300], Step [153/172], Loss: 6.2718\n",
      "Epoch [204/300], Step [154/172], Loss: 7.3286\n",
      "Epoch [204/300], Step [155/172], Loss: 5.9353\n",
      "Epoch [204/300], Step [156/172], Loss: 13.4134\n",
      "Epoch [204/300], Step [157/172], Loss: 8.9192\n",
      "Epoch [204/300], Step [158/172], Loss: 6.8106\n",
      "Epoch [204/300], Step [159/172], Loss: 9.6234\n",
      "Epoch [204/300], Step [160/172], Loss: 9.4717\n",
      "Epoch [204/300], Step [161/172], Loss: 6.9610\n",
      "Epoch [204/300], Step [162/172], Loss: 4.9518\n",
      "Epoch [204/300], Step [163/172], Loss: 6.4369\n",
      "Epoch [204/300], Step [164/172], Loss: 8.5783\n",
      "Epoch [204/300], Step [165/172], Loss: 6.3964\n",
      "Epoch [204/300], Step [166/172], Loss: 5.7026\n",
      "Epoch [204/300], Step [167/172], Loss: 10.4235\n",
      "Epoch [204/300], Step [168/172], Loss: 6.3730\n",
      "Epoch [204/300], Step [169/172], Loss: 6.6421\n",
      "Epoch [204/300], Step [170/172], Loss: 4.8572\n",
      "Epoch [204/300], Step [171/172], Loss: 8.3677\n",
      "Epoch [204/300], Step [172/172], Loss: 5.4126\n",
      "Epoch [205/300], Step [1/172], Loss: 49.5030\n",
      "Epoch [205/300], Step [2/172], Loss: 53.0384\n",
      "Epoch [205/300], Step [3/172], Loss: 46.9836\n",
      "Epoch [205/300], Step [4/172], Loss: 26.3702\n",
      "Epoch [205/300], Step [5/172], Loss: 43.1614\n",
      "Epoch [205/300], Step [6/172], Loss: 18.9725\n",
      "Epoch [205/300], Step [7/172], Loss: 28.6556\n",
      "Epoch [205/300], Step [8/172], Loss: 4.5830\n",
      "Epoch [205/300], Step [9/172], Loss: 30.4973\n",
      "Epoch [205/300], Step [10/172], Loss: 41.1501\n",
      "Epoch [205/300], Step [11/172], Loss: 55.3474\n",
      "Epoch [205/300], Step [12/172], Loss: 61.0663\n",
      "Epoch [205/300], Step [13/172], Loss: 34.7543\n",
      "Epoch [205/300], Step [14/172], Loss: 60.8398\n",
      "Epoch [205/300], Step [15/172], Loss: 53.5129\n",
      "Epoch [205/300], Step [16/172], Loss: 9.8252\n",
      "Epoch [205/300], Step [17/172], Loss: 42.7987\n",
      "Epoch [205/300], Step [18/172], Loss: 56.0366\n",
      "Epoch [205/300], Step [19/172], Loss: 80.0319\n",
      "Epoch [205/300], Step [20/172], Loss: 32.3509\n",
      "Epoch [205/300], Step [21/172], Loss: 84.1026\n",
      "Epoch [205/300], Step [22/172], Loss: 58.6100\n",
      "Epoch [205/300], Step [23/172], Loss: 1.8054\n",
      "Epoch [205/300], Step [24/172], Loss: 54.2826\n",
      "Epoch [205/300], Step [25/172], Loss: 39.8198\n",
      "Epoch [205/300], Step [26/172], Loss: 46.3076\n",
      "Epoch [205/300], Step [27/172], Loss: 59.3931\n",
      "Epoch [205/300], Step [28/172], Loss: 23.5619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [205/300], Step [29/172], Loss: 16.0929\n",
      "Epoch [205/300], Step [30/172], Loss: 61.1056\n",
      "Epoch [205/300], Step [31/172], Loss: 36.7028\n",
      "Epoch [205/300], Step [32/172], Loss: 43.9698\n",
      "Epoch [205/300], Step [33/172], Loss: 71.0363\n",
      "Epoch [205/300], Step [34/172], Loss: 2.5708\n",
      "Epoch [205/300], Step [35/172], Loss: 14.4936\n",
      "Epoch [205/300], Step [36/172], Loss: 17.6550\n",
      "Epoch [205/300], Step [37/172], Loss: 16.8930\n",
      "Epoch [205/300], Step [38/172], Loss: 31.2673\n",
      "Epoch [205/300], Step [39/172], Loss: 37.1074\n",
      "Epoch [205/300], Step [40/172], Loss: 21.7691\n",
      "Epoch [205/300], Step [41/172], Loss: 34.7428\n",
      "Epoch [205/300], Step [42/172], Loss: 39.6302\n",
      "Epoch [205/300], Step [43/172], Loss: 28.3569\n",
      "Epoch [205/300], Step [44/172], Loss: 21.8796\n",
      "Epoch [205/300], Step [45/172], Loss: 29.1342\n",
      "Epoch [205/300], Step [46/172], Loss: 17.3696\n",
      "Epoch [205/300], Step [47/172], Loss: 49.3565\n",
      "Epoch [205/300], Step [48/172], Loss: 61.0286\n",
      "Epoch [205/300], Step [49/172], Loss: 22.1210\n",
      "Epoch [205/300], Step [50/172], Loss: 46.8440\n",
      "Epoch [205/300], Step [51/172], Loss: 9.1642\n",
      "Epoch [205/300], Step [52/172], Loss: 20.5807\n",
      "Epoch [205/300], Step [53/172], Loss: 23.1116\n",
      "Epoch [205/300], Step [54/172], Loss: 15.0222\n",
      "Epoch [205/300], Step [55/172], Loss: 14.6373\n",
      "Epoch [205/300], Step [56/172], Loss: 19.2028\n",
      "Epoch [205/300], Step [57/172], Loss: 16.9565\n",
      "Epoch [205/300], Step [58/172], Loss: 12.8174\n",
      "Epoch [205/300], Step [59/172], Loss: 26.5504\n",
      "Epoch [205/300], Step [60/172], Loss: 22.0262\n",
      "Epoch [205/300], Step [61/172], Loss: 5.8909\n",
      "Epoch [205/300], Step [62/172], Loss: 18.1138\n",
      "Epoch [205/300], Step [63/172], Loss: 10.3098\n",
      "Epoch [205/300], Step [64/172], Loss: 10.7805\n",
      "Epoch [205/300], Step [65/172], Loss: 18.0980\n",
      "Epoch [205/300], Step [66/172], Loss: 6.3637\n",
      "Epoch [205/300], Step [67/172], Loss: 21.5345\n",
      "Epoch [205/300], Step [68/172], Loss: 4.1952\n",
      "Epoch [205/300], Step [69/172], Loss: 32.7035\n",
      "Epoch [205/300], Step [70/172], Loss: 35.7122\n",
      "Epoch [205/300], Step [71/172], Loss: 38.3091\n",
      "Epoch [205/300], Step [72/172], Loss: 37.4687\n",
      "Epoch [205/300], Step [73/172], Loss: 46.9912\n",
      "Epoch [205/300], Step [74/172], Loss: 24.2561\n",
      "Epoch [205/300], Step [75/172], Loss: 24.7072\n",
      "Epoch [205/300], Step [76/172], Loss: 27.0303\n",
      "Epoch [205/300], Step [77/172], Loss: 45.4553\n",
      "Epoch [205/300], Step [78/172], Loss: 35.2717\n",
      "Epoch [205/300], Step [79/172], Loss: 34.4397\n",
      "Epoch [205/300], Step [80/172], Loss: 47.8417\n",
      "Epoch [205/300], Step [81/172], Loss: 31.7415\n",
      "Epoch [205/300], Step [82/172], Loss: 34.1560\n",
      "Epoch [205/300], Step [83/172], Loss: 41.8784\n",
      "Epoch [205/300], Step [84/172], Loss: 31.7013\n",
      "Epoch [205/300], Step [85/172], Loss: 37.5505\n",
      "Epoch [205/300], Step [86/172], Loss: 31.8762\n",
      "Epoch [205/300], Step [87/172], Loss: 24.7377\n",
      "Epoch [205/300], Step [88/172], Loss: 22.3641\n",
      "Epoch [205/300], Step [89/172], Loss: 26.6850\n",
      "Epoch [205/300], Step [90/172], Loss: 19.6946\n",
      "Epoch [205/300], Step [91/172], Loss: 25.3966\n",
      "Epoch [205/300], Step [92/172], Loss: 19.0972\n",
      "Epoch [205/300], Step [93/172], Loss: 19.7162\n",
      "Epoch [205/300], Step [94/172], Loss: 27.2419\n",
      "Epoch [205/300], Step [95/172], Loss: 20.0961\n",
      "Epoch [205/300], Step [96/172], Loss: 19.8207\n",
      "Epoch [205/300], Step [97/172], Loss: 28.2537\n",
      "Epoch [205/300], Step [98/172], Loss: 19.2117\n",
      "Epoch [205/300], Step [99/172], Loss: 18.9570\n",
      "Epoch [205/300], Step [100/172], Loss: 16.0670\n",
      "Epoch [205/300], Step [101/172], Loss: 19.2021\n",
      "Epoch [205/300], Step [102/172], Loss: 16.6631\n",
      "Epoch [205/300], Step [103/172], Loss: 12.8495\n",
      "Epoch [205/300], Step [104/172], Loss: 18.9494\n",
      "Epoch [205/300], Step [105/172], Loss: 20.4955\n",
      "Epoch [205/300], Step [106/172], Loss: 15.7671\n",
      "Epoch [205/300], Step [107/172], Loss: 15.9671\n",
      "Epoch [205/300], Step [108/172], Loss: 14.6667\n",
      "Epoch [205/300], Step [109/172], Loss: 14.4290\n",
      "Epoch [205/300], Step [110/172], Loss: 16.5143\n",
      "Epoch [205/300], Step [111/172], Loss: 16.6975\n",
      "Epoch [205/300], Step [112/172], Loss: 16.9720\n",
      "Epoch [205/300], Step [113/172], Loss: 12.1513\n",
      "Epoch [205/300], Step [114/172], Loss: 13.9831\n",
      "Epoch [205/300], Step [115/172], Loss: 18.7718\n",
      "Epoch [205/300], Step [116/172], Loss: 14.4857\n",
      "Epoch [205/300], Step [117/172], Loss: 12.0081\n",
      "Epoch [205/300], Step [118/172], Loss: 13.5660\n",
      "Epoch [205/300], Step [119/172], Loss: 15.9727\n",
      "Epoch [205/300], Step [120/172], Loss: 10.4319\n",
      "Epoch [205/300], Step [121/172], Loss: 9.7292\n",
      "Epoch [205/300], Step [122/172], Loss: 11.3454\n",
      "Epoch [205/300], Step [123/172], Loss: 10.0944\n",
      "Epoch [205/300], Step [124/172], Loss: 7.4651\n",
      "Epoch [205/300], Step [125/172], Loss: 11.7182\n",
      "Epoch [205/300], Step [126/172], Loss: 11.2621\n",
      "Epoch [205/300], Step [127/172], Loss: 10.5737\n",
      "Epoch [205/300], Step [128/172], Loss: 10.2399\n",
      "Epoch [205/300], Step [129/172], Loss: 8.2650\n",
      "Epoch [205/300], Step [130/172], Loss: 12.7009\n",
      "Epoch [205/300], Step [131/172], Loss: 7.3646\n",
      "Epoch [205/300], Step [132/172], Loss: 9.0078\n",
      "Epoch [205/300], Step [133/172], Loss: 9.3898\n",
      "Epoch [205/300], Step [134/172], Loss: 11.0344\n",
      "Epoch [205/300], Step [135/172], Loss: 8.6435\n",
      "Epoch [205/300], Step [136/172], Loss: 7.7224\n",
      "Epoch [205/300], Step [137/172], Loss: 9.2007\n",
      "Epoch [205/300], Step [138/172], Loss: 6.9492\n",
      "Epoch [205/300], Step [139/172], Loss: 9.6030\n",
      "Epoch [205/300], Step [140/172], Loss: 10.0158\n",
      "Epoch [205/300], Step [141/172], Loss: 9.0451\n",
      "Epoch [205/300], Step [142/172], Loss: 14.1625\n",
      "Epoch [205/300], Step [143/172], Loss: 10.8017\n",
      "Epoch [205/300], Step [144/172], Loss: 9.0726\n",
      "Epoch [205/300], Step [145/172], Loss: 10.6332\n",
      "Epoch [205/300], Step [146/172], Loss: 9.6000\n",
      "Epoch [205/300], Step [147/172], Loss: 5.4445\n",
      "Epoch [205/300], Step [148/172], Loss: 6.2106\n",
      "Epoch [205/300], Step [149/172], Loss: 6.5753\n",
      "Epoch [205/300], Step [150/172], Loss: 5.7173\n",
      "Epoch [205/300], Step [151/172], Loss: 5.2522\n",
      "Epoch [205/300], Step [152/172], Loss: 7.4078\n",
      "Epoch [205/300], Step [153/172], Loss: 6.3036\n",
      "Epoch [205/300], Step [154/172], Loss: 7.3334\n",
      "Epoch [205/300], Step [155/172], Loss: 6.0059\n",
      "Epoch [205/300], Step [156/172], Loss: 13.4584\n",
      "Epoch [205/300], Step [157/172], Loss: 8.8633\n",
      "Epoch [205/300], Step [158/172], Loss: 6.8112\n",
      "Epoch [205/300], Step [159/172], Loss: 9.5299\n",
      "Epoch [205/300], Step [160/172], Loss: 9.4419\n",
      "Epoch [205/300], Step [161/172], Loss: 6.9574\n",
      "Epoch [205/300], Step [162/172], Loss: 4.9883\n",
      "Epoch [205/300], Step [163/172], Loss: 6.4506\n",
      "Epoch [205/300], Step [164/172], Loss: 8.5886\n",
      "Epoch [205/300], Step [165/172], Loss: 6.4559\n",
      "Epoch [205/300], Step [166/172], Loss: 5.8070\n",
      "Epoch [205/300], Step [167/172], Loss: 10.3297\n",
      "Epoch [205/300], Step [168/172], Loss: 6.4829\n",
      "Epoch [205/300], Step [169/172], Loss: 6.7194\n",
      "Epoch [205/300], Step [170/172], Loss: 4.9287\n",
      "Epoch [205/300], Step [171/172], Loss: 8.1992\n",
      "Epoch [205/300], Step [172/172], Loss: 5.4143\n",
      "Epoch [206/300], Step [1/172], Loss: 49.2355\n",
      "Epoch [206/300], Step [2/172], Loss: 53.0301\n",
      "Epoch [206/300], Step [3/172], Loss: 48.9264\n",
      "Epoch [206/300], Step [4/172], Loss: 26.2753\n",
      "Epoch [206/300], Step [5/172], Loss: 42.8116\n",
      "Epoch [206/300], Step [6/172], Loss: 18.9257\n",
      "Epoch [206/300], Step [7/172], Loss: 28.9261\n",
      "Epoch [206/300], Step [8/172], Loss: 4.4752\n",
      "Epoch [206/300], Step [9/172], Loss: 30.3445\n",
      "Epoch [206/300], Step [10/172], Loss: 40.8882\n",
      "Epoch [206/300], Step [11/172], Loss: 55.2297\n",
      "Epoch [206/300], Step [12/172], Loss: 60.9349\n",
      "Epoch [206/300], Step [13/172], Loss: 34.6934\n",
      "Epoch [206/300], Step [14/172], Loss: 60.7678\n",
      "Epoch [206/300], Step [15/172], Loss: 53.3028\n",
      "Epoch [206/300], Step [16/172], Loss: 9.5474\n",
      "Epoch [206/300], Step [17/172], Loss: 42.7918\n",
      "Epoch [206/300], Step [18/172], Loss: 56.1261\n",
      "Epoch [206/300], Step [19/172], Loss: 79.9587\n",
      "Epoch [206/300], Step [20/172], Loss: 32.2178\n",
      "Epoch [206/300], Step [21/172], Loss: 84.1738\n",
      "Epoch [206/300], Step [22/172], Loss: 58.2464\n",
      "Epoch [206/300], Step [23/172], Loss: 1.8090\n",
      "Epoch [206/300], Step [24/172], Loss: 54.4832\n",
      "Epoch [206/300], Step [25/172], Loss: 39.6702\n",
      "Epoch [206/300], Step [26/172], Loss: 46.4581\n",
      "Epoch [206/300], Step [27/172], Loss: 59.2231\n",
      "Epoch [206/300], Step [28/172], Loss: 23.9474\n",
      "Epoch [206/300], Step [29/172], Loss: 16.4390\n",
      "Epoch [206/300], Step [30/172], Loss: 60.9473\n",
      "Epoch [206/300], Step [31/172], Loss: 36.8232\n",
      "Epoch [206/300], Step [32/172], Loss: 43.9469\n",
      "Epoch [206/300], Step [33/172], Loss: 70.7876\n",
      "Epoch [206/300], Step [34/172], Loss: 2.6493\n",
      "Epoch [206/300], Step [35/172], Loss: 14.4909\n",
      "Epoch [206/300], Step [36/172], Loss: 17.4496\n",
      "Epoch [206/300], Step [37/172], Loss: 16.9656\n",
      "Epoch [206/300], Step [38/172], Loss: 31.3160\n",
      "Epoch [206/300], Step [39/172], Loss: 37.1394\n",
      "Epoch [206/300], Step [40/172], Loss: 22.0000\n",
      "Epoch [206/300], Step [41/172], Loss: 34.7479\n",
      "Epoch [206/300], Step [42/172], Loss: 39.8514\n",
      "Epoch [206/300], Step [43/172], Loss: 28.5139\n",
      "Epoch [206/300], Step [44/172], Loss: 21.8377\n",
      "Epoch [206/300], Step [45/172], Loss: 29.6545\n",
      "Epoch [206/300], Step [46/172], Loss: 17.5084\n",
      "Epoch [206/300], Step [47/172], Loss: 49.7190\n",
      "Epoch [206/300], Step [48/172], Loss: 60.4919\n",
      "Epoch [206/300], Step [49/172], Loss: 22.7209\n",
      "Epoch [206/300], Step [50/172], Loss: 47.0507\n",
      "Epoch [206/300], Step [51/172], Loss: 9.3777\n",
      "Epoch [206/300], Step [52/172], Loss: 20.9345\n",
      "Epoch [206/300], Step [53/172], Loss: 23.3569\n",
      "Epoch [206/300], Step [54/172], Loss: 15.3240\n",
      "Epoch [206/300], Step [55/172], Loss: 14.9283\n",
      "Epoch [206/300], Step [56/172], Loss: 19.7187\n",
      "Epoch [206/300], Step [57/172], Loss: 17.0181\n",
      "Epoch [206/300], Step [58/172], Loss: 13.0351\n",
      "Epoch [206/300], Step [59/172], Loss: 26.4237\n",
      "Epoch [206/300], Step [60/172], Loss: 22.3766\n",
      "Epoch [206/300], Step [61/172], Loss: 5.9937\n",
      "Epoch [206/300], Step [62/172], Loss: 18.0800\n",
      "Epoch [206/300], Step [63/172], Loss: 10.3838\n",
      "Epoch [206/300], Step [64/172], Loss: 10.9934\n",
      "Epoch [206/300], Step [65/172], Loss: 18.1570\n",
      "Epoch [206/300], Step [66/172], Loss: 6.4981\n",
      "Epoch [206/300], Step [67/172], Loss: 21.7799\n",
      "Epoch [206/300], Step [68/172], Loss: 4.4858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [206/300], Step [69/172], Loss: 32.5899\n",
      "Epoch [206/300], Step [70/172], Loss: 35.2743\n",
      "Epoch [206/300], Step [71/172], Loss: 37.9038\n",
      "Epoch [206/300], Step [72/172], Loss: 36.9492\n",
      "Epoch [206/300], Step [73/172], Loss: 46.6734\n",
      "Epoch [206/300], Step [74/172], Loss: 23.9260\n",
      "Epoch [206/300], Step [75/172], Loss: 24.2671\n",
      "Epoch [206/300], Step [76/172], Loss: 26.9307\n",
      "Epoch [206/300], Step [77/172], Loss: 45.0093\n",
      "Epoch [206/300], Step [78/172], Loss: 34.9989\n",
      "Epoch [206/300], Step [79/172], Loss: 34.1767\n",
      "Epoch [206/300], Step [80/172], Loss: 47.3975\n",
      "Epoch [206/300], Step [81/172], Loss: 31.6465\n",
      "Epoch [206/300], Step [82/172], Loss: 33.8913\n",
      "Epoch [206/300], Step [83/172], Loss: 41.7149\n",
      "Epoch [206/300], Step [84/172], Loss: 31.7170\n",
      "Epoch [206/300], Step [85/172], Loss: 37.6408\n",
      "Epoch [206/300], Step [86/172], Loss: 31.8726\n",
      "Epoch [206/300], Step [87/172], Loss: 24.6710\n",
      "Epoch [206/300], Step [88/172], Loss: 22.2315\n",
      "Epoch [206/300], Step [89/172], Loss: 26.7674\n",
      "Epoch [206/300], Step [90/172], Loss: 19.7318\n",
      "Epoch [206/300], Step [91/172], Loss: 25.4130\n",
      "Epoch [206/300], Step [92/172], Loss: 19.0867\n",
      "Epoch [206/300], Step [93/172], Loss: 19.6830\n",
      "Epoch [206/300], Step [94/172], Loss: 27.2114\n",
      "Epoch [206/300], Step [95/172], Loss: 20.0968\n",
      "Epoch [206/300], Step [96/172], Loss: 19.8652\n",
      "Epoch [206/300], Step [97/172], Loss: 28.2231\n",
      "Epoch [206/300], Step [98/172], Loss: 19.3082\n",
      "Epoch [206/300], Step [99/172], Loss: 19.0014\n",
      "Epoch [206/300], Step [100/172], Loss: 16.2719\n",
      "Epoch [206/300], Step [101/172], Loss: 19.2536\n",
      "Epoch [206/300], Step [102/172], Loss: 16.6915\n",
      "Epoch [206/300], Step [103/172], Loss: 12.9543\n",
      "Epoch [206/300], Step [104/172], Loss: 18.9742\n",
      "Epoch [206/300], Step [105/172], Loss: 20.6267\n",
      "Epoch [206/300], Step [106/172], Loss: 15.8726\n",
      "Epoch [206/300], Step [107/172], Loss: 16.0033\n",
      "Epoch [206/300], Step [108/172], Loss: 14.8053\n",
      "Epoch [206/300], Step [109/172], Loss: 14.5336\n",
      "Epoch [206/300], Step [110/172], Loss: 16.6334\n",
      "Epoch [206/300], Step [111/172], Loss: 16.9067\n",
      "Epoch [206/300], Step [112/172], Loss: 16.9552\n",
      "Epoch [206/300], Step [113/172], Loss: 12.2898\n",
      "Epoch [206/300], Step [114/172], Loss: 14.0433\n",
      "Epoch [206/300], Step [115/172], Loss: 18.7972\n",
      "Epoch [206/300], Step [116/172], Loss: 14.5533\n",
      "Epoch [206/300], Step [117/172], Loss: 12.1324\n",
      "Epoch [206/300], Step [118/172], Loss: 13.6149\n",
      "Epoch [206/300], Step [119/172], Loss: 15.9977\n",
      "Epoch [206/300], Step [120/172], Loss: 10.4979\n",
      "Epoch [206/300], Step [121/172], Loss: 9.8068\n",
      "Epoch [206/300], Step [122/172], Loss: 11.4829\n",
      "Epoch [206/300], Step [123/172], Loss: 10.1617\n",
      "Epoch [206/300], Step [124/172], Loss: 7.4635\n",
      "Epoch [206/300], Step [125/172], Loss: 11.7411\n",
      "Epoch [206/300], Step [126/172], Loss: 11.3390\n",
      "Epoch [206/300], Step [127/172], Loss: 10.5947\n",
      "Epoch [206/300], Step [128/172], Loss: 10.2938\n",
      "Epoch [206/300], Step [129/172], Loss: 8.3154\n",
      "Epoch [206/300], Step [130/172], Loss: 12.7762\n",
      "Epoch [206/300], Step [131/172], Loss: 7.4271\n",
      "Epoch [206/300], Step [132/172], Loss: 9.1094\n",
      "Epoch [206/300], Step [133/172], Loss: 9.4477\n",
      "Epoch [206/300], Step [134/172], Loss: 11.0533\n",
      "Epoch [206/300], Step [135/172], Loss: 8.6755\n",
      "Epoch [206/300], Step [136/172], Loss: 7.7260\n",
      "Epoch [206/300], Step [137/172], Loss: 9.1916\n",
      "Epoch [206/300], Step [138/172], Loss: 6.9639\n",
      "Epoch [206/300], Step [139/172], Loss: 9.5437\n",
      "Epoch [206/300], Step [140/172], Loss: 10.0565\n",
      "Epoch [206/300], Step [141/172], Loss: 9.0283\n",
      "Epoch [206/300], Step [142/172], Loss: 14.2186\n",
      "Epoch [206/300], Step [143/172], Loss: 10.8275\n",
      "Epoch [206/300], Step [144/172], Loss: 9.0413\n",
      "Epoch [206/300], Step [145/172], Loss: 10.7073\n",
      "Epoch [206/300], Step [146/172], Loss: 9.6372\n",
      "Epoch [206/300], Step [147/172], Loss: 5.4284\n",
      "Epoch [206/300], Step [148/172], Loss: 6.2379\n",
      "Epoch [206/300], Step [149/172], Loss: 6.5787\n",
      "Epoch [206/300], Step [150/172], Loss: 5.7634\n",
      "Epoch [206/300], Step [151/172], Loss: 5.2705\n",
      "Epoch [206/300], Step [152/172], Loss: 7.4605\n",
      "Epoch [206/300], Step [153/172], Loss: 6.3011\n",
      "Epoch [206/300], Step [154/172], Loss: 7.3514\n",
      "Epoch [206/300], Step [155/172], Loss: 5.9786\n",
      "Epoch [206/300], Step [156/172], Loss: 13.5094\n",
      "Epoch [206/300], Step [157/172], Loss: 8.8417\n",
      "Epoch [206/300], Step [158/172], Loss: 6.8277\n",
      "Epoch [206/300], Step [159/172], Loss: 9.6302\n",
      "Epoch [206/300], Step [160/172], Loss: 9.4310\n",
      "Epoch [206/300], Step [161/172], Loss: 6.9072\n",
      "Epoch [206/300], Step [162/172], Loss: 5.0104\n",
      "Epoch [206/300], Step [163/172], Loss: 6.4699\n",
      "Epoch [206/300], Step [164/172], Loss: 8.5871\n",
      "Epoch [206/300], Step [165/172], Loss: 6.5047\n",
      "Epoch [206/300], Step [166/172], Loss: 5.8540\n",
      "Epoch [206/300], Step [167/172], Loss: 10.3626\n",
      "Epoch [206/300], Step [168/172], Loss: 6.4923\n",
      "Epoch [206/300], Step [169/172], Loss: 6.7834\n",
      "Epoch [206/300], Step [170/172], Loss: 4.9802\n",
      "Epoch [206/300], Step [171/172], Loss: 8.3220\n",
      "Epoch [206/300], Step [172/172], Loss: 5.4335\n",
      "Epoch [207/300], Step [1/172], Loss: 48.9536\n",
      "Epoch [207/300], Step [2/172], Loss: 52.6896\n",
      "Epoch [207/300], Step [3/172], Loss: 48.2617\n",
      "Epoch [207/300], Step [4/172], Loss: 26.3236\n",
      "Epoch [207/300], Step [5/172], Loss: 42.7669\n",
      "Epoch [207/300], Step [6/172], Loss: 19.1580\n",
      "Epoch [207/300], Step [7/172], Loss: 29.1700\n",
      "Epoch [207/300], Step [8/172], Loss: 4.8866\n",
      "Epoch [207/300], Step [9/172], Loss: 30.2961\n",
      "Epoch [207/300], Step [10/172], Loss: 40.7295\n",
      "Epoch [207/300], Step [11/172], Loss: 55.2193\n",
      "Epoch [207/300], Step [12/172], Loss: 60.6946\n",
      "Epoch [207/300], Step [13/172], Loss: 34.6789\n",
      "Epoch [207/300], Step [14/172], Loss: 60.8297\n",
      "Epoch [207/300], Step [15/172], Loss: 52.9317\n",
      "Epoch [207/300], Step [16/172], Loss: 9.6207\n",
      "Epoch [207/300], Step [17/172], Loss: 42.6912\n",
      "Epoch [207/300], Step [18/172], Loss: 55.4519\n",
      "Epoch [207/300], Step [19/172], Loss: 79.3104\n",
      "Epoch [207/300], Step [20/172], Loss: 31.9638\n",
      "Epoch [207/300], Step [21/172], Loss: 84.1106\n",
      "Epoch [207/300], Step [22/172], Loss: 57.9014\n",
      "Epoch [207/300], Step [23/172], Loss: 1.7810\n",
      "Epoch [207/300], Step [24/172], Loss: 54.5802\n",
      "Epoch [207/300], Step [25/172], Loss: 39.6472\n",
      "Epoch [207/300], Step [26/172], Loss: 46.5069\n",
      "Epoch [207/300], Step [27/172], Loss: 58.7482\n",
      "Epoch [207/300], Step [28/172], Loss: 24.1260\n",
      "Epoch [207/300], Step [29/172], Loss: 16.2611\n",
      "Epoch [207/300], Step [30/172], Loss: 60.6935\n",
      "Epoch [207/300], Step [31/172], Loss: 36.5933\n",
      "Epoch [207/300], Step [32/172], Loss: 44.2857\n",
      "Epoch [207/300], Step [33/172], Loss: 71.2326\n",
      "Epoch [207/300], Step [34/172], Loss: 2.6918\n",
      "Epoch [207/300], Step [35/172], Loss: 14.5127\n",
      "Epoch [207/300], Step [36/172], Loss: 17.7950\n",
      "Epoch [207/300], Step [37/172], Loss: 17.0721\n",
      "Epoch [207/300], Step [38/172], Loss: 31.8514\n",
      "Epoch [207/300], Step [39/172], Loss: 37.4406\n",
      "Epoch [207/300], Step [40/172], Loss: 22.2331\n",
      "Epoch [207/300], Step [41/172], Loss: 34.9475\n",
      "Epoch [207/300], Step [42/172], Loss: 40.3052\n",
      "Epoch [207/300], Step [43/172], Loss: 28.8088\n",
      "Epoch [207/300], Step [44/172], Loss: 21.9114\n",
      "Epoch [207/300], Step [45/172], Loss: 30.0398\n",
      "Epoch [207/300], Step [46/172], Loss: 17.5099\n",
      "Epoch [207/300], Step [47/172], Loss: 49.9903\n",
      "Epoch [207/300], Step [48/172], Loss: 60.2194\n",
      "Epoch [207/300], Step [49/172], Loss: 22.8125\n",
      "Epoch [207/300], Step [50/172], Loss: 46.7387\n",
      "Epoch [207/300], Step [51/172], Loss: 9.4529\n",
      "Epoch [207/300], Step [52/172], Loss: 21.0375\n",
      "Epoch [207/300], Step [53/172], Loss: 23.3348\n",
      "Epoch [207/300], Step [54/172], Loss: 15.3686\n",
      "Epoch [207/300], Step [55/172], Loss: 15.0064\n",
      "Epoch [207/300], Step [56/172], Loss: 19.7904\n",
      "Epoch [207/300], Step [57/172], Loss: 16.9944\n",
      "Epoch [207/300], Step [58/172], Loss: 13.1267\n",
      "Epoch [207/300], Step [59/172], Loss: 26.2991\n",
      "Epoch [207/300], Step [60/172], Loss: 21.9545\n",
      "Epoch [207/300], Step [61/172], Loss: 5.9919\n",
      "Epoch [207/300], Step [62/172], Loss: 17.8201\n",
      "Epoch [207/300], Step [63/172], Loss: 10.5081\n",
      "Epoch [207/300], Step [64/172], Loss: 10.9390\n",
      "Epoch [207/300], Step [65/172], Loss: 17.9529\n",
      "Epoch [207/300], Step [66/172], Loss: 6.5397\n",
      "Epoch [207/300], Step [67/172], Loss: 21.6986\n",
      "Epoch [207/300], Step [68/172], Loss: 4.4050\n",
      "Epoch [207/300], Step [69/172], Loss: 32.3407\n",
      "Epoch [207/300], Step [70/172], Loss: 35.1745\n",
      "Epoch [207/300], Step [71/172], Loss: 37.9121\n",
      "Epoch [207/300], Step [72/172], Loss: 36.8155\n",
      "Epoch [207/300], Step [73/172], Loss: 46.4931\n",
      "Epoch [207/300], Step [74/172], Loss: 23.8267\n",
      "Epoch [207/300], Step [75/172], Loss: 24.2380\n",
      "Epoch [207/300], Step [76/172], Loss: 26.7616\n",
      "Epoch [207/300], Step [77/172], Loss: 44.9210\n",
      "Epoch [207/300], Step [78/172], Loss: 34.8714\n",
      "Epoch [207/300], Step [79/172], Loss: 34.0196\n",
      "Epoch [207/300], Step [80/172], Loss: 47.4560\n",
      "Epoch [207/300], Step [81/172], Loss: 31.5747\n",
      "Epoch [207/300], Step [82/172], Loss: 34.0707\n",
      "Epoch [207/300], Step [83/172], Loss: 41.5459\n",
      "Epoch [207/300], Step [84/172], Loss: 31.6183\n",
      "Epoch [207/300], Step [85/172], Loss: 37.6214\n",
      "Epoch [207/300], Step [86/172], Loss: 31.9129\n",
      "Epoch [207/300], Step [87/172], Loss: 24.6672\n",
      "Epoch [207/300], Step [88/172], Loss: 22.1675\n",
      "Epoch [207/300], Step [89/172], Loss: 26.7865\n",
      "Epoch [207/300], Step [90/172], Loss: 19.6050\n",
      "Epoch [207/300], Step [91/172], Loss: 25.3711\n",
      "Epoch [207/300], Step [92/172], Loss: 18.8797\n",
      "Epoch [207/300], Step [93/172], Loss: 19.5785\n",
      "Epoch [207/300], Step [94/172], Loss: 27.2158\n",
      "Epoch [207/300], Step [95/172], Loss: 19.9846\n",
      "Epoch [207/300], Step [96/172], Loss: 19.8626\n",
      "Epoch [207/300], Step [97/172], Loss: 28.1719\n",
      "Epoch [207/300], Step [98/172], Loss: 19.3563\n",
      "Epoch [207/300], Step [99/172], Loss: 19.0194\n",
      "Epoch [207/300], Step [100/172], Loss: 16.4361\n",
      "Epoch [207/300], Step [101/172], Loss: 19.2984\n",
      "Epoch [207/300], Step [102/172], Loss: 16.7687\n",
      "Epoch [207/300], Step [103/172], Loss: 13.0231\n",
      "Epoch [207/300], Step [104/172], Loss: 19.0918\n",
      "Epoch [207/300], Step [105/172], Loss: 20.8644\n",
      "Epoch [207/300], Step [106/172], Loss: 15.9392\n",
      "Epoch [207/300], Step [107/172], Loss: 16.0243\n",
      "Epoch [207/300], Step [108/172], Loss: 14.8549\n",
      "Epoch [207/300], Step [109/172], Loss: 14.6016\n",
      "Epoch [207/300], Step [110/172], Loss: 16.7309\n",
      "Epoch [207/300], Step [111/172], Loss: 17.0403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [207/300], Step [112/172], Loss: 16.8934\n",
      "Epoch [207/300], Step [113/172], Loss: 12.3597\n",
      "Epoch [207/300], Step [114/172], Loss: 14.0571\n",
      "Epoch [207/300], Step [115/172], Loss: 18.7626\n",
      "Epoch [207/300], Step [116/172], Loss: 14.6150\n",
      "Epoch [207/300], Step [117/172], Loss: 12.2079\n",
      "Epoch [207/300], Step [118/172], Loss: 13.5707\n",
      "Epoch [207/300], Step [119/172], Loss: 15.9576\n",
      "Epoch [207/300], Step [120/172], Loss: 10.5507\n",
      "Epoch [207/300], Step [121/172], Loss: 9.8513\n",
      "Epoch [207/300], Step [122/172], Loss: 11.5377\n",
      "Epoch [207/300], Step [123/172], Loss: 10.1482\n",
      "Epoch [207/300], Step [124/172], Loss: 7.4943\n",
      "Epoch [207/300], Step [125/172], Loss: 11.8154\n",
      "Epoch [207/300], Step [126/172], Loss: 11.2991\n",
      "Epoch [207/300], Step [127/172], Loss: 10.5312\n",
      "Epoch [207/300], Step [128/172], Loss: 10.2260\n",
      "Epoch [207/300], Step [129/172], Loss: 8.3602\n",
      "Epoch [207/300], Step [130/172], Loss: 12.7580\n",
      "Epoch [207/300], Step [131/172], Loss: 7.4316\n",
      "Epoch [207/300], Step [132/172], Loss: 9.1295\n",
      "Epoch [207/300], Step [133/172], Loss: 9.4573\n",
      "Epoch [207/300], Step [134/172], Loss: 10.9687\n",
      "Epoch [207/300], Step [135/172], Loss: 8.6814\n",
      "Epoch [207/300], Step [136/172], Loss: 7.7053\n",
      "Epoch [207/300], Step [137/172], Loss: 9.2471\n",
      "Epoch [207/300], Step [138/172], Loss: 6.9618\n",
      "Epoch [207/300], Step [139/172], Loss: 9.5899\n",
      "Epoch [207/300], Step [140/172], Loss: 10.0350\n",
      "Epoch [207/300], Step [141/172], Loss: 9.0087\n",
      "Epoch [207/300], Step [142/172], Loss: 14.1851\n",
      "Epoch [207/300], Step [143/172], Loss: 10.7924\n",
      "Epoch [207/300], Step [144/172], Loss: 9.0270\n",
      "Epoch [207/300], Step [145/172], Loss: 10.7327\n",
      "Epoch [207/300], Step [146/172], Loss: 9.6133\n",
      "Epoch [207/300], Step [147/172], Loss: 5.4425\n",
      "Epoch [207/300], Step [148/172], Loss: 6.2526\n",
      "Epoch [207/300], Step [149/172], Loss: 6.6389\n",
      "Epoch [207/300], Step [150/172], Loss: 5.7656\n",
      "Epoch [207/300], Step [151/172], Loss: 5.2460\n",
      "Epoch [207/300], Step [152/172], Loss: 7.4672\n",
      "Epoch [207/300], Step [153/172], Loss: 6.3106\n",
      "Epoch [207/300], Step [154/172], Loss: 7.3658\n",
      "Epoch [207/300], Step [155/172], Loss: 6.0589\n",
      "Epoch [207/300], Step [156/172], Loss: 13.3924\n",
      "Epoch [207/300], Step [157/172], Loss: 8.7526\n",
      "Epoch [207/300], Step [158/172], Loss: 6.7844\n",
      "Epoch [207/300], Step [159/172], Loss: 9.4499\n",
      "Epoch [207/300], Step [160/172], Loss: 9.2613\n",
      "Epoch [207/300], Step [161/172], Loss: 6.9596\n",
      "Epoch [207/300], Step [162/172], Loss: 5.0002\n",
      "Epoch [207/300], Step [163/172], Loss: 6.3151\n",
      "Epoch [207/300], Step [164/172], Loss: 8.6534\n",
      "Epoch [207/300], Step [165/172], Loss: 6.5213\n",
      "Epoch [207/300], Step [166/172], Loss: 5.8696\n",
      "Epoch [207/300], Step [167/172], Loss: 10.1376\n",
      "Epoch [207/300], Step [168/172], Loss: 6.5555\n",
      "Epoch [207/300], Step [169/172], Loss: 6.7611\n",
      "Epoch [207/300], Step [170/172], Loss: 4.9700\n",
      "Epoch [207/300], Step [171/172], Loss: 8.0782\n",
      "Epoch [207/300], Step [172/172], Loss: 5.4257\n",
      "Epoch [208/300], Step [1/172], Loss: 48.8729\n",
      "Epoch [208/300], Step [2/172], Loss: 52.9983\n",
      "Epoch [208/300], Step [3/172], Loss: 50.0401\n",
      "Epoch [208/300], Step [4/172], Loss: 26.7214\n",
      "Epoch [208/300], Step [5/172], Loss: 43.9865\n",
      "Epoch [208/300], Step [6/172], Loss: 19.3555\n",
      "Epoch [208/300], Step [7/172], Loss: 29.2678\n",
      "Epoch [208/300], Step [8/172], Loss: 4.5737\n",
      "Epoch [208/300], Step [9/172], Loss: 30.3916\n",
      "Epoch [208/300], Step [10/172], Loss: 40.6938\n",
      "Epoch [208/300], Step [11/172], Loss: 55.2870\n",
      "Epoch [208/300], Step [12/172], Loss: 60.5638\n",
      "Epoch [208/300], Step [13/172], Loss: 34.6953\n",
      "Epoch [208/300], Step [14/172], Loss: 61.2387\n",
      "Epoch [208/300], Step [15/172], Loss: 53.0004\n",
      "Epoch [208/300], Step [16/172], Loss: 10.1281\n",
      "Epoch [208/300], Step [17/172], Loss: 42.5194\n",
      "Epoch [208/300], Step [18/172], Loss: 55.3815\n",
      "Epoch [208/300], Step [19/172], Loss: 78.8717\n",
      "Epoch [208/300], Step [20/172], Loss: 32.0313\n",
      "Epoch [208/300], Step [21/172], Loss: 84.5842\n",
      "Epoch [208/300], Step [22/172], Loss: 57.7605\n",
      "Epoch [208/300], Step [23/172], Loss: 1.7511\n",
      "Epoch [208/300], Step [24/172], Loss: 54.5125\n",
      "Epoch [208/300], Step [25/172], Loss: 39.7490\n",
      "Epoch [208/300], Step [26/172], Loss: 46.3719\n",
      "Epoch [208/300], Step [27/172], Loss: 59.1098\n",
      "Epoch [208/300], Step [28/172], Loss: 24.1834\n",
      "Epoch [208/300], Step [29/172], Loss: 16.4005\n",
      "Epoch [208/300], Step [30/172], Loss: 60.8338\n",
      "Epoch [208/300], Step [31/172], Loss: 36.8768\n",
      "Epoch [208/300], Step [32/172], Loss: 44.2678\n",
      "Epoch [208/300], Step [33/172], Loss: 71.0961\n",
      "Epoch [208/300], Step [34/172], Loss: 2.6749\n",
      "Epoch [208/300], Step [35/172], Loss: 14.5070\n",
      "Epoch [208/300], Step [36/172], Loss: 17.5304\n",
      "Epoch [208/300], Step [37/172], Loss: 17.0289\n",
      "Epoch [208/300], Step [38/172], Loss: 32.0407\n",
      "Epoch [208/300], Step [39/172], Loss: 37.4259\n",
      "Epoch [208/300], Step [40/172], Loss: 22.4353\n",
      "Epoch [208/300], Step [41/172], Loss: 35.0599\n",
      "Epoch [208/300], Step [42/172], Loss: 40.4091\n",
      "Epoch [208/300], Step [43/172], Loss: 28.8239\n",
      "Epoch [208/300], Step [44/172], Loss: 22.0217\n",
      "Epoch [208/300], Step [45/172], Loss: 30.2939\n",
      "Epoch [208/300], Step [46/172], Loss: 17.5866\n",
      "Epoch [208/300], Step [47/172], Loss: 50.5373\n",
      "Epoch [208/300], Step [48/172], Loss: 60.2518\n",
      "Epoch [208/300], Step [49/172], Loss: 22.9215\n",
      "Epoch [208/300], Step [50/172], Loss: 46.2692\n",
      "Epoch [208/300], Step [51/172], Loss: 9.6191\n",
      "Epoch [208/300], Step [52/172], Loss: 21.2741\n",
      "Epoch [208/300], Step [53/172], Loss: 23.6061\n",
      "Epoch [208/300], Step [54/172], Loss: 15.5947\n",
      "Epoch [208/300], Step [55/172], Loss: 15.3369\n",
      "Epoch [208/300], Step [56/172], Loss: 20.3771\n",
      "Epoch [208/300], Step [57/172], Loss: 16.9355\n",
      "Epoch [208/300], Step [58/172], Loss: 13.2550\n",
      "Epoch [208/300], Step [59/172], Loss: 26.5362\n",
      "Epoch [208/300], Step [60/172], Loss: 21.5754\n",
      "Epoch [208/300], Step [61/172], Loss: 6.1429\n",
      "Epoch [208/300], Step [62/172], Loss: 18.0397\n",
      "Epoch [208/300], Step [63/172], Loss: 10.8251\n",
      "Epoch [208/300], Step [64/172], Loss: 11.1958\n",
      "Epoch [208/300], Step [65/172], Loss: 18.1424\n",
      "Epoch [208/300], Step [66/172], Loss: 6.6826\n",
      "Epoch [208/300], Step [67/172], Loss: 21.8229\n",
      "Epoch [208/300], Step [68/172], Loss: 4.4289\n",
      "Epoch [208/300], Step [69/172], Loss: 32.2521\n",
      "Epoch [208/300], Step [70/172], Loss: 34.6767\n",
      "Epoch [208/300], Step [71/172], Loss: 37.3549\n",
      "Epoch [208/300], Step [72/172], Loss: 36.2437\n",
      "Epoch [208/300], Step [73/172], Loss: 45.8373\n",
      "Epoch [208/300], Step [74/172], Loss: 23.4400\n",
      "Epoch [208/300], Step [75/172], Loss: 23.6134\n",
      "Epoch [208/300], Step [76/172], Loss: 26.4573\n",
      "Epoch [208/300], Step [77/172], Loss: 44.1236\n",
      "Epoch [208/300], Step [78/172], Loss: 34.3399\n",
      "Epoch [208/300], Step [79/172], Loss: 33.5010\n",
      "Epoch [208/300], Step [80/172], Loss: 46.8532\n",
      "Epoch [208/300], Step [81/172], Loss: 31.1417\n",
      "Epoch [208/300], Step [82/172], Loss: 33.5992\n",
      "Epoch [208/300], Step [83/172], Loss: 41.2446\n",
      "Epoch [208/300], Step [84/172], Loss: 31.5028\n",
      "Epoch [208/300], Step [85/172], Loss: 37.5305\n",
      "Epoch [208/300], Step [86/172], Loss: 31.7708\n",
      "Epoch [208/300], Step [87/172], Loss: 24.4508\n",
      "Epoch [208/300], Step [88/172], Loss: 21.9940\n",
      "Epoch [208/300], Step [89/172], Loss: 26.7706\n",
      "Epoch [208/300], Step [90/172], Loss: 19.7105\n",
      "Epoch [208/300], Step [91/172], Loss: 25.3724\n",
      "Epoch [208/300], Step [92/172], Loss: 18.8155\n",
      "Epoch [208/300], Step [93/172], Loss: 19.4598\n",
      "Epoch [208/300], Step [94/172], Loss: 27.1570\n",
      "Epoch [208/300], Step [95/172], Loss: 19.9824\n",
      "Epoch [208/300], Step [96/172], Loss: 19.9332\n",
      "Epoch [208/300], Step [97/172], Loss: 28.1535\n",
      "Epoch [208/300], Step [98/172], Loss: 19.4482\n",
      "Epoch [208/300], Step [99/172], Loss: 19.1014\n",
      "Epoch [208/300], Step [100/172], Loss: 16.6921\n",
      "Epoch [208/300], Step [101/172], Loss: 19.3766\n",
      "Epoch [208/300], Step [102/172], Loss: 16.7590\n",
      "Epoch [208/300], Step [103/172], Loss: 13.1776\n",
      "Epoch [208/300], Step [104/172], Loss: 19.1652\n",
      "Epoch [208/300], Step [105/172], Loss: 21.1359\n",
      "Epoch [208/300], Step [106/172], Loss: 16.1317\n",
      "Epoch [208/300], Step [107/172], Loss: 16.1419\n",
      "Epoch [208/300], Step [108/172], Loss: 14.9800\n",
      "Epoch [208/300], Step [109/172], Loss: 14.7429\n",
      "Epoch [208/300], Step [110/172], Loss: 16.9168\n",
      "Epoch [208/300], Step [111/172], Loss: 17.2651\n",
      "Epoch [208/300], Step [112/172], Loss: 16.8946\n",
      "Epoch [208/300], Step [113/172], Loss: 12.4162\n",
      "Epoch [208/300], Step [114/172], Loss: 14.1642\n",
      "Epoch [208/300], Step [115/172], Loss: 18.7154\n",
      "Epoch [208/300], Step [116/172], Loss: 14.6731\n",
      "Epoch [208/300], Step [117/172], Loss: 12.2391\n",
      "Epoch [208/300], Step [118/172], Loss: 13.4786\n",
      "Epoch [208/300], Step [119/172], Loss: 16.1038\n",
      "Epoch [208/300], Step [120/172], Loss: 10.6506\n",
      "Epoch [208/300], Step [121/172], Loss: 9.9923\n",
      "Epoch [208/300], Step [122/172], Loss: 11.7369\n",
      "Epoch [208/300], Step [123/172], Loss: 10.1907\n",
      "Epoch [208/300], Step [124/172], Loss: 7.4813\n",
      "Epoch [208/300], Step [125/172], Loss: 11.8804\n",
      "Epoch [208/300], Step [126/172], Loss: 11.3818\n",
      "Epoch [208/300], Step [127/172], Loss: 10.6340\n",
      "Epoch [208/300], Step [128/172], Loss: 10.2902\n",
      "Epoch [208/300], Step [129/172], Loss: 8.4285\n",
      "Epoch [208/300], Step [130/172], Loss: 12.8154\n",
      "Epoch [208/300], Step [131/172], Loss: 7.5339\n",
      "Epoch [208/300], Step [132/172], Loss: 9.2614\n",
      "Epoch [208/300], Step [133/172], Loss: 9.5912\n",
      "Epoch [208/300], Step [134/172], Loss: 11.0817\n",
      "Epoch [208/300], Step [135/172], Loss: 8.7767\n",
      "Epoch [208/300], Step [136/172], Loss: 7.6712\n",
      "Epoch [208/300], Step [137/172], Loss: 9.1997\n",
      "Epoch [208/300], Step [138/172], Loss: 7.0072\n",
      "Epoch [208/300], Step [139/172], Loss: 9.5360\n",
      "Epoch [208/300], Step [140/172], Loss: 10.1436\n",
      "Epoch [208/300], Step [141/172], Loss: 9.1319\n",
      "Epoch [208/300], Step [142/172], Loss: 14.1825\n",
      "Epoch [208/300], Step [143/172], Loss: 10.8190\n",
      "Epoch [208/300], Step [144/172], Loss: 9.0323\n",
      "Epoch [208/300], Step [145/172], Loss: 10.7624\n",
      "Epoch [208/300], Step [146/172], Loss: 9.6993\n",
      "Epoch [208/300], Step [147/172], Loss: 5.4735\n",
      "Epoch [208/300], Step [148/172], Loss: 6.3098\n",
      "Epoch [208/300], Step [149/172], Loss: 6.6949\n",
      "Epoch [208/300], Step [150/172], Loss: 5.8240\n",
      "Epoch [208/300], Step [151/172], Loss: 5.2660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [208/300], Step [152/172], Loss: 7.5991\n",
      "Epoch [208/300], Step [153/172], Loss: 6.3283\n",
      "Epoch [208/300], Step [154/172], Loss: 7.2885\n",
      "Epoch [208/300], Step [155/172], Loss: 6.0278\n",
      "Epoch [208/300], Step [156/172], Loss: 13.5025\n",
      "Epoch [208/300], Step [157/172], Loss: 8.6870\n",
      "Epoch [208/300], Step [158/172], Loss: 6.8280\n",
      "Epoch [208/300], Step [159/172], Loss: 9.6624\n",
      "Epoch [208/300], Step [160/172], Loss: 9.2760\n",
      "Epoch [208/300], Step [161/172], Loss: 7.0646\n",
      "Epoch [208/300], Step [162/172], Loss: 5.0457\n",
      "Epoch [208/300], Step [163/172], Loss: 6.3953\n",
      "Epoch [208/300], Step [164/172], Loss: 8.6559\n",
      "Epoch [208/300], Step [165/172], Loss: 6.6141\n",
      "Epoch [208/300], Step [166/172], Loss: 5.9066\n",
      "Epoch [208/300], Step [167/172], Loss: 10.2374\n",
      "Epoch [208/300], Step [168/172], Loss: 6.5321\n",
      "Epoch [208/300], Step [169/172], Loss: 6.8529\n",
      "Epoch [208/300], Step [170/172], Loss: 5.0772\n",
      "Epoch [208/300], Step [171/172], Loss: 8.1585\n",
      "Epoch [208/300], Step [172/172], Loss: 5.6110\n",
      "Epoch [209/300], Step [1/172], Loss: 48.8345\n",
      "Epoch [209/300], Step [2/172], Loss: 53.0689\n",
      "Epoch [209/300], Step [3/172], Loss: 48.7149\n",
      "Epoch [209/300], Step [4/172], Loss: 26.7012\n",
      "Epoch [209/300], Step [5/172], Loss: 42.6934\n",
      "Epoch [209/300], Step [6/172], Loss: 19.4259\n",
      "Epoch [209/300], Step [7/172], Loss: 29.1637\n",
      "Epoch [209/300], Step [8/172], Loss: 5.1199\n",
      "Epoch [209/300], Step [9/172], Loss: 30.2849\n",
      "Epoch [209/300], Step [10/172], Loss: 40.8663\n",
      "Epoch [209/300], Step [11/172], Loss: 55.6372\n",
      "Epoch [209/300], Step [12/172], Loss: 60.3835\n",
      "Epoch [209/300], Step [13/172], Loss: 34.7220\n",
      "Epoch [209/300], Step [14/172], Loss: 60.8632\n",
      "Epoch [209/300], Step [15/172], Loss: 52.3820\n",
      "Epoch [209/300], Step [16/172], Loss: 9.6135\n",
      "Epoch [209/300], Step [17/172], Loss: 42.3554\n",
      "Epoch [209/300], Step [18/172], Loss: 54.7628\n",
      "Epoch [209/300], Step [19/172], Loss: 77.7543\n",
      "Epoch [209/300], Step [20/172], Loss: 31.8205\n",
      "Epoch [209/300], Step [21/172], Loss: 84.0655\n",
      "Epoch [209/300], Step [22/172], Loss: 57.6176\n",
      "Epoch [209/300], Step [23/172], Loss: 1.7594\n",
      "Epoch [209/300], Step [24/172], Loss: 54.5658\n",
      "Epoch [209/300], Step [25/172], Loss: 39.7566\n",
      "Epoch [209/300], Step [26/172], Loss: 46.2930\n",
      "Epoch [209/300], Step [27/172], Loss: 58.9385\n",
      "Epoch [209/300], Step [28/172], Loss: 24.1377\n",
      "Epoch [209/300], Step [29/172], Loss: 16.1583\n",
      "Epoch [209/300], Step [30/172], Loss: 60.7699\n",
      "Epoch [209/300], Step [31/172], Loss: 37.0360\n",
      "Epoch [209/300], Step [32/172], Loss: 44.7103\n",
      "Epoch [209/300], Step [33/172], Loss: 71.9269\n",
      "Epoch [209/300], Step [34/172], Loss: 2.6971\n",
      "Epoch [209/300], Step [35/172], Loss: 14.6543\n",
      "Epoch [209/300], Step [36/172], Loss: 17.4324\n",
      "Epoch [209/300], Step [37/172], Loss: 17.0027\n",
      "Epoch [209/300], Step [38/172], Loss: 32.4201\n",
      "Epoch [209/300], Step [39/172], Loss: 37.4323\n",
      "Epoch [209/300], Step [40/172], Loss: 22.6190\n",
      "Epoch [209/300], Step [41/172], Loss: 35.5442\n",
      "Epoch [209/300], Step [42/172], Loss: 40.6312\n",
      "Epoch [209/300], Step [43/172], Loss: 29.0489\n",
      "Epoch [209/300], Step [44/172], Loss: 22.2728\n",
      "Epoch [209/300], Step [45/172], Loss: 30.8358\n",
      "Epoch [209/300], Step [46/172], Loss: 17.7415\n",
      "Epoch [209/300], Step [47/172], Loss: 50.7949\n",
      "Epoch [209/300], Step [48/172], Loss: 60.3043\n",
      "Epoch [209/300], Step [49/172], Loss: 23.1867\n",
      "Epoch [209/300], Step [50/172], Loss: 45.7792\n",
      "Epoch [209/300], Step [51/172], Loss: 9.6786\n",
      "Epoch [209/300], Step [52/172], Loss: 21.3623\n",
      "Epoch [209/300], Step [53/172], Loss: 23.5769\n",
      "Epoch [209/300], Step [54/172], Loss: 15.7742\n",
      "Epoch [209/300], Step [55/172], Loss: 15.4419\n",
      "Epoch [209/300], Step [56/172], Loss: 20.0440\n",
      "Epoch [209/300], Step [57/172], Loss: 16.8810\n",
      "Epoch [209/300], Step [58/172], Loss: 13.1920\n",
      "Epoch [209/300], Step [59/172], Loss: 26.1963\n",
      "Epoch [209/300], Step [60/172], Loss: 20.8570\n",
      "Epoch [209/300], Step [61/172], Loss: 6.0975\n",
      "Epoch [209/300], Step [62/172], Loss: 17.7614\n",
      "Epoch [209/300], Step [63/172], Loss: 10.9155\n",
      "Epoch [209/300], Step [64/172], Loss: 11.1647\n",
      "Epoch [209/300], Step [65/172], Loss: 17.9133\n",
      "Epoch [209/300], Step [66/172], Loss: 6.5773\n",
      "Epoch [209/300], Step [67/172], Loss: 21.5696\n",
      "Epoch [209/300], Step [68/172], Loss: 4.2621\n",
      "Epoch [209/300], Step [69/172], Loss: 31.8543\n",
      "Epoch [209/300], Step [70/172], Loss: 34.6483\n",
      "Epoch [209/300], Step [71/172], Loss: 37.3401\n",
      "Epoch [209/300], Step [72/172], Loss: 36.1271\n",
      "Epoch [209/300], Step [73/172], Loss: 45.7260\n",
      "Epoch [209/300], Step [74/172], Loss: 23.3913\n",
      "Epoch [209/300], Step [75/172], Loss: 23.4629\n",
      "Epoch [209/300], Step [76/172], Loss: 26.4951\n",
      "Epoch [209/300], Step [77/172], Loss: 44.1633\n",
      "Epoch [209/300], Step [78/172], Loss: 34.2306\n",
      "Epoch [209/300], Step [79/172], Loss: 33.3912\n",
      "Epoch [209/300], Step [80/172], Loss: 47.0254\n",
      "Epoch [209/300], Step [81/172], Loss: 31.0339\n",
      "Epoch [209/300], Step [82/172], Loss: 34.0068\n",
      "Epoch [209/300], Step [83/172], Loss: 41.2774\n",
      "Epoch [209/300], Step [84/172], Loss: 31.4857\n",
      "Epoch [209/300], Step [85/172], Loss: 37.5380\n",
      "Epoch [209/300], Step [86/172], Loss: 31.8721\n",
      "Epoch [209/300], Step [87/172], Loss: 24.4089\n",
      "Epoch [209/300], Step [88/172], Loss: 21.9949\n",
      "Epoch [209/300], Step [89/172], Loss: 26.9194\n",
      "Epoch [209/300], Step [90/172], Loss: 19.7378\n",
      "Epoch [209/300], Step [91/172], Loss: 25.3589\n",
      "Epoch [209/300], Step [92/172], Loss: 18.8542\n",
      "Epoch [209/300], Step [93/172], Loss: 19.5052\n",
      "Epoch [209/300], Step [94/172], Loss: 27.1029\n",
      "Epoch [209/300], Step [95/172], Loss: 20.0854\n",
      "Epoch [209/300], Step [96/172], Loss: 19.9844\n",
      "Epoch [209/300], Step [97/172], Loss: 28.1499\n",
      "Epoch [209/300], Step [98/172], Loss: 19.5177\n",
      "Epoch [209/300], Step [99/172], Loss: 19.1442\n",
      "Epoch [209/300], Step [100/172], Loss: 16.8017\n",
      "Epoch [209/300], Step [101/172], Loss: 19.3894\n",
      "Epoch [209/300], Step [102/172], Loss: 16.9282\n",
      "Epoch [209/300], Step [103/172], Loss: 13.2487\n",
      "Epoch [209/300], Step [104/172], Loss: 19.3034\n",
      "Epoch [209/300], Step [105/172], Loss: 21.4368\n",
      "Epoch [209/300], Step [106/172], Loss: 16.2372\n",
      "Epoch [209/300], Step [107/172], Loss: 16.2096\n",
      "Epoch [209/300], Step [108/172], Loss: 15.1629\n",
      "Epoch [209/300], Step [109/172], Loss: 14.8542\n",
      "Epoch [209/300], Step [110/172], Loss: 17.0855\n",
      "Epoch [209/300], Step [111/172], Loss: 17.4437\n",
      "Epoch [209/300], Step [112/172], Loss: 16.9183\n",
      "Epoch [209/300], Step [113/172], Loss: 12.6680\n",
      "Epoch [209/300], Step [114/172], Loss: 14.2727\n",
      "Epoch [209/300], Step [115/172], Loss: 18.6929\n",
      "Epoch [209/300], Step [116/172], Loss: 14.8220\n",
      "Epoch [209/300], Step [117/172], Loss: 12.3865\n",
      "Epoch [209/300], Step [118/172], Loss: 13.3656\n",
      "Epoch [209/300], Step [119/172], Loss: 16.1436\n",
      "Epoch [209/300], Step [120/172], Loss: 10.8084\n",
      "Epoch [209/300], Step [121/172], Loss: 10.1426\n",
      "Epoch [209/300], Step [122/172], Loss: 11.8947\n",
      "Epoch [209/300], Step [123/172], Loss: 10.2437\n",
      "Epoch [209/300], Step [124/172], Loss: 7.5445\n",
      "Epoch [209/300], Step [125/172], Loss: 12.0380\n",
      "Epoch [209/300], Step [126/172], Loss: 11.4786\n",
      "Epoch [209/300], Step [127/172], Loss: 10.7212\n",
      "Epoch [209/300], Step [128/172], Loss: 10.4027\n",
      "Epoch [209/300], Step [129/172], Loss: 8.5964\n",
      "Epoch [209/300], Step [130/172], Loss: 12.8354\n",
      "Epoch [209/300], Step [131/172], Loss: 7.6468\n",
      "Epoch [209/300], Step [132/172], Loss: 9.3950\n",
      "Epoch [209/300], Step [133/172], Loss: 9.7402\n",
      "Epoch [209/300], Step [134/172], Loss: 11.1478\n",
      "Epoch [209/300], Step [135/172], Loss: 8.9040\n",
      "Epoch [209/300], Step [136/172], Loss: 7.6694\n",
      "Epoch [209/300], Step [137/172], Loss: 9.3474\n",
      "Epoch [209/300], Step [138/172], Loss: 7.1704\n",
      "Epoch [209/300], Step [139/172], Loss: 9.6274\n",
      "Epoch [209/300], Step [140/172], Loss: 10.2373\n",
      "Epoch [209/300], Step [141/172], Loss: 9.1932\n",
      "Epoch [209/300], Step [142/172], Loss: 14.1630\n",
      "Epoch [209/300], Step [143/172], Loss: 10.8683\n",
      "Epoch [209/300], Step [144/172], Loss: 9.1436\n",
      "Epoch [209/300], Step [145/172], Loss: 10.8416\n",
      "Epoch [209/300], Step [146/172], Loss: 9.8475\n",
      "Epoch [209/300], Step [147/172], Loss: 5.5938\n",
      "Epoch [209/300], Step [148/172], Loss: 6.4394\n",
      "Epoch [209/300], Step [149/172], Loss: 6.8215\n",
      "Epoch [209/300], Step [150/172], Loss: 5.8690\n",
      "Epoch [209/300], Step [151/172], Loss: 5.3959\n",
      "Epoch [209/300], Step [152/172], Loss: 7.7105\n",
      "Epoch [209/300], Step [153/172], Loss: 6.4336\n",
      "Epoch [209/300], Step [154/172], Loss: 7.3368\n",
      "Epoch [209/300], Step [155/172], Loss: 6.2271\n",
      "Epoch [209/300], Step [156/172], Loss: 13.5029\n",
      "Epoch [209/300], Step [157/172], Loss: 8.6661\n",
      "Epoch [209/300], Step [158/172], Loss: 6.8815\n",
      "Epoch [209/300], Step [159/172], Loss: 9.4545\n",
      "Epoch [209/300], Step [160/172], Loss: 9.2221\n",
      "Epoch [209/300], Step [161/172], Loss: 7.1873\n",
      "Epoch [209/300], Step [162/172], Loss: 5.0996\n",
      "Epoch [209/300], Step [163/172], Loss: 6.3804\n",
      "Epoch [209/300], Step [164/172], Loss: 8.7687\n",
      "Epoch [209/300], Step [165/172], Loss: 6.7437\n",
      "Epoch [209/300], Step [166/172], Loss: 6.0417\n",
      "Epoch [209/300], Step [167/172], Loss: 10.0538\n",
      "Epoch [209/300], Step [168/172], Loss: 6.6531\n",
      "Epoch [209/300], Step [169/172], Loss: 6.9852\n",
      "Epoch [209/300], Step [170/172], Loss: 5.2161\n",
      "Epoch [209/300], Step [171/172], Loss: 7.9819\n",
      "Epoch [209/300], Step [172/172], Loss: 5.7371\n",
      "Epoch [210/300], Step [1/172], Loss: 48.8030\n",
      "Epoch [210/300], Step [2/172], Loss: 53.4398\n",
      "Epoch [210/300], Step [3/172], Loss: 50.8653\n",
      "Epoch [210/300], Step [4/172], Loss: 26.6055\n",
      "Epoch [210/300], Step [5/172], Loss: 44.6865\n",
      "Epoch [210/300], Step [6/172], Loss: 19.5585\n",
      "Epoch [210/300], Step [7/172], Loss: 29.1646\n",
      "Epoch [210/300], Step [8/172], Loss: 4.5496\n",
      "Epoch [210/300], Step [9/172], Loss: 30.0710\n",
      "Epoch [210/300], Step [10/172], Loss: 40.8574\n",
      "Epoch [210/300], Step [11/172], Loss: 54.9025\n",
      "Epoch [210/300], Step [12/172], Loss: 60.1603\n",
      "Epoch [210/300], Step [13/172], Loss: 34.8432\n",
      "Epoch [210/300], Step [14/172], Loss: 61.1508\n",
      "Epoch [210/300], Step [15/172], Loss: 52.4577\n",
      "Epoch [210/300], Step [16/172], Loss: 10.4771\n",
      "Epoch [210/300], Step [17/172], Loss: 42.3049\n",
      "Epoch [210/300], Step [18/172], Loss: 54.6224\n",
      "Epoch [210/300], Step [19/172], Loss: 78.0005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/300], Step [20/172], Loss: 30.9227\n",
      "Epoch [210/300], Step [21/172], Loss: 84.6364\n",
      "Epoch [210/300], Step [22/172], Loss: 57.1014\n",
      "Epoch [210/300], Step [23/172], Loss: 1.8825\n",
      "Epoch [210/300], Step [24/172], Loss: 54.7378\n",
      "Epoch [210/300], Step [25/172], Loss: 39.9897\n",
      "Epoch [210/300], Step [26/172], Loss: 46.5016\n",
      "Epoch [210/300], Step [27/172], Loss: 58.9483\n",
      "Epoch [210/300], Step [28/172], Loss: 24.2030\n",
      "Epoch [210/300], Step [29/172], Loss: 16.2462\n",
      "Epoch [210/300], Step [30/172], Loss: 60.6769\n",
      "Epoch [210/300], Step [31/172], Loss: 36.9236\n",
      "Epoch [210/300], Step [32/172], Loss: 44.7262\n",
      "Epoch [210/300], Step [33/172], Loss: 71.9495\n",
      "Epoch [210/300], Step [34/172], Loss: 2.7612\n",
      "Epoch [210/300], Step [35/172], Loss: 14.6435\n",
      "Epoch [210/300], Step [36/172], Loss: 17.2457\n",
      "Epoch [210/300], Step [37/172], Loss: 17.1611\n",
      "Epoch [210/300], Step [38/172], Loss: 32.7365\n",
      "Epoch [210/300], Step [39/172], Loss: 37.7550\n",
      "Epoch [210/300], Step [40/172], Loss: 23.1324\n",
      "Epoch [210/300], Step [41/172], Loss: 36.0164\n",
      "Epoch [210/300], Step [42/172], Loss: 41.2015\n",
      "Epoch [210/300], Step [43/172], Loss: 29.5339\n",
      "Epoch [210/300], Step [44/172], Loss: 22.3990\n",
      "Epoch [210/300], Step [45/172], Loss: 31.3250\n",
      "Epoch [210/300], Step [46/172], Loss: 17.8660\n",
      "Epoch [210/300], Step [47/172], Loss: 51.2938\n",
      "Epoch [210/300], Step [48/172], Loss: 60.1494\n",
      "Epoch [210/300], Step [49/172], Loss: 23.6348\n",
      "Epoch [210/300], Step [50/172], Loss: 45.4869\n",
      "Epoch [210/300], Step [51/172], Loss: 9.9327\n",
      "Epoch [210/300], Step [52/172], Loss: 21.8365\n",
      "Epoch [210/300], Step [53/172], Loss: 23.9820\n",
      "Epoch [210/300], Step [54/172], Loss: 16.1971\n",
      "Epoch [210/300], Step [55/172], Loss: 15.7575\n",
      "Epoch [210/300], Step [56/172], Loss: 20.1566\n",
      "Epoch [210/300], Step [57/172], Loss: 16.9087\n",
      "Epoch [210/300], Step [58/172], Loss: 13.3616\n",
      "Epoch [210/300], Step [59/172], Loss: 25.7459\n",
      "Epoch [210/300], Step [60/172], Loss: 20.6020\n",
      "Epoch [210/300], Step [61/172], Loss: 6.1225\n",
      "Epoch [210/300], Step [62/172], Loss: 17.4384\n",
      "Epoch [210/300], Step [63/172], Loss: 10.9337\n",
      "Epoch [210/300], Step [64/172], Loss: 11.4059\n",
      "Epoch [210/300], Step [65/172], Loss: 17.9757\n",
      "Epoch [210/300], Step [66/172], Loss: 6.6925\n",
      "Epoch [210/300], Step [67/172], Loss: 21.5985\n",
      "Epoch [210/300], Step [68/172], Loss: 4.4951\n",
      "Epoch [210/300], Step [69/172], Loss: 31.7618\n",
      "Epoch [210/300], Step [70/172], Loss: 34.2123\n",
      "Epoch [210/300], Step [71/172], Loss: 36.8961\n",
      "Epoch [210/300], Step [72/172], Loss: 35.6571\n",
      "Epoch [210/300], Step [73/172], Loss: 45.3404\n",
      "Epoch [210/300], Step [74/172], Loss: 23.0736\n",
      "Epoch [210/300], Step [75/172], Loss: 22.9081\n",
      "Epoch [210/300], Step [76/172], Loss: 26.2766\n",
      "Epoch [210/300], Step [77/172], Loss: 43.7868\n",
      "Epoch [210/300], Step [78/172], Loss: 33.8150\n",
      "Epoch [210/300], Step [79/172], Loss: 32.9658\n",
      "Epoch [210/300], Step [80/172], Loss: 46.6577\n",
      "Epoch [210/300], Step [81/172], Loss: 30.7147\n",
      "Epoch [210/300], Step [82/172], Loss: 33.5686\n",
      "Epoch [210/300], Step [83/172], Loss: 40.9750\n",
      "Epoch [210/300], Step [84/172], Loss: 31.4008\n",
      "Epoch [210/300], Step [85/172], Loss: 37.3294\n",
      "Epoch [210/300], Step [86/172], Loss: 31.7428\n",
      "Epoch [210/300], Step [87/172], Loss: 24.1942\n",
      "Epoch [210/300], Step [88/172], Loss: 21.6651\n",
      "Epoch [210/300], Step [89/172], Loss: 26.9706\n",
      "Epoch [210/300], Step [90/172], Loss: 19.4917\n",
      "Epoch [210/300], Step [91/172], Loss: 25.1868\n",
      "Epoch [210/300], Step [92/172], Loss: 18.6499\n",
      "Epoch [210/300], Step [93/172], Loss: 19.3020\n",
      "Epoch [210/300], Step [94/172], Loss: 26.8499\n",
      "Epoch [210/300], Step [95/172], Loss: 19.8277\n",
      "Epoch [210/300], Step [96/172], Loss: 19.9150\n",
      "Epoch [210/300], Step [97/172], Loss: 27.9681\n",
      "Epoch [210/300], Step [98/172], Loss: 19.3903\n",
      "Epoch [210/300], Step [99/172], Loss: 18.9628\n",
      "Epoch [210/300], Step [100/172], Loss: 16.7986\n",
      "Epoch [210/300], Step [101/172], Loss: 19.2938\n",
      "Epoch [210/300], Step [102/172], Loss: 16.7701\n",
      "Epoch [210/300], Step [103/172], Loss: 13.1761\n",
      "Epoch [210/300], Step [104/172], Loss: 19.1959\n",
      "Epoch [210/300], Step [105/172], Loss: 21.4102\n",
      "Epoch [210/300], Step [106/172], Loss: 16.2978\n",
      "Epoch [210/300], Step [107/172], Loss: 16.2913\n",
      "Epoch [210/300], Step [108/172], Loss: 15.1934\n",
      "Epoch [210/300], Step [109/172], Loss: 14.8779\n",
      "Epoch [210/300], Step [110/172], Loss: 16.9949\n",
      "Epoch [210/300], Step [111/172], Loss: 17.5704\n",
      "Epoch [210/300], Step [112/172], Loss: 16.8860\n",
      "Epoch [210/300], Step [113/172], Loss: 12.7176\n",
      "Epoch [210/300], Step [114/172], Loss: 14.2899\n",
      "Epoch [210/300], Step [115/172], Loss: 18.7122\n",
      "Epoch [210/300], Step [116/172], Loss: 14.8542\n",
      "Epoch [210/300], Step [117/172], Loss: 12.4890\n",
      "Epoch [210/300], Step [118/172], Loss: 13.4573\n",
      "Epoch [210/300], Step [119/172], Loss: 16.2637\n",
      "Epoch [210/300], Step [120/172], Loss: 10.8768\n",
      "Epoch [210/300], Step [121/172], Loss: 10.2547\n",
      "Epoch [210/300], Step [122/172], Loss: 12.0420\n",
      "Epoch [210/300], Step [123/172], Loss: 10.4740\n",
      "Epoch [210/300], Step [124/172], Loss: 7.6142\n",
      "Epoch [210/300], Step [125/172], Loss: 12.1244\n",
      "Epoch [210/300], Step [126/172], Loss: 11.3885\n",
      "Epoch [210/300], Step [127/172], Loss: 10.7181\n",
      "Epoch [210/300], Step [128/172], Loss: 10.5210\n",
      "Epoch [210/300], Step [129/172], Loss: 8.6713\n",
      "Epoch [210/300], Step [130/172], Loss: 12.9257\n",
      "Epoch [210/300], Step [131/172], Loss: 7.7310\n",
      "Epoch [210/300], Step [132/172], Loss: 9.4616\n",
      "Epoch [210/300], Step [133/172], Loss: 9.8567\n",
      "Epoch [210/300], Step [134/172], Loss: 11.3221\n",
      "Epoch [210/300], Step [135/172], Loss: 8.9549\n",
      "Epoch [210/300], Step [136/172], Loss: 7.8237\n",
      "Epoch [210/300], Step [137/172], Loss: 9.4130\n",
      "Epoch [210/300], Step [138/172], Loss: 7.1796\n",
      "Epoch [210/300], Step [139/172], Loss: 9.7372\n",
      "Epoch [210/300], Step [140/172], Loss: 10.2441\n",
      "Epoch [210/300], Step [141/172], Loss: 9.2397\n",
      "Epoch [210/300], Step [142/172], Loss: 14.2292\n",
      "Epoch [210/300], Step [143/172], Loss: 10.9295\n",
      "Epoch [210/300], Step [144/172], Loss: 9.1156\n",
      "Epoch [210/300], Step [145/172], Loss: 10.9179\n",
      "Epoch [210/300], Step [146/172], Loss: 9.7658\n",
      "Epoch [210/300], Step [147/172], Loss: 5.6285\n",
      "Epoch [210/300], Step [148/172], Loss: 6.4505\n",
      "Epoch [210/300], Step [149/172], Loss: 6.8824\n",
      "Epoch [210/300], Step [150/172], Loss: 5.9780\n",
      "Epoch [210/300], Step [151/172], Loss: 5.3858\n",
      "Epoch [210/300], Step [152/172], Loss: 7.8246\n",
      "Epoch [210/300], Step [153/172], Loss: 6.4959\n",
      "Epoch [210/300], Step [154/172], Loss: 7.3940\n",
      "Epoch [210/300], Step [155/172], Loss: 6.2411\n",
      "Epoch [210/300], Step [156/172], Loss: 13.4840\n",
      "Epoch [210/300], Step [157/172], Loss: 8.7745\n",
      "Epoch [210/300], Step [158/172], Loss: 6.8951\n",
      "Epoch [210/300], Step [159/172], Loss: 9.5972\n",
      "Epoch [210/300], Step [160/172], Loss: 9.1827\n",
      "Epoch [210/300], Step [161/172], Loss: 7.2832\n",
      "Epoch [210/300], Step [162/172], Loss: 5.1207\n",
      "Epoch [210/300], Step [163/172], Loss: 6.2661\n",
      "Epoch [210/300], Step [164/172], Loss: 8.7581\n",
      "Epoch [210/300], Step [165/172], Loss: 6.7030\n",
      "Epoch [210/300], Step [166/172], Loss: 6.1050\n",
      "Epoch [210/300], Step [167/172], Loss: 10.1402\n",
      "Epoch [210/300], Step [168/172], Loss: 6.7603\n",
      "Epoch [210/300], Step [169/172], Loss: 7.0762\n",
      "Epoch [210/300], Step [170/172], Loss: 5.2214\n",
      "Epoch [210/300], Step [171/172], Loss: 7.8545\n",
      "Epoch [210/300], Step [172/172], Loss: 5.9012\n",
      "Epoch [211/300], Step [1/172], Loss: 48.8691\n",
      "Epoch [211/300], Step [2/172], Loss: 52.2802\n",
      "Epoch [211/300], Step [3/172], Loss: 49.7283\n",
      "Epoch [211/300], Step [4/172], Loss: 26.6150\n",
      "Epoch [211/300], Step [5/172], Loss: 42.7487\n",
      "Epoch [211/300], Step [6/172], Loss: 19.5002\n",
      "Epoch [211/300], Step [7/172], Loss: 26.8342\n",
      "Epoch [211/300], Step [8/172], Loss: 5.1308\n",
      "Epoch [211/300], Step [9/172], Loss: 29.7773\n",
      "Epoch [211/300], Step [10/172], Loss: 40.6416\n",
      "Epoch [211/300], Step [11/172], Loss: 55.8472\n",
      "Epoch [211/300], Step [12/172], Loss: 60.2067\n",
      "Epoch [211/300], Step [13/172], Loss: 34.9012\n",
      "Epoch [211/300], Step [14/172], Loss: 61.7628\n",
      "Epoch [211/300], Step [15/172], Loss: 52.8502\n",
      "Epoch [211/300], Step [16/172], Loss: 9.0093\n",
      "Epoch [211/300], Step [17/172], Loss: 42.0322\n",
      "Epoch [211/300], Step [18/172], Loss: 54.7289\n",
      "Epoch [211/300], Step [19/172], Loss: 75.9482\n",
      "Epoch [211/300], Step [20/172], Loss: 34.8252\n",
      "Epoch [211/300], Step [21/172], Loss: 81.9377\n",
      "Epoch [211/300], Step [22/172], Loss: 58.6418\n",
      "Epoch [211/300], Step [23/172], Loss: 1.2900\n",
      "Epoch [211/300], Step [24/172], Loss: 54.6908\n",
      "Epoch [211/300], Step [25/172], Loss: 39.1043\n",
      "Epoch [211/300], Step [26/172], Loss: 46.5843\n",
      "Epoch [211/300], Step [27/172], Loss: 59.4120\n",
      "Epoch [211/300], Step [28/172], Loss: 24.8341\n",
      "Epoch [211/300], Step [29/172], Loss: 17.8266\n",
      "Epoch [211/300], Step [30/172], Loss: 60.5425\n",
      "Epoch [211/300], Step [31/172], Loss: 36.5010\n",
      "Epoch [211/300], Step [32/172], Loss: 44.9726\n",
      "Epoch [211/300], Step [33/172], Loss: 73.5190\n",
      "Epoch [211/300], Step [34/172], Loss: 3.3970\n",
      "Epoch [211/300], Step [35/172], Loss: 14.9484\n",
      "Epoch [211/300], Step [36/172], Loss: 18.6147\n",
      "Epoch [211/300], Step [37/172], Loss: 17.5265\n",
      "Epoch [211/300], Step [38/172], Loss: 33.5298\n",
      "Epoch [211/300], Step [39/172], Loss: 39.1436\n",
      "Epoch [211/300], Step [40/172], Loss: 23.7556\n",
      "Epoch [211/300], Step [41/172], Loss: 36.3155\n",
      "Epoch [211/300], Step [42/172], Loss: 41.5381\n",
      "Epoch [211/300], Step [43/172], Loss: 30.3722\n",
      "Epoch [211/300], Step [44/172], Loss: 23.0262\n",
      "Epoch [211/300], Step [45/172], Loss: 32.2034\n",
      "Epoch [211/300], Step [46/172], Loss: 19.1650\n",
      "Epoch [211/300], Step [47/172], Loss: 52.9086\n",
      "Epoch [211/300], Step [48/172], Loss: 62.1125\n",
      "Epoch [211/300], Step [49/172], Loss: 25.4656\n",
      "Epoch [211/300], Step [50/172], Loss: 45.6378\n",
      "Epoch [211/300], Step [51/172], Loss: 10.4926\n",
      "Epoch [211/300], Step [52/172], Loss: 23.0336\n",
      "Epoch [211/300], Step [53/172], Loss: 25.7178\n",
      "Epoch [211/300], Step [54/172], Loss: 16.8576\n",
      "Epoch [211/300], Step [55/172], Loss: 16.2432\n",
      "Epoch [211/300], Step [56/172], Loss: 21.2541\n",
      "Epoch [211/300], Step [57/172], Loss: 18.3788\n",
      "Epoch [211/300], Step [58/172], Loss: 13.6454\n",
      "Epoch [211/300], Step [59/172], Loss: 24.8136\n",
      "Epoch [211/300], Step [60/172], Loss: 20.4493\n",
      "Epoch [211/300], Step [61/172], Loss: 6.2949\n",
      "Epoch [211/300], Step [62/172], Loss: 17.4336\n",
      "Epoch [211/300], Step [63/172], Loss: 11.4609\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [211/300], Step [64/172], Loss: 12.0320\n",
      "Epoch [211/300], Step [65/172], Loss: 17.7124\n",
      "Epoch [211/300], Step [66/172], Loss: 6.6726\n",
      "Epoch [211/300], Step [67/172], Loss: 21.1867\n",
      "Epoch [211/300], Step [68/172], Loss: 4.7551\n",
      "Epoch [211/300], Step [69/172], Loss: 30.8985\n",
      "Epoch [211/300], Step [70/172], Loss: 34.7601\n",
      "Epoch [211/300], Step [71/172], Loss: 36.7465\n",
      "Epoch [211/300], Step [72/172], Loss: 35.2636\n",
      "Epoch [211/300], Step [73/172], Loss: 45.6114\n",
      "Epoch [211/300], Step [74/172], Loss: 23.5544\n",
      "Epoch [211/300], Step [75/172], Loss: 23.0868\n",
      "Epoch [211/300], Step [76/172], Loss: 26.9206\n",
      "Epoch [211/300], Step [77/172], Loss: 43.6836\n",
      "Epoch [211/300], Step [78/172], Loss: 33.8158\n",
      "Epoch [211/300], Step [79/172], Loss: 32.8748\n",
      "Epoch [211/300], Step [80/172], Loss: 46.0737\n",
      "Epoch [211/300], Step [81/172], Loss: 30.5634\n",
      "Epoch [211/300], Step [82/172], Loss: 33.4839\n",
      "Epoch [211/300], Step [83/172], Loss: 40.9632\n",
      "Epoch [211/300], Step [84/172], Loss: 31.1461\n",
      "Epoch [211/300], Step [85/172], Loss: 36.8746\n",
      "Epoch [211/300], Step [86/172], Loss: 31.6241\n",
      "Epoch [211/300], Step [87/172], Loss: 24.2254\n",
      "Epoch [211/300], Step [88/172], Loss: 21.8419\n",
      "Epoch [211/300], Step [89/172], Loss: 26.8586\n",
      "Epoch [211/300], Step [90/172], Loss: 19.8157\n",
      "Epoch [211/300], Step [91/172], Loss: 25.0178\n",
      "Epoch [211/300], Step [92/172], Loss: 18.7012\n",
      "Epoch [211/300], Step [93/172], Loss: 19.5262\n",
      "Epoch [211/300], Step [94/172], Loss: 26.7110\n",
      "Epoch [211/300], Step [95/172], Loss: 20.3679\n",
      "Epoch [211/300], Step [96/172], Loss: 20.0783\n",
      "Epoch [211/300], Step [97/172], Loss: 27.6770\n",
      "Epoch [211/300], Step [98/172], Loss: 19.2607\n",
      "Epoch [211/300], Step [99/172], Loss: 18.9622\n",
      "Epoch [211/300], Step [100/172], Loss: 16.7087\n",
      "Epoch [211/300], Step [101/172], Loss: 19.2692\n",
      "Epoch [211/300], Step [102/172], Loss: 16.3488\n",
      "Epoch [211/300], Step [103/172], Loss: 13.2448\n",
      "Epoch [211/300], Step [104/172], Loss: 19.2204\n",
      "Epoch [211/300], Step [105/172], Loss: 21.2921\n",
      "Epoch [211/300], Step [106/172], Loss: 16.3129\n",
      "Epoch [211/300], Step [107/172], Loss: 15.9982\n",
      "Epoch [211/300], Step [108/172], Loss: 14.9290\n",
      "Epoch [211/300], Step [109/172], Loss: 14.6920\n",
      "Epoch [211/300], Step [110/172], Loss: 16.9570\n",
      "Epoch [211/300], Step [111/172], Loss: 17.6382\n",
      "Epoch [211/300], Step [112/172], Loss: 16.9910\n",
      "Epoch [211/300], Step [113/172], Loss: 13.0297\n",
      "Epoch [211/300], Step [114/172], Loss: 14.5311\n",
      "Epoch [211/300], Step [115/172], Loss: 18.7426\n",
      "Epoch [211/300], Step [116/172], Loss: 15.2909\n",
      "Epoch [211/300], Step [117/172], Loss: 12.6403\n",
      "Epoch [211/300], Step [118/172], Loss: 13.3143\n",
      "Epoch [211/300], Step [119/172], Loss: 16.1855\n",
      "Epoch [211/300], Step [120/172], Loss: 10.9264\n",
      "Epoch [211/300], Step [121/172], Loss: 10.4444\n",
      "Epoch [211/300], Step [122/172], Loss: 11.3912\n",
      "Epoch [211/300], Step [123/172], Loss: 10.4987\n",
      "Epoch [211/300], Step [124/172], Loss: 7.9097\n",
      "Epoch [211/300], Step [125/172], Loss: 12.3936\n",
      "Epoch [211/300], Step [126/172], Loss: 12.0773\n",
      "Epoch [211/300], Step [127/172], Loss: 11.0800\n",
      "Epoch [211/300], Step [128/172], Loss: 11.2634\n",
      "Epoch [211/300], Step [129/172], Loss: 9.0064\n",
      "Epoch [211/300], Step [130/172], Loss: 13.0991\n",
      "Epoch [211/300], Step [131/172], Loss: 7.8183\n",
      "Epoch [211/300], Step [132/172], Loss: 9.9518\n",
      "Epoch [211/300], Step [133/172], Loss: 9.7049\n",
      "Epoch [211/300], Step [134/172], Loss: 10.9416\n",
      "Epoch [211/300], Step [135/172], Loss: 9.1103\n",
      "Epoch [211/300], Step [136/172], Loss: 8.1062\n",
      "Epoch [211/300], Step [137/172], Loss: 9.5274\n",
      "Epoch [211/300], Step [138/172], Loss: 7.6067\n",
      "Epoch [211/300], Step [139/172], Loss: 9.9900\n",
      "Epoch [211/300], Step [140/172], Loss: 10.6739\n",
      "Epoch [211/300], Step [141/172], Loss: 9.6064\n",
      "Epoch [211/300], Step [142/172], Loss: 14.0633\n",
      "Epoch [211/300], Step [143/172], Loss: 11.0663\n",
      "Epoch [211/300], Step [144/172], Loss: 9.3141\n",
      "Epoch [211/300], Step [145/172], Loss: 11.1013\n",
      "Epoch [211/300], Step [146/172], Loss: 10.0536\n",
      "Epoch [211/300], Step [147/172], Loss: 5.8133\n",
      "Epoch [211/300], Step [148/172], Loss: 6.6909\n",
      "Epoch [211/300], Step [149/172], Loss: 7.0917\n",
      "Epoch [211/300], Step [150/172], Loss: 6.1670\n",
      "Epoch [211/300], Step [151/172], Loss: 5.6315\n",
      "Epoch [211/300], Step [152/172], Loss: 7.9727\n",
      "Epoch [211/300], Step [153/172], Loss: 6.6419\n",
      "Epoch [211/300], Step [154/172], Loss: 7.4609\n",
      "Epoch [211/300], Step [155/172], Loss: 6.3981\n",
      "Epoch [211/300], Step [156/172], Loss: 13.5928\n",
      "Epoch [211/300], Step [157/172], Loss: 8.5421\n",
      "Epoch [211/300], Step [158/172], Loss: 6.9164\n",
      "Epoch [211/300], Step [159/172], Loss: 9.6423\n",
      "Epoch [211/300], Step [160/172], Loss: 9.0151\n",
      "Epoch [211/300], Step [161/172], Loss: 7.5994\n",
      "Epoch [211/300], Step [162/172], Loss: 5.1560\n",
      "Epoch [211/300], Step [163/172], Loss: 6.5951\n",
      "Epoch [211/300], Step [164/172], Loss: 9.0181\n",
      "Epoch [211/300], Step [165/172], Loss: 6.9604\n",
      "Epoch [211/300], Step [166/172], Loss: 6.2848\n",
      "Epoch [211/300], Step [167/172], Loss: 10.4425\n",
      "Epoch [211/300], Step [168/172], Loss: 6.7942\n",
      "Epoch [211/300], Step [169/172], Loss: 7.1195\n",
      "Epoch [211/300], Step [170/172], Loss: 5.4025\n",
      "Epoch [211/300], Step [171/172], Loss: 8.5589\n",
      "Epoch [211/300], Step [172/172], Loss: 6.1272\n",
      "Epoch [212/300], Step [1/172], Loss: 48.0499\n",
      "Epoch [212/300], Step [2/172], Loss: 54.2821\n",
      "Epoch [212/300], Step [3/172], Loss: 49.4034\n",
      "Epoch [212/300], Step [4/172], Loss: 26.7976\n",
      "Epoch [212/300], Step [5/172], Loss: 42.1949\n",
      "Epoch [212/300], Step [6/172], Loss: 20.8990\n",
      "Epoch [212/300], Step [7/172], Loss: 31.4064\n",
      "Epoch [212/300], Step [8/172], Loss: 4.5521\n",
      "Epoch [212/300], Step [9/172], Loss: 29.5932\n",
      "Epoch [212/300], Step [10/172], Loss: 41.1531\n",
      "Epoch [212/300], Step [11/172], Loss: 54.8859\n",
      "Epoch [212/300], Step [12/172], Loss: 59.2830\n",
      "Epoch [212/300], Step [13/172], Loss: 35.0086\n",
      "Epoch [212/300], Step [14/172], Loss: 60.6508\n",
      "Epoch [212/300], Step [15/172], Loss: 51.4334\n",
      "Epoch [212/300], Step [16/172], Loss: 10.8312\n",
      "Epoch [212/300], Step [17/172], Loss: 41.7859\n",
      "Epoch [212/300], Step [18/172], Loss: 53.8176\n",
      "Epoch [212/300], Step [19/172], Loss: 75.9721\n",
      "Epoch [212/300], Step [20/172], Loss: 31.6498\n",
      "Epoch [212/300], Step [21/172], Loss: 83.4518\n",
      "Epoch [212/300], Step [22/172], Loss: 58.0499\n",
      "Epoch [212/300], Step [23/172], Loss: 1.8609\n",
      "Epoch [212/300], Step [24/172], Loss: 54.3553\n",
      "Epoch [212/300], Step [25/172], Loss: 39.6012\n",
      "Epoch [212/300], Step [26/172], Loss: 46.1415\n",
      "Epoch [212/300], Step [27/172], Loss: 59.0718\n",
      "Epoch [212/300], Step [28/172], Loss: 24.0625\n",
      "Epoch [212/300], Step [29/172], Loss: 16.0191\n",
      "Epoch [212/300], Step [30/172], Loss: 60.1047\n",
      "Epoch [212/300], Step [31/172], Loss: 36.6837\n",
      "Epoch [212/300], Step [32/172], Loss: 44.7475\n",
      "Epoch [212/300], Step [33/172], Loss: 72.4741\n",
      "Epoch [212/300], Step [34/172], Loss: 2.8504\n",
      "Epoch [212/300], Step [35/172], Loss: 14.7876\n",
      "Epoch [212/300], Step [36/172], Loss: 16.9546\n",
      "Epoch [212/300], Step [37/172], Loss: 17.1507\n",
      "Epoch [212/300], Step [38/172], Loss: 33.1902\n",
      "Epoch [212/300], Step [39/172], Loss: 37.7956\n",
      "Epoch [212/300], Step [40/172], Loss: 23.5367\n",
      "Epoch [212/300], Step [41/172], Loss: 36.5980\n",
      "Epoch [212/300], Step [42/172], Loss: 41.5504\n",
      "Epoch [212/300], Step [43/172], Loss: 30.1358\n",
      "Epoch [212/300], Step [44/172], Loss: 22.2208\n",
      "Epoch [212/300], Step [45/172], Loss: 32.0073\n",
      "Epoch [212/300], Step [46/172], Loss: 18.2736\n",
      "Epoch [212/300], Step [47/172], Loss: 51.5058\n",
      "Epoch [212/300], Step [48/172], Loss: 59.6206\n",
      "Epoch [212/300], Step [49/172], Loss: 24.3293\n",
      "Epoch [212/300], Step [50/172], Loss: 44.8730\n",
      "Epoch [212/300], Step [51/172], Loss: 10.0531\n",
      "Epoch [212/300], Step [52/172], Loss: 22.1046\n",
      "Epoch [212/300], Step [53/172], Loss: 24.2911\n",
      "Epoch [212/300], Step [54/172], Loss: 16.5525\n",
      "Epoch [212/300], Step [55/172], Loss: 16.1097\n",
      "Epoch [212/300], Step [56/172], Loss: 19.3867\n",
      "Epoch [212/300], Step [57/172], Loss: 16.4871\n",
      "Epoch [212/300], Step [58/172], Loss: 13.4952\n",
      "Epoch [212/300], Step [59/172], Loss: 25.6403\n",
      "Epoch [212/300], Step [60/172], Loss: 19.2029\n",
      "Epoch [212/300], Step [61/172], Loss: 6.1548\n",
      "Epoch [212/300], Step [62/172], Loss: 16.8800\n",
      "Epoch [212/300], Step [63/172], Loss: 10.8341\n",
      "Epoch [212/300], Step [64/172], Loss: 11.3604\n",
      "Epoch [212/300], Step [65/172], Loss: 17.7392\n",
      "Epoch [212/300], Step [66/172], Loss: 6.6997\n",
      "Epoch [212/300], Step [67/172], Loss: 20.9349\n",
      "Epoch [212/300], Step [68/172], Loss: 4.1726\n",
      "Epoch [212/300], Step [69/172], Loss: 31.2435\n",
      "Epoch [212/300], Step [70/172], Loss: 34.1688\n",
      "Epoch [212/300], Step [71/172], Loss: 36.9418\n",
      "Epoch [212/300], Step [72/172], Loss: 35.4999\n",
      "Epoch [212/300], Step [73/172], Loss: 44.8111\n",
      "Epoch [212/300], Step [74/172], Loss: 23.0461\n",
      "Epoch [212/300], Step [75/172], Loss: 22.5432\n",
      "Epoch [212/300], Step [76/172], Loss: 26.5422\n",
      "Epoch [212/300], Step [77/172], Loss: 43.3216\n",
      "Epoch [212/300], Step [78/172], Loss: 33.4617\n",
      "Epoch [212/300], Step [79/172], Loss: 32.4172\n",
      "Epoch [212/300], Step [80/172], Loss: 45.8922\n",
      "Epoch [212/300], Step [81/172], Loss: 30.2786\n",
      "Epoch [212/300], Step [82/172], Loss: 33.2521\n",
      "Epoch [212/300], Step [83/172], Loss: 41.1452\n",
      "Epoch [212/300], Step [84/172], Loss: 31.5094\n",
      "Epoch [212/300], Step [85/172], Loss: 36.9861\n",
      "Epoch [212/300], Step [86/172], Loss: 31.6849\n",
      "Epoch [212/300], Step [87/172], Loss: 24.2061\n",
      "Epoch [212/300], Step [88/172], Loss: 21.6663\n",
      "Epoch [212/300], Step [89/172], Loss: 27.0792\n",
      "Epoch [212/300], Step [90/172], Loss: 19.4465\n",
      "Epoch [212/300], Step [91/172], Loss: 25.2668\n",
      "Epoch [212/300], Step [92/172], Loss: 18.6408\n",
      "Epoch [212/300], Step [93/172], Loss: 19.3038\n",
      "Epoch [212/300], Step [94/172], Loss: 26.6801\n",
      "Epoch [212/300], Step [95/172], Loss: 20.1547\n",
      "Epoch [212/300], Step [96/172], Loss: 20.1110\n",
      "Epoch [212/300], Step [97/172], Loss: 27.9908\n",
      "Epoch [212/300], Step [98/172], Loss: 19.4723\n",
      "Epoch [212/300], Step [99/172], Loss: 19.0732\n",
      "Epoch [212/300], Step [100/172], Loss: 16.9960\n",
      "Epoch [212/300], Step [101/172], Loss: 19.4600\n",
      "Epoch [212/300], Step [102/172], Loss: 16.7214\n",
      "Epoch [212/300], Step [103/172], Loss: 13.2380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [212/300], Step [104/172], Loss: 19.3787\n",
      "Epoch [212/300], Step [105/172], Loss: 21.5550\n",
      "Epoch [212/300], Step [106/172], Loss: 16.3069\n",
      "Epoch [212/300], Step [107/172], Loss: 16.3212\n",
      "Epoch [212/300], Step [108/172], Loss: 15.2308\n",
      "Epoch [212/300], Step [109/172], Loss: 14.8460\n",
      "Epoch [212/300], Step [110/172], Loss: 17.1161\n",
      "Epoch [212/300], Step [111/172], Loss: 17.7031\n",
      "Epoch [212/300], Step [112/172], Loss: 16.8754\n",
      "Epoch [212/300], Step [113/172], Loss: 13.0854\n",
      "Epoch [212/300], Step [114/172], Loss: 14.4058\n",
      "Epoch [212/300], Step [115/172], Loss: 18.6709\n",
      "Epoch [212/300], Step [116/172], Loss: 15.0372\n",
      "Epoch [212/300], Step [117/172], Loss: 12.4363\n",
      "Epoch [212/300], Step [118/172], Loss: 13.0071\n",
      "Epoch [212/300], Step [119/172], Loss: 16.3943\n",
      "Epoch [212/300], Step [120/172], Loss: 10.9577\n",
      "Epoch [212/300], Step [121/172], Loss: 10.3688\n",
      "Epoch [212/300], Step [122/172], Loss: 12.0825\n",
      "Epoch [212/300], Step [123/172], Loss: 10.4706\n",
      "Epoch [212/300], Step [124/172], Loss: 7.7415\n",
      "Epoch [212/300], Step [125/172], Loss: 12.3058\n",
      "Epoch [212/300], Step [126/172], Loss: 11.8119\n",
      "Epoch [212/300], Step [127/172], Loss: 10.9684\n",
      "Epoch [212/300], Step [128/172], Loss: 10.7791\n",
      "Epoch [212/300], Step [129/172], Loss: 8.8936\n",
      "Epoch [212/300], Step [130/172], Loss: 12.9807\n",
      "Epoch [212/300], Step [131/172], Loss: 7.8489\n",
      "Epoch [212/300], Step [132/172], Loss: 9.7462\n",
      "Epoch [212/300], Step [133/172], Loss: 9.9135\n",
      "Epoch [212/300], Step [134/172], Loss: 11.1613\n",
      "Epoch [212/300], Step [135/172], Loss: 9.1391\n",
      "Epoch [212/300], Step [136/172], Loss: 8.1270\n",
      "Epoch [212/300], Step [137/172], Loss: 9.4983\n",
      "Epoch [212/300], Step [138/172], Loss: 7.5105\n",
      "Epoch [212/300], Step [139/172], Loss: 9.7880\n",
      "Epoch [212/300], Step [140/172], Loss: 10.5289\n",
      "Epoch [212/300], Step [141/172], Loss: 9.3927\n",
      "Epoch [212/300], Step [142/172], Loss: 13.9722\n",
      "Epoch [212/300], Step [143/172], Loss: 11.0637\n",
      "Epoch [212/300], Step [144/172], Loss: 9.2521\n",
      "Epoch [212/300], Step [145/172], Loss: 10.8920\n",
      "Epoch [212/300], Step [146/172], Loss: 10.0114\n",
      "Epoch [212/300], Step [147/172], Loss: 5.7909\n",
      "Epoch [212/300], Step [148/172], Loss: 6.6614\n",
      "Epoch [212/300], Step [149/172], Loss: 7.1207\n",
      "Epoch [212/300], Step [150/172], Loss: 6.0831\n",
      "Epoch [212/300], Step [151/172], Loss: 5.5724\n",
      "Epoch [212/300], Step [152/172], Loss: 8.0310\n",
      "Epoch [212/300], Step [153/172], Loss: 6.6192\n",
      "Epoch [212/300], Step [154/172], Loss: 7.4663\n",
      "Epoch [212/300], Step [155/172], Loss: 6.4119\n",
      "Epoch [212/300], Step [156/172], Loss: 13.6381\n",
      "Epoch [212/300], Step [157/172], Loss: 8.5931\n",
      "Epoch [212/300], Step [158/172], Loss: 6.9953\n",
      "Epoch [212/300], Step [159/172], Loss: 9.4675\n",
      "Epoch [212/300], Step [160/172], Loss: 9.1556\n",
      "Epoch [212/300], Step [161/172], Loss: 7.7589\n",
      "Epoch [212/300], Step [162/172], Loss: 5.1969\n",
      "Epoch [212/300], Step [163/172], Loss: 6.4903\n",
      "Epoch [212/300], Step [164/172], Loss: 8.8618\n",
      "Epoch [212/300], Step [165/172], Loss: 6.9542\n",
      "Epoch [212/300], Step [166/172], Loss: 6.2092\n",
      "Epoch [212/300], Step [167/172], Loss: 9.9807\n",
      "Epoch [212/300], Step [168/172], Loss: 6.8498\n",
      "Epoch [212/300], Step [169/172], Loss: 7.1281\n",
      "Epoch [212/300], Step [170/172], Loss: 5.4666\n",
      "Epoch [212/300], Step [171/172], Loss: 7.8468\n",
      "Epoch [212/300], Step [172/172], Loss: 6.1676\n",
      "Epoch [213/300], Step [1/172], Loss: 48.2624\n",
      "Epoch [213/300], Step [2/172], Loss: 53.6014\n",
      "Epoch [213/300], Step [3/172], Loss: 51.2469\n",
      "Epoch [213/300], Step [4/172], Loss: 26.4600\n",
      "Epoch [213/300], Step [5/172], Loss: 43.8793\n",
      "Epoch [213/300], Step [6/172], Loss: 19.9991\n",
      "Epoch [213/300], Step [7/172], Loss: 29.3098\n",
      "Epoch [213/300], Step [8/172], Loss: 5.1348\n",
      "Epoch [213/300], Step [9/172], Loss: 29.7623\n",
      "Epoch [213/300], Step [10/172], Loss: 40.7737\n",
      "Epoch [213/300], Step [11/172], Loss: 54.2793\n",
      "Epoch [213/300], Step [12/172], Loss: 59.6444\n",
      "Epoch [213/300], Step [13/172], Loss: 35.1299\n",
      "Epoch [213/300], Step [14/172], Loss: 61.6864\n",
      "Epoch [213/300], Step [15/172], Loss: 51.4861\n",
      "Epoch [213/300], Step [16/172], Loss: 9.5178\n",
      "Epoch [213/300], Step [17/172], Loss: 42.4124\n",
      "Epoch [213/300], Step [18/172], Loss: 54.1721\n",
      "Epoch [213/300], Step [19/172], Loss: 77.1117\n",
      "Epoch [213/300], Step [20/172], Loss: 30.2331\n",
      "Epoch [213/300], Step [21/172], Loss: 83.3585\n",
      "Epoch [213/300], Step [22/172], Loss: 57.1409\n",
      "Epoch [213/300], Step [23/172], Loss: 2.2253\n",
      "Epoch [213/300], Step [24/172], Loss: 54.5475\n",
      "Epoch [213/300], Step [25/172], Loss: 39.5903\n",
      "Epoch [213/300], Step [26/172], Loss: 46.4082\n",
      "Epoch [213/300], Step [27/172], Loss: 58.4733\n",
      "Epoch [213/300], Step [28/172], Loss: 23.9423\n",
      "Epoch [213/300], Step [29/172], Loss: 15.9991\n",
      "Epoch [213/300], Step [30/172], Loss: 59.0469\n",
      "Epoch [213/300], Step [31/172], Loss: 36.6292\n",
      "Epoch [213/300], Step [32/172], Loss: 44.6053\n",
      "Epoch [213/300], Step [33/172], Loss: 72.1165\n",
      "Epoch [213/300], Step [34/172], Loss: 2.7458\n",
      "Epoch [213/300], Step [35/172], Loss: 14.5271\n",
      "Epoch [213/300], Step [36/172], Loss: 17.0211\n",
      "Epoch [213/300], Step [37/172], Loss: 17.1481\n",
      "Epoch [213/300], Step [38/172], Loss: 32.9349\n",
      "Epoch [213/300], Step [39/172], Loss: 37.6283\n",
      "Epoch [213/300], Step [40/172], Loss: 23.5599\n",
      "Epoch [213/300], Step [41/172], Loss: 36.2382\n",
      "Epoch [213/300], Step [42/172], Loss: 41.3129\n",
      "Epoch [213/300], Step [43/172], Loss: 30.1027\n",
      "Epoch [213/300], Step [44/172], Loss: 22.6401\n",
      "Epoch [213/300], Step [45/172], Loss: 32.4327\n",
      "Epoch [213/300], Step [46/172], Loss: 18.4928\n",
      "Epoch [213/300], Step [47/172], Loss: 51.8167\n",
      "Epoch [213/300], Step [48/172], Loss: 60.2653\n",
      "Epoch [213/300], Step [49/172], Loss: 24.5267\n",
      "Epoch [213/300], Step [50/172], Loss: 44.7595\n",
      "Epoch [213/300], Step [51/172], Loss: 10.3277\n",
      "Epoch [213/300], Step [52/172], Loss: 22.6980\n",
      "Epoch [213/300], Step [53/172], Loss: 24.7376\n",
      "Epoch [213/300], Step [54/172], Loss: 16.9706\n",
      "Epoch [213/300], Step [55/172], Loss: 16.4620\n",
      "Epoch [213/300], Step [56/172], Loss: 20.0292\n",
      "Epoch [213/300], Step [57/172], Loss: 16.6145\n",
      "Epoch [213/300], Step [58/172], Loss: 13.6449\n",
      "Epoch [213/300], Step [59/172], Loss: 25.6790\n",
      "Epoch [213/300], Step [60/172], Loss: 18.9396\n",
      "Epoch [213/300], Step [61/172], Loss: 6.3733\n",
      "Epoch [213/300], Step [62/172], Loss: 17.0270\n",
      "Epoch [213/300], Step [63/172], Loss: 11.3304\n",
      "Epoch [213/300], Step [64/172], Loss: 11.6699\n",
      "Epoch [213/300], Step [65/172], Loss: 17.8683\n",
      "Epoch [213/300], Step [66/172], Loss: 6.8127\n",
      "Epoch [213/300], Step [67/172], Loss: 21.3549\n",
      "Epoch [213/300], Step [68/172], Loss: 4.3352\n",
      "Epoch [213/300], Step [69/172], Loss: 31.0854\n",
      "Epoch [213/300], Step [70/172], Loss: 33.8196\n",
      "Epoch [213/300], Step [71/172], Loss: 36.5062\n",
      "Epoch [213/300], Step [72/172], Loss: 35.1438\n",
      "Epoch [213/300], Step [73/172], Loss: 44.5528\n",
      "Epoch [213/300], Step [74/172], Loss: 22.8582\n",
      "Epoch [213/300], Step [75/172], Loss: 22.2262\n",
      "Epoch [213/300], Step [76/172], Loss: 26.4250\n",
      "Epoch [213/300], Step [77/172], Loss: 43.0368\n",
      "Epoch [213/300], Step [78/172], Loss: 33.3966\n",
      "Epoch [213/300], Step [79/172], Loss: 32.3090\n",
      "Epoch [213/300], Step [80/172], Loss: 45.9559\n",
      "Epoch [213/300], Step [81/172], Loss: 29.9824\n",
      "Epoch [213/300], Step [82/172], Loss: 33.5625\n",
      "Epoch [213/300], Step [83/172], Loss: 41.1201\n",
      "Epoch [213/300], Step [84/172], Loss: 31.3573\n",
      "Epoch [213/300], Step [85/172], Loss: 36.9910\n",
      "Epoch [213/300], Step [86/172], Loss: 31.8651\n",
      "Epoch [213/300], Step [87/172], Loss: 24.0847\n",
      "Epoch [213/300], Step [88/172], Loss: 21.6239\n",
      "Epoch [213/300], Step [89/172], Loss: 27.2048\n",
      "Epoch [213/300], Step [90/172], Loss: 19.4438\n",
      "Epoch [213/300], Step [91/172], Loss: 25.1576\n",
      "Epoch [213/300], Step [92/172], Loss: 18.5552\n",
      "Epoch [213/300], Step [93/172], Loss: 19.2821\n",
      "Epoch [213/300], Step [94/172], Loss: 26.6654\n",
      "Epoch [213/300], Step [95/172], Loss: 20.2775\n",
      "Epoch [213/300], Step [96/172], Loss: 20.0192\n",
      "Epoch [213/300], Step [97/172], Loss: 27.7440\n",
      "Epoch [213/300], Step [98/172], Loss: 19.3932\n",
      "Epoch [213/300], Step [99/172], Loss: 19.0015\n",
      "Epoch [213/300], Step [100/172], Loss: 16.9508\n",
      "Epoch [213/300], Step [101/172], Loss: 19.4010\n",
      "Epoch [213/300], Step [102/172], Loss: 16.7050\n",
      "Epoch [213/300], Step [103/172], Loss: 13.2199\n",
      "Epoch [213/300], Step [104/172], Loss: 19.3382\n",
      "Epoch [213/300], Step [105/172], Loss: 21.6604\n",
      "Epoch [213/300], Step [106/172], Loss: 16.2608\n",
      "Epoch [213/300], Step [107/172], Loss: 16.2481\n",
      "Epoch [213/300], Step [108/172], Loss: 15.3088\n",
      "Epoch [213/300], Step [109/172], Loss: 14.7700\n",
      "Epoch [213/300], Step [110/172], Loss: 17.2279\n",
      "Epoch [213/300], Step [111/172], Loss: 17.7782\n",
      "Epoch [213/300], Step [112/172], Loss: 17.0247\n",
      "Epoch [213/300], Step [113/172], Loss: 13.2124\n",
      "Epoch [213/300], Step [114/172], Loss: 14.4351\n",
      "Epoch [213/300], Step [115/172], Loss: 18.6045\n",
      "Epoch [213/300], Step [116/172], Loss: 15.0838\n",
      "Epoch [213/300], Step [117/172], Loss: 12.4384\n",
      "Epoch [213/300], Step [118/172], Loss: 13.0073\n",
      "Epoch [213/300], Step [119/172], Loss: 16.3842\n",
      "Epoch [213/300], Step [120/172], Loss: 10.9952\n",
      "Epoch [213/300], Step [121/172], Loss: 10.3678\n",
      "Epoch [213/300], Step [122/172], Loss: 12.3360\n",
      "Epoch [213/300], Step [123/172], Loss: 10.7198\n",
      "Epoch [213/300], Step [124/172], Loss: 7.7220\n",
      "Epoch [213/300], Step [125/172], Loss: 12.3150\n",
      "Epoch [213/300], Step [126/172], Loss: 11.8589\n",
      "Epoch [213/300], Step [127/172], Loss: 11.0345\n",
      "Epoch [213/300], Step [128/172], Loss: 10.8886\n",
      "Epoch [213/300], Step [129/172], Loss: 8.9443\n",
      "Epoch [213/300], Step [130/172], Loss: 12.9960\n",
      "Epoch [213/300], Step [131/172], Loss: 7.9371\n",
      "Epoch [213/300], Step [132/172], Loss: 9.7744\n",
      "Epoch [213/300], Step [133/172], Loss: 10.0141\n",
      "Epoch [213/300], Step [134/172], Loss: 11.0959\n",
      "Epoch [213/300], Step [135/172], Loss: 9.0772\n",
      "Epoch [213/300], Step [136/172], Loss: 8.0396\n",
      "Epoch [213/300], Step [137/172], Loss: 9.5014\n",
      "Epoch [213/300], Step [138/172], Loss: 7.6660\n",
      "Epoch [213/300], Step [139/172], Loss: 9.7984\n",
      "Epoch [213/300], Step [140/172], Loss: 10.5676\n",
      "Epoch [213/300], Step [141/172], Loss: 9.4213\n",
      "Epoch [213/300], Step [142/172], Loss: 14.2036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [213/300], Step [143/172], Loss: 11.1267\n",
      "Epoch [213/300], Step [144/172], Loss: 9.2948\n",
      "Epoch [213/300], Step [145/172], Loss: 10.9786\n",
      "Epoch [213/300], Step [146/172], Loss: 10.0893\n",
      "Epoch [213/300], Step [147/172], Loss: 5.8126\n",
      "Epoch [213/300], Step [148/172], Loss: 6.6846\n",
      "Epoch [213/300], Step [149/172], Loss: 7.1296\n",
      "Epoch [213/300], Step [150/172], Loss: 6.1091\n",
      "Epoch [213/300], Step [151/172], Loss: 5.5998\n",
      "Epoch [213/300], Step [152/172], Loss: 8.0311\n",
      "Epoch [213/300], Step [153/172], Loss: 6.6263\n",
      "Epoch [213/300], Step [154/172], Loss: 7.4759\n",
      "Epoch [213/300], Step [155/172], Loss: 6.4288\n",
      "Epoch [213/300], Step [156/172], Loss: 13.7556\n",
      "Epoch [213/300], Step [157/172], Loss: 8.6198\n",
      "Epoch [213/300], Step [158/172], Loss: 7.0720\n",
      "Epoch [213/300], Step [159/172], Loss: 9.6565\n",
      "Epoch [213/300], Step [160/172], Loss: 9.2657\n",
      "Epoch [213/300], Step [161/172], Loss: 7.7484\n",
      "Epoch [213/300], Step [162/172], Loss: 5.1808\n",
      "Epoch [213/300], Step [163/172], Loss: 6.5207\n",
      "Epoch [213/300], Step [164/172], Loss: 8.9290\n",
      "Epoch [213/300], Step [165/172], Loss: 6.9664\n",
      "Epoch [213/300], Step [166/172], Loss: 6.2011\n",
      "Epoch [213/300], Step [167/172], Loss: 10.2076\n",
      "Epoch [213/300], Step [168/172], Loss: 6.8430\n",
      "Epoch [213/300], Step [169/172], Loss: 7.0799\n",
      "Epoch [213/300], Step [170/172], Loss: 5.5033\n",
      "Epoch [213/300], Step [171/172], Loss: 8.0516\n",
      "Epoch [213/300], Step [172/172], Loss: 6.2462\n",
      "Epoch [214/300], Step [1/172], Loss: 48.2652\n",
      "Epoch [214/300], Step [2/172], Loss: 52.9178\n",
      "Epoch [214/300], Step [3/172], Loss: 49.1663\n",
      "Epoch [214/300], Step [4/172], Loss: 26.1037\n",
      "Epoch [214/300], Step [5/172], Loss: 42.4326\n",
      "Epoch [214/300], Step [6/172], Loss: 20.1984\n",
      "Epoch [214/300], Step [7/172], Loss: 28.7463\n",
      "Epoch [214/300], Step [8/172], Loss: 5.0803\n",
      "Epoch [214/300], Step [9/172], Loss: 29.4149\n",
      "Epoch [214/300], Step [10/172], Loss: 41.2769\n",
      "Epoch [214/300], Step [11/172], Loss: 54.1865\n",
      "Epoch [214/300], Step [12/172], Loss: 59.0526\n",
      "Epoch [214/300], Step [13/172], Loss: 34.9029\n",
      "Epoch [214/300], Step [14/172], Loss: 60.9164\n",
      "Epoch [214/300], Step [15/172], Loss: 51.1640\n",
      "Epoch [214/300], Step [16/172], Loss: 10.2123\n",
      "Epoch [214/300], Step [17/172], Loss: 41.7834\n",
      "Epoch [214/300], Step [18/172], Loss: 53.8017\n",
      "Epoch [214/300], Step [19/172], Loss: 75.9729\n",
      "Epoch [214/300], Step [20/172], Loss: 30.9969\n",
      "Epoch [214/300], Step [21/172], Loss: 83.0594\n",
      "Epoch [214/300], Step [22/172], Loss: 56.7677\n",
      "Epoch [214/300], Step [23/172], Loss: 1.8731\n",
      "Epoch [214/300], Step [24/172], Loss: 54.4212\n",
      "Epoch [214/300], Step [25/172], Loss: 39.5905\n",
      "Epoch [214/300], Step [26/172], Loss: 46.2456\n",
      "Epoch [214/300], Step [27/172], Loss: 58.7932\n",
      "Epoch [214/300], Step [28/172], Loss: 24.0589\n",
      "Epoch [214/300], Step [29/172], Loss: 15.8672\n",
      "Epoch [214/300], Step [30/172], Loss: 59.1383\n",
      "Epoch [214/300], Step [31/172], Loss: 36.6366\n",
      "Epoch [214/300], Step [32/172], Loss: 44.7683\n",
      "Epoch [214/300], Step [33/172], Loss: 72.0315\n",
      "Epoch [214/300], Step [34/172], Loss: 2.7903\n",
      "Epoch [214/300], Step [35/172], Loss: 14.6764\n",
      "Epoch [214/300], Step [36/172], Loss: 16.8552\n",
      "Epoch [214/300], Step [37/172], Loss: 17.1424\n",
      "Epoch [214/300], Step [38/172], Loss: 33.1506\n",
      "Epoch [214/300], Step [39/172], Loss: 37.7895\n",
      "Epoch [214/300], Step [40/172], Loss: 23.6629\n",
      "Epoch [214/300], Step [41/172], Loss: 36.5471\n",
      "Epoch [214/300], Step [42/172], Loss: 41.3137\n",
      "Epoch [214/300], Step [43/172], Loss: 30.2666\n",
      "Epoch [214/300], Step [44/172], Loss: 22.4972\n",
      "Epoch [214/300], Step [45/172], Loss: 32.2476\n",
      "Epoch [214/300], Step [46/172], Loss: 18.0986\n",
      "Epoch [214/300], Step [47/172], Loss: 51.5911\n",
      "Epoch [214/300], Step [48/172], Loss: 59.6677\n",
      "Epoch [214/300], Step [49/172], Loss: 24.4779\n",
      "Epoch [214/300], Step [50/172], Loss: 44.6608\n",
      "Epoch [214/300], Step [51/172], Loss: 10.3200\n",
      "Epoch [214/300], Step [52/172], Loss: 22.7014\n",
      "Epoch [214/300], Step [53/172], Loss: 24.8988\n",
      "Epoch [214/300], Step [54/172], Loss: 17.2370\n",
      "Epoch [214/300], Step [55/172], Loss: 16.5657\n",
      "Epoch [214/300], Step [56/172], Loss: 20.0378\n",
      "Epoch [214/300], Step [57/172], Loss: 16.6592\n",
      "Epoch [214/300], Step [58/172], Loss: 13.9004\n",
      "Epoch [214/300], Step [59/172], Loss: 26.1725\n",
      "Epoch [214/300], Step [60/172], Loss: 19.0912\n",
      "Epoch [214/300], Step [61/172], Loss: 6.3819\n",
      "Epoch [214/300], Step [62/172], Loss: 17.0614\n",
      "Epoch [214/300], Step [63/172], Loss: 11.4306\n",
      "Epoch [214/300], Step [64/172], Loss: 11.8186\n",
      "Epoch [214/300], Step [65/172], Loss: 17.9487\n",
      "Epoch [214/300], Step [66/172], Loss: 6.8517\n",
      "Epoch [214/300], Step [67/172], Loss: 21.4489\n",
      "Epoch [214/300], Step [68/172], Loss: 4.1359\n",
      "Epoch [214/300], Step [69/172], Loss: 31.1129\n",
      "Epoch [214/300], Step [70/172], Loss: 33.5880\n",
      "Epoch [214/300], Step [71/172], Loss: 36.3523\n",
      "Epoch [214/300], Step [72/172], Loss: 34.9595\n",
      "Epoch [214/300], Step [73/172], Loss: 44.2239\n",
      "Epoch [214/300], Step [74/172], Loss: 22.8007\n",
      "Epoch [214/300], Step [75/172], Loss: 22.1147\n",
      "Epoch [214/300], Step [76/172], Loss: 26.4606\n",
      "Epoch [214/300], Step [77/172], Loss: 43.1222\n",
      "Epoch [214/300], Step [78/172], Loss: 33.3239\n",
      "Epoch [214/300], Step [79/172], Loss: 32.3316\n",
      "Epoch [214/300], Step [80/172], Loss: 46.3653\n",
      "Epoch [214/300], Step [81/172], Loss: 30.0751\n",
      "Epoch [214/300], Step [82/172], Loss: 33.9938\n",
      "Epoch [214/300], Step [83/172], Loss: 41.2410\n",
      "Epoch [214/300], Step [84/172], Loss: 31.5648\n",
      "Epoch [214/300], Step [85/172], Loss: 37.1679\n",
      "Epoch [214/300], Step [86/172], Loss: 31.9988\n",
      "Epoch [214/300], Step [87/172], Loss: 24.1448\n",
      "Epoch [214/300], Step [88/172], Loss: 21.6534\n",
      "Epoch [214/300], Step [89/172], Loss: 27.2428\n",
      "Epoch [214/300], Step [90/172], Loss: 19.4975\n",
      "Epoch [214/300], Step [91/172], Loss: 25.2072\n",
      "Epoch [214/300], Step [92/172], Loss: 18.5110\n",
      "Epoch [214/300], Step [93/172], Loss: 19.2456\n",
      "Epoch [214/300], Step [94/172], Loss: 26.6578\n",
      "Epoch [214/300], Step [95/172], Loss: 20.2591\n",
      "Epoch [214/300], Step [96/172], Loss: 20.0152\n",
      "Epoch [214/300], Step [97/172], Loss: 27.8613\n",
      "Epoch [214/300], Step [98/172], Loss: 19.4534\n",
      "Epoch [214/300], Step [99/172], Loss: 19.0585\n",
      "Epoch [214/300], Step [100/172], Loss: 17.0216\n",
      "Epoch [214/300], Step [101/172], Loss: 19.4571\n",
      "Epoch [214/300], Step [102/172], Loss: 16.8512\n",
      "Epoch [214/300], Step [103/172], Loss: 13.2381\n",
      "Epoch [214/300], Step [104/172], Loss: 19.3158\n",
      "Epoch [214/300], Step [105/172], Loss: 21.8716\n",
      "Epoch [214/300], Step [106/172], Loss: 16.3108\n",
      "Epoch [214/300], Step [107/172], Loss: 16.2497\n",
      "Epoch [214/300], Step [108/172], Loss: 15.2869\n",
      "Epoch [214/300], Step [109/172], Loss: 14.7514\n",
      "Epoch [214/300], Step [110/172], Loss: 17.1580\n",
      "Epoch [214/300], Step [111/172], Loss: 17.7706\n",
      "Epoch [214/300], Step [112/172], Loss: 16.8774\n",
      "Epoch [214/300], Step [113/172], Loss: 13.2291\n",
      "Epoch [214/300], Step [114/172], Loss: 14.4187\n",
      "Epoch [214/300], Step [115/172], Loss: 18.5720\n",
      "Epoch [214/300], Step [116/172], Loss: 14.9825\n",
      "Epoch [214/300], Step [117/172], Loss: 12.3902\n",
      "Epoch [214/300], Step [118/172], Loss: 12.6900\n",
      "Epoch [214/300], Step [119/172], Loss: 16.5018\n",
      "Epoch [214/300], Step [120/172], Loss: 10.9731\n",
      "Epoch [214/300], Step [121/172], Loss: 10.2495\n",
      "Epoch [214/300], Step [122/172], Loss: 12.3684\n",
      "Epoch [214/300], Step [123/172], Loss: 10.6936\n",
      "Epoch [214/300], Step [124/172], Loss: 7.6341\n",
      "Epoch [214/300], Step [125/172], Loss: 12.1195\n",
      "Epoch [214/300], Step [126/172], Loss: 11.8126\n",
      "Epoch [214/300], Step [127/172], Loss: 11.0432\n",
      "Epoch [214/300], Step [128/172], Loss: 10.9615\n",
      "Epoch [214/300], Step [129/172], Loss: 8.9213\n",
      "Epoch [214/300], Step [130/172], Loss: 12.9067\n",
      "Epoch [214/300], Step [131/172], Loss: 7.9156\n",
      "Epoch [214/300], Step [132/172], Loss: 9.8554\n",
      "Epoch [214/300], Step [133/172], Loss: 10.0495\n",
      "Epoch [214/300], Step [134/172], Loss: 11.2182\n",
      "Epoch [214/300], Step [135/172], Loss: 9.1068\n",
      "Epoch [214/300], Step [136/172], Loss: 7.9690\n",
      "Epoch [214/300], Step [137/172], Loss: 9.3528\n",
      "Epoch [214/300], Step [138/172], Loss: 7.6248\n",
      "Epoch [214/300], Step [139/172], Loss: 9.7232\n",
      "Epoch [214/300], Step [140/172], Loss: 10.5545\n",
      "Epoch [214/300], Step [141/172], Loss: 9.3556\n",
      "Epoch [214/300], Step [142/172], Loss: 14.0740\n",
      "Epoch [214/300], Step [143/172], Loss: 11.1588\n",
      "Epoch [214/300], Step [144/172], Loss: 9.2609\n",
      "Epoch [214/300], Step [145/172], Loss: 10.9705\n",
      "Epoch [214/300], Step [146/172], Loss: 10.0623\n",
      "Epoch [214/300], Step [147/172], Loss: 5.7718\n",
      "Epoch [214/300], Step [148/172], Loss: 6.6690\n",
      "Epoch [214/300], Step [149/172], Loss: 7.0868\n",
      "Epoch [214/300], Step [150/172], Loss: 6.0799\n",
      "Epoch [214/300], Step [151/172], Loss: 5.5954\n",
      "Epoch [214/300], Step [152/172], Loss: 8.1147\n",
      "Epoch [214/300], Step [153/172], Loss: 6.5457\n",
      "Epoch [214/300], Step [154/172], Loss: 7.3592\n",
      "Epoch [214/300], Step [155/172], Loss: 6.3413\n",
      "Epoch [214/300], Step [156/172], Loss: 13.7779\n",
      "Epoch [214/300], Step [157/172], Loss: 8.5838\n",
      "Epoch [214/300], Step [158/172], Loss: 7.0541\n",
      "Epoch [214/300], Step [159/172], Loss: 9.5002\n",
      "Epoch [214/300], Step [160/172], Loss: 9.2363\n",
      "Epoch [214/300], Step [161/172], Loss: 7.6879\n",
      "Epoch [214/300], Step [162/172], Loss: 5.1921\n",
      "Epoch [214/300], Step [163/172], Loss: 6.4356\n",
      "Epoch [214/300], Step [164/172], Loss: 9.0874\n",
      "Epoch [214/300], Step [165/172], Loss: 6.9585\n",
      "Epoch [214/300], Step [166/172], Loss: 6.1360\n",
      "Epoch [214/300], Step [167/172], Loss: 10.0203\n",
      "Epoch [214/300], Step [168/172], Loss: 6.7319\n",
      "Epoch [214/300], Step [169/172], Loss: 7.0537\n",
      "Epoch [214/300], Step [170/172], Loss: 5.4468\n",
      "Epoch [214/300], Step [171/172], Loss: 7.9486\n",
      "Epoch [214/300], Step [172/172], Loss: 6.1448\n",
      "Epoch [215/300], Step [1/172], Loss: 48.0840\n",
      "Epoch [215/300], Step [2/172], Loss: 52.4597\n",
      "Epoch [215/300], Step [3/172], Loss: 49.9709\n",
      "Epoch [215/300], Step [4/172], Loss: 25.8676\n",
      "Epoch [215/300], Step [5/172], Loss: 43.1492\n",
      "Epoch [215/300], Step [6/172], Loss: 19.9673\n",
      "Epoch [215/300], Step [7/172], Loss: 28.5930\n",
      "Epoch [215/300], Step [8/172], Loss: 4.3216\n",
      "Epoch [215/300], Step [9/172], Loss: 29.2778\n",
      "Epoch [215/300], Step [10/172], Loss: 41.2551\n",
      "Epoch [215/300], Step [11/172], Loss: 53.9883\n",
      "Epoch [215/300], Step [12/172], Loss: 58.7238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [215/300], Step [13/172], Loss: 34.8308\n",
      "Epoch [215/300], Step [14/172], Loss: 60.8061\n",
      "Epoch [215/300], Step [15/172], Loss: 51.1132\n",
      "Epoch [215/300], Step [16/172], Loss: 11.0190\n",
      "Epoch [215/300], Step [17/172], Loss: 41.4020\n",
      "Epoch [215/300], Step [18/172], Loss: 53.4225\n",
      "Epoch [215/300], Step [19/172], Loss: 75.8979\n",
      "Epoch [215/300], Step [20/172], Loss: 29.7155\n",
      "Epoch [215/300], Step [21/172], Loss: 82.9542\n",
      "Epoch [215/300], Step [22/172], Loss: 56.1482\n",
      "Epoch [215/300], Step [23/172], Loss: 2.2011\n",
      "Epoch [215/300], Step [24/172], Loss: 54.2237\n",
      "Epoch [215/300], Step [25/172], Loss: 39.7670\n",
      "Epoch [215/300], Step [26/172], Loss: 46.0790\n",
      "Epoch [215/300], Step [27/172], Loss: 58.2796\n",
      "Epoch [215/300], Step [28/172], Loss: 23.9783\n",
      "Epoch [215/300], Step [29/172], Loss: 15.7702\n",
      "Epoch [215/300], Step [30/172], Loss: 59.0198\n",
      "Epoch [215/300], Step [31/172], Loss: 36.8108\n",
      "Epoch [215/300], Step [32/172], Loss: 44.6417\n",
      "Epoch [215/300], Step [33/172], Loss: 72.0399\n",
      "Epoch [215/300], Step [34/172], Loss: 2.8013\n",
      "Epoch [215/300], Step [35/172], Loss: 14.7201\n",
      "Epoch [215/300], Step [36/172], Loss: 16.8309\n",
      "Epoch [215/300], Step [37/172], Loss: 17.2094\n",
      "Epoch [215/300], Step [38/172], Loss: 33.4700\n",
      "Epoch [215/300], Step [39/172], Loss: 37.8040\n",
      "Epoch [215/300], Step [40/172], Loss: 23.8106\n",
      "Epoch [215/300], Step [41/172], Loss: 37.1694\n",
      "Epoch [215/300], Step [42/172], Loss: 41.8288\n",
      "Epoch [215/300], Step [43/172], Loss: 30.4434\n",
      "Epoch [215/300], Step [44/172], Loss: 22.9677\n",
      "Epoch [215/300], Step [45/172], Loss: 32.8873\n",
      "Epoch [215/300], Step [46/172], Loss: 18.3122\n",
      "Epoch [215/300], Step [47/172], Loss: 52.0926\n",
      "Epoch [215/300], Step [48/172], Loss: 60.7242\n",
      "Epoch [215/300], Step [49/172], Loss: 24.6704\n",
      "Epoch [215/300], Step [50/172], Loss: 45.0536\n",
      "Epoch [215/300], Step [51/172], Loss: 10.3535\n",
      "Epoch [215/300], Step [52/172], Loss: 22.8166\n",
      "Epoch [215/300], Step [53/172], Loss: 25.1647\n",
      "Epoch [215/300], Step [54/172], Loss: 17.4842\n",
      "Epoch [215/300], Step [55/172], Loss: 16.7563\n",
      "Epoch [215/300], Step [56/172], Loss: 19.5817\n",
      "Epoch [215/300], Step [57/172], Loss: 16.6920\n",
      "Epoch [215/300], Step [58/172], Loss: 13.9456\n",
      "Epoch [215/300], Step [59/172], Loss: 26.3562\n",
      "Epoch [215/300], Step [60/172], Loss: 18.8701\n",
      "Epoch [215/300], Step [61/172], Loss: 6.5182\n",
      "Epoch [215/300], Step [62/172], Loss: 17.2015\n",
      "Epoch [215/300], Step [63/172], Loss: 11.1652\n",
      "Epoch [215/300], Step [64/172], Loss: 11.7219\n",
      "Epoch [215/300], Step [65/172], Loss: 17.8647\n",
      "Epoch [215/300], Step [66/172], Loss: 6.8480\n",
      "Epoch [215/300], Step [67/172], Loss: 21.3634\n",
      "Epoch [215/300], Step [68/172], Loss: 4.2202\n",
      "Epoch [215/300], Step [69/172], Loss: 30.7260\n",
      "Epoch [215/300], Step [70/172], Loss: 33.6869\n",
      "Epoch [215/300], Step [71/172], Loss: 36.4974\n",
      "Epoch [215/300], Step [72/172], Loss: 34.7994\n",
      "Epoch [215/300], Step [73/172], Loss: 44.4375\n",
      "Epoch [215/300], Step [74/172], Loss: 22.6742\n",
      "Epoch [215/300], Step [75/172], Loss: 22.1458\n",
      "Epoch [215/300], Step [76/172], Loss: 26.6577\n",
      "Epoch [215/300], Step [77/172], Loss: 42.9529\n",
      "Epoch [215/300], Step [78/172], Loss: 33.2224\n",
      "Epoch [215/300], Step [79/172], Loss: 32.0659\n",
      "Epoch [215/300], Step [80/172], Loss: 45.8295\n",
      "Epoch [215/300], Step [81/172], Loss: 29.7424\n",
      "Epoch [215/300], Step [82/172], Loss: 33.1329\n",
      "Epoch [215/300], Step [83/172], Loss: 40.9700\n",
      "Epoch [215/300], Step [84/172], Loss: 31.2988\n",
      "Epoch [215/300], Step [85/172], Loss: 37.0897\n",
      "Epoch [215/300], Step [86/172], Loss: 31.9526\n",
      "Epoch [215/300], Step [87/172], Loss: 24.0777\n",
      "Epoch [215/300], Step [88/172], Loss: 21.5568\n",
      "Epoch [215/300], Step [89/172], Loss: 27.2740\n",
      "Epoch [215/300], Step [90/172], Loss: 19.4626\n",
      "Epoch [215/300], Step [91/172], Loss: 25.2648\n",
      "Epoch [215/300], Step [92/172], Loss: 18.5254\n",
      "Epoch [215/300], Step [93/172], Loss: 19.1995\n",
      "Epoch [215/300], Step [94/172], Loss: 26.6291\n",
      "Epoch [215/300], Step [95/172], Loss: 20.3462\n",
      "Epoch [215/300], Step [96/172], Loss: 20.0763\n",
      "Epoch [215/300], Step [97/172], Loss: 27.8809\n",
      "Epoch [215/300], Step [98/172], Loss: 19.6012\n",
      "Epoch [215/300], Step [99/172], Loss: 19.2330\n",
      "Epoch [215/300], Step [100/172], Loss: 17.3345\n",
      "Epoch [215/300], Step [101/172], Loss: 19.6164\n",
      "Epoch [215/300], Step [102/172], Loss: 16.6190\n",
      "Epoch [215/300], Step [103/172], Loss: 13.3378\n",
      "Epoch [215/300], Step [104/172], Loss: 19.5378\n",
      "Epoch [215/300], Step [105/172], Loss: 21.9456\n",
      "Epoch [215/300], Step [106/172], Loss: 16.4741\n",
      "Epoch [215/300], Step [107/172], Loss: 16.3890\n",
      "Epoch [215/300], Step [108/172], Loss: 15.5365\n",
      "Epoch [215/300], Step [109/172], Loss: 14.8338\n",
      "Epoch [215/300], Step [110/172], Loss: 17.2778\n",
      "Epoch [215/300], Step [111/172], Loss: 18.0179\n",
      "Epoch [215/300], Step [112/172], Loss: 16.9295\n",
      "Epoch [215/300], Step [113/172], Loss: 13.3166\n",
      "Epoch [215/300], Step [114/172], Loss: 14.5125\n",
      "Epoch [215/300], Step [115/172], Loss: 18.5733\n",
      "Epoch [215/300], Step [116/172], Loss: 14.9733\n",
      "Epoch [215/300], Step [117/172], Loss: 12.3202\n",
      "Epoch [215/300], Step [118/172], Loss: 12.6570\n",
      "Epoch [215/300], Step [119/172], Loss: 16.6248\n",
      "Epoch [215/300], Step [120/172], Loss: 11.0179\n",
      "Epoch [215/300], Step [121/172], Loss: 10.4098\n",
      "Epoch [215/300], Step [122/172], Loss: 12.5160\n",
      "Epoch [215/300], Step [123/172], Loss: 10.7729\n",
      "Epoch [215/300], Step [124/172], Loss: 7.7275\n",
      "Epoch [215/300], Step [125/172], Loss: 12.4776\n",
      "Epoch [215/300], Step [126/172], Loss: 12.0110\n",
      "Epoch [215/300], Step [127/172], Loss: 10.9828\n",
      "Epoch [215/300], Step [128/172], Loss: 10.8956\n",
      "Epoch [215/300], Step [129/172], Loss: 9.0346\n",
      "Epoch [215/300], Step [130/172], Loss: 13.0532\n",
      "Epoch [215/300], Step [131/172], Loss: 8.0183\n",
      "Epoch [215/300], Step [132/172], Loss: 9.8654\n",
      "Epoch [215/300], Step [133/172], Loss: 10.1239\n",
      "Epoch [215/300], Step [134/172], Loss: 11.2062\n",
      "Epoch [215/300], Step [135/172], Loss: 9.1507\n",
      "Epoch [215/300], Step [136/172], Loss: 8.0225\n",
      "Epoch [215/300], Step [137/172], Loss: 9.3842\n",
      "Epoch [215/300], Step [138/172], Loss: 7.8702\n",
      "Epoch [215/300], Step [139/172], Loss: 9.9164\n",
      "Epoch [215/300], Step [140/172], Loss: 10.6377\n",
      "Epoch [215/300], Step [141/172], Loss: 9.4506\n",
      "Epoch [215/300], Step [142/172], Loss: 14.1160\n",
      "Epoch [215/300], Step [143/172], Loss: 11.2188\n",
      "Epoch [215/300], Step [144/172], Loss: 9.4142\n",
      "Epoch [215/300], Step [145/172], Loss: 10.8854\n",
      "Epoch [215/300], Step [146/172], Loss: 10.1407\n",
      "Epoch [215/300], Step [147/172], Loss: 5.9127\n",
      "Epoch [215/300], Step [148/172], Loss: 6.7603\n",
      "Epoch [215/300], Step [149/172], Loss: 7.2094\n",
      "Epoch [215/300], Step [150/172], Loss: 6.1683\n",
      "Epoch [215/300], Step [151/172], Loss: 5.6631\n",
      "Epoch [215/300], Step [152/172], Loss: 8.2015\n",
      "Epoch [215/300], Step [153/172], Loss: 6.6707\n",
      "Epoch [215/300], Step [154/172], Loss: 7.3931\n",
      "Epoch [215/300], Step [155/172], Loss: 6.4873\n",
      "Epoch [215/300], Step [156/172], Loss: 13.8550\n",
      "Epoch [215/300], Step [157/172], Loss: 8.6271\n",
      "Epoch [215/300], Step [158/172], Loss: 7.0780\n",
      "Epoch [215/300], Step [159/172], Loss: 9.6042\n",
      "Epoch [215/300], Step [160/172], Loss: 9.2022\n",
      "Epoch [215/300], Step [161/172], Loss: 7.8560\n",
      "Epoch [215/300], Step [162/172], Loss: 5.2575\n",
      "Epoch [215/300], Step [163/172], Loss: 6.5719\n",
      "Epoch [215/300], Step [164/172], Loss: 8.8017\n",
      "Epoch [215/300], Step [165/172], Loss: 7.0464\n",
      "Epoch [215/300], Step [166/172], Loss: 6.2405\n",
      "Epoch [215/300], Step [167/172], Loss: 10.1903\n",
      "Epoch [215/300], Step [168/172], Loss: 6.8514\n",
      "Epoch [215/300], Step [169/172], Loss: 7.0554\n",
      "Epoch [215/300], Step [170/172], Loss: 5.5521\n",
      "Epoch [215/300], Step [171/172], Loss: 8.2640\n",
      "Epoch [215/300], Step [172/172], Loss: 6.3401\n",
      "Epoch [216/300], Step [1/172], Loss: 48.1499\n",
      "Epoch [216/300], Step [2/172], Loss: 52.5149\n",
      "Epoch [216/300], Step [3/172], Loss: 48.5241\n",
      "Epoch [216/300], Step [4/172], Loss: 25.9056\n",
      "Epoch [216/300], Step [5/172], Loss: 42.4566\n",
      "Epoch [216/300], Step [6/172], Loss: 20.1714\n",
      "Epoch [216/300], Step [7/172], Loss: 29.4861\n",
      "Epoch [216/300], Step [8/172], Loss: 6.1311\n",
      "Epoch [216/300], Step [9/172], Loss: 29.6314\n",
      "Epoch [216/300], Step [10/172], Loss: 40.9322\n",
      "Epoch [216/300], Step [11/172], Loss: 53.8738\n",
      "Epoch [216/300], Step [12/172], Loss: 58.7201\n",
      "Epoch [216/300], Step [13/172], Loss: 35.0348\n",
      "Epoch [216/300], Step [14/172], Loss: 60.9851\n",
      "Epoch [216/300], Step [15/172], Loss: 50.7536\n",
      "Epoch [216/300], Step [16/172], Loss: 9.5211\n",
      "Epoch [216/300], Step [17/172], Loss: 41.5507\n",
      "Epoch [216/300], Step [18/172], Loss: 53.4628\n",
      "Epoch [216/300], Step [19/172], Loss: 75.3358\n",
      "Epoch [216/300], Step [20/172], Loss: 30.1040\n",
      "Epoch [216/300], Step [21/172], Loss: 83.0779\n",
      "Epoch [216/300], Step [22/172], Loss: 56.3395\n",
      "Epoch [216/300], Step [23/172], Loss: 2.1063\n",
      "Epoch [216/300], Step [24/172], Loss: 54.2641\n",
      "Epoch [216/300], Step [25/172], Loss: 39.3571\n",
      "Epoch [216/300], Step [26/172], Loss: 46.0194\n",
      "Epoch [216/300], Step [27/172], Loss: 58.0436\n",
      "Epoch [216/300], Step [28/172], Loss: 24.0988\n",
      "Epoch [216/300], Step [29/172], Loss: 15.8593\n",
      "Epoch [216/300], Step [30/172], Loss: 58.3339\n",
      "Epoch [216/300], Step [31/172], Loss: 36.3969\n",
      "Epoch [216/300], Step [32/172], Loss: 44.7230\n",
      "Epoch [216/300], Step [33/172], Loss: 71.7762\n",
      "Epoch [216/300], Step [34/172], Loss: 2.7894\n",
      "Epoch [216/300], Step [35/172], Loss: 14.7098\n",
      "Epoch [216/300], Step [36/172], Loss: 17.0458\n",
      "Epoch [216/300], Step [37/172], Loss: 17.2045\n",
      "Epoch [216/300], Step [38/172], Loss: 33.2589\n",
      "Epoch [216/300], Step [39/172], Loss: 37.8710\n",
      "Epoch [216/300], Step [40/172], Loss: 23.9131\n",
      "Epoch [216/300], Step [41/172], Loss: 36.7844\n",
      "Epoch [216/300], Step [42/172], Loss: 41.8209\n",
      "Epoch [216/300], Step [43/172], Loss: 30.6024\n",
      "Epoch [216/300], Step [44/172], Loss: 22.9438\n",
      "Epoch [216/300], Step [45/172], Loss: 33.2059\n",
      "Epoch [216/300], Step [46/172], Loss: 18.2125\n",
      "Epoch [216/300], Step [47/172], Loss: 52.0988\n",
      "Epoch [216/300], Step [48/172], Loss: 60.7040\n",
      "Epoch [216/300], Step [49/172], Loss: 24.9925\n",
      "Epoch [216/300], Step [50/172], Loss: 44.6174\n",
      "Epoch [216/300], Step [51/172], Loss: 10.6143\n",
      "Epoch [216/300], Step [52/172], Loss: 23.1598\n",
      "Epoch [216/300], Step [53/172], Loss: 25.2082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [216/300], Step [54/172], Loss: 17.8946\n",
      "Epoch [216/300], Step [55/172], Loss: 17.0187\n",
      "Epoch [216/300], Step [56/172], Loss: 20.1975\n",
      "Epoch [216/300], Step [57/172], Loss: 16.9786\n",
      "Epoch [216/300], Step [58/172], Loss: 14.2730\n",
      "Epoch [216/300], Step [59/172], Loss: 26.5568\n",
      "Epoch [216/300], Step [60/172], Loss: 18.8304\n",
      "Epoch [216/300], Step [61/172], Loss: 6.5356\n",
      "Epoch [216/300], Step [62/172], Loss: 16.8739\n",
      "Epoch [216/300], Step [63/172], Loss: 11.3865\n",
      "Epoch [216/300], Step [64/172], Loss: 11.8898\n",
      "Epoch [216/300], Step [65/172], Loss: 17.8151\n",
      "Epoch [216/300], Step [66/172], Loss: 6.8005\n",
      "Epoch [216/300], Step [67/172], Loss: 21.5050\n",
      "Epoch [216/300], Step [68/172], Loss: 4.1522\n",
      "Epoch [216/300], Step [69/172], Loss: 30.6975\n",
      "Epoch [216/300], Step [70/172], Loss: 33.5961\n",
      "Epoch [216/300], Step [71/172], Loss: 36.4001\n",
      "Epoch [216/300], Step [72/172], Loss: 34.7288\n",
      "Epoch [216/300], Step [73/172], Loss: 44.2416\n",
      "Epoch [216/300], Step [74/172], Loss: 22.5385\n",
      "Epoch [216/300], Step [75/172], Loss: 21.8011\n",
      "Epoch [216/300], Step [76/172], Loss: 26.5839\n",
      "Epoch [216/300], Step [77/172], Loss: 42.9965\n",
      "Epoch [216/300], Step [78/172], Loss: 33.2625\n",
      "Epoch [216/300], Step [79/172], Loss: 32.2294\n",
      "Epoch [216/300], Step [80/172], Loss: 46.4322\n",
      "Epoch [216/300], Step [81/172], Loss: 30.0026\n",
      "Epoch [216/300], Step [82/172], Loss: 34.3107\n",
      "Epoch [216/300], Step [83/172], Loss: 41.0799\n",
      "Epoch [216/300], Step [84/172], Loss: 31.4770\n",
      "Epoch [216/300], Step [85/172], Loss: 37.1204\n",
      "Epoch [216/300], Step [86/172], Loss: 32.0323\n",
      "Epoch [216/300], Step [87/172], Loss: 24.0907\n",
      "Epoch [216/300], Step [88/172], Loss: 21.5819\n",
      "Epoch [216/300], Step [89/172], Loss: 27.2539\n",
      "Epoch [216/300], Step [90/172], Loss: 19.4727\n",
      "Epoch [216/300], Step [91/172], Loss: 25.2552\n",
      "Epoch [216/300], Step [92/172], Loss: 18.5322\n",
      "Epoch [216/300], Step [93/172], Loss: 19.0760\n",
      "Epoch [216/300], Step [94/172], Loss: 26.6265\n",
      "Epoch [216/300], Step [95/172], Loss: 20.1709\n",
      "Epoch [216/300], Step [96/172], Loss: 19.9965\n",
      "Epoch [216/300], Step [97/172], Loss: 27.9476\n",
      "Epoch [216/300], Step [98/172], Loss: 19.6616\n",
      "Epoch [216/300], Step [99/172], Loss: 19.2563\n",
      "Epoch [216/300], Step [100/172], Loss: 17.3789\n",
      "Epoch [216/300], Step [101/172], Loss: 19.5999\n",
      "Epoch [216/300], Step [102/172], Loss: 16.9929\n",
      "Epoch [216/300], Step [103/172], Loss: 13.3741\n",
      "Epoch [216/300], Step [104/172], Loss: 19.5706\n",
      "Epoch [216/300], Step [105/172], Loss: 22.3278\n",
      "Epoch [216/300], Step [106/172], Loss: 16.5675\n",
      "Epoch [216/300], Step [107/172], Loss: 16.3803\n",
      "Epoch [216/300], Step [108/172], Loss: 15.5954\n",
      "Epoch [216/300], Step [109/172], Loss: 14.9327\n",
      "Epoch [216/300], Step [110/172], Loss: 17.2634\n",
      "Epoch [216/300], Step [111/172], Loss: 18.0978\n",
      "Epoch [216/300], Step [112/172], Loss: 16.9555\n",
      "Epoch [216/300], Step [113/172], Loss: 13.2907\n",
      "Epoch [216/300], Step [114/172], Loss: 14.5867\n",
      "Epoch [216/300], Step [115/172], Loss: 18.5553\n",
      "Epoch [216/300], Step [116/172], Loss: 14.9052\n",
      "Epoch [216/300], Step [117/172], Loss: 12.3674\n",
      "Epoch [216/300], Step [118/172], Loss: 12.7065\n",
      "Epoch [216/300], Step [119/172], Loss: 16.5548\n",
      "Epoch [216/300], Step [120/172], Loss: 11.0377\n",
      "Epoch [216/300], Step [121/172], Loss: 10.4441\n",
      "Epoch [216/300], Step [122/172], Loss: 12.6361\n",
      "Epoch [216/300], Step [123/172], Loss: 10.8758\n",
      "Epoch [216/300], Step [124/172], Loss: 7.7459\n",
      "Epoch [216/300], Step [125/172], Loss: 12.4827\n",
      "Epoch [216/300], Step [126/172], Loss: 11.9220\n",
      "Epoch [216/300], Step [127/172], Loss: 11.0283\n",
      "Epoch [216/300], Step [128/172], Loss: 10.8949\n",
      "Epoch [216/300], Step [129/172], Loss: 9.1309\n",
      "Epoch [216/300], Step [130/172], Loss: 12.9929\n",
      "Epoch [216/300], Step [131/172], Loss: 8.0888\n",
      "Epoch [216/300], Step [132/172], Loss: 9.9063\n",
      "Epoch [216/300], Step [133/172], Loss: 10.2099\n",
      "Epoch [216/300], Step [134/172], Loss: 11.1819\n",
      "Epoch [216/300], Step [135/172], Loss: 9.1825\n",
      "Epoch [216/300], Step [136/172], Loss: 8.1053\n",
      "Epoch [216/300], Step [137/172], Loss: 9.4052\n",
      "Epoch [216/300], Step [138/172], Loss: 7.9335\n",
      "Epoch [216/300], Step [139/172], Loss: 9.8405\n",
      "Epoch [216/300], Step [140/172], Loss: 10.6322\n",
      "Epoch [216/300], Step [141/172], Loss: 9.5008\n",
      "Epoch [216/300], Step [142/172], Loss: 14.0187\n",
      "Epoch [216/300], Step [143/172], Loss: 11.2039\n",
      "Epoch [216/300], Step [144/172], Loss: 9.3994\n",
      "Epoch [216/300], Step [145/172], Loss: 10.9376\n",
      "Epoch [216/300], Step [146/172], Loss: 10.0716\n",
      "Epoch [216/300], Step [147/172], Loss: 5.9459\n",
      "Epoch [216/300], Step [148/172], Loss: 6.8313\n",
      "Epoch [216/300], Step [149/172], Loss: 7.2666\n",
      "Epoch [216/300], Step [150/172], Loss: 6.1691\n",
      "Epoch [216/300], Step [151/172], Loss: 5.6654\n",
      "Epoch [216/300], Step [152/172], Loss: 8.2131\n",
      "Epoch [216/300], Step [153/172], Loss: 6.7028\n",
      "Epoch [216/300], Step [154/172], Loss: 7.3647\n",
      "Epoch [216/300], Step [155/172], Loss: 6.5735\n",
      "Epoch [216/300], Step [156/172], Loss: 13.7976\n",
      "Epoch [216/300], Step [157/172], Loss: 8.5936\n",
      "Epoch [216/300], Step [158/172], Loss: 7.1012\n",
      "Epoch [216/300], Step [159/172], Loss: 9.4992\n",
      "Epoch [216/300], Step [160/172], Loss: 9.1034\n",
      "Epoch [216/300], Step [161/172], Loss: 7.9778\n",
      "Epoch [216/300], Step [162/172], Loss: 5.2141\n",
      "Epoch [216/300], Step [163/172], Loss: 6.5054\n",
      "Epoch [216/300], Step [164/172], Loss: 9.0942\n",
      "Epoch [216/300], Step [165/172], Loss: 7.0971\n",
      "Epoch [216/300], Step [166/172], Loss: 6.2765\n",
      "Epoch [216/300], Step [167/172], Loss: 10.1046\n",
      "Epoch [216/300], Step [168/172], Loss: 6.8950\n",
      "Epoch [216/300], Step [169/172], Loss: 7.1130\n",
      "Epoch [216/300], Step [170/172], Loss: 5.6054\n",
      "Epoch [216/300], Step [171/172], Loss: 7.9831\n",
      "Epoch [216/300], Step [172/172], Loss: 6.4356\n",
      "Epoch [217/300], Step [1/172], Loss: 48.0040\n",
      "Epoch [217/300], Step [2/172], Loss: 52.4743\n",
      "Epoch [217/300], Step [3/172], Loss: 48.7824\n",
      "Epoch [217/300], Step [4/172], Loss: 25.6055\n",
      "Epoch [217/300], Step [5/172], Loss: 42.3653\n",
      "Epoch [217/300], Step [6/172], Loss: 20.3532\n",
      "Epoch [217/300], Step [7/172], Loss: 28.8835\n",
      "Epoch [217/300], Step [8/172], Loss: 4.4379\n",
      "Epoch [217/300], Step [9/172], Loss: 29.1677\n",
      "Epoch [217/300], Step [10/172], Loss: 41.2065\n",
      "Epoch [217/300], Step [11/172], Loss: 53.7699\n",
      "Epoch [217/300], Step [12/172], Loss: 58.1824\n",
      "Epoch [217/300], Step [13/172], Loss: 34.9046\n",
      "Epoch [217/300], Step [14/172], Loss: 61.1101\n",
      "Epoch [217/300], Step [15/172], Loss: 51.0001\n",
      "Epoch [217/300], Step [16/172], Loss: 11.3929\n",
      "Epoch [217/300], Step [17/172], Loss: 41.0505\n",
      "Epoch [217/300], Step [18/172], Loss: 53.3777\n",
      "Epoch [217/300], Step [19/172], Loss: 74.9425\n",
      "Epoch [217/300], Step [20/172], Loss: 29.7826\n",
      "Epoch [217/300], Step [21/172], Loss: 82.9238\n",
      "Epoch [217/300], Step [22/172], Loss: 56.0090\n",
      "Epoch [217/300], Step [23/172], Loss: 1.8855\n",
      "Epoch [217/300], Step [24/172], Loss: 54.1891\n",
      "Epoch [217/300], Step [25/172], Loss: 39.4705\n",
      "Epoch [217/300], Step [26/172], Loss: 46.2739\n",
      "Epoch [217/300], Step [27/172], Loss: 58.0951\n",
      "Epoch [217/300], Step [28/172], Loss: 24.2443\n",
      "Epoch [217/300], Step [29/172], Loss: 15.8954\n",
      "Epoch [217/300], Step [30/172], Loss: 58.6757\n",
      "Epoch [217/300], Step [31/172], Loss: 36.7980\n",
      "Epoch [217/300], Step [32/172], Loss: 44.8672\n",
      "Epoch [217/300], Step [33/172], Loss: 71.9002\n",
      "Epoch [217/300], Step [34/172], Loss: 2.8200\n",
      "Epoch [217/300], Step [35/172], Loss: 14.8165\n",
      "Epoch [217/300], Step [36/172], Loss: 17.1282\n",
      "Epoch [217/300], Step [37/172], Loss: 17.4121\n",
      "Epoch [217/300], Step [38/172], Loss: 33.7524\n",
      "Epoch [217/300], Step [39/172], Loss: 38.0361\n",
      "Epoch [217/300], Step [40/172], Loss: 24.1608\n",
      "Epoch [217/300], Step [41/172], Loss: 37.1519\n",
      "Epoch [217/300], Step [42/172], Loss: 42.2685\n",
      "Epoch [217/300], Step [43/172], Loss: 30.7861\n",
      "Epoch [217/300], Step [44/172], Loss: 23.1601\n",
      "Epoch [217/300], Step [45/172], Loss: 33.6387\n",
      "Epoch [217/300], Step [46/172], Loss: 18.2650\n",
      "Epoch [217/300], Step [47/172], Loss: 52.2126\n",
      "Epoch [217/300], Step [48/172], Loss: 59.4241\n",
      "Epoch [217/300], Step [49/172], Loss: 25.2303\n",
      "Epoch [217/300], Step [50/172], Loss: 44.5053\n",
      "Epoch [217/300], Step [51/172], Loss: 10.7024\n",
      "Epoch [217/300], Step [52/172], Loss: 23.5265\n",
      "Epoch [217/300], Step [53/172], Loss: 25.6558\n",
      "Epoch [217/300], Step [54/172], Loss: 18.2678\n",
      "Epoch [217/300], Step [55/172], Loss: 17.3208\n",
      "Epoch [217/300], Step [56/172], Loss: 20.2190\n",
      "Epoch [217/300], Step [57/172], Loss: 16.9542\n",
      "Epoch [217/300], Step [58/172], Loss: 14.4703\n",
      "Epoch [217/300], Step [59/172], Loss: 27.0110\n",
      "Epoch [217/300], Step [60/172], Loss: 19.0679\n",
      "Epoch [217/300], Step [61/172], Loss: 6.7171\n",
      "Epoch [217/300], Step [62/172], Loss: 17.0245\n",
      "Epoch [217/300], Step [63/172], Loss: 11.4880\n",
      "Epoch [217/300], Step [64/172], Loss: 12.1197\n",
      "Epoch [217/300], Step [65/172], Loss: 18.1145\n",
      "Epoch [217/300], Step [66/172], Loss: 6.9309\n",
      "Epoch [217/300], Step [67/172], Loss: 21.3047\n",
      "Epoch [217/300], Step [68/172], Loss: 4.3857\n",
      "Epoch [217/300], Step [69/172], Loss: 30.3791\n",
      "Epoch [217/300], Step [70/172], Loss: 33.3176\n",
      "Epoch [217/300], Step [71/172], Loss: 36.1784\n",
      "Epoch [217/300], Step [72/172], Loss: 34.3262\n",
      "Epoch [217/300], Step [73/172], Loss: 43.7354\n",
      "Epoch [217/300], Step [74/172], Loss: 22.3851\n",
      "Epoch [217/300], Step [75/172], Loss: 21.6448\n",
      "Epoch [217/300], Step [76/172], Loss: 26.7907\n",
      "Epoch [217/300], Step [77/172], Loss: 42.6647\n",
      "Epoch [217/300], Step [78/172], Loss: 33.0350\n",
      "Epoch [217/300], Step [79/172], Loss: 32.0592\n",
      "Epoch [217/300], Step [80/172], Loss: 46.0080\n",
      "Epoch [217/300], Step [81/172], Loss: 29.7485\n",
      "Epoch [217/300], Step [82/172], Loss: 33.7235\n",
      "Epoch [217/300], Step [83/172], Loss: 40.9341\n",
      "Epoch [217/300], Step [84/172], Loss: 31.4205\n",
      "Epoch [217/300], Step [85/172], Loss: 36.9404\n",
      "Epoch [217/300], Step [86/172], Loss: 31.9835\n",
      "Epoch [217/300], Step [87/172], Loss: 24.0463\n",
      "Epoch [217/300], Step [88/172], Loss: 21.5224\n",
      "Epoch [217/300], Step [89/172], Loss: 27.2715\n",
      "Epoch [217/300], Step [90/172], Loss: 19.5189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [217/300], Step [91/172], Loss: 25.2800\n",
      "Epoch [217/300], Step [92/172], Loss: 18.5326\n",
      "Epoch [217/300], Step [93/172], Loss: 19.0486\n",
      "Epoch [217/300], Step [94/172], Loss: 26.4786\n",
      "Epoch [217/300], Step [95/172], Loss: 20.2411\n",
      "Epoch [217/300], Step [96/172], Loss: 20.1150\n",
      "Epoch [217/300], Step [97/172], Loss: 28.0579\n",
      "Epoch [217/300], Step [98/172], Loss: 19.7512\n",
      "Epoch [217/300], Step [99/172], Loss: 19.3572\n",
      "Epoch [217/300], Step [100/172], Loss: 17.5936\n",
      "Epoch [217/300], Step [101/172], Loss: 19.7344\n",
      "Epoch [217/300], Step [102/172], Loss: 16.9130\n",
      "Epoch [217/300], Step [103/172], Loss: 13.5222\n",
      "Epoch [217/300], Step [104/172], Loss: 19.6731\n",
      "Epoch [217/300], Step [105/172], Loss: 22.4593\n",
      "Epoch [217/300], Step [106/172], Loss: 16.6545\n",
      "Epoch [217/300], Step [107/172], Loss: 16.4877\n",
      "Epoch [217/300], Step [108/172], Loss: 15.7297\n",
      "Epoch [217/300], Step [109/172], Loss: 14.9862\n",
      "Epoch [217/300], Step [110/172], Loss: 17.3214\n",
      "Epoch [217/300], Step [111/172], Loss: 18.3055\n",
      "Epoch [217/300], Step [112/172], Loss: 16.8731\n",
      "Epoch [217/300], Step [113/172], Loss: 13.4581\n",
      "Epoch [217/300], Step [114/172], Loss: 14.5957\n",
      "Epoch [217/300], Step [115/172], Loss: 18.5309\n",
      "Epoch [217/300], Step [116/172], Loss: 14.9326\n",
      "Epoch [217/300], Step [117/172], Loss: 12.3237\n",
      "Epoch [217/300], Step [118/172], Loss: 12.4539\n",
      "Epoch [217/300], Step [119/172], Loss: 16.7290\n",
      "Epoch [217/300], Step [120/172], Loss: 11.1226\n",
      "Epoch [217/300], Step [121/172], Loss: 10.4980\n",
      "Epoch [217/300], Step [122/172], Loss: 12.5851\n",
      "Epoch [217/300], Step [123/172], Loss: 10.9319\n",
      "Epoch [217/300], Step [124/172], Loss: 7.7874\n",
      "Epoch [217/300], Step [125/172], Loss: 12.5715\n",
      "Epoch [217/300], Step [126/172], Loss: 12.1924\n",
      "Epoch [217/300], Step [127/172], Loss: 10.9821\n",
      "Epoch [217/300], Step [128/172], Loss: 10.8467\n",
      "Epoch [217/300], Step [129/172], Loss: 9.1441\n",
      "Epoch [217/300], Step [130/172], Loss: 13.0213\n",
      "Epoch [217/300], Step [131/172], Loss: 8.1053\n",
      "Epoch [217/300], Step [132/172], Loss: 9.9466\n",
      "Epoch [217/300], Step [133/172], Loss: 10.2215\n",
      "Epoch [217/300], Step [134/172], Loss: 11.1855\n",
      "Epoch [217/300], Step [135/172], Loss: 9.2668\n",
      "Epoch [217/300], Step [136/172], Loss: 8.4187\n",
      "Epoch [217/300], Step [137/172], Loss: 9.3896\n",
      "Epoch [217/300], Step [138/172], Loss: 8.0935\n",
      "Epoch [217/300], Step [139/172], Loss: 9.9197\n",
      "Epoch [217/300], Step [140/172], Loss: 10.7136\n",
      "Epoch [217/300], Step [141/172], Loss: 9.6385\n",
      "Epoch [217/300], Step [142/172], Loss: 13.9465\n",
      "Epoch [217/300], Step [143/172], Loss: 11.2209\n",
      "Epoch [217/300], Step [144/172], Loss: 9.4229\n",
      "Epoch [217/300], Step [145/172], Loss: 10.8893\n",
      "Epoch [217/300], Step [146/172], Loss: 10.1413\n",
      "Epoch [217/300], Step [147/172], Loss: 6.0017\n",
      "Epoch [217/300], Step [148/172], Loss: 6.8903\n",
      "Epoch [217/300], Step [149/172], Loss: 7.3614\n",
      "Epoch [217/300], Step [150/172], Loss: 6.2566\n",
      "Epoch [217/300], Step [151/172], Loss: 5.7705\n",
      "Epoch [217/300], Step [152/172], Loss: 8.3466\n",
      "Epoch [217/300], Step [153/172], Loss: 6.7574\n",
      "Epoch [217/300], Step [154/172], Loss: 7.3100\n",
      "Epoch [217/300], Step [155/172], Loss: 6.6097\n",
      "Epoch [217/300], Step [156/172], Loss: 13.8425\n",
      "Epoch [217/300], Step [157/172], Loss: 8.5450\n",
      "Epoch [217/300], Step [158/172], Loss: 7.1186\n",
      "Epoch [217/300], Step [159/172], Loss: 9.3980\n",
      "Epoch [217/300], Step [160/172], Loss: 9.0719\n",
      "Epoch [217/300], Step [161/172], Loss: 8.1257\n",
      "Epoch [217/300], Step [162/172], Loss: 5.2708\n",
      "Epoch [217/300], Step [163/172], Loss: 6.5597\n",
      "Epoch [217/300], Step [164/172], Loss: 9.0260\n",
      "Epoch [217/300], Step [165/172], Loss: 7.1289\n",
      "Epoch [217/300], Step [166/172], Loss: 6.2508\n",
      "Epoch [217/300], Step [167/172], Loss: 9.9493\n",
      "Epoch [217/300], Step [168/172], Loss: 6.8769\n",
      "Epoch [217/300], Step [169/172], Loss: 7.1333\n",
      "Epoch [217/300], Step [170/172], Loss: 5.6910\n",
      "Epoch [217/300], Step [171/172], Loss: 8.0941\n",
      "Epoch [217/300], Step [172/172], Loss: 6.5239\n",
      "Epoch [218/300], Step [1/172], Loss: 47.9829\n",
      "Epoch [218/300], Step [2/172], Loss: 52.8655\n",
      "Epoch [218/300], Step [3/172], Loss: 46.9279\n",
      "Epoch [218/300], Step [4/172], Loss: 25.6565\n",
      "Epoch [218/300], Step [5/172], Loss: 43.2614\n",
      "Epoch [218/300], Step [6/172], Loss: 20.4287\n",
      "Epoch [218/300], Step [7/172], Loss: 29.4844\n",
      "Epoch [218/300], Step [8/172], Loss: 5.2145\n",
      "Epoch [218/300], Step [9/172], Loss: 29.2732\n",
      "Epoch [218/300], Step [10/172], Loss: 40.7522\n",
      "Epoch [218/300], Step [11/172], Loss: 53.5344\n",
      "Epoch [218/300], Step [12/172], Loss: 58.2329\n",
      "Epoch [218/300], Step [13/172], Loss: 34.9204\n",
      "Epoch [218/300], Step [14/172], Loss: 61.0385\n",
      "Epoch [218/300], Step [15/172], Loss: 50.4722\n",
      "Epoch [218/300], Step [16/172], Loss: 9.6561\n",
      "Epoch [218/300], Step [17/172], Loss: 41.2836\n",
      "Epoch [218/300], Step [18/172], Loss: 52.9389\n",
      "Epoch [218/300], Step [19/172], Loss: 75.2291\n",
      "Epoch [218/300], Step [20/172], Loss: 28.9544\n",
      "Epoch [218/300], Step [21/172], Loss: 83.8711\n",
      "Epoch [218/300], Step [22/172], Loss: 55.9289\n",
      "Epoch [218/300], Step [23/172], Loss: 2.3715\n",
      "Epoch [218/300], Step [24/172], Loss: 54.0758\n",
      "Epoch [218/300], Step [25/172], Loss: 39.3150\n",
      "Epoch [218/300], Step [26/172], Loss: 45.9790\n",
      "Epoch [218/300], Step [27/172], Loss: 57.6380\n",
      "Epoch [218/300], Step [28/172], Loss: 23.9995\n",
      "Epoch [218/300], Step [29/172], Loss: 15.7430\n",
      "Epoch [218/300], Step [30/172], Loss: 58.1273\n",
      "Epoch [218/300], Step [31/172], Loss: 36.5610\n",
      "Epoch [218/300], Step [32/172], Loss: 44.8850\n",
      "Epoch [218/300], Step [33/172], Loss: 71.8597\n",
      "Epoch [218/300], Step [34/172], Loss: 2.7843\n",
      "Epoch [218/300], Step [35/172], Loss: 14.6886\n",
      "Epoch [218/300], Step [36/172], Loss: 17.0683\n",
      "Epoch [218/300], Step [37/172], Loss: 17.3994\n",
      "Epoch [218/300], Step [38/172], Loss: 33.8450\n",
      "Epoch [218/300], Step [39/172], Loss: 37.8329\n",
      "Epoch [218/300], Step [40/172], Loss: 24.2819\n",
      "Epoch [218/300], Step [41/172], Loss: 37.0776\n",
      "Epoch [218/300], Step [42/172], Loss: 42.4838\n",
      "Epoch [218/300], Step [43/172], Loss: 30.9205\n",
      "Epoch [218/300], Step [44/172], Loss: 23.2659\n",
      "Epoch [218/300], Step [45/172], Loss: 33.6558\n",
      "Epoch [218/300], Step [46/172], Loss: 18.4358\n",
      "Epoch [218/300], Step [47/172], Loss: 52.8551\n",
      "Epoch [218/300], Step [48/172], Loss: 60.7118\n",
      "Epoch [218/300], Step [49/172], Loss: 25.5587\n",
      "Epoch [218/300], Step [50/172], Loss: 44.6792\n",
      "Epoch [218/300], Step [51/172], Loss: 10.8392\n",
      "Epoch [218/300], Step [52/172], Loss: 23.8002\n",
      "Epoch [218/300], Step [53/172], Loss: 25.8174\n",
      "Epoch [218/300], Step [54/172], Loss: 18.5798\n",
      "Epoch [218/300], Step [55/172], Loss: 17.6143\n",
      "Epoch [218/300], Step [56/172], Loss: 20.1406\n",
      "Epoch [218/300], Step [57/172], Loss: 16.9346\n",
      "Epoch [218/300], Step [58/172], Loss: 14.7186\n",
      "Epoch [218/300], Step [59/172], Loss: 27.2218\n",
      "Epoch [218/300], Step [60/172], Loss: 18.8133\n",
      "Epoch [218/300], Step [61/172], Loss: 6.7557\n",
      "Epoch [218/300], Step [62/172], Loss: 17.0891\n",
      "Epoch [218/300], Step [63/172], Loss: 11.4885\n",
      "Epoch [218/300], Step [64/172], Loss: 12.0640\n",
      "Epoch [218/300], Step [65/172], Loss: 17.9348\n",
      "Epoch [218/300], Step [66/172], Loss: 6.9186\n",
      "Epoch [218/300], Step [67/172], Loss: 21.4571\n",
      "Epoch [218/300], Step [68/172], Loss: 4.3753\n",
      "Epoch [218/300], Step [69/172], Loss: 30.3127\n",
      "Epoch [218/300], Step [70/172], Loss: 33.3455\n",
      "Epoch [218/300], Step [71/172], Loss: 36.1707\n",
      "Epoch [218/300], Step [72/172], Loss: 34.1680\n",
      "Epoch [218/300], Step [73/172], Loss: 43.5076\n",
      "Epoch [218/300], Step [74/172], Loss: 22.3438\n",
      "Epoch [218/300], Step [75/172], Loss: 21.3207\n",
      "Epoch [218/300], Step [76/172], Loss: 26.8082\n",
      "Epoch [218/300], Step [77/172], Loss: 42.6521\n",
      "Epoch [218/300], Step [78/172], Loss: 32.9665\n",
      "Epoch [218/300], Step [79/172], Loss: 31.9983\n",
      "Epoch [218/300], Step [80/172], Loss: 46.2462\n",
      "Epoch [218/300], Step [81/172], Loss: 29.5050\n",
      "Epoch [218/300], Step [82/172], Loss: 34.1420\n",
      "Epoch [218/300], Step [83/172], Loss: 40.7818\n",
      "Epoch [218/300], Step [84/172], Loss: 31.2688\n",
      "Epoch [218/300], Step [85/172], Loss: 36.8558\n",
      "Epoch [218/300], Step [86/172], Loss: 32.0604\n",
      "Epoch [218/300], Step [87/172], Loss: 23.9081\n",
      "Epoch [218/300], Step [88/172], Loss: 21.5620\n",
      "Epoch [218/300], Step [89/172], Loss: 27.3764\n",
      "Epoch [218/300], Step [90/172], Loss: 19.6022\n",
      "Epoch [218/300], Step [91/172], Loss: 25.2664\n",
      "Epoch [218/300], Step [92/172], Loss: 18.5414\n",
      "Epoch [218/300], Step [93/172], Loss: 19.0449\n",
      "Epoch [218/300], Step [94/172], Loss: 26.4106\n",
      "Epoch [218/300], Step [95/172], Loss: 20.2751\n",
      "Epoch [218/300], Step [96/172], Loss: 20.1075\n",
      "Epoch [218/300], Step [97/172], Loss: 27.9784\n",
      "Epoch [218/300], Step [98/172], Loss: 19.8016\n",
      "Epoch [218/300], Step [99/172], Loss: 19.4079\n",
      "Epoch [218/300], Step [100/172], Loss: 17.6946\n",
      "Epoch [218/300], Step [101/172], Loss: 19.7038\n",
      "Epoch [218/300], Step [102/172], Loss: 17.0680\n",
      "Epoch [218/300], Step [103/172], Loss: 13.5885\n",
      "Epoch [218/300], Step [104/172], Loss: 19.6989\n",
      "Epoch [218/300], Step [105/172], Loss: 22.7108\n",
      "Epoch [218/300], Step [106/172], Loss: 16.7346\n",
      "Epoch [218/300], Step [107/172], Loss: 16.4217\n",
      "Epoch [218/300], Step [108/172], Loss: 15.8435\n",
      "Epoch [218/300], Step [109/172], Loss: 15.0470\n",
      "Epoch [218/300], Step [110/172], Loss: 17.4109\n",
      "Epoch [218/300], Step [111/172], Loss: 18.3967\n",
      "Epoch [218/300], Step [112/172], Loss: 16.8300\n",
      "Epoch [218/300], Step [113/172], Loss: 13.5467\n",
      "Epoch [218/300], Step [114/172], Loss: 14.6515\n",
      "Epoch [218/300], Step [115/172], Loss: 18.5529\n",
      "Epoch [218/300], Step [116/172], Loss: 14.8533\n",
      "Epoch [218/300], Step [117/172], Loss: 12.4073\n",
      "Epoch [218/300], Step [118/172], Loss: 12.5556\n",
      "Epoch [218/300], Step [119/172], Loss: 16.8340\n",
      "Epoch [218/300], Step [120/172], Loss: 11.1281\n",
      "Epoch [218/300], Step [121/172], Loss: 10.5340\n",
      "Epoch [218/300], Step [122/172], Loss: 12.7260\n",
      "Epoch [218/300], Step [123/172], Loss: 11.0521\n",
      "Epoch [218/300], Step [124/172], Loss: 7.7529\n",
      "Epoch [218/300], Step [125/172], Loss: 12.5586\n",
      "Epoch [218/300], Step [126/172], Loss: 12.1731\n",
      "Epoch [218/300], Step [127/172], Loss: 11.0814\n",
      "Epoch [218/300], Step [128/172], Loss: 10.9934\n",
      "Epoch [218/300], Step [129/172], Loss: 9.2691\n",
      "Epoch [218/300], Step [130/172], Loss: 13.0957\n",
      "Epoch [218/300], Step [131/172], Loss: 8.2101\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [218/300], Step [132/172], Loss: 10.0625\n",
      "Epoch [218/300], Step [133/172], Loss: 10.3450\n",
      "Epoch [218/300], Step [134/172], Loss: 11.2017\n",
      "Epoch [218/300], Step [135/172], Loss: 9.2734\n",
      "Epoch [218/300], Step [136/172], Loss: 8.2476\n",
      "Epoch [218/300], Step [137/172], Loss: 9.3163\n",
      "Epoch [218/300], Step [138/172], Loss: 8.2775\n",
      "Epoch [218/300], Step [139/172], Loss: 9.8520\n",
      "Epoch [218/300], Step [140/172], Loss: 10.7330\n",
      "Epoch [218/300], Step [141/172], Loss: 9.6821\n",
      "Epoch [218/300], Step [142/172], Loss: 14.0276\n",
      "Epoch [218/300], Step [143/172], Loss: 11.2173\n",
      "Epoch [218/300], Step [144/172], Loss: 9.4564\n",
      "Epoch [218/300], Step [145/172], Loss: 11.0213\n",
      "Epoch [218/300], Step [146/172], Loss: 10.2260\n",
      "Epoch [218/300], Step [147/172], Loss: 6.0680\n",
      "Epoch [218/300], Step [148/172], Loss: 6.9963\n",
      "Epoch [218/300], Step [149/172], Loss: 7.4263\n",
      "Epoch [218/300], Step [150/172], Loss: 6.2844\n",
      "Epoch [218/300], Step [151/172], Loss: 5.8361\n",
      "Epoch [218/300], Step [152/172], Loss: 8.4439\n",
      "Epoch [218/300], Step [153/172], Loss: 6.7890\n",
      "Epoch [218/300], Step [154/172], Loss: 7.3227\n",
      "Epoch [218/300], Step [155/172], Loss: 6.6032\n",
      "Epoch [218/300], Step [156/172], Loss: 13.9534\n",
      "Epoch [218/300], Step [157/172], Loss: 8.4512\n",
      "Epoch [218/300], Step [158/172], Loss: 7.1743\n",
      "Epoch [218/300], Step [159/172], Loss: 9.2748\n",
      "Epoch [218/300], Step [160/172], Loss: 9.1232\n",
      "Epoch [218/300], Step [161/172], Loss: 8.2213\n",
      "Epoch [218/300], Step [162/172], Loss: 5.2969\n",
      "Epoch [218/300], Step [163/172], Loss: 6.6210\n",
      "Epoch [218/300], Step [164/172], Loss: 9.1480\n",
      "Epoch [218/300], Step [165/172], Loss: 7.1618\n",
      "Epoch [218/300], Step [166/172], Loss: 6.3720\n",
      "Epoch [218/300], Step [167/172], Loss: 10.1019\n",
      "Epoch [218/300], Step [168/172], Loss: 6.9795\n",
      "Epoch [218/300], Step [169/172], Loss: 7.0890\n",
      "Epoch [218/300], Step [170/172], Loss: 5.7791\n",
      "Epoch [218/300], Step [171/172], Loss: 8.3532\n",
      "Epoch [218/300], Step [172/172], Loss: 6.5217\n",
      "Epoch [219/300], Step [1/172], Loss: 47.8884\n",
      "Epoch [219/300], Step [2/172], Loss: 52.6469\n",
      "Epoch [219/300], Step [3/172], Loss: 47.9220\n",
      "Epoch [219/300], Step [4/172], Loss: 25.4231\n",
      "Epoch [219/300], Step [5/172], Loss: 42.8817\n",
      "Epoch [219/300], Step [6/172], Loss: 20.9186\n",
      "Epoch [219/300], Step [7/172], Loss: 29.4042\n",
      "Epoch [219/300], Step [8/172], Loss: 4.6585\n",
      "Epoch [219/300], Step [9/172], Loss: 28.9329\n",
      "Epoch [219/300], Step [10/172], Loss: 41.3316\n",
      "Epoch [219/300], Step [11/172], Loss: 53.4210\n",
      "Epoch [219/300], Step [12/172], Loss: 58.0185\n",
      "Epoch [219/300], Step [13/172], Loss: 35.0537\n",
      "Epoch [219/300], Step [14/172], Loss: 60.7878\n",
      "Epoch [219/300], Step [15/172], Loss: 50.4621\n",
      "Epoch [219/300], Step [16/172], Loss: 10.6013\n",
      "Epoch [219/300], Step [17/172], Loss: 41.0876\n",
      "Epoch [219/300], Step [18/172], Loss: 52.7519\n",
      "Epoch [219/300], Step [19/172], Loss: 74.9978\n",
      "Epoch [219/300], Step [20/172], Loss: 28.9995\n",
      "Epoch [219/300], Step [21/172], Loss: 83.2727\n",
      "Epoch [219/300], Step [22/172], Loss: 55.3149\n",
      "Epoch [219/300], Step [23/172], Loss: 2.2370\n",
      "Epoch [219/300], Step [24/172], Loss: 54.2825\n",
      "Epoch [219/300], Step [25/172], Loss: 39.3926\n",
      "Epoch [219/300], Step [26/172], Loss: 45.9003\n",
      "Epoch [219/300], Step [27/172], Loss: 57.6207\n",
      "Epoch [219/300], Step [28/172], Loss: 23.9765\n",
      "Epoch [219/300], Step [29/172], Loss: 15.6384\n",
      "Epoch [219/300], Step [30/172], Loss: 58.2084\n",
      "Epoch [219/300], Step [31/172], Loss: 36.9155\n",
      "Epoch [219/300], Step [32/172], Loss: 44.8241\n",
      "Epoch [219/300], Step [33/172], Loss: 71.6876\n",
      "Epoch [219/300], Step [34/172], Loss: 2.8373\n",
      "Epoch [219/300], Step [35/172], Loss: 14.8306\n",
      "Epoch [219/300], Step [36/172], Loss: 17.0135\n",
      "Epoch [219/300], Step [37/172], Loss: 17.5211\n",
      "Epoch [219/300], Step [38/172], Loss: 34.2154\n",
      "Epoch [219/300], Step [39/172], Loss: 37.9028\n",
      "Epoch [219/300], Step [40/172], Loss: 24.4836\n",
      "Epoch [219/300], Step [41/172], Loss: 37.2994\n",
      "Epoch [219/300], Step [42/172], Loss: 42.9136\n",
      "Epoch [219/300], Step [43/172], Loss: 30.9110\n",
      "Epoch [219/300], Step [44/172], Loss: 23.4718\n",
      "Epoch [219/300], Step [45/172], Loss: 34.2076\n",
      "Epoch [219/300], Step [46/172], Loss: 18.3347\n",
      "Epoch [219/300], Step [47/172], Loss: 52.8864\n",
      "Epoch [219/300], Step [48/172], Loss: 60.3276\n",
      "Epoch [219/300], Step [49/172], Loss: 25.7682\n",
      "Epoch [219/300], Step [50/172], Loss: 45.0807\n",
      "Epoch [219/300], Step [51/172], Loss: 10.9700\n",
      "Epoch [219/300], Step [52/172], Loss: 24.0522\n",
      "Epoch [219/300], Step [53/172], Loss: 26.0602\n",
      "Epoch [219/300], Step [54/172], Loss: 18.8348\n",
      "Epoch [219/300], Step [55/172], Loss: 17.9566\n",
      "Epoch [219/300], Step [56/172], Loss: 20.2023\n",
      "Epoch [219/300], Step [57/172], Loss: 17.1001\n",
      "Epoch [219/300], Step [58/172], Loss: 14.9832\n",
      "Epoch [219/300], Step [59/172], Loss: 27.3668\n",
      "Epoch [219/300], Step [60/172], Loss: 18.8275\n",
      "Epoch [219/300], Step [61/172], Loss: 6.9360\n",
      "Epoch [219/300], Step [62/172], Loss: 17.0867\n",
      "Epoch [219/300], Step [63/172], Loss: 11.6847\n",
      "Epoch [219/300], Step [64/172], Loss: 12.4128\n",
      "Epoch [219/300], Step [65/172], Loss: 18.2927\n",
      "Epoch [219/300], Step [66/172], Loss: 7.0096\n",
      "Epoch [219/300], Step [67/172], Loss: 21.4408\n",
      "Epoch [219/300], Step [68/172], Loss: 4.6278\n",
      "Epoch [219/300], Step [69/172], Loss: 29.9443\n",
      "Epoch [219/300], Step [70/172], Loss: 33.2237\n",
      "Epoch [219/300], Step [71/172], Loss: 35.8825\n",
      "Epoch [219/300], Step [72/172], Loss: 33.8892\n",
      "Epoch [219/300], Step [73/172], Loss: 43.2901\n",
      "Epoch [219/300], Step [74/172], Loss: 22.0689\n",
      "Epoch [219/300], Step [75/172], Loss: 21.2366\n",
      "Epoch [219/300], Step [76/172], Loss: 26.9362\n",
      "Epoch [219/300], Step [77/172], Loss: 42.5417\n",
      "Epoch [219/300], Step [78/172], Loss: 32.7964\n",
      "Epoch [219/300], Step [79/172], Loss: 31.8669\n",
      "Epoch [219/300], Step [80/172], Loss: 46.2446\n",
      "Epoch [219/300], Step [81/172], Loss: 29.5327\n",
      "Epoch [219/300], Step [82/172], Loss: 34.1283\n",
      "Epoch [219/300], Step [83/172], Loss: 40.7338\n",
      "Epoch [219/300], Step [84/172], Loss: 31.3987\n",
      "Epoch [219/300], Step [85/172], Loss: 36.9621\n",
      "Epoch [219/300], Step [86/172], Loss: 32.1335\n",
      "Epoch [219/300], Step [87/172], Loss: 23.9147\n",
      "Epoch [219/300], Step [88/172], Loss: 21.4634\n",
      "Epoch [219/300], Step [89/172], Loss: 27.3870\n",
      "Epoch [219/300], Step [90/172], Loss: 19.5217\n",
      "Epoch [219/300], Step [91/172], Loss: 25.3919\n",
      "Epoch [219/300], Step [92/172], Loss: 18.5905\n",
      "Epoch [219/300], Step [93/172], Loss: 18.8878\n",
      "Epoch [219/300], Step [94/172], Loss: 26.3327\n",
      "Epoch [219/300], Step [95/172], Loss: 20.3156\n",
      "Epoch [219/300], Step [96/172], Loss: 20.0699\n",
      "Epoch [219/300], Step [97/172], Loss: 28.0257\n",
      "Epoch [219/300], Step [98/172], Loss: 19.8746\n",
      "Epoch [219/300], Step [99/172], Loss: 19.4877\n",
      "Epoch [219/300], Step [100/172], Loss: 17.8695\n",
      "Epoch [219/300], Step [101/172], Loss: 19.8389\n",
      "Epoch [219/300], Step [102/172], Loss: 17.2046\n",
      "Epoch [219/300], Step [103/172], Loss: 13.6426\n",
      "Epoch [219/300], Step [104/172], Loss: 19.8031\n",
      "Epoch [219/300], Step [105/172], Loss: 22.9150\n",
      "Epoch [219/300], Step [106/172], Loss: 16.7848\n",
      "Epoch [219/300], Step [107/172], Loss: 16.5720\n",
      "Epoch [219/300], Step [108/172], Loss: 16.0091\n",
      "Epoch [219/300], Step [109/172], Loss: 15.1371\n",
      "Epoch [219/300], Step [110/172], Loss: 17.5342\n",
      "Epoch [219/300], Step [111/172], Loss: 18.4786\n",
      "Epoch [219/300], Step [112/172], Loss: 16.8597\n",
      "Epoch [219/300], Step [113/172], Loss: 13.5207\n",
      "Epoch [219/300], Step [114/172], Loss: 14.6825\n",
      "Epoch [219/300], Step [115/172], Loss: 18.4122\n",
      "Epoch [219/300], Step [116/172], Loss: 14.8103\n",
      "Epoch [219/300], Step [117/172], Loss: 12.3911\n",
      "Epoch [219/300], Step [118/172], Loss: 12.5070\n",
      "Epoch [219/300], Step [119/172], Loss: 16.9739\n",
      "Epoch [219/300], Step [120/172], Loss: 11.2214\n",
      "Epoch [219/300], Step [121/172], Loss: 10.5805\n",
      "Epoch [219/300], Step [122/172], Loss: 12.8644\n",
      "Epoch [219/300], Step [123/172], Loss: 11.0926\n",
      "Epoch [219/300], Step [124/172], Loss: 7.8596\n",
      "Epoch [219/300], Step [125/172], Loss: 12.7882\n",
      "Epoch [219/300], Step [126/172], Loss: 12.2079\n",
      "Epoch [219/300], Step [127/172], Loss: 11.0722\n",
      "Epoch [219/300], Step [128/172], Loss: 10.9586\n",
      "Epoch [219/300], Step [129/172], Loss: 9.3143\n",
      "Epoch [219/300], Step [130/172], Loss: 13.0899\n",
      "Epoch [219/300], Step [131/172], Loss: 8.2883\n",
      "Epoch [219/300], Step [132/172], Loss: 10.0723\n",
      "Epoch [219/300], Step [133/172], Loss: 10.4132\n",
      "Epoch [219/300], Step [134/172], Loss: 11.2239\n",
      "Epoch [219/300], Step [135/172], Loss: 9.3708\n",
      "Epoch [219/300], Step [136/172], Loss: 8.4035\n",
      "Epoch [219/300], Step [137/172], Loss: 9.3672\n",
      "Epoch [219/300], Step [138/172], Loss: 8.2437\n",
      "Epoch [219/300], Step [139/172], Loss: 9.9392\n",
      "Epoch [219/300], Step [140/172], Loss: 10.7453\n",
      "Epoch [219/300], Step [141/172], Loss: 9.6912\n",
      "Epoch [219/300], Step [142/172], Loss: 13.9902\n",
      "Epoch [219/300], Step [143/172], Loss: 11.2573\n",
      "Epoch [219/300], Step [144/172], Loss: 9.4377\n",
      "Epoch [219/300], Step [145/172], Loss: 10.9907\n",
      "Epoch [219/300], Step [146/172], Loss: 10.1583\n",
      "Epoch [219/300], Step [147/172], Loss: 6.1113\n",
      "Epoch [219/300], Step [148/172], Loss: 7.0285\n",
      "Epoch [219/300], Step [149/172], Loss: 7.4966\n",
      "Epoch [219/300], Step [150/172], Loss: 6.3359\n",
      "Epoch [219/300], Step [151/172], Loss: 5.8649\n",
      "Epoch [219/300], Step [152/172], Loss: 8.5550\n",
      "Epoch [219/300], Step [153/172], Loss: 6.9029\n",
      "Epoch [219/300], Step [154/172], Loss: 7.2971\n",
      "Epoch [219/300], Step [155/172], Loss: 6.7328\n",
      "Epoch [219/300], Step [156/172], Loss: 13.9561\n",
      "Epoch [219/300], Step [157/172], Loss: 8.5008\n",
      "Epoch [219/300], Step [158/172], Loss: 7.2400\n",
      "Epoch [219/300], Step [159/172], Loss: 9.2105\n",
      "Epoch [219/300], Step [160/172], Loss: 9.0324\n",
      "Epoch [219/300], Step [161/172], Loss: 8.3754\n",
      "Epoch [219/300], Step [162/172], Loss: 5.3384\n",
      "Epoch [219/300], Step [163/172], Loss: 6.5904\n",
      "Epoch [219/300], Step [164/172], Loss: 9.2030\n",
      "Epoch [219/300], Step [165/172], Loss: 7.2053\n",
      "Epoch [219/300], Step [166/172], Loss: 6.3082\n",
      "Epoch [219/300], Step [167/172], Loss: 10.0326\n",
      "Epoch [219/300], Step [168/172], Loss: 6.9703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [219/300], Step [169/172], Loss: 7.2152\n",
      "Epoch [219/300], Step [170/172], Loss: 5.8762\n",
      "Epoch [219/300], Step [171/172], Loss: 8.1059\n",
      "Epoch [219/300], Step [172/172], Loss: 6.6677\n",
      "Epoch [220/300], Step [1/172], Loss: 47.7776\n",
      "Epoch [220/300], Step [2/172], Loss: 51.7810\n",
      "Epoch [220/300], Step [3/172], Loss: 48.0759\n",
      "Epoch [220/300], Step [4/172], Loss: 25.1229\n",
      "Epoch [220/300], Step [5/172], Loss: 43.3044\n",
      "Epoch [220/300], Step [6/172], Loss: 20.6990\n",
      "Epoch [220/300], Step [7/172], Loss: 27.7476\n",
      "Epoch [220/300], Step [8/172], Loss: 4.9381\n",
      "Epoch [220/300], Step [9/172], Loss: 28.8225\n",
      "Epoch [220/300], Step [10/172], Loss: 41.3025\n",
      "Epoch [220/300], Step [11/172], Loss: 53.6246\n",
      "Epoch [220/300], Step [12/172], Loss: 57.4946\n",
      "Epoch [220/300], Step [13/172], Loss: 34.7683\n",
      "Epoch [220/300], Step [14/172], Loss: 60.8879\n",
      "Epoch [220/300], Step [15/172], Loss: 50.5735\n",
      "Epoch [220/300], Step [16/172], Loss: 10.3585\n",
      "Epoch [220/300], Step [17/172], Loss: 41.0709\n",
      "Epoch [220/300], Step [18/172], Loss: 53.0352\n",
      "Epoch [220/300], Step [19/172], Loss: 74.3441\n",
      "Epoch [220/300], Step [20/172], Loss: 30.1518\n",
      "Epoch [220/300], Step [21/172], Loss: 82.5720\n",
      "Epoch [220/300], Step [22/172], Loss: 55.4737\n",
      "Epoch [220/300], Step [23/172], Loss: 1.7769\n",
      "Epoch [220/300], Step [24/172], Loss: 53.9673\n",
      "Epoch [220/300], Step [25/172], Loss: 38.9361\n",
      "Epoch [220/300], Step [26/172], Loss: 46.0649\n",
      "Epoch [220/300], Step [27/172], Loss: 58.4179\n",
      "Epoch [220/300], Step [28/172], Loss: 24.1708\n",
      "Epoch [220/300], Step [29/172], Loss: 15.7171\n",
      "Epoch [220/300], Step [30/172], Loss: 58.3994\n",
      "Epoch [220/300], Step [31/172], Loss: 36.6137\n",
      "Epoch [220/300], Step [32/172], Loss: 44.8278\n",
      "Epoch [220/300], Step [33/172], Loss: 73.0656\n",
      "Epoch [220/300], Step [34/172], Loss: 2.8691\n",
      "Epoch [220/300], Step [35/172], Loss: 14.8476\n",
      "Epoch [220/300], Step [36/172], Loss: 17.4298\n",
      "Epoch [220/300], Step [37/172], Loss: 17.5373\n",
      "Epoch [220/300], Step [38/172], Loss: 34.2444\n",
      "Epoch [220/300], Step [39/172], Loss: 38.3965\n",
      "Epoch [220/300], Step [40/172], Loss: 24.4833\n",
      "Epoch [220/300], Step [41/172], Loss: 37.4251\n",
      "Epoch [220/300], Step [42/172], Loss: 42.4454\n",
      "Epoch [220/300], Step [43/172], Loss: 31.0554\n",
      "Epoch [220/300], Step [44/172], Loss: 23.9926\n",
      "Epoch [220/300], Step [45/172], Loss: 34.0770\n",
      "Epoch [220/300], Step [46/172], Loss: 18.8831\n",
      "Epoch [220/300], Step [47/172], Loss: 53.5165\n",
      "Epoch [220/300], Step [48/172], Loss: 62.0959\n",
      "Epoch [220/300], Step [49/172], Loss: 26.5610\n",
      "Epoch [220/300], Step [50/172], Loss: 45.2892\n",
      "Epoch [220/300], Step [51/172], Loss: 11.2610\n",
      "Epoch [220/300], Step [52/172], Loss: 24.5487\n",
      "Epoch [220/300], Step [53/172], Loss: 27.0866\n",
      "Epoch [220/300], Step [54/172], Loss: 19.2657\n",
      "Epoch [220/300], Step [55/172], Loss: 18.2151\n",
      "Epoch [220/300], Step [56/172], Loss: 21.0599\n",
      "Epoch [220/300], Step [57/172], Loss: 19.3020\n",
      "Epoch [220/300], Step [58/172], Loss: 15.5563\n",
      "Epoch [220/300], Step [59/172], Loss: 27.4177\n",
      "Epoch [220/300], Step [60/172], Loss: 19.6964\n",
      "Epoch [220/300], Step [61/172], Loss: 7.2114\n",
      "Epoch [220/300], Step [62/172], Loss: 17.2271\n",
      "Epoch [220/300], Step [63/172], Loss: 12.3108\n",
      "Epoch [220/300], Step [64/172], Loss: 13.0550\n",
      "Epoch [220/300], Step [65/172], Loss: 18.1341\n",
      "Epoch [220/300], Step [66/172], Loss: 7.0837\n",
      "Epoch [220/300], Step [67/172], Loss: 21.7716\n",
      "Epoch [220/300], Step [68/172], Loss: 5.0024\n",
      "Epoch [220/300], Step [69/172], Loss: 29.4374\n",
      "Epoch [220/300], Step [70/172], Loss: 34.0683\n",
      "Epoch [220/300], Step [71/172], Loss: 35.7802\n",
      "Epoch [220/300], Step [72/172], Loss: 33.4952\n",
      "Epoch [220/300], Step [73/172], Loss: 43.6000\n",
      "Epoch [220/300], Step [74/172], Loss: 22.5187\n",
      "Epoch [220/300], Step [75/172], Loss: 21.2783\n",
      "Epoch [220/300], Step [76/172], Loss: 27.9362\n",
      "Epoch [220/300], Step [77/172], Loss: 42.9193\n",
      "Epoch [220/300], Step [78/172], Loss: 33.1307\n",
      "Epoch [220/300], Step [79/172], Loss: 31.8084\n",
      "Epoch [220/300], Step [80/172], Loss: 46.4716\n",
      "Epoch [220/300], Step [81/172], Loss: 29.2758\n",
      "Epoch [220/300], Step [82/172], Loss: 34.3872\n",
      "Epoch [220/300], Step [83/172], Loss: 40.7368\n",
      "Epoch [220/300], Step [84/172], Loss: 31.3262\n",
      "Epoch [220/300], Step [85/172], Loss: 36.7197\n",
      "Epoch [220/300], Step [86/172], Loss: 32.3540\n",
      "Epoch [220/300], Step [87/172], Loss: 23.9848\n",
      "Epoch [220/300], Step [88/172], Loss: 21.8049\n",
      "Epoch [220/300], Step [89/172], Loss: 27.5147\n",
      "Epoch [220/300], Step [90/172], Loss: 19.9379\n",
      "Epoch [220/300], Step [91/172], Loss: 25.3534\n",
      "Epoch [220/300], Step [92/172], Loss: 18.5606\n",
      "Epoch [220/300], Step [93/172], Loss: 19.0016\n",
      "Epoch [220/300], Step [94/172], Loss: 26.3270\n",
      "Epoch [220/300], Step [95/172], Loss: 20.8750\n",
      "Epoch [220/300], Step [96/172], Loss: 20.1807\n",
      "Epoch [220/300], Step [97/172], Loss: 28.1312\n",
      "Epoch [220/300], Step [98/172], Loss: 19.8578\n",
      "Epoch [220/300], Step [99/172], Loss: 19.5747\n",
      "Epoch [220/300], Step [100/172], Loss: 17.8566\n",
      "Epoch [220/300], Step [101/172], Loss: 19.8542\n",
      "Epoch [220/300], Step [102/172], Loss: 17.0946\n",
      "Epoch [220/300], Step [103/172], Loss: 13.7252\n",
      "Epoch [220/300], Step [104/172], Loss: 20.0027\n",
      "Epoch [220/300], Step [105/172], Loss: 22.9146\n",
      "Epoch [220/300], Step [106/172], Loss: 16.7928\n",
      "Epoch [220/300], Step [107/172], Loss: 16.5330\n",
      "Epoch [220/300], Step [108/172], Loss: 15.9908\n",
      "Epoch [220/300], Step [109/172], Loss: 15.0151\n",
      "Epoch [220/300], Step [110/172], Loss: 17.6982\n",
      "Epoch [220/300], Step [111/172], Loss: 18.5692\n",
      "Epoch [220/300], Step [112/172], Loss: 16.8801\n",
      "Epoch [220/300], Step [113/172], Loss: 13.8158\n",
      "Epoch [220/300], Step [114/172], Loss: 14.7847\n",
      "Epoch [220/300], Step [115/172], Loss: 18.3180\n",
      "Epoch [220/300], Step [116/172], Loss: 14.9270\n",
      "Epoch [220/300], Step [117/172], Loss: 12.5049\n",
      "Epoch [220/300], Step [118/172], Loss: 12.3342\n",
      "Epoch [220/300], Step [119/172], Loss: 17.1272\n",
      "Epoch [220/300], Step [120/172], Loss: 11.2635\n",
      "Epoch [220/300], Step [121/172], Loss: 10.6121\n",
      "Epoch [220/300], Step [122/172], Loss: 12.4499\n",
      "Epoch [220/300], Step [123/172], Loss: 11.1527\n",
      "Epoch [220/300], Step [124/172], Loss: 8.0169\n",
      "Epoch [220/300], Step [125/172], Loss: 12.9599\n",
      "Epoch [220/300], Step [126/172], Loss: 12.7082\n",
      "Epoch [220/300], Step [127/172], Loss: 11.2461\n",
      "Epoch [220/300], Step [128/172], Loss: 11.2458\n",
      "Epoch [220/300], Step [129/172], Loss: 9.5066\n",
      "Epoch [220/300], Step [130/172], Loss: 13.1908\n",
      "Epoch [220/300], Step [131/172], Loss: 8.3122\n",
      "Epoch [220/300], Step [132/172], Loss: 10.2688\n",
      "Epoch [220/300], Step [133/172], Loss: 10.2715\n",
      "Epoch [220/300], Step [134/172], Loss: 11.0367\n",
      "Epoch [220/300], Step [135/172], Loss: 9.5618\n",
      "Epoch [220/300], Step [136/172], Loss: 8.6772\n",
      "Epoch [220/300], Step [137/172], Loss: 9.3535\n",
      "Epoch [220/300], Step [138/172], Loss: 8.5253\n",
      "Epoch [220/300], Step [139/172], Loss: 10.1552\n",
      "Epoch [220/300], Step [140/172], Loss: 11.0678\n",
      "Epoch [220/300], Step [141/172], Loss: 9.8770\n",
      "Epoch [220/300], Step [142/172], Loss: 13.9786\n",
      "Epoch [220/300], Step [143/172], Loss: 11.2549\n",
      "Epoch [220/300], Step [144/172], Loss: 9.5265\n",
      "Epoch [220/300], Step [145/172], Loss: 11.1459\n",
      "Epoch [220/300], Step [146/172], Loss: 10.4951\n",
      "Epoch [220/300], Step [147/172], Loss: 6.2244\n",
      "Epoch [220/300], Step [148/172], Loss: 7.1902\n",
      "Epoch [220/300], Step [149/172], Loss: 7.6749\n",
      "Epoch [220/300], Step [150/172], Loss: 6.4140\n",
      "Epoch [220/300], Step [151/172], Loss: 6.0371\n",
      "Epoch [220/300], Step [152/172], Loss: 8.6885\n",
      "Epoch [220/300], Step [153/172], Loss: 7.0552\n",
      "Epoch [220/300], Step [154/172], Loss: 7.3239\n",
      "Epoch [220/300], Step [155/172], Loss: 6.9620\n",
      "Epoch [220/300], Step [156/172], Loss: 13.9444\n",
      "Epoch [220/300], Step [157/172], Loss: 8.3334\n",
      "Epoch [220/300], Step [158/172], Loss: 7.2816\n",
      "Epoch [220/300], Step [159/172], Loss: 9.3099\n",
      "Epoch [220/300], Step [160/172], Loss: 8.9346\n",
      "Epoch [220/300], Step [161/172], Loss: 8.7371\n",
      "Epoch [220/300], Step [162/172], Loss: 5.3894\n",
      "Epoch [220/300], Step [163/172], Loss: 6.6929\n",
      "Epoch [220/300], Step [164/172], Loss: 9.3034\n",
      "Epoch [220/300], Step [165/172], Loss: 7.3317\n",
      "Epoch [220/300], Step [166/172], Loss: 6.3737\n",
      "Epoch [220/300], Step [167/172], Loss: 9.9131\n",
      "Epoch [220/300], Step [168/172], Loss: 7.0321\n",
      "Epoch [220/300], Step [169/172], Loss: 7.0929\n",
      "Epoch [220/300], Step [170/172], Loss: 6.0214\n",
      "Epoch [220/300], Step [171/172], Loss: 8.3824\n",
      "Epoch [220/300], Step [172/172], Loss: 6.6584\n",
      "Epoch [221/300], Step [1/172], Loss: 47.8219\n",
      "Epoch [221/300], Step [2/172], Loss: 54.3127\n",
      "Epoch [221/300], Step [3/172], Loss: 46.6492\n",
      "Epoch [221/300], Step [4/172], Loss: 24.9558\n",
      "Epoch [221/300], Step [5/172], Loss: 43.7811\n",
      "Epoch [221/300], Step [6/172], Loss: 21.2273\n",
      "Epoch [221/300], Step [7/172], Loss: 30.3157\n",
      "Epoch [221/300], Step [8/172], Loss: 4.4806\n",
      "Epoch [221/300], Step [9/172], Loss: 28.6744\n",
      "Epoch [221/300], Step [10/172], Loss: 41.9898\n",
      "Epoch [221/300], Step [11/172], Loss: 53.0503\n",
      "Epoch [221/300], Step [12/172], Loss: 57.0549\n",
      "Epoch [221/300], Step [13/172], Loss: 34.7494\n",
      "Epoch [221/300], Step [14/172], Loss: 59.9951\n",
      "Epoch [221/300], Step [15/172], Loss: 49.5944\n",
      "Epoch [221/300], Step [16/172], Loss: 10.5102\n",
      "Epoch [221/300], Step [17/172], Loss: 40.9433\n",
      "Epoch [221/300], Step [18/172], Loss: 52.9380\n",
      "Epoch [221/300], Step [19/172], Loss: 74.4645\n",
      "Epoch [221/300], Step [20/172], Loss: 27.4380\n",
      "Epoch [221/300], Step [21/172], Loss: 82.7910\n",
      "Epoch [221/300], Step [22/172], Loss: 54.5790\n",
      "Epoch [221/300], Step [23/172], Loss: 2.3813\n",
      "Epoch [221/300], Step [24/172], Loss: 53.9104\n",
      "Epoch [221/300], Step [25/172], Loss: 39.4471\n",
      "Epoch [221/300], Step [26/172], Loss: 46.1860\n",
      "Epoch [221/300], Step [27/172], Loss: 57.3630\n",
      "Epoch [221/300], Step [28/172], Loss: 23.8499\n",
      "Epoch [221/300], Step [29/172], Loss: 15.3885\n",
      "Epoch [221/300], Step [30/172], Loss: 58.1794\n",
      "Epoch [221/300], Step [31/172], Loss: 36.8950\n",
      "Epoch [221/300], Step [32/172], Loss: 44.6540\n",
      "Epoch [221/300], Step [33/172], Loss: 71.7146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [221/300], Step [34/172], Loss: 2.7369\n",
      "Epoch [221/300], Step [35/172], Loss: 14.7809\n",
      "Epoch [221/300], Step [36/172], Loss: 16.9611\n",
      "Epoch [221/300], Step [37/172], Loss: 17.3841\n",
      "Epoch [221/300], Step [38/172], Loss: 33.9506\n",
      "Epoch [221/300], Step [39/172], Loss: 38.0328\n",
      "Epoch [221/300], Step [40/172], Loss: 24.6022\n",
      "Epoch [221/300], Step [41/172], Loss: 38.2220\n",
      "Epoch [221/300], Step [42/172], Loss: 43.1976\n",
      "Epoch [221/300], Step [43/172], Loss: 31.3084\n",
      "Epoch [221/300], Step [44/172], Loss: 24.1063\n",
      "Epoch [221/300], Step [45/172], Loss: 35.0025\n",
      "Epoch [221/300], Step [46/172], Loss: 18.5474\n",
      "Epoch [221/300], Step [47/172], Loss: 53.5308\n",
      "Epoch [221/300], Step [48/172], Loss: 60.8953\n",
      "Epoch [221/300], Step [49/172], Loss: 25.9623\n",
      "Epoch [221/300], Step [50/172], Loss: 45.4056\n",
      "Epoch [221/300], Step [51/172], Loss: 11.0775\n",
      "Epoch [221/300], Step [52/172], Loss: 24.2727\n",
      "Epoch [221/300], Step [53/172], Loss: 26.6262\n",
      "Epoch [221/300], Step [54/172], Loss: 19.5448\n",
      "Epoch [221/300], Step [55/172], Loss: 18.4519\n",
      "Epoch [221/300], Step [56/172], Loss: 20.0174\n",
      "Epoch [221/300], Step [57/172], Loss: 17.5328\n",
      "Epoch [221/300], Step [58/172], Loss: 15.3913\n",
      "Epoch [221/300], Step [59/172], Loss: 28.1255\n",
      "Epoch [221/300], Step [60/172], Loss: 18.7934\n",
      "Epoch [221/300], Step [61/172], Loss: 7.1812\n",
      "Epoch [221/300], Step [62/172], Loss: 17.0795\n",
      "Epoch [221/300], Step [63/172], Loss: 11.9279\n",
      "Epoch [221/300], Step [64/172], Loss: 12.6794\n",
      "Epoch [221/300], Step [65/172], Loss: 18.4680\n",
      "Epoch [221/300], Step [66/172], Loss: 7.3076\n",
      "Epoch [221/300], Step [67/172], Loss: 21.9956\n",
      "Epoch [221/300], Step [68/172], Loss: 4.3554\n",
      "Epoch [221/300], Step [69/172], Loss: 29.8626\n",
      "Epoch [221/300], Step [70/172], Loss: 33.5879\n",
      "Epoch [221/300], Step [71/172], Loss: 35.7857\n",
      "Epoch [221/300], Step [72/172], Loss: 33.6857\n",
      "Epoch [221/300], Step [73/172], Loss: 42.8607\n",
      "Epoch [221/300], Step [74/172], Loss: 22.1176\n",
      "Epoch [221/300], Step [75/172], Loss: 20.7485\n",
      "Epoch [221/300], Step [76/172], Loss: 27.5115\n",
      "Epoch [221/300], Step [77/172], Loss: 42.5000\n",
      "Epoch [221/300], Step [78/172], Loss: 32.6476\n",
      "Epoch [221/300], Step [79/172], Loss: 31.4516\n",
      "Epoch [221/300], Step [80/172], Loss: 46.2307\n",
      "Epoch [221/300], Step [81/172], Loss: 28.8166\n",
      "Epoch [221/300], Step [82/172], Loss: 33.9977\n",
      "Epoch [221/300], Step [83/172], Loss: 40.6777\n",
      "Epoch [221/300], Step [84/172], Loss: 31.2494\n",
      "Epoch [221/300], Step [85/172], Loss: 36.3649\n",
      "Epoch [221/300], Step [86/172], Loss: 32.0897\n",
      "Epoch [221/300], Step [87/172], Loss: 23.7107\n",
      "Epoch [221/300], Step [88/172], Loss: 21.4191\n",
      "Epoch [221/300], Step [89/172], Loss: 27.2700\n",
      "Epoch [221/300], Step [90/172], Loss: 19.6005\n",
      "Epoch [221/300], Step [91/172], Loss: 25.4363\n",
      "Epoch [221/300], Step [92/172], Loss: 18.5781\n",
      "Epoch [221/300], Step [93/172], Loss: 18.9304\n",
      "Epoch [221/300], Step [94/172], Loss: 26.0621\n",
      "Epoch [221/300], Step [95/172], Loss: 20.5337\n",
      "Epoch [221/300], Step [96/172], Loss: 20.1607\n",
      "Epoch [221/300], Step [97/172], Loss: 28.1996\n",
      "Epoch [221/300], Step [98/172], Loss: 19.7129\n",
      "Epoch [221/300], Step [99/172], Loss: 19.4921\n",
      "Epoch [221/300], Step [100/172], Loss: 17.8468\n",
      "Epoch [221/300], Step [101/172], Loss: 19.8880\n",
      "Epoch [221/300], Step [102/172], Loss: 17.3099\n",
      "Epoch [221/300], Step [103/172], Loss: 13.7190\n",
      "Epoch [221/300], Step [104/172], Loss: 19.8739\n",
      "Epoch [221/300], Step [105/172], Loss: 23.0013\n",
      "Epoch [221/300], Step [106/172], Loss: 16.7696\n",
      "Epoch [221/300], Step [107/172], Loss: 16.6665\n",
      "Epoch [221/300], Step [108/172], Loss: 16.1803\n",
      "Epoch [221/300], Step [109/172], Loss: 15.2492\n",
      "Epoch [221/300], Step [110/172], Loss: 18.0386\n",
      "Epoch [221/300], Step [111/172], Loss: 18.5939\n",
      "Epoch [221/300], Step [112/172], Loss: 16.9130\n",
      "Epoch [221/300], Step [113/172], Loss: 14.0561\n",
      "Epoch [221/300], Step [114/172], Loss: 14.8045\n",
      "Epoch [221/300], Step [115/172], Loss: 18.4704\n",
      "Epoch [221/300], Step [116/172], Loss: 15.0662\n",
      "Epoch [221/300], Step [117/172], Loss: 12.5135\n",
      "Epoch [221/300], Step [118/172], Loss: 12.4189\n",
      "Epoch [221/300], Step [119/172], Loss: 17.3660\n",
      "Epoch [221/300], Step [120/172], Loss: 11.4093\n",
      "Epoch [221/300], Step [121/172], Loss: 10.5218\n",
      "Epoch [221/300], Step [122/172], Loss: 13.1254\n",
      "Epoch [221/300], Step [123/172], Loss: 11.3385\n",
      "Epoch [221/300], Step [124/172], Loss: 7.8074\n",
      "Epoch [221/300], Step [125/172], Loss: 12.7220\n",
      "Epoch [221/300], Step [126/172], Loss: 12.5235\n",
      "Epoch [221/300], Step [127/172], Loss: 11.3601\n",
      "Epoch [221/300], Step [128/172], Loss: 11.1329\n",
      "Epoch [221/300], Step [129/172], Loss: 9.3084\n",
      "Epoch [221/300], Step [130/172], Loss: 13.0451\n",
      "Epoch [221/300], Step [131/172], Loss: 8.3428\n",
      "Epoch [221/300], Step [132/172], Loss: 10.1231\n",
      "Epoch [221/300], Step [133/172], Loss: 10.5266\n",
      "Epoch [221/300], Step [134/172], Loss: 11.2260\n",
      "Epoch [221/300], Step [135/172], Loss: 9.5086\n",
      "Epoch [221/300], Step [136/172], Loss: 8.7070\n",
      "Epoch [221/300], Step [137/172], Loss: 9.2652\n",
      "Epoch [221/300], Step [138/172], Loss: 8.5820\n",
      "Epoch [221/300], Step [139/172], Loss: 9.8814\n",
      "Epoch [221/300], Step [140/172], Loss: 11.0307\n",
      "Epoch [221/300], Step [141/172], Loss: 9.8635\n",
      "Epoch [221/300], Step [142/172], Loss: 14.3259\n",
      "Epoch [221/300], Step [143/172], Loss: 11.2872\n",
      "Epoch [221/300], Step [144/172], Loss: 9.4604\n",
      "Epoch [221/300], Step [145/172], Loss: 11.1714\n",
      "Epoch [221/300], Step [146/172], Loss: 10.6378\n",
      "Epoch [221/300], Step [147/172], Loss: 6.0837\n",
      "Epoch [221/300], Step [148/172], Loss: 7.1083\n",
      "Epoch [221/300], Step [149/172], Loss: 7.5871\n",
      "Epoch [221/300], Step [150/172], Loss: 6.3206\n",
      "Epoch [221/300], Step [151/172], Loss: 6.0320\n",
      "Epoch [221/300], Step [152/172], Loss: 8.7386\n",
      "Epoch [221/300], Step [153/172], Loss: 6.8594\n",
      "Epoch [221/300], Step [154/172], Loss: 7.2469\n",
      "Epoch [221/300], Step [155/172], Loss: 6.6971\n",
      "Epoch [221/300], Step [156/172], Loss: 14.0642\n",
      "Epoch [221/300], Step [157/172], Loss: 8.3578\n",
      "Epoch [221/300], Step [158/172], Loss: 7.3579\n",
      "Epoch [221/300], Step [159/172], Loss: 9.5509\n",
      "Epoch [221/300], Step [160/172], Loss: 9.1768\n",
      "Epoch [221/300], Step [161/172], Loss: 8.6104\n",
      "Epoch [221/300], Step [162/172], Loss: 5.3637\n",
      "Epoch [221/300], Step [163/172], Loss: 6.5955\n",
      "Epoch [221/300], Step [164/172], Loss: 9.1853\n",
      "Epoch [221/300], Step [165/172], Loss: 7.2641\n",
      "Epoch [221/300], Step [166/172], Loss: 6.2347\n",
      "Epoch [221/300], Step [167/172], Loss: 9.8846\n",
      "Epoch [221/300], Step [168/172], Loss: 6.8534\n",
      "Epoch [221/300], Step [169/172], Loss: 7.1444\n",
      "Epoch [221/300], Step [170/172], Loss: 6.0131\n",
      "Epoch [221/300], Step [171/172], Loss: 8.0178\n",
      "Epoch [221/300], Step [172/172], Loss: 6.4552\n",
      "Epoch [222/300], Step [1/172], Loss: 47.1376\n",
      "Epoch [222/300], Step [2/172], Loss: 52.9774\n",
      "Epoch [222/300], Step [3/172], Loss: 46.3196\n",
      "Epoch [222/300], Step [4/172], Loss: 24.5935\n",
      "Epoch [222/300], Step [5/172], Loss: 43.9780\n",
      "Epoch [222/300], Step [6/172], Loss: 20.8176\n",
      "Epoch [222/300], Step [7/172], Loss: 29.1693\n",
      "Epoch [222/300], Step [8/172], Loss: 5.3027\n",
      "Epoch [222/300], Step [9/172], Loss: 28.7385\n",
      "Epoch [222/300], Step [10/172], Loss: 41.2666\n",
      "Epoch [222/300], Step [11/172], Loss: 52.7491\n",
      "Epoch [222/300], Step [12/172], Loss: 56.8463\n",
      "Epoch [222/300], Step [13/172], Loss: 34.4611\n",
      "Epoch [222/300], Step [14/172], Loss: 61.0635\n",
      "Epoch [222/300], Step [15/172], Loss: 49.3486\n",
      "Epoch [222/300], Step [16/172], Loss: 9.2056\n",
      "Epoch [222/300], Step [17/172], Loss: 41.1337\n",
      "Epoch [222/300], Step [18/172], Loss: 52.8518\n",
      "Epoch [222/300], Step [19/172], Loss: 74.5380\n",
      "Epoch [222/300], Step [20/172], Loss: 28.1786\n",
      "Epoch [222/300], Step [21/172], Loss: 82.6012\n",
      "Epoch [222/300], Step [22/172], Loss: 54.6908\n",
      "Epoch [222/300], Step [23/172], Loss: 2.3512\n",
      "Epoch [222/300], Step [24/172], Loss: 53.6813\n",
      "Epoch [222/300], Step [25/172], Loss: 38.9412\n",
      "Epoch [222/300], Step [26/172], Loss: 45.8277\n",
      "Epoch [222/300], Step [27/172], Loss: 57.1329\n",
      "Epoch [222/300], Step [28/172], Loss: 23.5345\n",
      "Epoch [222/300], Step [29/172], Loss: 15.1305\n",
      "Epoch [222/300], Step [30/172], Loss: 57.8353\n",
      "Epoch [222/300], Step [31/172], Loss: 36.7444\n",
      "Epoch [222/300], Step [32/172], Loss: 44.5040\n",
      "Epoch [222/300], Step [33/172], Loss: 71.4516\n",
      "Epoch [222/300], Step [34/172], Loss: 2.7557\n",
      "Epoch [222/300], Step [35/172], Loss: 14.7061\n",
      "Epoch [222/300], Step [36/172], Loss: 17.0552\n",
      "Epoch [222/300], Step [37/172], Loss: 17.4667\n",
      "Epoch [222/300], Step [38/172], Loss: 34.0919\n",
      "Epoch [222/300], Step [39/172], Loss: 37.7157\n",
      "Epoch [222/300], Step [40/172], Loss: 24.6296\n",
      "Epoch [222/300], Step [41/172], Loss: 37.8320\n",
      "Epoch [222/300], Step [42/172], Loss: 43.1044\n",
      "Epoch [222/300], Step [43/172], Loss: 31.1855\n",
      "Epoch [222/300], Step [44/172], Loss: 24.3767\n",
      "Epoch [222/300], Step [45/172], Loss: 35.2139\n",
      "Epoch [222/300], Step [46/172], Loss: 18.3381\n",
      "Epoch [222/300], Step [47/172], Loss: 53.4371\n",
      "Epoch [222/300], Step [48/172], Loss: 60.3855\n",
      "Epoch [222/300], Step [49/172], Loss: 26.2956\n",
      "Epoch [222/300], Step [50/172], Loss: 45.2818\n",
      "Epoch [222/300], Step [51/172], Loss: 11.1992\n",
      "Epoch [222/300], Step [52/172], Loss: 24.3688\n",
      "Epoch [222/300], Step [53/172], Loss: 26.6307\n",
      "Epoch [222/300], Step [54/172], Loss: 19.8074\n",
      "Epoch [222/300], Step [55/172], Loss: 18.8003\n",
      "Epoch [222/300], Step [56/172], Loss: 20.4971\n",
      "Epoch [222/300], Step [57/172], Loss: 17.8552\n",
      "Epoch [222/300], Step [58/172], Loss: 15.7391\n",
      "Epoch [222/300], Step [59/172], Loss: 27.9539\n",
      "Epoch [222/300], Step [60/172], Loss: 19.0070\n",
      "Epoch [222/300], Step [61/172], Loss: 7.3490\n",
      "Epoch [222/300], Step [62/172], Loss: 16.7881\n",
      "Epoch [222/300], Step [63/172], Loss: 12.0838\n",
      "Epoch [222/300], Step [64/172], Loss: 12.8355\n",
      "Epoch [222/300], Step [65/172], Loss: 18.6617\n",
      "Epoch [222/300], Step [66/172], Loss: 7.3226\n",
      "Epoch [222/300], Step [67/172], Loss: 22.2574\n",
      "Epoch [222/300], Step [68/172], Loss: 5.1209\n",
      "Epoch [222/300], Step [69/172], Loss: 29.7222\n",
      "Epoch [222/300], Step [70/172], Loss: 33.3484\n",
      "Epoch [222/300], Step [71/172], Loss: 35.5966\n",
      "Epoch [222/300], Step [72/172], Loss: 33.3737\n",
      "Epoch [222/300], Step [73/172], Loss: 42.7278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [222/300], Step [74/172], Loss: 21.9094\n",
      "Epoch [222/300], Step [75/172], Loss: 20.7038\n",
      "Epoch [222/300], Step [76/172], Loss: 27.3717\n",
      "Epoch [222/300], Step [77/172], Loss: 42.3428\n",
      "Epoch [222/300], Step [78/172], Loss: 32.7767\n",
      "Epoch [222/300], Step [79/172], Loss: 31.6335\n",
      "Epoch [222/300], Step [80/172], Loss: 46.6826\n",
      "Epoch [222/300], Step [81/172], Loss: 28.8091\n",
      "Epoch [222/300], Step [82/172], Loss: 34.7308\n",
      "Epoch [222/300], Step [83/172], Loss: 40.6333\n",
      "Epoch [222/300], Step [84/172], Loss: 31.3244\n",
      "Epoch [222/300], Step [85/172], Loss: 36.6128\n",
      "Epoch [222/300], Step [86/172], Loss: 32.5559\n",
      "Epoch [222/300], Step [87/172], Loss: 23.7673\n",
      "Epoch [222/300], Step [88/172], Loss: 21.6099\n",
      "Epoch [222/300], Step [89/172], Loss: 27.6168\n",
      "Epoch [222/300], Step [90/172], Loss: 19.9112\n",
      "Epoch [222/300], Step [91/172], Loss: 25.5464\n",
      "Epoch [222/300], Step [92/172], Loss: 18.6408\n",
      "Epoch [222/300], Step [93/172], Loss: 18.9055\n",
      "Epoch [222/300], Step [94/172], Loss: 26.1702\n",
      "Epoch [222/300], Step [95/172], Loss: 20.6584\n",
      "Epoch [222/300], Step [96/172], Loss: 20.2932\n",
      "Epoch [222/300], Step [97/172], Loss: 28.2222\n",
      "Epoch [222/300], Step [98/172], Loss: 19.8641\n",
      "Epoch [222/300], Step [99/172], Loss: 19.6237\n",
      "Epoch [222/300], Step [100/172], Loss: 18.0361\n",
      "Epoch [222/300], Step [101/172], Loss: 19.9920\n",
      "Epoch [222/300], Step [102/172], Loss: 17.5471\n",
      "Epoch [222/300], Step [103/172], Loss: 13.8359\n",
      "Epoch [222/300], Step [104/172], Loss: 20.0285\n",
      "Epoch [222/300], Step [105/172], Loss: 23.3923\n",
      "Epoch [222/300], Step [106/172], Loss: 16.9663\n",
      "Epoch [222/300], Step [107/172], Loss: 16.6628\n",
      "Epoch [222/300], Step [108/172], Loss: 16.3325\n",
      "Epoch [222/300], Step [109/172], Loss: 15.2699\n",
      "Epoch [222/300], Step [110/172], Loss: 18.0893\n",
      "Epoch [222/300], Step [111/172], Loss: 18.7361\n",
      "Epoch [222/300], Step [112/172], Loss: 17.1209\n",
      "Epoch [222/300], Step [113/172], Loss: 14.2218\n",
      "Epoch [222/300], Step [114/172], Loss: 14.8842\n",
      "Epoch [222/300], Step [115/172], Loss: 18.4143\n",
      "Epoch [222/300], Step [116/172], Loss: 14.9709\n",
      "Epoch [222/300], Step [117/172], Loss: 12.6820\n",
      "Epoch [222/300], Step [118/172], Loss: 12.4844\n",
      "Epoch [222/300], Step [119/172], Loss: 17.2964\n",
      "Epoch [222/300], Step [120/172], Loss: 11.5172\n",
      "Epoch [222/300], Step [121/172], Loss: 10.5478\n",
      "Epoch [222/300], Step [122/172], Loss: 13.3723\n",
      "Epoch [222/300], Step [123/172], Loss: 11.6131\n",
      "Epoch [222/300], Step [124/172], Loss: 7.9547\n",
      "Epoch [222/300], Step [125/172], Loss: 13.0499\n",
      "Epoch [222/300], Step [126/172], Loss: 12.7244\n",
      "Epoch [222/300], Step [127/172], Loss: 11.3672\n",
      "Epoch [222/300], Step [128/172], Loss: 11.1149\n",
      "Epoch [222/300], Step [129/172], Loss: 9.4339\n",
      "Epoch [222/300], Step [130/172], Loss: 13.2452\n",
      "Epoch [222/300], Step [131/172], Loss: 8.5056\n",
      "Epoch [222/300], Step [132/172], Loss: 10.2839\n",
      "Epoch [222/300], Step [133/172], Loss: 10.6131\n",
      "Epoch [222/300], Step [134/172], Loss: 11.1103\n",
      "Epoch [222/300], Step [135/172], Loss: 9.5306\n",
      "Epoch [222/300], Step [136/172], Loss: 8.7060\n",
      "Epoch [222/300], Step [137/172], Loss: 9.4002\n",
      "Epoch [222/300], Step [138/172], Loss: 8.7384\n",
      "Epoch [222/300], Step [139/172], Loss: 10.1045\n",
      "Epoch [222/300], Step [140/172], Loss: 11.1185\n",
      "Epoch [222/300], Step [141/172], Loss: 9.8940\n",
      "Epoch [222/300], Step [142/172], Loss: 14.2671\n",
      "Epoch [222/300], Step [143/172], Loss: 11.4793\n",
      "Epoch [222/300], Step [144/172], Loss: 9.5097\n",
      "Epoch [222/300], Step [145/172], Loss: 11.3845\n",
      "Epoch [222/300], Step [146/172], Loss: 10.5566\n",
      "Epoch [222/300], Step [147/172], Loss: 6.2189\n",
      "Epoch [222/300], Step [148/172], Loss: 7.2454\n",
      "Epoch [222/300], Step [149/172], Loss: 7.7482\n",
      "Epoch [222/300], Step [150/172], Loss: 6.3548\n",
      "Epoch [222/300], Step [151/172], Loss: 6.0856\n",
      "Epoch [222/300], Step [152/172], Loss: 8.7913\n",
      "Epoch [222/300], Step [153/172], Loss: 7.0217\n",
      "Epoch [222/300], Step [154/172], Loss: 7.3391\n",
      "Epoch [222/300], Step [155/172], Loss: 6.9369\n",
      "Epoch [222/300], Step [156/172], Loss: 14.2320\n",
      "Epoch [222/300], Step [157/172], Loss: 8.4603\n",
      "Epoch [222/300], Step [158/172], Loss: 7.5302\n",
      "Epoch [222/300], Step [159/172], Loss: 9.3500\n",
      "Epoch [222/300], Step [160/172], Loss: 9.3050\n",
      "Epoch [222/300], Step [161/172], Loss: 8.8482\n",
      "Epoch [222/300], Step [162/172], Loss: 5.4179\n",
      "Epoch [222/300], Step [163/172], Loss: 6.7049\n",
      "Epoch [222/300], Step [164/172], Loss: 9.3550\n",
      "Epoch [222/300], Step [165/172], Loss: 7.3918\n",
      "Epoch [222/300], Step [166/172], Loss: 6.2739\n",
      "Epoch [222/300], Step [167/172], Loss: 10.1175\n",
      "Epoch [222/300], Step [168/172], Loss: 6.9855\n",
      "Epoch [222/300], Step [169/172], Loss: 7.3462\n",
      "Epoch [222/300], Step [170/172], Loss: 6.1519\n",
      "Epoch [222/300], Step [171/172], Loss: 8.2333\n",
      "Epoch [222/300], Step [172/172], Loss: 6.5964\n",
      "Epoch [223/300], Step [1/172], Loss: 46.8265\n",
      "Epoch [223/300], Step [2/172], Loss: 51.9143\n",
      "Epoch [223/300], Step [3/172], Loss: 48.3740\n",
      "Epoch [223/300], Step [4/172], Loss: 24.2857\n",
      "Epoch [223/300], Step [5/172], Loss: 43.0954\n",
      "Epoch [223/300], Step [6/172], Loss: 20.8698\n",
      "Epoch [223/300], Step [7/172], Loss: 28.2501\n",
      "Epoch [223/300], Step [8/172], Loss: 4.8675\n",
      "Epoch [223/300], Step [9/172], Loss: 28.3721\n",
      "Epoch [223/300], Step [10/172], Loss: 41.7132\n",
      "Epoch [223/300], Step [11/172], Loss: 52.5987\n",
      "Epoch [223/300], Step [12/172], Loss: 56.0851\n",
      "Epoch [223/300], Step [13/172], Loss: 34.0795\n",
      "Epoch [223/300], Step [14/172], Loss: 60.6060\n",
      "Epoch [223/300], Step [15/172], Loss: 49.3662\n",
      "Epoch [223/300], Step [16/172], Loss: 10.4696\n",
      "Epoch [223/300], Step [17/172], Loss: 40.6335\n",
      "Epoch [223/300], Step [18/172], Loss: 52.6557\n",
      "Epoch [223/300], Step [19/172], Loss: 73.8348\n",
      "Epoch [223/300], Step [20/172], Loss: 28.2891\n",
      "Epoch [223/300], Step [21/172], Loss: 82.3348\n",
      "Epoch [223/300], Step [22/172], Loss: 54.0221\n",
      "Epoch [223/300], Step [23/172], Loss: 2.1258\n",
      "Epoch [223/300], Step [24/172], Loss: 53.4211\n",
      "Epoch [223/300], Step [25/172], Loss: 38.9558\n",
      "Epoch [223/300], Step [26/172], Loss: 45.9506\n",
      "Epoch [223/300], Step [27/172], Loss: 56.9678\n",
      "Epoch [223/300], Step [28/172], Loss: 23.4473\n",
      "Epoch [223/300], Step [29/172], Loss: 15.1618\n",
      "Epoch [223/300], Step [30/172], Loss: 58.0941\n",
      "Epoch [223/300], Step [31/172], Loss: 36.9529\n",
      "Epoch [223/300], Step [32/172], Loss: 44.5219\n",
      "Epoch [223/300], Step [33/172], Loss: 71.0365\n",
      "Epoch [223/300], Step [34/172], Loss: 2.7065\n",
      "Epoch [223/300], Step [35/172], Loss: 14.7804\n",
      "Epoch [223/300], Step [36/172], Loss: 17.2271\n",
      "Epoch [223/300], Step [37/172], Loss: 17.4633\n",
      "Epoch [223/300], Step [38/172], Loss: 33.9100\n",
      "Epoch [223/300], Step [39/172], Loss: 38.0495\n",
      "Epoch [223/300], Step [40/172], Loss: 24.8876\n",
      "Epoch [223/300], Step [41/172], Loss: 38.3269\n",
      "Epoch [223/300], Step [42/172], Loss: 43.1950\n",
      "Epoch [223/300], Step [43/172], Loss: 31.3157\n",
      "Epoch [223/300], Step [44/172], Loss: 24.4876\n",
      "Epoch [223/300], Step [45/172], Loss: 35.3472\n",
      "Epoch [223/300], Step [46/172], Loss: 18.6557\n",
      "Epoch [223/300], Step [47/172], Loss: 53.6560\n",
      "Epoch [223/300], Step [48/172], Loss: 61.0579\n",
      "Epoch [223/300], Step [49/172], Loss: 26.4705\n",
      "Epoch [223/300], Step [50/172], Loss: 46.1570\n",
      "Epoch [223/300], Step [51/172], Loss: 11.2249\n",
      "Epoch [223/300], Step [52/172], Loss: 24.6665\n",
      "Epoch [223/300], Step [53/172], Loss: 27.0345\n",
      "Epoch [223/300], Step [54/172], Loss: 20.2022\n",
      "Epoch [223/300], Step [55/172], Loss: 19.0852\n",
      "Epoch [223/300], Step [56/172], Loss: 20.4706\n",
      "Epoch [223/300], Step [57/172], Loss: 17.7618\n",
      "Epoch [223/300], Step [58/172], Loss: 16.1719\n",
      "Epoch [223/300], Step [59/172], Loss: 28.7571\n",
      "Epoch [223/300], Step [60/172], Loss: 19.1638\n",
      "Epoch [223/300], Step [61/172], Loss: 7.6144\n",
      "Epoch [223/300], Step [62/172], Loss: 17.1995\n",
      "Epoch [223/300], Step [63/172], Loss: 12.6345\n",
      "Epoch [223/300], Step [64/172], Loss: 13.1499\n",
      "Epoch [223/300], Step [65/172], Loss: 19.0692\n",
      "Epoch [223/300], Step [66/172], Loss: 7.5489\n",
      "Epoch [223/300], Step [67/172], Loss: 22.9104\n",
      "Epoch [223/300], Step [68/172], Loss: 5.4283\n",
      "Epoch [223/300], Step [69/172], Loss: 29.6259\n",
      "Epoch [223/300], Step [70/172], Loss: 33.3422\n",
      "Epoch [223/300], Step [71/172], Loss: 35.3120\n",
      "Epoch [223/300], Step [72/172], Loss: 33.0090\n",
      "Epoch [223/300], Step [73/172], Loss: 42.1138\n",
      "Epoch [223/300], Step [74/172], Loss: 21.9695\n",
      "Epoch [223/300], Step [75/172], Loss: 20.8061\n",
      "Epoch [223/300], Step [76/172], Loss: 27.3745\n",
      "Epoch [223/300], Step [77/172], Loss: 42.4414\n",
      "Epoch [223/300], Step [78/172], Loss: 32.9210\n",
      "Epoch [223/300], Step [79/172], Loss: 31.8694\n",
      "Epoch [223/300], Step [80/172], Loss: 47.3706\n",
      "Epoch [223/300], Step [81/172], Loss: 28.9528\n",
      "Epoch [223/300], Step [82/172], Loss: 35.3542\n",
      "Epoch [223/300], Step [83/172], Loss: 40.9953\n",
      "Epoch [223/300], Step [84/172], Loss: 31.6870\n",
      "Epoch [223/300], Step [85/172], Loss: 36.8604\n",
      "Epoch [223/300], Step [86/172], Loss: 32.9178\n",
      "Epoch [223/300], Step [87/172], Loss: 23.9103\n",
      "Epoch [223/300], Step [88/172], Loss: 21.9304\n",
      "Epoch [223/300], Step [89/172], Loss: 27.9170\n",
      "Epoch [223/300], Step [90/172], Loss: 20.1387\n",
      "Epoch [223/300], Step [91/172], Loss: 25.7229\n",
      "Epoch [223/300], Step [92/172], Loss: 18.7535\n",
      "Epoch [223/300], Step [93/172], Loss: 19.1032\n",
      "Epoch [223/300], Step [94/172], Loss: 26.1701\n",
      "Epoch [223/300], Step [95/172], Loss: 20.7471\n",
      "Epoch [223/300], Step [96/172], Loss: 20.4492\n",
      "Epoch [223/300], Step [97/172], Loss: 28.5599\n",
      "Epoch [223/300], Step [98/172], Loss: 20.0462\n",
      "Epoch [223/300], Step [99/172], Loss: 19.7919\n",
      "Epoch [223/300], Step [100/172], Loss: 18.2300\n",
      "Epoch [223/300], Step [101/172], Loss: 20.1030\n",
      "Epoch [223/300], Step [102/172], Loss: 17.7062\n",
      "Epoch [223/300], Step [103/172], Loss: 13.9776\n",
      "Epoch [223/300], Step [104/172], Loss: 20.2122\n",
      "Epoch [223/300], Step [105/172], Loss: 23.7852\n",
      "Epoch [223/300], Step [106/172], Loss: 17.1152\n",
      "Epoch [223/300], Step [107/172], Loss: 16.7989\n",
      "Epoch [223/300], Step [108/172], Loss: 16.4476\n",
      "Epoch [223/300], Step [109/172], Loss: 15.3365\n",
      "Epoch [223/300], Step [110/172], Loss: 18.2755\n",
      "Epoch [223/300], Step [111/172], Loss: 18.8998\n",
      "Epoch [223/300], Step [112/172], Loss: 17.1353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [223/300], Step [113/172], Loss: 14.4523\n",
      "Epoch [223/300], Step [114/172], Loss: 14.9397\n",
      "Epoch [223/300], Step [115/172], Loss: 18.6662\n",
      "Epoch [223/300], Step [116/172], Loss: 15.1274\n",
      "Epoch [223/300], Step [117/172], Loss: 12.6638\n",
      "Epoch [223/300], Step [118/172], Loss: 12.5218\n",
      "Epoch [223/300], Step [119/172], Loss: 17.5892\n",
      "Epoch [223/300], Step [120/172], Loss: 11.5758\n",
      "Epoch [223/300], Step [121/172], Loss: 10.6442\n",
      "Epoch [223/300], Step [122/172], Loss: 13.2483\n",
      "Epoch [223/300], Step [123/172], Loss: 11.7501\n",
      "Epoch [223/300], Step [124/172], Loss: 8.0097\n",
      "Epoch [223/300], Step [125/172], Loss: 13.1349\n",
      "Epoch [223/300], Step [126/172], Loss: 12.9278\n",
      "Epoch [223/300], Step [127/172], Loss: 11.3827\n",
      "Epoch [223/300], Step [128/172], Loss: 11.2664\n",
      "Epoch [223/300], Step [129/172], Loss: 9.5475\n",
      "Epoch [223/300], Step [130/172], Loss: 13.3664\n",
      "Epoch [223/300], Step [131/172], Loss: 8.5469\n",
      "Epoch [223/300], Step [132/172], Loss: 10.3884\n",
      "Epoch [223/300], Step [133/172], Loss: 10.6513\n",
      "Epoch [223/300], Step [134/172], Loss: 11.1946\n",
      "Epoch [223/300], Step [135/172], Loss: 9.6711\n",
      "Epoch [223/300], Step [136/172], Loss: 8.7990\n",
      "Epoch [223/300], Step [137/172], Loss: 9.4838\n",
      "Epoch [223/300], Step [138/172], Loss: 8.9760\n",
      "Epoch [223/300], Step [139/172], Loss: 10.3789\n",
      "Epoch [223/300], Step [140/172], Loss: 11.2711\n",
      "Epoch [223/300], Step [141/172], Loss: 10.0777\n",
      "Epoch [223/300], Step [142/172], Loss: 14.3497\n",
      "Epoch [223/300], Step [143/172], Loss: 11.6448\n",
      "Epoch [223/300], Step [144/172], Loss: 9.6615\n",
      "Epoch [223/300], Step [145/172], Loss: 11.3447\n",
      "Epoch [223/300], Step [146/172], Loss: 10.9078\n",
      "Epoch [223/300], Step [147/172], Loss: 6.3157\n",
      "Epoch [223/300], Step [148/172], Loss: 7.3702\n",
      "Epoch [223/300], Step [149/172], Loss: 7.8626\n",
      "Epoch [223/300], Step [150/172], Loss: 6.4397\n",
      "Epoch [223/300], Step [151/172], Loss: 6.1433\n",
      "Epoch [223/300], Step [152/172], Loss: 8.9610\n",
      "Epoch [223/300], Step [153/172], Loss: 7.0881\n",
      "Epoch [223/300], Step [154/172], Loss: 7.4092\n",
      "Epoch [223/300], Step [155/172], Loss: 7.0569\n",
      "Epoch [223/300], Step [156/172], Loss: 14.2734\n",
      "Epoch [223/300], Step [157/172], Loss: 8.6165\n",
      "Epoch [223/300], Step [158/172], Loss: 7.5231\n",
      "Epoch [223/300], Step [159/172], Loss: 9.5783\n",
      "Epoch [223/300], Step [160/172], Loss: 9.2880\n",
      "Epoch [223/300], Step [161/172], Loss: 9.1105\n",
      "Epoch [223/300], Step [162/172], Loss: 5.4859\n",
      "Epoch [223/300], Step [163/172], Loss: 6.8936\n",
      "Epoch [223/300], Step [164/172], Loss: 9.5164\n",
      "Epoch [223/300], Step [165/172], Loss: 7.4434\n",
      "Epoch [223/300], Step [166/172], Loss: 6.5079\n",
      "Epoch [223/300], Step [167/172], Loss: 10.2669\n",
      "Epoch [223/300], Step [168/172], Loss: 7.2234\n",
      "Epoch [223/300], Step [169/172], Loss: 7.3294\n",
      "Epoch [223/300], Step [170/172], Loss: 6.2217\n",
      "Epoch [223/300], Step [171/172], Loss: 8.6746\n",
      "Epoch [223/300], Step [172/172], Loss: 6.6616\n",
      "Epoch [224/300], Step [1/172], Loss: 46.7518\n",
      "Epoch [224/300], Step [2/172], Loss: 51.4084\n",
      "Epoch [224/300], Step [3/172], Loss: 46.2897\n",
      "Epoch [224/300], Step [4/172], Loss: 23.9972\n",
      "Epoch [224/300], Step [5/172], Loss: 42.6812\n",
      "Epoch [224/300], Step [6/172], Loss: 20.8645\n",
      "Epoch [224/300], Step [7/172], Loss: 29.1168\n",
      "Epoch [224/300], Step [8/172], Loss: 4.4492\n",
      "Epoch [224/300], Step [9/172], Loss: 28.1175\n",
      "Epoch [224/300], Step [10/172], Loss: 40.8446\n",
      "Epoch [224/300], Step [11/172], Loss: 52.5076\n",
      "Epoch [224/300], Step [12/172], Loss: 55.6472\n",
      "Epoch [224/300], Step [13/172], Loss: 34.0379\n",
      "Epoch [224/300], Step [14/172], Loss: 59.0766\n",
      "Epoch [224/300], Step [15/172], Loss: 48.9393\n",
      "Epoch [224/300], Step [16/172], Loss: 10.6274\n",
      "Epoch [224/300], Step [17/172], Loss: 40.2728\n",
      "Epoch [224/300], Step [18/172], Loss: 52.0859\n",
      "Epoch [224/300], Step [19/172], Loss: 73.1488\n",
      "Epoch [224/300], Step [20/172], Loss: 28.5539\n",
      "Epoch [224/300], Step [21/172], Loss: 82.0017\n",
      "Epoch [224/300], Step [22/172], Loss: 53.8614\n",
      "Epoch [224/300], Step [23/172], Loss: 2.0088\n",
      "Epoch [224/300], Step [24/172], Loss: 52.9263\n",
      "Epoch [224/300], Step [25/172], Loss: 38.7649\n",
      "Epoch [224/300], Step [26/172], Loss: 45.5417\n",
      "Epoch [224/300], Step [27/172], Loss: 56.7131\n",
      "Epoch [224/300], Step [28/172], Loss: 23.1928\n",
      "Epoch [224/300], Step [29/172], Loss: 15.0562\n",
      "Epoch [224/300], Step [30/172], Loss: 57.6781\n",
      "Epoch [224/300], Step [31/172], Loss: 36.5082\n",
      "Epoch [224/300], Step [32/172], Loss: 44.3884\n",
      "Epoch [224/300], Step [33/172], Loss: 70.7049\n",
      "Epoch [224/300], Step [34/172], Loss: 2.6944\n",
      "Epoch [224/300], Step [35/172], Loss: 14.6911\n",
      "Epoch [224/300], Step [36/172], Loss: 17.1074\n",
      "Epoch [224/300], Step [37/172], Loss: 17.3067\n",
      "Epoch [224/300], Step [38/172], Loss: 33.6248\n",
      "Epoch [224/300], Step [39/172], Loss: 37.7482\n",
      "Epoch [224/300], Step [40/172], Loss: 24.8501\n",
      "Epoch [224/300], Step [41/172], Loss: 38.2243\n",
      "Epoch [224/300], Step [42/172], Loss: 43.3866\n",
      "Epoch [224/300], Step [43/172], Loss: 31.3386\n",
      "Epoch [224/300], Step [44/172], Loss: 24.1500\n",
      "Epoch [224/300], Step [45/172], Loss: 35.0885\n",
      "Epoch [224/300], Step [46/172], Loss: 18.1514\n",
      "Epoch [224/300], Step [47/172], Loss: 53.4494\n",
      "Epoch [224/300], Step [48/172], Loss: 61.5041\n",
      "Epoch [224/300], Step [49/172], Loss: 26.7395\n",
      "Epoch [224/300], Step [50/172], Loss: 46.4956\n",
      "Epoch [224/300], Step [51/172], Loss: 11.1635\n",
      "Epoch [224/300], Step [52/172], Loss: 24.6535\n",
      "Epoch [224/300], Step [53/172], Loss: 27.0684\n",
      "Epoch [224/300], Step [54/172], Loss: 20.4313\n",
      "Epoch [224/300], Step [55/172], Loss: 19.3064\n",
      "Epoch [224/300], Step [56/172], Loss: 20.2655\n",
      "Epoch [224/300], Step [57/172], Loss: 18.3544\n",
      "Epoch [224/300], Step [58/172], Loss: 16.3599\n",
      "Epoch [224/300], Step [59/172], Loss: 29.2844\n",
      "Epoch [224/300], Step [60/172], Loss: 19.4284\n",
      "Epoch [224/300], Step [61/172], Loss: 7.6884\n",
      "Epoch [224/300], Step [62/172], Loss: 17.3838\n",
      "Epoch [224/300], Step [63/172], Loss: 12.5185\n",
      "Epoch [224/300], Step [64/172], Loss: 13.1487\n",
      "Epoch [224/300], Step [65/172], Loss: 19.2423\n",
      "Epoch [224/300], Step [66/172], Loss: 7.6121\n",
      "Epoch [224/300], Step [67/172], Loss: 23.1956\n",
      "Epoch [224/300], Step [68/172], Loss: 5.2700\n",
      "Epoch [224/300], Step [69/172], Loss: 29.8447\n",
      "Epoch [224/300], Step [70/172], Loss: 32.9611\n",
      "Epoch [224/300], Step [71/172], Loss: 35.2091\n",
      "Epoch [224/300], Step [72/172], Loss: 32.7066\n",
      "Epoch [224/300], Step [73/172], Loss: 41.9098\n",
      "Epoch [224/300], Step [74/172], Loss: 21.6582\n",
      "Epoch [224/300], Step [75/172], Loss: 20.4595\n",
      "Epoch [224/300], Step [76/172], Loss: 27.4682\n",
      "Epoch [224/300], Step [77/172], Loss: 42.3701\n",
      "Epoch [224/300], Step [78/172], Loss: 32.7832\n",
      "Epoch [224/300], Step [79/172], Loss: 31.7225\n",
      "Epoch [224/300], Step [80/172], Loss: 47.4254\n",
      "Epoch [224/300], Step [81/172], Loss: 29.0448\n",
      "Epoch [224/300], Step [82/172], Loss: 35.3230\n",
      "Epoch [224/300], Step [83/172], Loss: 40.9127\n",
      "Epoch [224/300], Step [84/172], Loss: 32.0399\n",
      "Epoch [224/300], Step [85/172], Loss: 37.1232\n",
      "Epoch [224/300], Step [86/172], Loss: 33.1691\n",
      "Epoch [224/300], Step [87/172], Loss: 23.8559\n",
      "Epoch [224/300], Step [88/172], Loss: 21.7523\n",
      "Epoch [224/300], Step [89/172], Loss: 27.8960\n",
      "Epoch [224/300], Step [90/172], Loss: 20.2114\n",
      "Epoch [224/300], Step [91/172], Loss: 25.8013\n",
      "Epoch [224/300], Step [92/172], Loss: 18.7118\n",
      "Epoch [224/300], Step [93/172], Loss: 18.9550\n",
      "Epoch [224/300], Step [94/172], Loss: 26.1241\n",
      "Epoch [224/300], Step [95/172], Loss: 20.8148\n",
      "Epoch [224/300], Step [96/172], Loss: 20.3602\n",
      "Epoch [224/300], Step [97/172], Loss: 28.6754\n",
      "Epoch [224/300], Step [98/172], Loss: 20.0576\n",
      "Epoch [224/300], Step [99/172], Loss: 19.8069\n",
      "Epoch [224/300], Step [100/172], Loss: 18.3385\n",
      "Epoch [224/300], Step [101/172], Loss: 20.1635\n",
      "Epoch [224/300], Step [102/172], Loss: 17.8034\n",
      "Epoch [224/300], Step [103/172], Loss: 13.9825\n",
      "Epoch [224/300], Step [104/172], Loss: 20.2486\n",
      "Epoch [224/300], Step [105/172], Loss: 23.8702\n",
      "Epoch [224/300], Step [106/172], Loss: 17.1114\n",
      "Epoch [224/300], Step [107/172], Loss: 16.8745\n",
      "Epoch [224/300], Step [108/172], Loss: 16.5773\n",
      "Epoch [224/300], Step [109/172], Loss: 15.4455\n",
      "Epoch [224/300], Step [110/172], Loss: 18.2498\n",
      "Epoch [224/300], Step [111/172], Loss: 18.9081\n",
      "Epoch [224/300], Step [112/172], Loss: 17.1317\n",
      "Epoch [224/300], Step [113/172], Loss: 14.4538\n",
      "Epoch [224/300], Step [114/172], Loss: 14.9771\n",
      "Epoch [224/300], Step [115/172], Loss: 18.4692\n",
      "Epoch [224/300], Step [116/172], Loss: 14.9866\n",
      "Epoch [224/300], Step [117/172], Loss: 12.6598\n",
      "Epoch [224/300], Step [118/172], Loss: 12.4757\n",
      "Epoch [224/300], Step [119/172], Loss: 17.7371\n",
      "Epoch [224/300], Step [120/172], Loss: 11.5214\n",
      "Epoch [224/300], Step [121/172], Loss: 10.5956\n",
      "Epoch [224/300], Step [122/172], Loss: 13.5000\n",
      "Epoch [224/300], Step [123/172], Loss: 11.7755\n",
      "Epoch [224/300], Step [124/172], Loss: 7.9368\n",
      "Epoch [224/300], Step [125/172], Loss: 13.0957\n",
      "Epoch [224/300], Step [126/172], Loss: 12.8534\n",
      "Epoch [224/300], Step [127/172], Loss: 11.4641\n",
      "Epoch [224/300], Step [128/172], Loss: 11.1916\n",
      "Epoch [224/300], Step [129/172], Loss: 9.5392\n",
      "Epoch [224/300], Step [130/172], Loss: 13.1468\n",
      "Epoch [224/300], Step [131/172], Loss: 8.5870\n",
      "Epoch [224/300], Step [132/172], Loss: 10.4462\n",
      "Epoch [224/300], Step [133/172], Loss: 10.7086\n",
      "Epoch [224/300], Step [134/172], Loss: 11.2130\n",
      "Epoch [224/300], Step [135/172], Loss: 9.6679\n",
      "Epoch [224/300], Step [136/172], Loss: 8.8450\n",
      "Epoch [224/300], Step [137/172], Loss: 9.3505\n",
      "Epoch [224/300], Step [138/172], Loss: 8.8114\n",
      "Epoch [224/300], Step [139/172], Loss: 10.1978\n",
      "Epoch [224/300], Step [140/172], Loss: 11.2227\n",
      "Epoch [224/300], Step [141/172], Loss: 9.9384\n",
      "Epoch [224/300], Step [142/172], Loss: 14.1427\n",
      "Epoch [224/300], Step [143/172], Loss: 11.5867\n",
      "Epoch [224/300], Step [144/172], Loss: 9.5473\n",
      "Epoch [224/300], Step [145/172], Loss: 11.1918\n",
      "Epoch [224/300], Step [146/172], Loss: 10.7242\n",
      "Epoch [224/300], Step [147/172], Loss: 6.2467\n",
      "Epoch [224/300], Step [148/172], Loss: 7.3417\n",
      "Epoch [224/300], Step [149/172], Loss: 7.8139\n",
      "Epoch [224/300], Step [150/172], Loss: 6.3969\n",
      "Epoch [224/300], Step [151/172], Loss: 6.0963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [224/300], Step [152/172], Loss: 9.0229\n",
      "Epoch [224/300], Step [153/172], Loss: 7.0336\n",
      "Epoch [224/300], Step [154/172], Loss: 7.3226\n",
      "Epoch [224/300], Step [155/172], Loss: 6.9410\n",
      "Epoch [224/300], Step [156/172], Loss: 14.2381\n",
      "Epoch [224/300], Step [157/172], Loss: 8.5377\n",
      "Epoch [224/300], Step [158/172], Loss: 7.5467\n",
      "Epoch [224/300], Step [159/172], Loss: 9.4174\n",
      "Epoch [224/300], Step [160/172], Loss: 9.2353\n",
      "Epoch [224/300], Step [161/172], Loss: 9.0139\n",
      "Epoch [224/300], Step [162/172], Loss: 5.4695\n",
      "Epoch [224/300], Step [163/172], Loss: 6.7903\n",
      "Epoch [224/300], Step [164/172], Loss: 9.4037\n",
      "Epoch [224/300], Step [165/172], Loss: 7.4096\n",
      "Epoch [224/300], Step [166/172], Loss: 6.4849\n",
      "Epoch [224/300], Step [167/172], Loss: 10.1090\n",
      "Epoch [224/300], Step [168/172], Loss: 7.1277\n",
      "Epoch [224/300], Step [169/172], Loss: 7.2576\n",
      "Epoch [224/300], Step [170/172], Loss: 6.2244\n",
      "Epoch [224/300], Step [171/172], Loss: 8.2831\n",
      "Epoch [224/300], Step [172/172], Loss: 6.4549\n",
      "Epoch [225/300], Step [1/172], Loss: 46.6442\n",
      "Epoch [225/300], Step [2/172], Loss: 51.2564\n",
      "Epoch [225/300], Step [3/172], Loss: 46.7615\n",
      "Epoch [225/300], Step [4/172], Loss: 23.8675\n",
      "Epoch [225/300], Step [5/172], Loss: 43.1430\n",
      "Epoch [225/300], Step [6/172], Loss: 21.0395\n",
      "Epoch [225/300], Step [7/172], Loss: 28.3707\n",
      "Epoch [225/300], Step [8/172], Loss: 4.8518\n",
      "Epoch [225/300], Step [9/172], Loss: 28.3883\n",
      "Epoch [225/300], Step [10/172], Loss: 40.9749\n",
      "Epoch [225/300], Step [11/172], Loss: 52.4402\n",
      "Epoch [225/300], Step [12/172], Loss: 55.6154\n",
      "Epoch [225/300], Step [13/172], Loss: 34.1046\n",
      "Epoch [225/300], Step [14/172], Loss: 60.0317\n",
      "Epoch [225/300], Step [15/172], Loss: 48.6825\n",
      "Epoch [225/300], Step [16/172], Loss: 9.7902\n",
      "Epoch [225/300], Step [17/172], Loss: 40.4160\n",
      "Epoch [225/300], Step [18/172], Loss: 52.2126\n",
      "Epoch [225/300], Step [19/172], Loss: 73.3986\n",
      "Epoch [225/300], Step [20/172], Loss: 27.3709\n",
      "Epoch [225/300], Step [21/172], Loss: 82.1895\n",
      "Epoch [225/300], Step [22/172], Loss: 53.2093\n",
      "Epoch [225/300], Step [23/172], Loss: 1.9913\n",
      "Epoch [225/300], Step [24/172], Loss: 52.9820\n",
      "Epoch [225/300], Step [25/172], Loss: 38.7151\n",
      "Epoch [225/300], Step [26/172], Loss: 45.5025\n",
      "Epoch [225/300], Step [27/172], Loss: 55.9112\n",
      "Epoch [225/300], Step [28/172], Loss: 23.0465\n",
      "Epoch [225/300], Step [29/172], Loss: 14.8934\n",
      "Epoch [225/300], Step [30/172], Loss: 57.2990\n",
      "Epoch [225/300], Step [31/172], Loss: 36.5155\n",
      "Epoch [225/300], Step [32/172], Loss: 44.4287\n",
      "Epoch [225/300], Step [33/172], Loss: 70.8671\n",
      "Epoch [225/300], Step [34/172], Loss: 2.6658\n",
      "Epoch [225/300], Step [35/172], Loss: 14.6465\n",
      "Epoch [225/300], Step [36/172], Loss: 17.2394\n",
      "Epoch [225/300], Step [37/172], Loss: 17.3028\n",
      "Epoch [225/300], Step [38/172], Loss: 33.5866\n",
      "Epoch [225/300], Step [39/172], Loss: 37.8877\n",
      "Epoch [225/300], Step [40/172], Loss: 24.9589\n",
      "Epoch [225/300], Step [41/172], Loss: 38.0907\n",
      "Epoch [225/300], Step [42/172], Loss: 43.4119\n",
      "Epoch [225/300], Step [43/172], Loss: 31.1226\n",
      "Epoch [225/300], Step [44/172], Loss: 24.2536\n",
      "Epoch [225/300], Step [45/172], Loss: 35.4928\n",
      "Epoch [225/300], Step [46/172], Loss: 18.4873\n",
      "Epoch [225/300], Step [47/172], Loss: 53.7485\n",
      "Epoch [225/300], Step [48/172], Loss: 61.5968\n",
      "Epoch [225/300], Step [49/172], Loss: 26.7328\n",
      "Epoch [225/300], Step [50/172], Loss: 46.3077\n",
      "Epoch [225/300], Step [51/172], Loss: 11.2424\n",
      "Epoch [225/300], Step [52/172], Loss: 25.0690\n",
      "Epoch [225/300], Step [53/172], Loss: 27.3177\n",
      "Epoch [225/300], Step [54/172], Loss: 20.8887\n",
      "Epoch [225/300], Step [55/172], Loss: 19.8034\n",
      "Epoch [225/300], Step [56/172], Loss: 20.5543\n",
      "Epoch [225/300], Step [57/172], Loss: 18.4085\n",
      "Epoch [225/300], Step [58/172], Loss: 16.6519\n",
      "Epoch [225/300], Step [59/172], Loss: 29.3829\n",
      "Epoch [225/300], Step [60/172], Loss: 19.1198\n",
      "Epoch [225/300], Step [61/172], Loss: 8.0004\n",
      "Epoch [225/300], Step [62/172], Loss: 17.2946\n",
      "Epoch [225/300], Step [63/172], Loss: 12.8678\n",
      "Epoch [225/300], Step [64/172], Loss: 13.6222\n",
      "Epoch [225/300], Step [65/172], Loss: 19.3902\n",
      "Epoch [225/300], Step [66/172], Loss: 7.7221\n",
      "Epoch [225/300], Step [67/172], Loss: 23.4746\n",
      "Epoch [225/300], Step [68/172], Loss: 5.5188\n",
      "Epoch [225/300], Step [69/172], Loss: 29.4769\n",
      "Epoch [225/300], Step [70/172], Loss: 32.9873\n",
      "Epoch [225/300], Step [71/172], Loss: 34.8952\n",
      "Epoch [225/300], Step [72/172], Loss: 32.2212\n",
      "Epoch [225/300], Step [73/172], Loss: 41.2487\n",
      "Epoch [225/300], Step [74/172], Loss: 21.6354\n",
      "Epoch [225/300], Step [75/172], Loss: 20.4679\n",
      "Epoch [225/300], Step [76/172], Loss: 27.3865\n",
      "Epoch [225/300], Step [77/172], Loss: 42.1007\n",
      "Epoch [225/300], Step [78/172], Loss: 32.5789\n",
      "Epoch [225/300], Step [79/172], Loss: 31.5278\n",
      "Epoch [225/300], Step [80/172], Loss: 47.2630\n",
      "Epoch [225/300], Step [81/172], Loss: 28.9492\n",
      "Epoch [225/300], Step [82/172], Loss: 35.5377\n",
      "Epoch [225/300], Step [83/172], Loss: 41.0201\n",
      "Epoch [225/300], Step [84/172], Loss: 32.1770\n",
      "Epoch [225/300], Step [85/172], Loss: 36.9577\n",
      "Epoch [225/300], Step [86/172], Loss: 33.4230\n",
      "Epoch [225/300], Step [87/172], Loss: 23.9301\n",
      "Epoch [225/300], Step [88/172], Loss: 21.9089\n",
      "Epoch [225/300], Step [89/172], Loss: 27.9941\n",
      "Epoch [225/300], Step [90/172], Loss: 20.4259\n",
      "Epoch [225/300], Step [91/172], Loss: 25.8588\n",
      "Epoch [225/300], Step [92/172], Loss: 18.7931\n",
      "Epoch [225/300], Step [93/172], Loss: 19.0464\n",
      "Epoch [225/300], Step [94/172], Loss: 26.0969\n",
      "Epoch [225/300], Step [95/172], Loss: 21.0628\n",
      "Epoch [225/300], Step [96/172], Loss: 20.4819\n",
      "Epoch [225/300], Step [97/172], Loss: 28.7791\n",
      "Epoch [225/300], Step [98/172], Loss: 20.0705\n",
      "Epoch [225/300], Step [99/172], Loss: 19.9593\n",
      "Epoch [225/300], Step [100/172], Loss: 18.3907\n",
      "Epoch [225/300], Step [101/172], Loss: 20.2782\n",
      "Epoch [225/300], Step [102/172], Loss: 17.9209\n",
      "Epoch [225/300], Step [103/172], Loss: 14.0485\n",
      "Epoch [225/300], Step [104/172], Loss: 20.3937\n",
      "Epoch [225/300], Step [105/172], Loss: 24.1478\n",
      "Epoch [225/300], Step [106/172], Loss: 17.1452\n",
      "Epoch [225/300], Step [107/172], Loss: 16.9361\n",
      "Epoch [225/300], Step [108/172], Loss: 16.7636\n",
      "Epoch [225/300], Step [109/172], Loss: 15.5006\n",
      "Epoch [225/300], Step [110/172], Loss: 18.5275\n",
      "Epoch [225/300], Step [111/172], Loss: 19.0478\n",
      "Epoch [225/300], Step [112/172], Loss: 17.2899\n",
      "Epoch [225/300], Step [113/172], Loss: 14.6942\n",
      "Epoch [225/300], Step [114/172], Loss: 15.0829\n",
      "Epoch [225/300], Step [115/172], Loss: 18.6394\n",
      "Epoch [225/300], Step [116/172], Loss: 15.0704\n",
      "Epoch [225/300], Step [117/172], Loss: 12.6181\n",
      "Epoch [225/300], Step [118/172], Loss: 12.5777\n",
      "Epoch [225/300], Step [119/172], Loss: 17.9090\n",
      "Epoch [225/300], Step [120/172], Loss: 11.5287\n",
      "Epoch [225/300], Step [121/172], Loss: 10.6510\n",
      "Epoch [225/300], Step [122/172], Loss: 13.5997\n",
      "Epoch [225/300], Step [123/172], Loss: 11.8881\n",
      "Epoch [225/300], Step [124/172], Loss: 8.0402\n",
      "Epoch [225/300], Step [125/172], Loss: 13.2900\n",
      "Epoch [225/300], Step [126/172], Loss: 13.1375\n",
      "Epoch [225/300], Step [127/172], Loss: 11.5193\n",
      "Epoch [225/300], Step [128/172], Loss: 11.2874\n",
      "Epoch [225/300], Step [129/172], Loss: 9.6590\n",
      "Epoch [225/300], Step [130/172], Loss: 13.2934\n",
      "Epoch [225/300], Step [131/172], Loss: 8.7005\n",
      "Epoch [225/300], Step [132/172], Loss: 10.5472\n",
      "Epoch [225/300], Step [133/172], Loss: 10.7371\n",
      "Epoch [225/300], Step [134/172], Loss: 11.2033\n",
      "Epoch [225/300], Step [135/172], Loss: 9.7629\n",
      "Epoch [225/300], Step [136/172], Loss: 8.9565\n",
      "Epoch [225/300], Step [137/172], Loss: 9.5759\n",
      "Epoch [225/300], Step [138/172], Loss: 9.2039\n",
      "Epoch [225/300], Step [139/172], Loss: 10.5152\n",
      "Epoch [225/300], Step [140/172], Loss: 11.3985\n",
      "Epoch [225/300], Step [141/172], Loss: 10.1534\n",
      "Epoch [225/300], Step [142/172], Loss: 14.2671\n",
      "Epoch [225/300], Step [143/172], Loss: 11.6773\n",
      "Epoch [225/300], Step [144/172], Loss: 9.7839\n",
      "Epoch [225/300], Step [145/172], Loss: 11.1054\n",
      "Epoch [225/300], Step [146/172], Loss: 11.0728\n",
      "Epoch [225/300], Step [147/172], Loss: 6.3900\n",
      "Epoch [225/300], Step [148/172], Loss: 7.4512\n",
      "Epoch [225/300], Step [149/172], Loss: 7.9714\n",
      "Epoch [225/300], Step [150/172], Loss: 6.5374\n",
      "Epoch [225/300], Step [151/172], Loss: 6.2116\n",
      "Epoch [225/300], Step [152/172], Loss: 9.1249\n",
      "Epoch [225/300], Step [153/172], Loss: 7.1563\n",
      "Epoch [225/300], Step [154/172], Loss: 7.4090\n",
      "Epoch [225/300], Step [155/172], Loss: 7.1500\n",
      "Epoch [225/300], Step [156/172], Loss: 14.3815\n",
      "Epoch [225/300], Step [157/172], Loss: 8.6977\n",
      "Epoch [225/300], Step [158/172], Loss: 7.6043\n",
      "Epoch [225/300], Step [159/172], Loss: 9.6179\n",
      "Epoch [225/300], Step [160/172], Loss: 9.3288\n",
      "Epoch [225/300], Step [161/172], Loss: 9.2477\n",
      "Epoch [225/300], Step [162/172], Loss: 5.5543\n",
      "Epoch [225/300], Step [163/172], Loss: 6.9287\n",
      "Epoch [225/300], Step [164/172], Loss: 9.5527\n",
      "Epoch [225/300], Step [165/172], Loss: 7.4786\n",
      "Epoch [225/300], Step [166/172], Loss: 6.6267\n",
      "Epoch [225/300], Step [167/172], Loss: 10.2198\n",
      "Epoch [225/300], Step [168/172], Loss: 7.3322\n",
      "Epoch [225/300], Step [169/172], Loss: 7.3689\n",
      "Epoch [225/300], Step [170/172], Loss: 6.3179\n",
      "Epoch [225/300], Step [171/172], Loss: 8.5647\n",
      "Epoch [225/300], Step [172/172], Loss: 6.5447\n",
      "Epoch [226/300], Step [1/172], Loss: 46.4168\n",
      "Epoch [226/300], Step [2/172], Loss: 51.9473\n",
      "Epoch [226/300], Step [3/172], Loss: 45.8597\n",
      "Epoch [226/300], Step [4/172], Loss: 23.6193\n",
      "Epoch [226/300], Step [5/172], Loss: 42.7305\n",
      "Epoch [226/300], Step [6/172], Loss: 20.8807\n",
      "Epoch [226/300], Step [7/172], Loss: 28.3684\n",
      "Epoch [226/300], Step [8/172], Loss: 4.5948\n",
      "Epoch [226/300], Step [9/172], Loss: 28.0641\n",
      "Epoch [226/300], Step [10/172], Loss: 40.5733\n",
      "Epoch [226/300], Step [11/172], Loss: 52.0895\n",
      "Epoch [226/300], Step [12/172], Loss: 55.3070\n",
      "Epoch [226/300], Step [13/172], Loss: 33.9957\n",
      "Epoch [226/300], Step [14/172], Loss: 58.9200\n",
      "Epoch [226/300], Step [15/172], Loss: 48.7767\n",
      "Epoch [226/300], Step [16/172], Loss: 10.1805\n",
      "Epoch [226/300], Step [17/172], Loss: 40.0640\n",
      "Epoch [226/300], Step [18/172], Loss: 51.5747\n",
      "Epoch [226/300], Step [19/172], Loss: 72.4811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [226/300], Step [20/172], Loss: 27.2561\n",
      "Epoch [226/300], Step [21/172], Loss: 81.3695\n",
      "Epoch [226/300], Step [22/172], Loss: 53.3041\n",
      "Epoch [226/300], Step [23/172], Loss: 2.0278\n",
      "Epoch [226/300], Step [24/172], Loss: 52.8130\n",
      "Epoch [226/300], Step [25/172], Loss: 38.3532\n",
      "Epoch [226/300], Step [26/172], Loss: 45.2781\n",
      "Epoch [226/300], Step [27/172], Loss: 56.0982\n",
      "Epoch [226/300], Step [28/172], Loss: 22.7227\n",
      "Epoch [226/300], Step [29/172], Loss: 14.8741\n",
      "Epoch [226/300], Step [30/172], Loss: 56.7963\n",
      "Epoch [226/300], Step [31/172], Loss: 36.2548\n",
      "Epoch [226/300], Step [32/172], Loss: 44.1719\n",
      "Epoch [226/300], Step [33/172], Loss: 70.1083\n",
      "Epoch [226/300], Step [34/172], Loss: 2.5792\n",
      "Epoch [226/300], Step [35/172], Loss: 14.7003\n",
      "Epoch [226/300], Step [36/172], Loss: 17.1789\n",
      "Epoch [226/300], Step [37/172], Loss: 17.3531\n",
      "Epoch [226/300], Step [38/172], Loss: 33.5247\n",
      "Epoch [226/300], Step [39/172], Loss: 37.8759\n",
      "Epoch [226/300], Step [40/172], Loss: 24.9686\n",
      "Epoch [226/300], Step [41/172], Loss: 38.2271\n",
      "Epoch [226/300], Step [42/172], Loss: 43.5989\n",
      "Epoch [226/300], Step [43/172], Loss: 31.4175\n",
      "Epoch [226/300], Step [44/172], Loss: 24.3266\n",
      "Epoch [226/300], Step [45/172], Loss: 35.6489\n",
      "Epoch [226/300], Step [46/172], Loss: 18.3442\n",
      "Epoch [226/300], Step [47/172], Loss: 53.8095\n",
      "Epoch [226/300], Step [48/172], Loss: 61.1296\n",
      "Epoch [226/300], Step [49/172], Loss: 26.7388\n",
      "Epoch [226/300], Step [50/172], Loss: 46.9764\n",
      "Epoch [226/300], Step [51/172], Loss: 11.3315\n",
      "Epoch [226/300], Step [52/172], Loss: 25.1001\n",
      "Epoch [226/300], Step [53/172], Loss: 27.2113\n",
      "Epoch [226/300], Step [54/172], Loss: 20.7951\n",
      "Epoch [226/300], Step [55/172], Loss: 19.6094\n",
      "Epoch [226/300], Step [56/172], Loss: 20.5605\n",
      "Epoch [226/300], Step [57/172], Loss: 18.7267\n",
      "Epoch [226/300], Step [58/172], Loss: 16.6359\n",
      "Epoch [226/300], Step [59/172], Loss: 29.5880\n",
      "Epoch [226/300], Step [60/172], Loss: 19.7009\n",
      "Epoch [226/300], Step [61/172], Loss: 7.8669\n",
      "Epoch [226/300], Step [62/172], Loss: 17.4278\n",
      "Epoch [226/300], Step [63/172], Loss: 12.8588\n",
      "Epoch [226/300], Step [64/172], Loss: 13.6377\n",
      "Epoch [226/300], Step [65/172], Loss: 19.5922\n",
      "Epoch [226/300], Step [66/172], Loss: 7.7070\n",
      "Epoch [226/300], Step [67/172], Loss: 23.9341\n",
      "Epoch [226/300], Step [68/172], Loss: 5.5965\n",
      "Epoch [226/300], Step [69/172], Loss: 29.7327\n",
      "Epoch [226/300], Step [70/172], Loss: 32.4670\n",
      "Epoch [226/300], Step [71/172], Loss: 34.7611\n",
      "Epoch [226/300], Step [72/172], Loss: 32.1414\n",
      "Epoch [226/300], Step [73/172], Loss: 41.1274\n",
      "Epoch [226/300], Step [74/172], Loss: 21.3526\n",
      "Epoch [226/300], Step [75/172], Loss: 20.4757\n",
      "Epoch [226/300], Step [76/172], Loss: 27.3972\n",
      "Epoch [226/300], Step [77/172], Loss: 42.2752\n",
      "Epoch [226/300], Step [78/172], Loss: 32.5400\n",
      "Epoch [226/300], Step [79/172], Loss: 31.6349\n",
      "Epoch [226/300], Step [80/172], Loss: 47.5166\n",
      "Epoch [226/300], Step [81/172], Loss: 28.9787\n",
      "Epoch [226/300], Step [82/172], Loss: 35.6678\n",
      "Epoch [226/300], Step [83/172], Loss: 40.9604\n",
      "Epoch [226/300], Step [84/172], Loss: 32.4640\n",
      "Epoch [226/300], Step [85/172], Loss: 37.2153\n",
      "Epoch [226/300], Step [86/172], Loss: 33.6694\n",
      "Epoch [226/300], Step [87/172], Loss: 23.9965\n",
      "Epoch [226/300], Step [88/172], Loss: 21.8946\n",
      "Epoch [226/300], Step [89/172], Loss: 28.2799\n",
      "Epoch [226/300], Step [90/172], Loss: 20.6601\n",
      "Epoch [226/300], Step [91/172], Loss: 26.0911\n",
      "Epoch [226/300], Step [92/172], Loss: 18.8993\n",
      "Epoch [226/300], Step [93/172], Loss: 18.9561\n",
      "Epoch [226/300], Step [94/172], Loss: 26.0940\n",
      "Epoch [226/300], Step [95/172], Loss: 20.9309\n",
      "Epoch [226/300], Step [96/172], Loss: 20.5239\n",
      "Epoch [226/300], Step [97/172], Loss: 29.0914\n",
      "Epoch [226/300], Step [98/172], Loss: 20.1722\n",
      "Epoch [226/300], Step [99/172], Loss: 19.9978\n",
      "Epoch [226/300], Step [100/172], Loss: 18.5133\n",
      "Epoch [226/300], Step [101/172], Loss: 20.4007\n",
      "Epoch [226/300], Step [102/172], Loss: 18.0498\n",
      "Epoch [226/300], Step [103/172], Loss: 14.1133\n",
      "Epoch [226/300], Step [104/172], Loss: 20.4455\n",
      "Epoch [226/300], Step [105/172], Loss: 24.2030\n",
      "Epoch [226/300], Step [106/172], Loss: 17.1244\n",
      "Epoch [226/300], Step [107/172], Loss: 17.0291\n",
      "Epoch [226/300], Step [108/172], Loss: 16.8522\n",
      "Epoch [226/300], Step [109/172], Loss: 15.6227\n",
      "Epoch [226/300], Step [110/172], Loss: 18.5493\n",
      "Epoch [226/300], Step [111/172], Loss: 19.0932\n",
      "Epoch [226/300], Step [112/172], Loss: 17.3771\n",
      "Epoch [226/300], Step [113/172], Loss: 14.7778\n",
      "Epoch [226/300], Step [114/172], Loss: 15.1735\n",
      "Epoch [226/300], Step [115/172], Loss: 18.5871\n",
      "Epoch [226/300], Step [116/172], Loss: 15.0745\n",
      "Epoch [226/300], Step [117/172], Loss: 12.7639\n",
      "Epoch [226/300], Step [118/172], Loss: 12.6201\n",
      "Epoch [226/300], Step [119/172], Loss: 18.0233\n",
      "Epoch [226/300], Step [120/172], Loss: 11.5197\n",
      "Epoch [226/300], Step [121/172], Loss: 10.6063\n",
      "Epoch [226/300], Step [122/172], Loss: 13.7801\n",
      "Epoch [226/300], Step [123/172], Loss: 12.0192\n",
      "Epoch [226/300], Step [124/172], Loss: 8.0243\n",
      "Epoch [226/300], Step [125/172], Loss: 13.2777\n",
      "Epoch [226/300], Step [126/172], Loss: 13.1451\n",
      "Epoch [226/300], Step [127/172], Loss: 11.6116\n",
      "Epoch [226/300], Step [128/172], Loss: 11.3395\n",
      "Epoch [226/300], Step [129/172], Loss: 9.6675\n",
      "Epoch [226/300], Step [130/172], Loss: 13.2221\n",
      "Epoch [226/300], Step [131/172], Loss: 8.7845\n",
      "Epoch [226/300], Step [132/172], Loss: 10.6458\n",
      "Epoch [226/300], Step [133/172], Loss: 10.7739\n",
      "Epoch [226/300], Step [134/172], Loss: 11.2118\n",
      "Epoch [226/300], Step [135/172], Loss: 9.8003\n",
      "Epoch [226/300], Step [136/172], Loss: 9.1040\n",
      "Epoch [226/300], Step [137/172], Loss: 9.5101\n",
      "Epoch [226/300], Step [138/172], Loss: 9.1269\n",
      "Epoch [226/300], Step [139/172], Loss: 10.4484\n",
      "Epoch [226/300], Step [140/172], Loss: 11.4116\n",
      "Epoch [226/300], Step [141/172], Loss: 10.1065\n",
      "Epoch [226/300], Step [142/172], Loss: 14.2218\n",
      "Epoch [226/300], Step [143/172], Loss: 11.6995\n",
      "Epoch [226/300], Step [144/172], Loss: 9.7704\n",
      "Epoch [226/300], Step [145/172], Loss: 11.1756\n",
      "Epoch [226/300], Step [146/172], Loss: 10.9425\n",
      "Epoch [226/300], Step [147/172], Loss: 6.3732\n",
      "Epoch [226/300], Step [148/172], Loss: 7.4829\n",
      "Epoch [226/300], Step [149/172], Loss: 7.9987\n",
      "Epoch [226/300], Step [150/172], Loss: 6.4958\n",
      "Epoch [226/300], Step [151/172], Loss: 6.2026\n",
      "Epoch [226/300], Step [152/172], Loss: 9.2067\n",
      "Epoch [226/300], Step [153/172], Loss: 7.1715\n",
      "Epoch [226/300], Step [154/172], Loss: 7.3759\n",
      "Epoch [226/300], Step [155/172], Loss: 7.2184\n",
      "Epoch [226/300], Step [156/172], Loss: 14.3980\n",
      "Epoch [226/300], Step [157/172], Loss: 8.6797\n",
      "Epoch [226/300], Step [158/172], Loss: 7.6684\n",
      "Epoch [226/300], Step [159/172], Loss: 9.4859\n",
      "Epoch [226/300], Step [160/172], Loss: 9.3119\n",
      "Epoch [226/300], Step [161/172], Loss: 9.2922\n",
      "Epoch [226/300], Step [162/172], Loss: 5.5557\n",
      "Epoch [226/300], Step [163/172], Loss: 6.8635\n",
      "Epoch [226/300], Step [164/172], Loss: 9.4932\n",
      "Epoch [226/300], Step [165/172], Loss: 7.5584\n",
      "Epoch [226/300], Step [166/172], Loss: 6.5981\n",
      "Epoch [226/300], Step [167/172], Loss: 10.1205\n",
      "Epoch [226/300], Step [168/172], Loss: 7.2583\n",
      "Epoch [226/300], Step [169/172], Loss: 7.3866\n",
      "Epoch [226/300], Step [170/172], Loss: 6.3798\n",
      "Epoch [226/300], Step [171/172], Loss: 8.3021\n",
      "Epoch [226/300], Step [172/172], Loss: 6.4525\n",
      "Epoch [227/300], Step [1/172], Loss: 46.3266\n",
      "Epoch [227/300], Step [2/172], Loss: 51.5828\n",
      "Epoch [227/300], Step [3/172], Loss: 46.9851\n",
      "Epoch [227/300], Step [4/172], Loss: 23.5078\n",
      "Epoch [227/300], Step [5/172], Loss: 43.1809\n",
      "Epoch [227/300], Step [6/172], Loss: 20.8320\n",
      "Epoch [227/300], Step [7/172], Loss: 27.5060\n",
      "Epoch [227/300], Step [8/172], Loss: 4.6689\n",
      "Epoch [227/300], Step [9/172], Loss: 27.8580\n",
      "Epoch [227/300], Step [10/172], Loss: 40.0429\n",
      "Epoch [227/300], Step [11/172], Loss: 51.9022\n",
      "Epoch [227/300], Step [12/172], Loss: 55.0316\n",
      "Epoch [227/300], Step [13/172], Loss: 33.9266\n",
      "Epoch [227/300], Step [14/172], Loss: 59.7759\n",
      "Epoch [227/300], Step [15/172], Loss: 48.4995\n",
      "Epoch [227/300], Step [16/172], Loss: 9.7636\n",
      "Epoch [227/300], Step [17/172], Loss: 40.1374\n",
      "Epoch [227/300], Step [18/172], Loss: 51.8429\n",
      "Epoch [227/300], Step [19/172], Loss: 72.5937\n",
      "Epoch [227/300], Step [20/172], Loss: 26.9071\n",
      "Epoch [227/300], Step [21/172], Loss: 81.2551\n",
      "Epoch [227/300], Step [22/172], Loss: 52.5502\n",
      "Epoch [227/300], Step [23/172], Loss: 1.9990\n",
      "Epoch [227/300], Step [24/172], Loss: 52.8394\n",
      "Epoch [227/300], Step [25/172], Loss: 38.2754\n",
      "Epoch [227/300], Step [26/172], Loss: 45.2877\n",
      "Epoch [227/300], Step [27/172], Loss: 55.6205\n",
      "Epoch [227/300], Step [28/172], Loss: 22.7886\n",
      "Epoch [227/300], Step [29/172], Loss: 14.9691\n",
      "Epoch [227/300], Step [30/172], Loss: 57.1619\n",
      "Epoch [227/300], Step [31/172], Loss: 36.5288\n",
      "Epoch [227/300], Step [32/172], Loss: 44.3551\n",
      "Epoch [227/300], Step [33/172], Loss: 70.1842\n",
      "Epoch [227/300], Step [34/172], Loss: 2.5863\n",
      "Epoch [227/300], Step [35/172], Loss: 14.6424\n",
      "Epoch [227/300], Step [36/172], Loss: 17.1801\n",
      "Epoch [227/300], Step [37/172], Loss: 17.2853\n",
      "Epoch [227/300], Step [38/172], Loss: 33.3506\n",
      "Epoch [227/300], Step [39/172], Loss: 37.7963\n",
      "Epoch [227/300], Step [40/172], Loss: 24.9394\n",
      "Epoch [227/300], Step [41/172], Loss: 37.8494\n",
      "Epoch [227/300], Step [42/172], Loss: 43.4042\n",
      "Epoch [227/300], Step [43/172], Loss: 31.0827\n",
      "Epoch [227/300], Step [44/172], Loss: 24.4453\n",
      "Epoch [227/300], Step [45/172], Loss: 35.9501\n",
      "Epoch [227/300], Step [46/172], Loss: 18.6185\n",
      "Epoch [227/300], Step [47/172], Loss: 54.0243\n",
      "Epoch [227/300], Step [48/172], Loss: 62.0996\n",
      "Epoch [227/300], Step [49/172], Loss: 26.8461\n",
      "Epoch [227/300], Step [50/172], Loss: 47.2939\n",
      "Epoch [227/300], Step [51/172], Loss: 11.5050\n",
      "Epoch [227/300], Step [52/172], Loss: 25.7772\n",
      "Epoch [227/300], Step [53/172], Loss: 27.6684\n",
      "Epoch [227/300], Step [54/172], Loss: 20.9681\n",
      "Epoch [227/300], Step [55/172], Loss: 20.1523\n",
      "Epoch [227/300], Step [56/172], Loss: 20.6206\n",
      "Epoch [227/300], Step [57/172], Loss: 18.5935\n",
      "Epoch [227/300], Step [58/172], Loss: 16.8720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [227/300], Step [59/172], Loss: 29.9437\n",
      "Epoch [227/300], Step [60/172], Loss: 19.5021\n",
      "Epoch [227/300], Step [61/172], Loss: 8.1403\n",
      "Epoch [227/300], Step [62/172], Loss: 17.4153\n",
      "Epoch [227/300], Step [63/172], Loss: 12.9862\n",
      "Epoch [227/300], Step [64/172], Loss: 14.0677\n",
      "Epoch [227/300], Step [65/172], Loss: 19.8934\n",
      "Epoch [227/300], Step [66/172], Loss: 7.9802\n",
      "Epoch [227/300], Step [67/172], Loss: 24.2259\n",
      "Epoch [227/300], Step [68/172], Loss: 5.8351\n",
      "Epoch [227/300], Step [69/172], Loss: 29.5234\n",
      "Epoch [227/300], Step [70/172], Loss: 32.1783\n",
      "Epoch [227/300], Step [71/172], Loss: 34.4292\n",
      "Epoch [227/300], Step [72/172], Loss: 31.7215\n",
      "Epoch [227/300], Step [73/172], Loss: 40.5927\n",
      "Epoch [227/300], Step [74/172], Loss: 21.2480\n",
      "Epoch [227/300], Step [75/172], Loss: 19.9696\n",
      "Epoch [227/300], Step [76/172], Loss: 27.2227\n",
      "Epoch [227/300], Step [77/172], Loss: 41.8342\n",
      "Epoch [227/300], Step [78/172], Loss: 32.2056\n",
      "Epoch [227/300], Step [79/172], Loss: 31.1221\n",
      "Epoch [227/300], Step [80/172], Loss: 47.0969\n",
      "Epoch [227/300], Step [81/172], Loss: 28.6637\n",
      "Epoch [227/300], Step [82/172], Loss: 35.5162\n",
      "Epoch [227/300], Step [83/172], Loss: 40.9564\n",
      "Epoch [227/300], Step [84/172], Loss: 32.4176\n",
      "Epoch [227/300], Step [85/172], Loss: 37.2473\n",
      "Epoch [227/300], Step [86/172], Loss: 33.6667\n",
      "Epoch [227/300], Step [87/172], Loss: 23.9821\n",
      "Epoch [227/300], Step [88/172], Loss: 21.7717\n",
      "Epoch [227/300], Step [89/172], Loss: 28.1546\n",
      "Epoch [227/300], Step [90/172], Loss: 20.7500\n",
      "Epoch [227/300], Step [91/172], Loss: 26.1371\n",
      "Epoch [227/300], Step [92/172], Loss: 18.9332\n",
      "Epoch [227/300], Step [93/172], Loss: 19.0112\n",
      "Epoch [227/300], Step [94/172], Loss: 26.1797\n",
      "Epoch [227/300], Step [95/172], Loss: 21.1146\n",
      "Epoch [227/300], Step [96/172], Loss: 20.5174\n",
      "Epoch [227/300], Step [97/172], Loss: 29.1343\n",
      "Epoch [227/300], Step [98/172], Loss: 20.2338\n",
      "Epoch [227/300], Step [99/172], Loss: 20.1065\n",
      "Epoch [227/300], Step [100/172], Loss: 18.7340\n",
      "Epoch [227/300], Step [101/172], Loss: 20.5516\n",
      "Epoch [227/300], Step [102/172], Loss: 18.1543\n",
      "Epoch [227/300], Step [103/172], Loss: 14.1611\n",
      "Epoch [227/300], Step [104/172], Loss: 20.5441\n",
      "Epoch [227/300], Step [105/172], Loss: 24.4861\n",
      "Epoch [227/300], Step [106/172], Loss: 17.3038\n",
      "Epoch [227/300], Step [107/172], Loss: 17.0791\n",
      "Epoch [227/300], Step [108/172], Loss: 17.0540\n",
      "Epoch [227/300], Step [109/172], Loss: 15.7697\n",
      "Epoch [227/300], Step [110/172], Loss: 18.7100\n",
      "Epoch [227/300], Step [111/172], Loss: 19.2135\n",
      "Epoch [227/300], Step [112/172], Loss: 17.4549\n",
      "Epoch [227/300], Step [113/172], Loss: 14.9426\n",
      "Epoch [227/300], Step [114/172], Loss: 15.2701\n",
      "Epoch [227/300], Step [115/172], Loss: 18.6061\n",
      "Epoch [227/300], Step [116/172], Loss: 15.0763\n",
      "Epoch [227/300], Step [117/172], Loss: 12.8310\n",
      "Epoch [227/300], Step [118/172], Loss: 12.7077\n",
      "Epoch [227/300], Step [119/172], Loss: 18.2083\n",
      "Epoch [227/300], Step [120/172], Loss: 11.5921\n",
      "Epoch [227/300], Step [121/172], Loss: 10.6030\n",
      "Epoch [227/300], Step [122/172], Loss: 13.8361\n",
      "Epoch [227/300], Step [123/172], Loss: 12.1625\n",
      "Epoch [227/300], Step [124/172], Loss: 7.9637\n",
      "Epoch [227/300], Step [125/172], Loss: 13.3019\n",
      "Epoch [227/300], Step [126/172], Loss: 13.1287\n",
      "Epoch [227/300], Step [127/172], Loss: 11.5459\n",
      "Epoch [227/300], Step [128/172], Loss: 11.2886\n",
      "Epoch [227/300], Step [129/172], Loss: 9.6544\n",
      "Epoch [227/300], Step [130/172], Loss: 13.2382\n",
      "Epoch [227/300], Step [131/172], Loss: 8.7939\n",
      "Epoch [227/300], Step [132/172], Loss: 10.7148\n",
      "Epoch [227/300], Step [133/172], Loss: 10.8360\n",
      "Epoch [227/300], Step [134/172], Loss: 11.1603\n",
      "Epoch [227/300], Step [135/172], Loss: 9.7499\n",
      "Epoch [227/300], Step [136/172], Loss: 9.0361\n",
      "Epoch [227/300], Step [137/172], Loss: 9.5022\n",
      "Epoch [227/300], Step [138/172], Loss: 9.2012\n",
      "Epoch [227/300], Step [139/172], Loss: 10.4822\n",
      "Epoch [227/300], Step [140/172], Loss: 11.4274\n",
      "Epoch [227/300], Step [141/172], Loss: 10.1468\n",
      "Epoch [227/300], Step [142/172], Loss: 14.3599\n",
      "Epoch [227/300], Step [143/172], Loss: 11.7916\n",
      "Epoch [227/300], Step [144/172], Loss: 9.8131\n",
      "Epoch [227/300], Step [145/172], Loss: 11.2728\n",
      "Epoch [227/300], Step [146/172], Loss: 10.9679\n",
      "Epoch [227/300], Step [147/172], Loss: 6.3372\n",
      "Epoch [227/300], Step [148/172], Loss: 7.4294\n",
      "Epoch [227/300], Step [149/172], Loss: 7.9225\n",
      "Epoch [227/300], Step [150/172], Loss: 6.5243\n",
      "Epoch [227/300], Step [151/172], Loss: 6.1993\n",
      "Epoch [227/300], Step [152/172], Loss: 9.1876\n",
      "Epoch [227/300], Step [153/172], Loss: 7.1024\n",
      "Epoch [227/300], Step [154/172], Loss: 7.3055\n",
      "Epoch [227/300], Step [155/172], Loss: 7.1064\n",
      "Epoch [227/300], Step [156/172], Loss: 14.5096\n",
      "Epoch [227/300], Step [157/172], Loss: 8.7318\n",
      "Epoch [227/300], Step [158/172], Loss: 7.6559\n",
      "Epoch [227/300], Step [159/172], Loss: 9.5366\n",
      "Epoch [227/300], Step [160/172], Loss: 9.4356\n",
      "Epoch [227/300], Step [161/172], Loss: 9.0915\n",
      "Epoch [227/300], Step [162/172], Loss: 5.5748\n",
      "Epoch [227/300], Step [163/172], Loss: 6.9186\n",
      "Epoch [227/300], Step [164/172], Loss: 9.5508\n",
      "Epoch [227/300], Step [165/172], Loss: 7.5180\n",
      "Epoch [227/300], Step [166/172], Loss: 6.5670\n",
      "Epoch [227/300], Step [167/172], Loss: 10.3105\n",
      "Epoch [227/300], Step [168/172], Loss: 7.1796\n",
      "Epoch [227/300], Step [169/172], Loss: 7.4995\n",
      "Epoch [227/300], Step [170/172], Loss: 6.3622\n",
      "Epoch [227/300], Step [171/172], Loss: 8.3223\n",
      "Epoch [227/300], Step [172/172], Loss: 6.3064\n",
      "Epoch [228/300], Step [1/172], Loss: 46.0480\n",
      "Epoch [228/300], Step [2/172], Loss: 50.3551\n",
      "Epoch [228/300], Step [3/172], Loss: 46.4261\n",
      "Epoch [228/300], Step [4/172], Loss: 23.3389\n",
      "Epoch [228/300], Step [5/172], Loss: 42.0750\n",
      "Epoch [228/300], Step [6/172], Loss: 20.9162\n",
      "Epoch [228/300], Step [7/172], Loss: 27.6576\n",
      "Epoch [228/300], Step [8/172], Loss: 4.8553\n",
      "Epoch [228/300], Step [9/172], Loss: 27.7622\n",
      "Epoch [228/300], Step [10/172], Loss: 40.0872\n",
      "Epoch [228/300], Step [11/172], Loss: 52.0091\n",
      "Epoch [228/300], Step [12/172], Loss: 54.7372\n",
      "Epoch [228/300], Step [13/172], Loss: 33.8516\n",
      "Epoch [228/300], Step [14/172], Loss: 59.7676\n",
      "Epoch [228/300], Step [15/172], Loss: 48.3346\n",
      "Epoch [228/300], Step [16/172], Loss: 9.8283\n",
      "Epoch [228/300], Step [17/172], Loss: 39.7588\n",
      "Epoch [228/300], Step [18/172], Loss: 51.3401\n",
      "Epoch [228/300], Step [19/172], Loss: 72.1449\n",
      "Epoch [228/300], Step [20/172], Loss: 28.6418\n",
      "Epoch [228/300], Step [21/172], Loss: 81.0825\n",
      "Epoch [228/300], Step [22/172], Loss: 52.4742\n",
      "Epoch [228/300], Step [23/172], Loss: 2.0032\n",
      "Epoch [228/300], Step [24/172], Loss: 52.7121\n",
      "Epoch [228/300], Step [25/172], Loss: 38.2870\n",
      "Epoch [228/300], Step [26/172], Loss: 45.1189\n",
      "Epoch [228/300], Step [27/172], Loss: 55.3768\n",
      "Epoch [228/300], Step [28/172], Loss: 22.6909\n",
      "Epoch [228/300], Step [29/172], Loss: 14.9605\n",
      "Epoch [228/300], Step [30/172], Loss: 57.3604\n",
      "Epoch [228/300], Step [31/172], Loss: 36.6970\n",
      "Epoch [228/300], Step [32/172], Loss: 44.4747\n",
      "Epoch [228/300], Step [33/172], Loss: 70.1817\n",
      "Epoch [228/300], Step [34/172], Loss: 2.6160\n",
      "Epoch [228/300], Step [35/172], Loss: 14.7423\n",
      "Epoch [228/300], Step [36/172], Loss: 17.4281\n",
      "Epoch [228/300], Step [37/172], Loss: 17.2541\n",
      "Epoch [228/300], Step [38/172], Loss: 33.4224\n",
      "Epoch [228/300], Step [39/172], Loss: 37.8090\n",
      "Epoch [228/300], Step [40/172], Loss: 25.0559\n",
      "Epoch [228/300], Step [41/172], Loss: 37.8616\n",
      "Epoch [228/300], Step [42/172], Loss: 43.6453\n",
      "Epoch [228/300], Step [43/172], Loss: 31.1998\n",
      "Epoch [228/300], Step [44/172], Loss: 24.5901\n",
      "Epoch [228/300], Step [45/172], Loss: 36.1399\n",
      "Epoch [228/300], Step [46/172], Loss: 18.4155\n",
      "Epoch [228/300], Step [47/172], Loss: 54.0967\n",
      "Epoch [228/300], Step [48/172], Loss: 60.8815\n",
      "Epoch [228/300], Step [49/172], Loss: 27.0333\n",
      "Epoch [228/300], Step [50/172], Loss: 47.3050\n",
      "Epoch [228/300], Step [51/172], Loss: 11.3893\n",
      "Epoch [228/300], Step [52/172], Loss: 25.6267\n",
      "Epoch [228/300], Step [53/172], Loss: 27.6226\n",
      "Epoch [228/300], Step [54/172], Loss: 20.8909\n",
      "Epoch [228/300], Step [55/172], Loss: 20.1515\n",
      "Epoch [228/300], Step [56/172], Loss: 20.6019\n",
      "Epoch [228/300], Step [57/172], Loss: 18.7824\n",
      "Epoch [228/300], Step [58/172], Loss: 16.8399\n",
      "Epoch [228/300], Step [59/172], Loss: 30.1903\n",
      "Epoch [228/300], Step [60/172], Loss: 19.8015\n",
      "Epoch [228/300], Step [61/172], Loss: 8.2249\n",
      "Epoch [228/300], Step [62/172], Loss: 17.6165\n",
      "Epoch [228/300], Step [63/172], Loss: 13.3320\n",
      "Epoch [228/300], Step [64/172], Loss: 13.9343\n",
      "Epoch [228/300], Step [65/172], Loss: 19.8972\n",
      "Epoch [228/300], Step [66/172], Loss: 7.9981\n",
      "Epoch [228/300], Step [67/172], Loss: 24.5485\n",
      "Epoch [228/300], Step [68/172], Loss: 5.9353\n",
      "Epoch [228/300], Step [69/172], Loss: 29.5395\n",
      "Epoch [228/300], Step [70/172], Loss: 32.4778\n",
      "Epoch [228/300], Step [71/172], Loss: 34.4539\n",
      "Epoch [228/300], Step [72/172], Loss: 31.3760\n",
      "Epoch [228/300], Step [73/172], Loss: 40.3306\n",
      "Epoch [228/300], Step [74/172], Loss: 21.0147\n",
      "Epoch [228/300], Step [75/172], Loss: 20.0535\n",
      "Epoch [228/300], Step [76/172], Loss: 27.2912\n",
      "Epoch [228/300], Step [77/172], Loss: 41.6074\n",
      "Epoch [228/300], Step [78/172], Loss: 32.0702\n",
      "Epoch [228/300], Step [79/172], Loss: 31.0132\n",
      "Epoch [228/300], Step [80/172], Loss: 47.0844\n",
      "Epoch [228/300], Step [81/172], Loss: 28.4603\n",
      "Epoch [228/300], Step [82/172], Loss: 35.8378\n",
      "Epoch [228/300], Step [83/172], Loss: 40.4549\n",
      "Epoch [228/300], Step [84/172], Loss: 32.1709\n",
      "Epoch [228/300], Step [85/172], Loss: 37.0446\n",
      "Epoch [228/300], Step [86/172], Loss: 33.6386\n",
      "Epoch [228/300], Step [87/172], Loss: 23.8486\n",
      "Epoch [228/300], Step [88/172], Loss: 21.8134\n",
      "Epoch [228/300], Step [89/172], Loss: 28.0914\n",
      "Epoch [228/300], Step [90/172], Loss: 20.9129\n",
      "Epoch [228/300], Step [91/172], Loss: 26.1047\n",
      "Epoch [228/300], Step [92/172], Loss: 18.8390\n",
      "Epoch [228/300], Step [93/172], Loss: 18.8299\n",
      "Epoch [228/300], Step [94/172], Loss: 26.1469\n",
      "Epoch [228/300], Step [95/172], Loss: 21.1195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [228/300], Step [96/172], Loss: 20.4739\n",
      "Epoch [228/300], Step [97/172], Loss: 29.2083\n",
      "Epoch [228/300], Step [98/172], Loss: 20.2758\n",
      "Epoch [228/300], Step [99/172], Loss: 20.1617\n",
      "Epoch [228/300], Step [100/172], Loss: 18.8179\n",
      "Epoch [228/300], Step [101/172], Loss: 20.5726\n",
      "Epoch [228/300], Step [102/172], Loss: 18.3520\n",
      "Epoch [228/300], Step [103/172], Loss: 14.2303\n",
      "Epoch [228/300], Step [104/172], Loss: 20.6784\n",
      "Epoch [228/300], Step [105/172], Loss: 24.8147\n",
      "Epoch [228/300], Step [106/172], Loss: 17.3264\n",
      "Epoch [228/300], Step [107/172], Loss: 17.2017\n",
      "Epoch [228/300], Step [108/172], Loss: 17.1527\n",
      "Epoch [228/300], Step [109/172], Loss: 15.9869\n",
      "Epoch [228/300], Step [110/172], Loss: 18.4966\n",
      "Epoch [228/300], Step [111/172], Loss: 19.2818\n",
      "Epoch [228/300], Step [112/172], Loss: 17.4469\n",
      "Epoch [228/300], Step [113/172], Loss: 14.6954\n",
      "Epoch [228/300], Step [114/172], Loss: 15.3512\n",
      "Epoch [228/300], Step [115/172], Loss: 18.8607\n",
      "Epoch [228/300], Step [116/172], Loss: 14.9091\n",
      "Epoch [228/300], Step [117/172], Loss: 12.9081\n",
      "Epoch [228/300], Step [118/172], Loss: 12.7793\n",
      "Epoch [228/300], Step [119/172], Loss: 18.2804\n",
      "Epoch [228/300], Step [120/172], Loss: 11.6137\n",
      "Epoch [228/300], Step [121/172], Loss: 10.6529\n",
      "Epoch [228/300], Step [122/172], Loss: 13.9412\n",
      "Epoch [228/300], Step [123/172], Loss: 12.3547\n",
      "Epoch [228/300], Step [124/172], Loss: 8.0468\n",
      "Epoch [228/300], Step [125/172], Loss: 13.4013\n",
      "Epoch [228/300], Step [126/172], Loss: 13.2450\n",
      "Epoch [228/300], Step [127/172], Loss: 11.4567\n",
      "Epoch [228/300], Step [128/172], Loss: 11.0976\n",
      "Epoch [228/300], Step [129/172], Loss: 9.7623\n",
      "Epoch [228/300], Step [130/172], Loss: 13.2551\n",
      "Epoch [228/300], Step [131/172], Loss: 8.8897\n",
      "Epoch [228/300], Step [132/172], Loss: 10.7721\n",
      "Epoch [228/300], Step [133/172], Loss: 10.9416\n",
      "Epoch [228/300], Step [134/172], Loss: 11.2304\n",
      "Epoch [228/300], Step [135/172], Loss: 9.8701\n",
      "Epoch [228/300], Step [136/172], Loss: 9.1135\n",
      "Epoch [228/300], Step [137/172], Loss: 9.5747\n",
      "Epoch [228/300], Step [138/172], Loss: 9.2961\n",
      "Epoch [228/300], Step [139/172], Loss: 10.7447\n",
      "Epoch [228/300], Step [140/172], Loss: 11.3293\n",
      "Epoch [228/300], Step [141/172], Loss: 10.2647\n",
      "Epoch [228/300], Step [142/172], Loss: 14.3101\n",
      "Epoch [228/300], Step [143/172], Loss: 11.7668\n",
      "Epoch [228/300], Step [144/172], Loss: 9.9111\n",
      "Epoch [228/300], Step [145/172], Loss: 11.3724\n",
      "Epoch [228/300], Step [146/172], Loss: 10.8996\n",
      "Epoch [228/300], Step [147/172], Loss: 6.4194\n",
      "Epoch [228/300], Step [148/172], Loss: 7.5107\n",
      "Epoch [228/300], Step [149/172], Loss: 8.0221\n",
      "Epoch [228/300], Step [150/172], Loss: 6.5260\n",
      "Epoch [228/300], Step [151/172], Loss: 6.1586\n",
      "Epoch [228/300], Step [152/172], Loss: 9.2912\n",
      "Epoch [228/300], Step [153/172], Loss: 7.1735\n",
      "Epoch [228/300], Step [154/172], Loss: 7.3125\n",
      "Epoch [228/300], Step [155/172], Loss: 7.3293\n",
      "Epoch [228/300], Step [156/172], Loss: 14.5129\n",
      "Epoch [228/300], Step [157/172], Loss: 8.7199\n",
      "Epoch [228/300], Step [158/172], Loss: 7.6739\n",
      "Epoch [228/300], Step [159/172], Loss: 9.3794\n",
      "Epoch [228/300], Step [160/172], Loss: 9.4414\n",
      "Epoch [228/300], Step [161/172], Loss: 9.3809\n",
      "Epoch [228/300], Step [162/172], Loss: 5.5899\n",
      "Epoch [228/300], Step [163/172], Loss: 6.9079\n",
      "Epoch [228/300], Step [164/172], Loss: 9.6738\n",
      "Epoch [228/300], Step [165/172], Loss: 7.5212\n",
      "Epoch [228/300], Step [166/172], Loss: 6.6234\n",
      "Epoch [228/300], Step [167/172], Loss: 10.6159\n",
      "Epoch [228/300], Step [168/172], Loss: 7.2892\n",
      "Epoch [228/300], Step [169/172], Loss: 7.6403\n",
      "Epoch [228/300], Step [170/172], Loss: 6.3634\n",
      "Epoch [228/300], Step [171/172], Loss: 8.4854\n",
      "Epoch [228/300], Step [172/172], Loss: 6.4678\n",
      "Epoch [229/300], Step [1/172], Loss: 45.7068\n",
      "Epoch [229/300], Step [2/172], Loss: 49.0708\n",
      "Epoch [229/300], Step [3/172], Loss: 46.7166\n",
      "Epoch [229/300], Step [4/172], Loss: 23.0153\n",
      "Epoch [229/300], Step [5/172], Loss: 41.5481\n",
      "Epoch [229/300], Step [6/172], Loss: 21.2358\n",
      "Epoch [229/300], Step [7/172], Loss: 28.3326\n",
      "Epoch [229/300], Step [8/172], Loss: 4.6656\n",
      "Epoch [229/300], Step [9/172], Loss: 27.8695\n",
      "Epoch [229/300], Step [10/172], Loss: 41.0451\n",
      "Epoch [229/300], Step [11/172], Loss: 52.3513\n",
      "Epoch [229/300], Step [12/172], Loss: 54.0252\n",
      "Epoch [229/300], Step [13/172], Loss: 33.7975\n",
      "Epoch [229/300], Step [14/172], Loss: 59.0188\n",
      "Epoch [229/300], Step [15/172], Loss: 48.2374\n",
      "Epoch [229/300], Step [16/172], Loss: 10.2522\n",
      "Epoch [229/300], Step [17/172], Loss: 39.4727\n",
      "Epoch [229/300], Step [18/172], Loss: 51.1318\n",
      "Epoch [229/300], Step [19/172], Loss: 71.2306\n",
      "Epoch [229/300], Step [20/172], Loss: 30.3860\n",
      "Epoch [229/300], Step [21/172], Loss: 80.4098\n",
      "Epoch [229/300], Step [22/172], Loss: 53.1194\n",
      "Epoch [229/300], Step [23/172], Loss: 1.7390\n",
      "Epoch [229/300], Step [24/172], Loss: 52.0147\n",
      "Epoch [229/300], Step [25/172], Loss: 38.1806\n",
      "Epoch [229/300], Step [26/172], Loss: 44.7625\n",
      "Epoch [229/300], Step [27/172], Loss: 55.5993\n",
      "Epoch [229/300], Step [28/172], Loss: 22.5677\n",
      "Epoch [229/300], Step [29/172], Loss: 14.8642\n",
      "Epoch [229/300], Step [30/172], Loss: 57.3813\n",
      "Epoch [229/300], Step [31/172], Loss: 36.6955\n",
      "Epoch [229/300], Step [32/172], Loss: 44.1646\n",
      "Epoch [229/300], Step [33/172], Loss: 70.2614\n",
      "Epoch [229/300], Step [34/172], Loss: 2.6371\n",
      "Epoch [229/300], Step [35/172], Loss: 14.7773\n",
      "Epoch [229/300], Step [36/172], Loss: 17.3540\n",
      "Epoch [229/300], Step [37/172], Loss: 17.3005\n",
      "Epoch [229/300], Step [38/172], Loss: 33.7904\n",
      "Epoch [229/300], Step [39/172], Loss: 38.0303\n",
      "Epoch [229/300], Step [40/172], Loss: 25.1632\n",
      "Epoch [229/300], Step [41/172], Loss: 38.7034\n",
      "Epoch [229/300], Step [42/172], Loss: 44.2576\n",
      "Epoch [229/300], Step [43/172], Loss: 31.7143\n",
      "Epoch [229/300], Step [44/172], Loss: 24.8947\n",
      "Epoch [229/300], Step [45/172], Loss: 36.5702\n",
      "Epoch [229/300], Step [46/172], Loss: 18.6645\n",
      "Epoch [229/300], Step [47/172], Loss: 54.5581\n",
      "Epoch [229/300], Step [48/172], Loss: 60.8740\n",
      "Epoch [229/300], Step [49/172], Loss: 27.6669\n",
      "Epoch [229/300], Step [50/172], Loss: 47.2004\n",
      "Epoch [229/300], Step [51/172], Loss: 11.5865\n",
      "Epoch [229/300], Step [52/172], Loss: 26.1217\n",
      "Epoch [229/300], Step [53/172], Loss: 28.0650\n",
      "Epoch [229/300], Step [54/172], Loss: 21.8365\n",
      "Epoch [229/300], Step [55/172], Loss: 20.8392\n",
      "Epoch [229/300], Step [56/172], Loss: 20.7932\n",
      "Epoch [229/300], Step [57/172], Loss: 18.8325\n",
      "Epoch [229/300], Step [58/172], Loss: 17.2954\n",
      "Epoch [229/300], Step [59/172], Loss: 30.4448\n",
      "Epoch [229/300], Step [60/172], Loss: 19.6543\n",
      "Epoch [229/300], Step [61/172], Loss: 8.4877\n",
      "Epoch [229/300], Step [62/172], Loss: 17.4983\n",
      "Epoch [229/300], Step [63/172], Loss: 13.6612\n",
      "Epoch [229/300], Step [64/172], Loss: 14.3733\n",
      "Epoch [229/300], Step [65/172], Loss: 20.0057\n",
      "Epoch [229/300], Step [66/172], Loss: 8.1658\n",
      "Epoch [229/300], Step [67/172], Loss: 24.6068\n",
      "Epoch [229/300], Step [68/172], Loss: 5.8982\n",
      "Epoch [229/300], Step [69/172], Loss: 29.4194\n",
      "Epoch [229/300], Step [70/172], Loss: 33.0069\n",
      "Epoch [229/300], Step [71/172], Loss: 34.3331\n",
      "Epoch [229/300], Step [72/172], Loss: 31.0920\n",
      "Epoch [229/300], Step [73/172], Loss: 39.9792\n",
      "Epoch [229/300], Step [74/172], Loss: 20.8760\n",
      "Epoch [229/300], Step [75/172], Loss: 19.7469\n",
      "Epoch [229/300], Step [76/172], Loss: 27.4223\n",
      "Epoch [229/300], Step [77/172], Loss: 41.6888\n",
      "Epoch [229/300], Step [78/172], Loss: 32.0459\n",
      "Epoch [229/300], Step [79/172], Loss: 30.9551\n",
      "Epoch [229/300], Step [80/172], Loss: 47.1263\n",
      "Epoch [229/300], Step [81/172], Loss: 28.3310\n",
      "Epoch [229/300], Step [82/172], Loss: 35.8884\n",
      "Epoch [229/300], Step [83/172], Loss: 40.1895\n",
      "Epoch [229/300], Step [84/172], Loss: 32.2172\n",
      "Epoch [229/300], Step [85/172], Loss: 36.7717\n",
      "Epoch [229/300], Step [86/172], Loss: 33.3857\n",
      "Epoch [229/300], Step [87/172], Loss: 23.7088\n",
      "Epoch [229/300], Step [88/172], Loss: 21.9207\n",
      "Epoch [229/300], Step [89/172], Loss: 28.0540\n",
      "Epoch [229/300], Step [90/172], Loss: 20.6496\n",
      "Epoch [229/300], Step [91/172], Loss: 26.0495\n",
      "Epoch [229/300], Step [92/172], Loss: 18.7640\n",
      "Epoch [229/300], Step [93/172], Loss: 18.6588\n",
      "Epoch [229/300], Step [94/172], Loss: 25.8920\n",
      "Epoch [229/300], Step [95/172], Loss: 20.9962\n",
      "Epoch [229/300], Step [96/172], Loss: 20.5703\n",
      "Epoch [229/300], Step [97/172], Loss: 29.2433\n",
      "Epoch [229/300], Step [98/172], Loss: 20.3020\n",
      "Epoch [229/300], Step [99/172], Loss: 20.1750\n",
      "Epoch [229/300], Step [100/172], Loss: 18.9234\n",
      "Epoch [229/300], Step [101/172], Loss: 20.6177\n",
      "Epoch [229/300], Step [102/172], Loss: 18.3401\n",
      "Epoch [229/300], Step [103/172], Loss: 14.2354\n",
      "Epoch [229/300], Step [104/172], Loss: 20.7235\n",
      "Epoch [229/300], Step [105/172], Loss: 24.9087\n",
      "Epoch [229/300], Step [106/172], Loss: 17.3438\n",
      "Epoch [229/300], Step [107/172], Loss: 17.3250\n",
      "Epoch [229/300], Step [108/172], Loss: 17.2974\n",
      "Epoch [229/300], Step [109/172], Loss: 16.0868\n",
      "Epoch [229/300], Step [110/172], Loss: 18.6481\n",
      "Epoch [229/300], Step [111/172], Loss: 19.3547\n",
      "Epoch [229/300], Step [112/172], Loss: 17.2372\n",
      "Epoch [229/300], Step [113/172], Loss: 14.7666\n",
      "Epoch [229/300], Step [114/172], Loss: 15.4549\n",
      "Epoch [229/300], Step [115/172], Loss: 18.9379\n",
      "Epoch [229/300], Step [116/172], Loss: 14.8783\n",
      "Epoch [229/300], Step [117/172], Loss: 12.8986\n",
      "Epoch [229/300], Step [118/172], Loss: 12.7879\n",
      "Epoch [229/300], Step [119/172], Loss: 18.3309\n",
      "Epoch [229/300], Step [120/172], Loss: 11.5813\n",
      "Epoch [229/300], Step [121/172], Loss: 10.7469\n",
      "Epoch [229/300], Step [122/172], Loss: 13.8961\n",
      "Epoch [229/300], Step [123/172], Loss: 12.4402\n",
      "Epoch [229/300], Step [124/172], Loss: 8.0034\n",
      "Epoch [229/300], Step [125/172], Loss: 13.4302\n",
      "Epoch [229/300], Step [126/172], Loss: 13.3787\n",
      "Epoch [229/300], Step [127/172], Loss: 11.5230\n",
      "Epoch [229/300], Step [128/172], Loss: 11.0543\n",
      "Epoch [229/300], Step [129/172], Loss: 9.7865\n",
      "Epoch [229/300], Step [130/172], Loss: 13.2140\n",
      "Epoch [229/300], Step [131/172], Loss: 8.8879\n",
      "Epoch [229/300], Step [132/172], Loss: 10.7921\n",
      "Epoch [229/300], Step [133/172], Loss: 10.9331\n",
      "Epoch [229/300], Step [134/172], Loss: 11.2110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [229/300], Step [135/172], Loss: 9.9271\n",
      "Epoch [229/300], Step [136/172], Loss: 9.2816\n",
      "Epoch [229/300], Step [137/172], Loss: 9.4782\n",
      "Epoch [229/300], Step [138/172], Loss: 9.3801\n",
      "Epoch [229/300], Step [139/172], Loss: 10.6639\n",
      "Epoch [229/300], Step [140/172], Loss: 11.4413\n",
      "Epoch [229/300], Step [141/172], Loss: 10.2797\n",
      "Epoch [229/300], Step [142/172], Loss: 14.0935\n",
      "Epoch [229/300], Step [143/172], Loss: 11.7464\n",
      "Epoch [229/300], Step [144/172], Loss: 9.9719\n",
      "Epoch [229/300], Step [145/172], Loss: 11.1238\n",
      "Epoch [229/300], Step [146/172], Loss: 11.0375\n",
      "Epoch [229/300], Step [147/172], Loss: 6.3886\n",
      "Epoch [229/300], Step [148/172], Loss: 7.5396\n",
      "Epoch [229/300], Step [149/172], Loss: 8.0210\n",
      "Epoch [229/300], Step [150/172], Loss: 6.5700\n",
      "Epoch [229/300], Step [151/172], Loss: 6.2092\n",
      "Epoch [229/300], Step [152/172], Loss: 9.3550\n",
      "Epoch [229/300], Step [153/172], Loss: 7.0519\n",
      "Epoch [229/300], Step [154/172], Loss: 7.2972\n",
      "Epoch [229/300], Step [155/172], Loss: 7.2102\n",
      "Epoch [229/300], Step [156/172], Loss: 14.5262\n",
      "Epoch [229/300], Step [157/172], Loss: 8.7406\n",
      "Epoch [229/300], Step [158/172], Loss: 7.6667\n",
      "Epoch [229/300], Step [159/172], Loss: 9.4010\n",
      "Epoch [229/300], Step [160/172], Loss: 9.3831\n",
      "Epoch [229/300], Step [161/172], Loss: 9.3833\n",
      "Epoch [229/300], Step [162/172], Loss: 5.6000\n",
      "Epoch [229/300], Step [163/172], Loss: 6.8773\n",
      "Epoch [229/300], Step [164/172], Loss: 9.6327\n",
      "Epoch [229/300], Step [165/172], Loss: 7.5168\n",
      "Epoch [229/300], Step [166/172], Loss: 6.6637\n",
      "Epoch [229/300], Step [167/172], Loss: 10.4197\n",
      "Epoch [229/300], Step [168/172], Loss: 7.2744\n",
      "Epoch [229/300], Step [169/172], Loss: 7.4583\n",
      "Epoch [229/300], Step [170/172], Loss: 6.3515\n",
      "Epoch [229/300], Step [171/172], Loss: 8.7295\n",
      "Epoch [229/300], Step [172/172], Loss: 6.2866\n",
      "Epoch [230/300], Step [1/172], Loss: 45.4119\n",
      "Epoch [230/300], Step [2/172], Loss: 49.6889\n",
      "Epoch [230/300], Step [3/172], Loss: 45.4358\n",
      "Epoch [230/300], Step [4/172], Loss: 23.1589\n",
      "Epoch [230/300], Step [5/172], Loss: 42.0721\n",
      "Epoch [230/300], Step [6/172], Loss: 21.1823\n",
      "Epoch [230/300], Step [7/172], Loss: 28.6767\n",
      "Epoch [230/300], Step [8/172], Loss: 4.7404\n",
      "Epoch [230/300], Step [9/172], Loss: 27.5889\n",
      "Epoch [230/300], Step [10/172], Loss: 40.0248\n",
      "Epoch [230/300], Step [11/172], Loss: 51.6038\n",
      "Epoch [230/300], Step [12/172], Loss: 54.1140\n",
      "Epoch [230/300], Step [13/172], Loss: 33.7592\n",
      "Epoch [230/300], Step [14/172], Loss: 58.8969\n",
      "Epoch [230/300], Step [15/172], Loss: 47.8683\n",
      "Epoch [230/300], Step [16/172], Loss: 9.8943\n",
      "Epoch [230/300], Step [17/172], Loss: 39.5507\n",
      "Epoch [230/300], Step [18/172], Loss: 50.6629\n",
      "Epoch [230/300], Step [19/172], Loss: 71.2962\n",
      "Epoch [230/300], Step [20/172], Loss: 27.0774\n",
      "Epoch [230/300], Step [21/172], Loss: 80.2402\n",
      "Epoch [230/300], Step [22/172], Loss: 52.5849\n",
      "Epoch [230/300], Step [23/172], Loss: 1.9307\n",
      "Epoch [230/300], Step [24/172], Loss: 52.1331\n",
      "Epoch [230/300], Step [25/172], Loss: 37.5621\n",
      "Epoch [230/300], Step [26/172], Loss: 44.5571\n",
      "Epoch [230/300], Step [27/172], Loss: 54.9322\n",
      "Epoch [230/300], Step [28/172], Loss: 22.2504\n",
      "Epoch [230/300], Step [29/172], Loss: 14.6947\n",
      "Epoch [230/300], Step [30/172], Loss: 55.8109\n",
      "Epoch [230/300], Step [31/172], Loss: 35.8102\n",
      "Epoch [230/300], Step [32/172], Loss: 44.0268\n",
      "Epoch [230/300], Step [33/172], Loss: 69.3931\n",
      "Epoch [230/300], Step [34/172], Loss: 2.5843\n",
      "Epoch [230/300], Step [35/172], Loss: 14.7381\n",
      "Epoch [230/300], Step [36/172], Loss: 16.9968\n",
      "Epoch [230/300], Step [37/172], Loss: 17.1047\n",
      "Epoch [230/300], Step [38/172], Loss: 33.0797\n",
      "Epoch [230/300], Step [39/172], Loss: 37.4393\n",
      "Epoch [230/300], Step [40/172], Loss: 24.8745\n",
      "Epoch [230/300], Step [41/172], Loss: 37.6802\n",
      "Epoch [230/300], Step [42/172], Loss: 43.4904\n",
      "Epoch [230/300], Step [43/172], Loss: 31.1357\n",
      "Epoch [230/300], Step [44/172], Loss: 24.4576\n",
      "Epoch [230/300], Step [45/172], Loss: 36.4775\n",
      "Epoch [230/300], Step [46/172], Loss: 18.6830\n",
      "Epoch [230/300], Step [47/172], Loss: 54.1492\n",
      "Epoch [230/300], Step [48/172], Loss: 61.6485\n",
      "Epoch [230/300], Step [49/172], Loss: 27.6472\n",
      "Epoch [230/300], Step [50/172], Loss: 46.6945\n",
      "Epoch [230/300], Step [51/172], Loss: 11.7163\n",
      "Epoch [230/300], Step [52/172], Loss: 26.5467\n",
      "Epoch [230/300], Step [53/172], Loss: 28.0840\n",
      "Epoch [230/300], Step [54/172], Loss: 21.8996\n",
      "Epoch [230/300], Step [55/172], Loss: 21.0929\n",
      "Epoch [230/300], Step [56/172], Loss: 20.8847\n",
      "Epoch [230/300], Step [57/172], Loss: 18.6710\n",
      "Epoch [230/300], Step [58/172], Loss: 17.5242\n",
      "Epoch [230/300], Step [59/172], Loss: 30.8420\n",
      "Epoch [230/300], Step [60/172], Loss: 19.2175\n",
      "Epoch [230/300], Step [61/172], Loss: 8.6721\n",
      "Epoch [230/300], Step [62/172], Loss: 17.5214\n",
      "Epoch [230/300], Step [63/172], Loss: 13.6914\n",
      "Epoch [230/300], Step [64/172], Loss: 14.5765\n",
      "Epoch [230/300], Step [65/172], Loss: 20.2780\n",
      "Epoch [230/300], Step [66/172], Loss: 8.3488\n",
      "Epoch [230/300], Step [67/172], Loss: 24.7720\n",
      "Epoch [230/300], Step [68/172], Loss: 6.2213\n",
      "Epoch [230/300], Step [69/172], Loss: 29.1082\n",
      "Epoch [230/300], Step [70/172], Loss: 32.4663\n",
      "Epoch [230/300], Step [71/172], Loss: 34.0509\n",
      "Epoch [230/300], Step [72/172], Loss: 30.9319\n",
      "Epoch [230/300], Step [73/172], Loss: 39.4961\n",
      "Epoch [230/300], Step [74/172], Loss: 21.0949\n",
      "Epoch [230/300], Step [75/172], Loss: 19.8583\n",
      "Epoch [230/300], Step [76/172], Loss: 27.3440\n",
      "Epoch [230/300], Step [77/172], Loss: 41.3359\n",
      "Epoch [230/300], Step [78/172], Loss: 31.8857\n",
      "Epoch [230/300], Step [79/172], Loss: 30.6896\n",
      "Epoch [230/300], Step [80/172], Loss: 47.2075\n",
      "Epoch [230/300], Step [81/172], Loss: 28.1007\n",
      "Epoch [230/300], Step [82/172], Loss: 35.9705\n",
      "Epoch [230/300], Step [83/172], Loss: 40.6320\n",
      "Epoch [230/300], Step [84/172], Loss: 32.4425\n",
      "Epoch [230/300], Step [85/172], Loss: 37.0085\n",
      "Epoch [230/300], Step [86/172], Loss: 33.7159\n",
      "Epoch [230/300], Step [87/172], Loss: 23.6885\n",
      "Epoch [230/300], Step [88/172], Loss: 21.8529\n",
      "Epoch [230/300], Step [89/172], Loss: 28.2556\n",
      "Epoch [230/300], Step [90/172], Loss: 20.8796\n",
      "Epoch [230/300], Step [91/172], Loss: 26.1481\n",
      "Epoch [230/300], Step [92/172], Loss: 18.9274\n",
      "Epoch [230/300], Step [93/172], Loss: 18.8771\n",
      "Epoch [230/300], Step [94/172], Loss: 25.9747\n",
      "Epoch [230/300], Step [95/172], Loss: 21.4020\n",
      "Epoch [230/300], Step [96/172], Loss: 20.5304\n",
      "Epoch [230/300], Step [97/172], Loss: 29.3445\n",
      "Epoch [230/300], Step [98/172], Loss: 20.1459\n",
      "Epoch [230/300], Step [99/172], Loss: 20.1178\n",
      "Epoch [230/300], Step [100/172], Loss: 18.8571\n",
      "Epoch [230/300], Step [101/172], Loss: 20.5816\n",
      "Epoch [230/300], Step [102/172], Loss: 18.4567\n",
      "Epoch [230/300], Step [103/172], Loss: 14.1600\n",
      "Epoch [230/300], Step [104/172], Loss: 20.6993\n",
      "Epoch [230/300], Step [105/172], Loss: 25.0240\n",
      "Epoch [230/300], Step [106/172], Loss: 17.3321\n",
      "Epoch [230/300], Step [107/172], Loss: 17.3796\n",
      "Epoch [230/300], Step [108/172], Loss: 17.3808\n",
      "Epoch [230/300], Step [109/172], Loss: 16.0742\n",
      "Epoch [230/300], Step [110/172], Loss: 18.9382\n",
      "Epoch [230/300], Step [111/172], Loss: 19.3912\n",
      "Epoch [230/300], Step [112/172], Loss: 17.3791\n",
      "Epoch [230/300], Step [113/172], Loss: 15.1749\n",
      "Epoch [230/300], Step [114/172], Loss: 15.4536\n",
      "Epoch [230/300], Step [115/172], Loss: 18.9688\n",
      "Epoch [230/300], Step [116/172], Loss: 15.1436\n",
      "Epoch [230/300], Step [117/172], Loss: 12.9867\n",
      "Epoch [230/300], Step [118/172], Loss: 12.8739\n",
      "Epoch [230/300], Step [119/172], Loss: 18.7042\n",
      "Epoch [230/300], Step [120/172], Loss: 11.6272\n",
      "Epoch [230/300], Step [121/172], Loss: 10.7232\n",
      "Epoch [230/300], Step [122/172], Loss: 14.0867\n",
      "Epoch [230/300], Step [123/172], Loss: 12.6577\n",
      "Epoch [230/300], Step [124/172], Loss: 8.0447\n",
      "Epoch [230/300], Step [125/172], Loss: 13.5310\n",
      "Epoch [230/300], Step [126/172], Loss: 13.4935\n",
      "Epoch [230/300], Step [127/172], Loss: 11.6436\n",
      "Epoch [230/300], Step [128/172], Loss: 11.2177\n",
      "Epoch [230/300], Step [129/172], Loss: 9.8528\n",
      "Epoch [230/300], Step [130/172], Loss: 13.2791\n",
      "Epoch [230/300], Step [131/172], Loss: 8.9821\n",
      "Epoch [230/300], Step [132/172], Loss: 10.8767\n",
      "Epoch [230/300], Step [133/172], Loss: 11.0151\n",
      "Epoch [230/300], Step [134/172], Loss: 11.3036\n",
      "Epoch [230/300], Step [135/172], Loss: 10.0281\n",
      "Epoch [230/300], Step [136/172], Loss: 9.3453\n",
      "Epoch [230/300], Step [137/172], Loss: 9.6911\n",
      "Epoch [230/300], Step [138/172], Loss: 9.6673\n",
      "Epoch [230/300], Step [139/172], Loss: 10.8967\n",
      "Epoch [230/300], Step [140/172], Loss: 11.5980\n",
      "Epoch [230/300], Step [141/172], Loss: 10.3535\n",
      "Epoch [230/300], Step [142/172], Loss: 14.4061\n",
      "Epoch [230/300], Step [143/172], Loss: 11.8604\n",
      "Epoch [230/300], Step [144/172], Loss: 10.0264\n",
      "Epoch [230/300], Step [145/172], Loss: 11.2871\n",
      "Epoch [230/300], Step [146/172], Loss: 11.2353\n",
      "Epoch [230/300], Step [147/172], Loss: 6.5221\n",
      "Epoch [230/300], Step [148/172], Loss: 7.6192\n",
      "Epoch [230/300], Step [149/172], Loss: 8.1983\n",
      "Epoch [230/300], Step [150/172], Loss: 6.6789\n",
      "Epoch [230/300], Step [151/172], Loss: 6.4266\n",
      "Epoch [230/300], Step [152/172], Loss: 9.5222\n",
      "Epoch [230/300], Step [153/172], Loss: 7.1703\n",
      "Epoch [230/300], Step [154/172], Loss: 7.3652\n",
      "Epoch [230/300], Step [155/172], Loss: 7.4086\n",
      "Epoch [230/300], Step [156/172], Loss: 14.5489\n",
      "Epoch [230/300], Step [157/172], Loss: 8.7759\n",
      "Epoch [230/300], Step [158/172], Loss: 7.7785\n",
      "Epoch [230/300], Step [159/172], Loss: 9.7246\n",
      "Epoch [230/300], Step [160/172], Loss: 9.5002\n",
      "Epoch [230/300], Step [161/172], Loss: 9.5868\n",
      "Epoch [230/300], Step [162/172], Loss: 5.6799\n",
      "Epoch [230/300], Step [163/172], Loss: 7.0086\n",
      "Epoch [230/300], Step [164/172], Loss: 9.7676\n",
      "Epoch [230/300], Step [165/172], Loss: 7.6256\n",
      "Epoch [230/300], Step [166/172], Loss: 6.7367\n",
      "Epoch [230/300], Step [167/172], Loss: 10.5414\n",
      "Epoch [230/300], Step [168/172], Loss: 7.3878\n",
      "Epoch [230/300], Step [169/172], Loss: 7.5099\n",
      "Epoch [230/300], Step [170/172], Loss: 6.4970\n",
      "Epoch [230/300], Step [171/172], Loss: 8.9228\n",
      "Epoch [230/300], Step [172/172], Loss: 6.4301\n",
      "Epoch [231/300], Step [1/172], Loss: 45.4147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [231/300], Step [2/172], Loss: 49.9119\n",
      "Epoch [231/300], Step [3/172], Loss: 44.7089\n",
      "Epoch [231/300], Step [4/172], Loss: 22.9032\n",
      "Epoch [231/300], Step [5/172], Loss: 42.7413\n",
      "Epoch [231/300], Step [6/172], Loss: 21.4670\n",
      "Epoch [231/300], Step [7/172], Loss: 28.4661\n",
      "Epoch [231/300], Step [8/172], Loss: 4.4768\n",
      "Epoch [231/300], Step [9/172], Loss: 27.4082\n",
      "Epoch [231/300], Step [10/172], Loss: 40.4352\n",
      "Epoch [231/300], Step [11/172], Loss: 51.6166\n",
      "Epoch [231/300], Step [12/172], Loss: 53.8797\n",
      "Epoch [231/300], Step [13/172], Loss: 33.6518\n",
      "Epoch [231/300], Step [14/172], Loss: 58.6844\n",
      "Epoch [231/300], Step [15/172], Loss: 47.7113\n",
      "Epoch [231/300], Step [16/172], Loss: 9.8681\n",
      "Epoch [231/300], Step [17/172], Loss: 39.4169\n",
      "Epoch [231/300], Step [18/172], Loss: 50.6454\n",
      "Epoch [231/300], Step [19/172], Loss: 70.6020\n",
      "Epoch [231/300], Step [20/172], Loss: 27.0695\n",
      "Epoch [231/300], Step [21/172], Loss: 79.7248\n",
      "Epoch [231/300], Step [22/172], Loss: 51.9405\n",
      "Epoch [231/300], Step [23/172], Loss: 1.9025\n",
      "Epoch [231/300], Step [24/172], Loss: 51.8991\n",
      "Epoch [231/300], Step [25/172], Loss: 37.6017\n",
      "Epoch [231/300], Step [26/172], Loss: 44.7388\n",
      "Epoch [231/300], Step [27/172], Loss: 54.7448\n",
      "Epoch [231/300], Step [28/172], Loss: 22.1353\n",
      "Epoch [231/300], Step [29/172], Loss: 14.6194\n",
      "Epoch [231/300], Step [30/172], Loss: 55.8790\n",
      "Epoch [231/300], Step [31/172], Loss: 35.8286\n",
      "Epoch [231/300], Step [32/172], Loss: 43.8052\n",
      "Epoch [231/300], Step [33/172], Loss: 69.0451\n",
      "Epoch [231/300], Step [34/172], Loss: 2.5550\n",
      "Epoch [231/300], Step [35/172], Loss: 14.7100\n",
      "Epoch [231/300], Step [36/172], Loss: 17.0007\n",
      "Epoch [231/300], Step [37/172], Loss: 17.1748\n",
      "Epoch [231/300], Step [38/172], Loss: 33.3022\n",
      "Epoch [231/300], Step [39/172], Loss: 37.3872\n",
      "Epoch [231/300], Step [40/172], Loss: 24.9393\n",
      "Epoch [231/300], Step [41/172], Loss: 37.8300\n",
      "Epoch [231/300], Step [42/172], Loss: 43.5899\n",
      "Epoch [231/300], Step [43/172], Loss: 31.2568\n",
      "Epoch [231/300], Step [44/172], Loss: 24.5706\n",
      "Epoch [231/300], Step [45/172], Loss: 36.7474\n",
      "Epoch [231/300], Step [46/172], Loss: 18.3702\n",
      "Epoch [231/300], Step [47/172], Loss: 54.0677\n",
      "Epoch [231/300], Step [48/172], Loss: 60.2748\n",
      "Epoch [231/300], Step [49/172], Loss: 27.7912\n",
      "Epoch [231/300], Step [50/172], Loss: 47.0493\n",
      "Epoch [231/300], Step [51/172], Loss: 11.7505\n",
      "Epoch [231/300], Step [52/172], Loss: 26.3236\n",
      "Epoch [231/300], Step [53/172], Loss: 27.9489\n",
      "Epoch [231/300], Step [54/172], Loss: 21.6207\n",
      "Epoch [231/300], Step [55/172], Loss: 20.8056\n",
      "Epoch [231/300], Step [56/172], Loss: 20.3074\n",
      "Epoch [231/300], Step [57/172], Loss: 18.8880\n",
      "Epoch [231/300], Step [58/172], Loss: 17.4224\n",
      "Epoch [231/300], Step [59/172], Loss: 30.5731\n",
      "Epoch [231/300], Step [60/172], Loss: 19.5784\n",
      "Epoch [231/300], Step [61/172], Loss: 8.4271\n",
      "Epoch [231/300], Step [62/172], Loss: 17.1224\n",
      "Epoch [231/300], Step [63/172], Loss: 13.3977\n",
      "Epoch [231/300], Step [64/172], Loss: 14.5665\n",
      "Epoch [231/300], Step [65/172], Loss: 20.2036\n",
      "Epoch [231/300], Step [66/172], Loss: 8.1943\n",
      "Epoch [231/300], Step [67/172], Loss: 24.7306\n",
      "Epoch [231/300], Step [68/172], Loss: 5.9982\n",
      "Epoch [231/300], Step [69/172], Loss: 29.2254\n",
      "Epoch [231/300], Step [70/172], Loss: 32.5646\n",
      "Epoch [231/300], Step [71/172], Loss: 33.9064\n",
      "Epoch [231/300], Step [72/172], Loss: 30.9404\n",
      "Epoch [231/300], Step [73/172], Loss: 39.4623\n",
      "Epoch [231/300], Step [74/172], Loss: 20.7642\n",
      "Epoch [231/300], Step [75/172], Loss: 19.7443\n",
      "Epoch [231/300], Step [76/172], Loss: 27.2902\n",
      "Epoch [231/300], Step [77/172], Loss: 41.6369\n",
      "Epoch [231/300], Step [78/172], Loss: 31.5906\n",
      "Epoch [231/300], Step [79/172], Loss: 30.7323\n",
      "Epoch [231/300], Step [80/172], Loss: 47.1278\n",
      "Epoch [231/300], Step [81/172], Loss: 28.0030\n",
      "Epoch [231/300], Step [82/172], Loss: 35.9603\n",
      "Epoch [231/300], Step [83/172], Loss: 40.4129\n",
      "Epoch [231/300], Step [84/172], Loss: 32.3916\n",
      "Epoch [231/300], Step [85/172], Loss: 36.7607\n",
      "Epoch [231/300], Step [86/172], Loss: 33.5134\n",
      "Epoch [231/300], Step [87/172], Loss: 23.6333\n",
      "Epoch [231/300], Step [88/172], Loss: 21.7711\n",
      "Epoch [231/300], Step [89/172], Loss: 28.3309\n",
      "Epoch [231/300], Step [90/172], Loss: 20.8429\n",
      "Epoch [231/300], Step [91/172], Loss: 26.2480\n",
      "Epoch [231/300], Step [92/172], Loss: 18.9750\n",
      "Epoch [231/300], Step [93/172], Loss: 18.7181\n",
      "Epoch [231/300], Step [94/172], Loss: 25.8936\n",
      "Epoch [231/300], Step [95/172], Loss: 21.2582\n",
      "Epoch [231/300], Step [96/172], Loss: 20.5108\n",
      "Epoch [231/300], Step [97/172], Loss: 29.5282\n",
      "Epoch [231/300], Step [98/172], Loss: 20.3040\n",
      "Epoch [231/300], Step [99/172], Loss: 20.1980\n",
      "Epoch [231/300], Step [100/172], Loss: 19.0555\n",
      "Epoch [231/300], Step [101/172], Loss: 20.7183\n",
      "Epoch [231/300], Step [102/172], Loss: 18.6077\n",
      "Epoch [231/300], Step [103/172], Loss: 14.2382\n",
      "Epoch [231/300], Step [104/172], Loss: 20.7314\n",
      "Epoch [231/300], Step [105/172], Loss: 25.0425\n",
      "Epoch [231/300], Step [106/172], Loss: 17.4143\n",
      "Epoch [231/300], Step [107/172], Loss: 17.4440\n",
      "Epoch [231/300], Step [108/172], Loss: 17.4797\n",
      "Epoch [231/300], Step [109/172], Loss: 16.1595\n",
      "Epoch [231/300], Step [110/172], Loss: 18.9045\n",
      "Epoch [231/300], Step [111/172], Loss: 19.3131\n",
      "Epoch [231/300], Step [112/172], Loss: 17.2445\n",
      "Epoch [231/300], Step [113/172], Loss: 15.0939\n",
      "Epoch [231/300], Step [114/172], Loss: 15.4333\n",
      "Epoch [231/300], Step [115/172], Loss: 18.7900\n",
      "Epoch [231/300], Step [116/172], Loss: 14.9642\n",
      "Epoch [231/300], Step [117/172], Loss: 12.9991\n",
      "Epoch [231/300], Step [118/172], Loss: 12.9452\n",
      "Epoch [231/300], Step [119/172], Loss: 18.6516\n",
      "Epoch [231/300], Step [120/172], Loss: 11.6045\n",
      "Epoch [231/300], Step [121/172], Loss: 10.6494\n",
      "Epoch [231/300], Step [122/172], Loss: 14.2088\n",
      "Epoch [231/300], Step [123/172], Loss: 12.5944\n",
      "Epoch [231/300], Step [124/172], Loss: 7.8870\n",
      "Epoch [231/300], Step [125/172], Loss: 13.3751\n",
      "Epoch [231/300], Step [126/172], Loss: 13.1616\n",
      "Epoch [231/300], Step [127/172], Loss: 11.5263\n",
      "Epoch [231/300], Step [128/172], Loss: 11.1426\n",
      "Epoch [231/300], Step [129/172], Loss: 9.7108\n",
      "Epoch [231/300], Step [130/172], Loss: 13.1316\n",
      "Epoch [231/300], Step [131/172], Loss: 8.9549\n",
      "Epoch [231/300], Step [132/172], Loss: 10.8994\n",
      "Epoch [231/300], Step [133/172], Loss: 11.0559\n",
      "Epoch [231/300], Step [134/172], Loss: 11.2755\n",
      "Epoch [231/300], Step [135/172], Loss: 9.8905\n",
      "Epoch [231/300], Step [136/172], Loss: 9.1809\n",
      "Epoch [231/300], Step [137/172], Loss: 9.4298\n",
      "Epoch [231/300], Step [138/172], Loss: 9.4061\n",
      "Epoch [231/300], Step [139/172], Loss: 10.5256\n",
      "Epoch [231/300], Step [140/172], Loss: 11.4885\n",
      "Epoch [231/300], Step [141/172], Loss: 10.0709\n",
      "Epoch [231/300], Step [142/172], Loss: 13.9874\n",
      "Epoch [231/300], Step [143/172], Loss: 11.7732\n",
      "Epoch [231/300], Step [144/172], Loss: 9.7435\n",
      "Epoch [231/300], Step [145/172], Loss: 11.2244\n",
      "Epoch [231/300], Step [146/172], Loss: 10.8126\n",
      "Epoch [231/300], Step [147/172], Loss: 6.4108\n",
      "Epoch [231/300], Step [148/172], Loss: 7.5937\n",
      "Epoch [231/300], Step [149/172], Loss: 8.0141\n",
      "Epoch [231/300], Step [150/172], Loss: 6.4902\n",
      "Epoch [231/300], Step [151/172], Loss: 6.3065\n",
      "Epoch [231/300], Step [152/172], Loss: 9.4755\n",
      "Epoch [231/300], Step [153/172], Loss: 6.9447\n",
      "Epoch [231/300], Step [154/172], Loss: 7.2704\n",
      "Epoch [231/300], Step [155/172], Loss: 7.1715\n",
      "Epoch [231/300], Step [156/172], Loss: 14.4622\n",
      "Epoch [231/300], Step [157/172], Loss: 8.5835\n",
      "Epoch [231/300], Step [158/172], Loss: 7.6782\n",
      "Epoch [231/300], Step [159/172], Loss: 9.2762\n",
      "Epoch [231/300], Step [160/172], Loss: 9.2842\n",
      "Epoch [231/300], Step [161/172], Loss: 9.3364\n",
      "Epoch [231/300], Step [162/172], Loss: 5.5821\n",
      "Epoch [231/300], Step [163/172], Loss: 6.8406\n",
      "Epoch [231/300], Step [164/172], Loss: 9.6257\n",
      "Epoch [231/300], Step [165/172], Loss: 7.4938\n",
      "Epoch [231/300], Step [166/172], Loss: 6.4530\n",
      "Epoch [231/300], Step [167/172], Loss: 10.1407\n",
      "Epoch [231/300], Step [168/172], Loss: 7.2188\n",
      "Epoch [231/300], Step [169/172], Loss: 7.3707\n",
      "Epoch [231/300], Step [170/172], Loss: 6.2883\n",
      "Epoch [231/300], Step [171/172], Loss: 8.1970\n",
      "Epoch [231/300], Step [172/172], Loss: 6.1450\n",
      "Epoch [232/300], Step [1/172], Loss: 45.4851\n",
      "Epoch [232/300], Step [2/172], Loss: 49.9268\n",
      "Epoch [232/300], Step [3/172], Loss: 53.6278\n",
      "Epoch [232/300], Step [4/172], Loss: 22.9609\n",
      "Epoch [232/300], Step [5/172], Loss: 41.9869\n",
      "Epoch [232/300], Step [6/172], Loss: 21.3330\n",
      "Epoch [232/300], Step [7/172], Loss: 27.5835\n",
      "Epoch [232/300], Step [8/172], Loss: 4.8739\n",
      "Epoch [232/300], Step [9/172], Loss: 27.6092\n",
      "Epoch [232/300], Step [10/172], Loss: 40.1387\n",
      "Epoch [232/300], Step [11/172], Loss: 51.7378\n",
      "Epoch [232/300], Step [12/172], Loss: 53.8234\n",
      "Epoch [232/300], Step [13/172], Loss: 33.9092\n",
      "Epoch [232/300], Step [14/172], Loss: 60.0679\n",
      "Epoch [232/300], Step [15/172], Loss: 47.0871\n",
      "Epoch [232/300], Step [16/172], Loss: 9.8149\n",
      "Epoch [232/300], Step [17/172], Loss: 39.4538\n",
      "Epoch [232/300], Step [18/172], Loss: 51.0503\n",
      "Epoch [232/300], Step [19/172], Loss: 71.0167\n",
      "Epoch [232/300], Step [20/172], Loss: 27.0234\n",
      "Epoch [232/300], Step [21/172], Loss: 80.4406\n",
      "Epoch [232/300], Step [22/172], Loss: 51.9544\n",
      "Epoch [232/300], Step [23/172], Loss: 1.8723\n",
      "Epoch [232/300], Step [24/172], Loss: 52.3170\n",
      "Epoch [232/300], Step [25/172], Loss: 37.7181\n",
      "Epoch [232/300], Step [26/172], Loss: 44.8242\n",
      "Epoch [232/300], Step [27/172], Loss: 54.4635\n",
      "Epoch [232/300], Step [28/172], Loss: 22.0835\n",
      "Epoch [232/300], Step [29/172], Loss: 14.5566\n",
      "Epoch [232/300], Step [30/172], Loss: 56.3800\n",
      "Epoch [232/300], Step [31/172], Loss: 35.9244\n",
      "Epoch [232/300], Step [32/172], Loss: 43.8599\n",
      "Epoch [232/300], Step [33/172], Loss: 69.3607\n",
      "Epoch [232/300], Step [34/172], Loss: 2.5158\n",
      "Epoch [232/300], Step [35/172], Loss: 14.7219\n",
      "Epoch [232/300], Step [36/172], Loss: 16.9455\n",
      "Epoch [232/300], Step [37/172], Loss: 17.1057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [232/300], Step [38/172], Loss: 33.2216\n",
      "Epoch [232/300], Step [39/172], Loss: 37.4140\n",
      "Epoch [232/300], Step [40/172], Loss: 24.8645\n",
      "Epoch [232/300], Step [41/172], Loss: 37.7040\n",
      "Epoch [232/300], Step [42/172], Loss: 43.7858\n",
      "Epoch [232/300], Step [43/172], Loss: 31.2748\n",
      "Epoch [232/300], Step [44/172], Loss: 24.4813\n",
      "Epoch [232/300], Step [45/172], Loss: 36.6699\n",
      "Epoch [232/300], Step [46/172], Loss: 18.4358\n",
      "Epoch [232/300], Step [47/172], Loss: 54.1360\n",
      "Epoch [232/300], Step [48/172], Loss: 60.5730\n",
      "Epoch [232/300], Step [49/172], Loss: 27.6114\n",
      "Epoch [232/300], Step [50/172], Loss: 46.7317\n",
      "Epoch [232/300], Step [51/172], Loss: 11.8268\n",
      "Epoch [232/300], Step [52/172], Loss: 26.7474\n",
      "Epoch [232/300], Step [53/172], Loss: 28.1368\n",
      "Epoch [232/300], Step [54/172], Loss: 22.1686\n",
      "Epoch [232/300], Step [55/172], Loss: 21.4037\n",
      "Epoch [232/300], Step [56/172], Loss: 20.5212\n",
      "Epoch [232/300], Step [57/172], Loss: 18.9253\n",
      "Epoch [232/300], Step [58/172], Loss: 17.7313\n",
      "Epoch [232/300], Step [59/172], Loss: 30.9024\n",
      "Epoch [232/300], Step [60/172], Loss: 19.2131\n",
      "Epoch [232/300], Step [61/172], Loss: 8.7281\n",
      "Epoch [232/300], Step [62/172], Loss: 17.2833\n",
      "Epoch [232/300], Step [63/172], Loss: 13.8758\n",
      "Epoch [232/300], Step [64/172], Loss: 15.0196\n",
      "Epoch [232/300], Step [65/172], Loss: 20.4801\n",
      "Epoch [232/300], Step [66/172], Loss: 8.3403\n",
      "Epoch [232/300], Step [67/172], Loss: 24.8143\n",
      "Epoch [232/300], Step [68/172], Loss: 6.0554\n",
      "Epoch [232/300], Step [69/172], Loss: 29.0688\n",
      "Epoch [232/300], Step [70/172], Loss: 32.6203\n",
      "Epoch [232/300], Step [71/172], Loss: 33.7921\n",
      "Epoch [232/300], Step [72/172], Loss: 30.5881\n",
      "Epoch [232/300], Step [73/172], Loss: 39.0556\n",
      "Epoch [232/300], Step [74/172], Loss: 20.7008\n",
      "Epoch [232/300], Step [75/172], Loss: 19.5874\n",
      "Epoch [232/300], Step [76/172], Loss: 27.2751\n",
      "Epoch [232/300], Step [77/172], Loss: 41.4838\n",
      "Epoch [232/300], Step [78/172], Loss: 31.4523\n",
      "Epoch [232/300], Step [79/172], Loss: 30.4589\n",
      "Epoch [232/300], Step [80/172], Loss: 47.0017\n",
      "Epoch [232/300], Step [81/172], Loss: 27.9792\n",
      "Epoch [232/300], Step [82/172], Loss: 36.0005\n",
      "Epoch [232/300], Step [83/172], Loss: 40.3203\n",
      "Epoch [232/300], Step [84/172], Loss: 32.4293\n",
      "Epoch [232/300], Step [85/172], Loss: 36.4870\n",
      "Epoch [232/300], Step [86/172], Loss: 33.2769\n",
      "Epoch [232/300], Step [87/172], Loss: 23.6884\n",
      "Epoch [232/300], Step [88/172], Loss: 21.7668\n",
      "Epoch [232/300], Step [89/172], Loss: 28.2841\n",
      "Epoch [232/300], Step [90/172], Loss: 20.9404\n",
      "Epoch [232/300], Step [91/172], Loss: 26.2816\n",
      "Epoch [232/300], Step [92/172], Loss: 18.9866\n",
      "Epoch [232/300], Step [93/172], Loss: 18.7524\n",
      "Epoch [232/300], Step [94/172], Loss: 25.8222\n",
      "Epoch [232/300], Step [95/172], Loss: 21.4126\n",
      "Epoch [232/300], Step [96/172], Loss: 20.6436\n",
      "Epoch [232/300], Step [97/172], Loss: 29.6173\n",
      "Epoch [232/300], Step [98/172], Loss: 20.3337\n",
      "Epoch [232/300], Step [99/172], Loss: 20.2394\n",
      "Epoch [232/300], Step [100/172], Loss: 19.1805\n",
      "Epoch [232/300], Step [101/172], Loss: 20.8000\n",
      "Epoch [232/300], Step [102/172], Loss: 18.7206\n",
      "Epoch [232/300], Step [103/172], Loss: 14.2989\n",
      "Epoch [232/300], Step [104/172], Loss: 20.8896\n",
      "Epoch [232/300], Step [105/172], Loss: 25.4120\n",
      "Epoch [232/300], Step [106/172], Loss: 17.5184\n",
      "Epoch [232/300], Step [107/172], Loss: 17.5570\n",
      "Epoch [232/300], Step [108/172], Loss: 17.6432\n",
      "Epoch [232/300], Step [109/172], Loss: 16.2922\n",
      "Epoch [232/300], Step [110/172], Loss: 19.0815\n",
      "Epoch [232/300], Step [111/172], Loss: 19.4754\n",
      "Epoch [232/300], Step [112/172], Loss: 17.4584\n",
      "Epoch [232/300], Step [113/172], Loss: 15.3180\n",
      "Epoch [232/300], Step [114/172], Loss: 15.6622\n",
      "Epoch [232/300], Step [115/172], Loss: 18.9628\n",
      "Epoch [232/300], Step [116/172], Loss: 15.1069\n",
      "Epoch [232/300], Step [117/172], Loss: 13.1735\n",
      "Epoch [232/300], Step [118/172], Loss: 13.0057\n",
      "Epoch [232/300], Step [119/172], Loss: 18.9050\n",
      "Epoch [232/300], Step [120/172], Loss: 11.7171\n",
      "Epoch [232/300], Step [121/172], Loss: 10.7360\n",
      "Epoch [232/300], Step [122/172], Loss: 14.2820\n",
      "Epoch [232/300], Step [123/172], Loss: 12.9706\n",
      "Epoch [232/300], Step [124/172], Loss: 8.0856\n",
      "Epoch [232/300], Step [125/172], Loss: 13.6576\n",
      "Epoch [232/300], Step [126/172], Loss: 13.4424\n",
      "Epoch [232/300], Step [127/172], Loss: 11.7423\n",
      "Epoch [232/300], Step [128/172], Loss: 11.2445\n",
      "Epoch [232/300], Step [129/172], Loss: 9.9503\n",
      "Epoch [232/300], Step [130/172], Loss: 13.4301\n",
      "Epoch [232/300], Step [131/172], Loss: 9.0923\n",
      "Epoch [232/300], Step [132/172], Loss: 10.9872\n",
      "Epoch [232/300], Step [133/172], Loss: 11.1804\n",
      "Epoch [232/300], Step [134/172], Loss: 11.3795\n",
      "Epoch [232/300], Step [135/172], Loss: 10.1513\n",
      "Epoch [232/300], Step [136/172], Loss: 9.5004\n",
      "Epoch [232/300], Step [137/172], Loss: 9.6452\n",
      "Epoch [232/300], Step [138/172], Loss: 9.5028\n",
      "Epoch [232/300], Step [139/172], Loss: 10.9109\n",
      "Epoch [232/300], Step [140/172], Loss: 11.6582\n",
      "Epoch [232/300], Step [141/172], Loss: 10.3609\n",
      "Epoch [232/300], Step [142/172], Loss: 14.4123\n",
      "Epoch [232/300], Step [143/172], Loss: 11.9431\n",
      "Epoch [232/300], Step [144/172], Loss: 10.0515\n",
      "Epoch [232/300], Step [145/172], Loss: 11.3931\n",
      "Epoch [232/300], Step [146/172], Loss: 11.2822\n",
      "Epoch [232/300], Step [147/172], Loss: 6.5635\n",
      "Epoch [232/300], Step [148/172], Loss: 7.6919\n",
      "Epoch [232/300], Step [149/172], Loss: 8.2424\n",
      "Epoch [232/300], Step [150/172], Loss: 6.6734\n",
      "Epoch [232/300], Step [151/172], Loss: 6.4302\n",
      "Epoch [232/300], Step [152/172], Loss: 9.6443\n",
      "Epoch [232/300], Step [153/172], Loss: 7.1130\n",
      "Epoch [232/300], Step [154/172], Loss: 7.3608\n",
      "Epoch [232/300], Step [155/172], Loss: 7.4810\n",
      "Epoch [232/300], Step [156/172], Loss: 14.7233\n",
      "Epoch [232/300], Step [157/172], Loss: 8.8260\n",
      "Epoch [232/300], Step [158/172], Loss: 7.8613\n",
      "Epoch [232/300], Step [159/172], Loss: 9.6722\n",
      "Epoch [232/300], Step [160/172], Loss: 9.5670\n",
      "Epoch [232/300], Step [161/172], Loss: 9.6923\n",
      "Epoch [232/300], Step [162/172], Loss: 5.7395\n",
      "Epoch [232/300], Step [163/172], Loss: 7.0783\n",
      "Epoch [232/300], Step [164/172], Loss: 9.8847\n",
      "Epoch [232/300], Step [165/172], Loss: 7.7000\n",
      "Epoch [232/300], Step [166/172], Loss: 6.7426\n",
      "Epoch [232/300], Step [167/172], Loss: 10.6653\n",
      "Epoch [232/300], Step [168/172], Loss: 7.3728\n",
      "Epoch [232/300], Step [169/172], Loss: 7.6736\n",
      "Epoch [232/300], Step [170/172], Loss: 6.5384\n",
      "Epoch [232/300], Step [171/172], Loss: 8.6915\n",
      "Epoch [232/300], Step [172/172], Loss: 6.4664\n",
      "Epoch [233/300], Step [1/172], Loss: 45.0085\n",
      "Epoch [233/300], Step [2/172], Loss: 49.3934\n",
      "Epoch [233/300], Step [3/172], Loss: 47.0714\n",
      "Epoch [233/300], Step [4/172], Loss: 22.3995\n",
      "Epoch [233/300], Step [5/172], Loss: 41.5504\n",
      "Epoch [233/300], Step [6/172], Loss: 20.9381\n",
      "Epoch [233/300], Step [7/172], Loss: 27.3488\n",
      "Epoch [233/300], Step [8/172], Loss: 4.3421\n",
      "Epoch [233/300], Step [9/172], Loss: 27.2348\n",
      "Epoch [233/300], Step [10/172], Loss: 40.4054\n",
      "Epoch [233/300], Step [11/172], Loss: 51.5183\n",
      "Epoch [233/300], Step [12/172], Loss: 53.3429\n",
      "Epoch [233/300], Step [13/172], Loss: 33.3523\n",
      "Epoch [233/300], Step [14/172], Loss: 57.5589\n",
      "Epoch [233/300], Step [15/172], Loss: 47.1162\n",
      "Epoch [233/300], Step [16/172], Loss: 10.0331\n",
      "Epoch [233/300], Step [17/172], Loss: 39.1353\n",
      "Epoch [233/300], Step [18/172], Loss: 50.8172\n",
      "Epoch [233/300], Step [19/172], Loss: 70.4165\n",
      "Epoch [233/300], Step [20/172], Loss: 26.8363\n",
      "Epoch [233/300], Step [21/172], Loss: 79.6431\n",
      "Epoch [233/300], Step [22/172], Loss: 51.2767\n",
      "Epoch [233/300], Step [23/172], Loss: 1.8324\n",
      "Epoch [233/300], Step [24/172], Loss: 51.4741\n",
      "Epoch [233/300], Step [25/172], Loss: 37.4680\n",
      "Epoch [233/300], Step [26/172], Loss: 44.5568\n",
      "Epoch [233/300], Step [27/172], Loss: 54.3349\n",
      "Epoch [233/300], Step [28/172], Loss: 21.9630\n",
      "Epoch [233/300], Step [29/172], Loss: 14.5006\n",
      "Epoch [233/300], Step [30/172], Loss: 56.1136\n",
      "Epoch [233/300], Step [31/172], Loss: 35.8932\n",
      "Epoch [233/300], Step [32/172], Loss: 43.7540\n",
      "Epoch [233/300], Step [33/172], Loss: 69.1752\n",
      "Epoch [233/300], Step [34/172], Loss: 2.5341\n",
      "Epoch [233/300], Step [35/172], Loss: 14.7727\n",
      "Epoch [233/300], Step [36/172], Loss: 16.8465\n",
      "Epoch [233/300], Step [37/172], Loss: 17.2161\n",
      "Epoch [233/300], Step [38/172], Loss: 33.4044\n",
      "Epoch [233/300], Step [39/172], Loss: 37.5264\n",
      "Epoch [233/300], Step [40/172], Loss: 24.9305\n",
      "Epoch [233/300], Step [41/172], Loss: 37.9250\n",
      "Epoch [233/300], Step [42/172], Loss: 43.7542\n",
      "Epoch [233/300], Step [43/172], Loss: 31.5323\n",
      "Epoch [233/300], Step [44/172], Loss: 24.8568\n",
      "Epoch [233/300], Step [45/172], Loss: 36.9785\n",
      "Epoch [233/300], Step [46/172], Loss: 18.8310\n",
      "Epoch [233/300], Step [47/172], Loss: 54.5843\n",
      "Epoch [233/300], Step [48/172], Loss: 60.6360\n",
      "Epoch [233/300], Step [49/172], Loss: 27.9369\n",
      "Epoch [233/300], Step [50/172], Loss: 47.5518\n",
      "Epoch [233/300], Step [51/172], Loss: 11.9607\n",
      "Epoch [233/300], Step [52/172], Loss: 27.0842\n",
      "Epoch [233/300], Step [53/172], Loss: 28.5201\n",
      "Epoch [233/300], Step [54/172], Loss: 22.4656\n",
      "Epoch [233/300], Step [55/172], Loss: 21.7342\n",
      "Epoch [233/300], Step [56/172], Loss: 20.6687\n",
      "Epoch [233/300], Step [57/172], Loss: 18.8926\n",
      "Epoch [233/300], Step [58/172], Loss: 17.9150\n",
      "Epoch [233/300], Step [59/172], Loss: 31.3462\n",
      "Epoch [233/300], Step [60/172], Loss: 19.1933\n",
      "Epoch [233/300], Step [61/172], Loss: 8.8846\n",
      "Epoch [233/300], Step [62/172], Loss: 17.2423\n",
      "Epoch [233/300], Step [63/172], Loss: 13.8683\n",
      "Epoch [233/300], Step [64/172], Loss: 15.1483\n",
      "Epoch [233/300], Step [65/172], Loss: 20.7921\n",
      "Epoch [233/300], Step [66/172], Loss: 8.4309\n",
      "Epoch [233/300], Step [67/172], Loss: 24.9194\n",
      "Epoch [233/300], Step [68/172], Loss: 6.4360\n",
      "Epoch [233/300], Step [69/172], Loss: 28.9384\n",
      "Epoch [233/300], Step [70/172], Loss: 32.6574\n",
      "Epoch [233/300], Step [71/172], Loss: 33.7092\n",
      "Epoch [233/300], Step [72/172], Loss: 30.4144\n",
      "Epoch [233/300], Step [73/172], Loss: 38.8132\n",
      "Epoch [233/300], Step [74/172], Loss: 20.7222\n",
      "Epoch [233/300], Step [75/172], Loss: 19.4343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [233/300], Step [76/172], Loss: 27.2802\n",
      "Epoch [233/300], Step [77/172], Loss: 41.4896\n",
      "Epoch [233/300], Step [78/172], Loss: 31.3566\n",
      "Epoch [233/300], Step [79/172], Loss: 30.5931\n",
      "Epoch [233/300], Step [80/172], Loss: 47.0500\n",
      "Epoch [233/300], Step [81/172], Loss: 27.8564\n",
      "Epoch [233/300], Step [82/172], Loss: 35.8132\n",
      "Epoch [233/300], Step [83/172], Loss: 40.3113\n",
      "Epoch [233/300], Step [84/172], Loss: 32.4903\n",
      "Epoch [233/300], Step [85/172], Loss: 36.5096\n",
      "Epoch [233/300], Step [86/172], Loss: 33.3691\n",
      "Epoch [233/300], Step [87/172], Loss: 23.7105\n",
      "Epoch [233/300], Step [88/172], Loss: 21.8007\n",
      "Epoch [233/300], Step [89/172], Loss: 28.3804\n",
      "Epoch [233/300], Step [90/172], Loss: 20.9766\n",
      "Epoch [233/300], Step [91/172], Loss: 26.3251\n",
      "Epoch [233/300], Step [92/172], Loss: 19.0535\n",
      "Epoch [233/300], Step [93/172], Loss: 18.7397\n",
      "Epoch [233/300], Step [94/172], Loss: 25.7764\n",
      "Epoch [233/300], Step [95/172], Loss: 21.5437\n",
      "Epoch [233/300], Step [96/172], Loss: 20.7447\n",
      "Epoch [233/300], Step [97/172], Loss: 29.6961\n",
      "Epoch [233/300], Step [98/172], Loss: 20.3332\n",
      "Epoch [233/300], Step [99/172], Loss: 20.2489\n",
      "Epoch [233/300], Step [100/172], Loss: 19.2427\n",
      "Epoch [233/300], Step [101/172], Loss: 20.8962\n",
      "Epoch [233/300], Step [102/172], Loss: 18.6058\n",
      "Epoch [233/300], Step [103/172], Loss: 14.2503\n",
      "Epoch [233/300], Step [104/172], Loss: 20.9128\n",
      "Epoch [233/300], Step [105/172], Loss: 25.2918\n",
      "Epoch [233/300], Step [106/172], Loss: 17.5210\n",
      "Epoch [233/300], Step [107/172], Loss: 17.5450\n",
      "Epoch [233/300], Step [108/172], Loss: 17.6574\n",
      "Epoch [233/300], Step [109/172], Loss: 16.2779\n",
      "Epoch [233/300], Step [110/172], Loss: 19.2358\n",
      "Epoch [233/300], Step [111/172], Loss: 19.3769\n",
      "Epoch [233/300], Step [112/172], Loss: 17.3295\n",
      "Epoch [233/300], Step [113/172], Loss: 15.3993\n",
      "Epoch [233/300], Step [114/172], Loss: 15.6872\n",
      "Epoch [233/300], Step [115/172], Loss: 19.0165\n",
      "Epoch [233/300], Step [116/172], Loss: 15.0929\n",
      "Epoch [233/300], Step [117/172], Loss: 13.1984\n",
      "Epoch [233/300], Step [118/172], Loss: 12.9962\n",
      "Epoch [233/300], Step [119/172], Loss: 18.9870\n",
      "Epoch [233/300], Step [120/172], Loss: 11.6890\n",
      "Epoch [233/300], Step [121/172], Loss: 10.7325\n",
      "Epoch [233/300], Step [122/172], Loss: 14.3033\n",
      "Epoch [233/300], Step [123/172], Loss: 12.7044\n",
      "Epoch [233/300], Step [124/172], Loss: 8.0525\n",
      "Epoch [233/300], Step [125/172], Loss: 13.6805\n",
      "Epoch [233/300], Step [126/172], Loss: 13.4483\n",
      "Epoch [233/300], Step [127/172], Loss: 11.6879\n",
      "Epoch [233/300], Step [128/172], Loss: 11.1740\n",
      "Epoch [233/300], Step [129/172], Loss: 9.9223\n",
      "Epoch [233/300], Step [130/172], Loss: 13.4905\n",
      "Epoch [233/300], Step [131/172], Loss: 9.0738\n",
      "Epoch [233/300], Step [132/172], Loss: 11.0111\n",
      "Epoch [233/300], Step [133/172], Loss: 11.1386\n",
      "Epoch [233/300], Step [134/172], Loss: 11.3022\n",
      "Epoch [233/300], Step [135/172], Loss: 10.1076\n",
      "Epoch [233/300], Step [136/172], Loss: 9.5370\n",
      "Epoch [233/300], Step [137/172], Loss: 9.5934\n",
      "Epoch [233/300], Step [138/172], Loss: 9.5231\n",
      "Epoch [233/300], Step [139/172], Loss: 10.9140\n",
      "Epoch [233/300], Step [140/172], Loss: 11.7160\n",
      "Epoch [233/300], Step [141/172], Loss: 10.2861\n",
      "Epoch [233/300], Step [142/172], Loss: 14.3313\n",
      "Epoch [233/300], Step [143/172], Loss: 11.9749\n",
      "Epoch [233/300], Step [144/172], Loss: 9.9433\n",
      "Epoch [233/300], Step [145/172], Loss: 11.3439\n",
      "Epoch [233/300], Step [146/172], Loss: 11.3389\n",
      "Epoch [233/300], Step [147/172], Loss: 6.5711\n",
      "Epoch [233/300], Step [148/172], Loss: 7.7399\n",
      "Epoch [233/300], Step [149/172], Loss: 8.2764\n",
      "Epoch [233/300], Step [150/172], Loss: 6.6838\n",
      "Epoch [233/300], Step [151/172], Loss: 6.4391\n",
      "Epoch [233/300], Step [152/172], Loss: 9.6909\n",
      "Epoch [233/300], Step [153/172], Loss: 7.0540\n",
      "Epoch [233/300], Step [154/172], Loss: 7.3950\n",
      "Epoch [233/300], Step [155/172], Loss: 7.4705\n",
      "Epoch [233/300], Step [156/172], Loss: 14.7306\n",
      "Epoch [233/300], Step [157/172], Loss: 8.8703\n",
      "Epoch [233/300], Step [158/172], Loss: 7.8758\n",
      "Epoch [233/300], Step [159/172], Loss: 9.6604\n",
      "Epoch [233/300], Step [160/172], Loss: 9.5703\n",
      "Epoch [233/300], Step [161/172], Loss: 9.7726\n",
      "Epoch [233/300], Step [162/172], Loss: 5.7682\n",
      "Epoch [233/300], Step [163/172], Loss: 7.1297\n",
      "Epoch [233/300], Step [164/172], Loss: 9.7900\n",
      "Epoch [233/300], Step [165/172], Loss: 7.7385\n",
      "Epoch [233/300], Step [166/172], Loss: 6.6841\n",
      "Epoch [233/300], Step [167/172], Loss: 10.7023\n",
      "Epoch [233/300], Step [168/172], Loss: 7.3393\n",
      "Epoch [233/300], Step [169/172], Loss: 7.6384\n",
      "Epoch [233/300], Step [170/172], Loss: 6.5174\n",
      "Epoch [233/300], Step [171/172], Loss: 8.7186\n",
      "Epoch [233/300], Step [172/172], Loss: 6.3988\n",
      "Epoch [234/300], Step [1/172], Loss: 44.6933\n",
      "Epoch [234/300], Step [2/172], Loss: 48.7597\n",
      "Epoch [234/300], Step [3/172], Loss: 46.0044\n",
      "Epoch [234/300], Step [4/172], Loss: 22.3408\n",
      "Epoch [234/300], Step [5/172], Loss: 41.5980\n",
      "Epoch [234/300], Step [6/172], Loss: 20.9820\n",
      "Epoch [234/300], Step [7/172], Loss: 27.6850\n",
      "Epoch [234/300], Step [8/172], Loss: 4.8513\n",
      "Epoch [234/300], Step [9/172], Loss: 27.1744\n",
      "Epoch [234/300], Step [10/172], Loss: 39.4566\n",
      "Epoch [234/300], Step [11/172], Loss: 50.9917\n",
      "Epoch [234/300], Step [12/172], Loss: 53.2419\n",
      "Epoch [234/300], Step [13/172], Loss: 33.0478\n",
      "Epoch [234/300], Step [14/172], Loss: 58.1750\n",
      "Epoch [234/300], Step [15/172], Loss: 47.0388\n",
      "Epoch [234/300], Step [16/172], Loss: 9.5233\n",
      "Epoch [234/300], Step [17/172], Loss: 39.3132\n",
      "Epoch [234/300], Step [18/172], Loss: 50.3514\n",
      "Epoch [234/300], Step [19/172], Loss: 70.4151\n",
      "Epoch [234/300], Step [20/172], Loss: 26.4397\n",
      "Epoch [234/300], Step [21/172], Loss: 79.6475\n",
      "Epoch [234/300], Step [22/172], Loss: 51.7178\n",
      "Epoch [234/300], Step [23/172], Loss: 2.0096\n",
      "Epoch [234/300], Step [24/172], Loss: 51.6301\n",
      "Epoch [234/300], Step [25/172], Loss: 36.8284\n",
      "Epoch [234/300], Step [26/172], Loss: 44.4689\n",
      "Epoch [234/300], Step [27/172], Loss: 53.9888\n",
      "Epoch [234/300], Step [28/172], Loss: 21.7419\n",
      "Epoch [234/300], Step [29/172], Loss: 14.5073\n",
      "Epoch [234/300], Step [30/172], Loss: 55.1944\n",
      "Epoch [234/300], Step [31/172], Loss: 35.4602\n",
      "Epoch [234/300], Step [32/172], Loss: 43.6049\n",
      "Epoch [234/300], Step [33/172], Loss: 68.5834\n",
      "Epoch [234/300], Step [34/172], Loss: 2.4805\n",
      "Epoch [234/300], Step [35/172], Loss: 14.7066\n",
      "Epoch [234/300], Step [36/172], Loss: 17.1153\n",
      "Epoch [234/300], Step [37/172], Loss: 17.0645\n",
      "Epoch [234/300], Step [38/172], Loss: 32.9093\n",
      "Epoch [234/300], Step [39/172], Loss: 37.2773\n",
      "Epoch [234/300], Step [40/172], Loss: 25.0011\n",
      "Epoch [234/300], Step [41/172], Loss: 37.4572\n",
      "Epoch [234/300], Step [42/172], Loss: 43.5997\n",
      "Epoch [234/300], Step [43/172], Loss: 31.4727\n",
      "Epoch [234/300], Step [44/172], Loss: 24.5854\n",
      "Epoch [234/300], Step [45/172], Loss: 37.2679\n",
      "Epoch [234/300], Step [46/172], Loss: 18.3720\n",
      "Epoch [234/300], Step [47/172], Loss: 54.3065\n",
      "Epoch [234/300], Step [48/172], Loss: 60.0408\n",
      "Epoch [234/300], Step [49/172], Loss: 27.8925\n",
      "Epoch [234/300], Step [50/172], Loss: 47.0011\n",
      "Epoch [234/300], Step [51/172], Loss: 11.8633\n",
      "Epoch [234/300], Step [52/172], Loss: 26.9099\n",
      "Epoch [234/300], Step [53/172], Loss: 28.2382\n",
      "Epoch [234/300], Step [54/172], Loss: 22.3094\n",
      "Epoch [234/300], Step [55/172], Loss: 21.6881\n",
      "Epoch [234/300], Step [56/172], Loss: 20.5440\n",
      "Epoch [234/300], Step [57/172], Loss: 19.1099\n",
      "Epoch [234/300], Step [58/172], Loss: 18.1202\n",
      "Epoch [234/300], Step [59/172], Loss: 31.6746\n",
      "Epoch [234/300], Step [60/172], Loss: 19.9150\n",
      "Epoch [234/300], Step [61/172], Loss: 8.9105\n",
      "Epoch [234/300], Step [62/172], Loss: 17.0837\n",
      "Epoch [234/300], Step [63/172], Loss: 13.7251\n",
      "Epoch [234/300], Step [64/172], Loss: 15.0458\n",
      "Epoch [234/300], Step [65/172], Loss: 21.0722\n",
      "Epoch [234/300], Step [66/172], Loss: 8.6233\n",
      "Epoch [234/300], Step [67/172], Loss: 25.1412\n",
      "Epoch [234/300], Step [68/172], Loss: 6.5974\n",
      "Epoch [234/300], Step [69/172], Loss: 28.9855\n",
      "Epoch [234/300], Step [70/172], Loss: 32.2636\n",
      "Epoch [234/300], Step [71/172], Loss: 33.4262\n",
      "Epoch [234/300], Step [72/172], Loss: 30.2035\n",
      "Epoch [234/300], Step [73/172], Loss: 38.5185\n",
      "Epoch [234/300], Step [74/172], Loss: 20.6352\n",
      "Epoch [234/300], Step [75/172], Loss: 19.2725\n",
      "Epoch [234/300], Step [76/172], Loss: 27.0980\n",
      "Epoch [234/300], Step [77/172], Loss: 41.2397\n",
      "Epoch [234/300], Step [78/172], Loss: 31.1768\n",
      "Epoch [234/300], Step [79/172], Loss: 30.3330\n",
      "Epoch [234/300], Step [80/172], Loss: 46.9044\n",
      "Epoch [234/300], Step [81/172], Loss: 27.6613\n",
      "Epoch [234/300], Step [82/172], Loss: 36.1246\n",
      "Epoch [234/300], Step [83/172], Loss: 40.1994\n",
      "Epoch [234/300], Step [84/172], Loss: 32.3433\n",
      "Epoch [234/300], Step [85/172], Loss: 36.3689\n",
      "Epoch [234/300], Step [86/172], Loss: 33.3120\n",
      "Epoch [234/300], Step [87/172], Loss: 23.6322\n",
      "Epoch [234/300], Step [88/172], Loss: 21.7420\n",
      "Epoch [234/300], Step [89/172], Loss: 28.6380\n",
      "Epoch [234/300], Step [90/172], Loss: 20.8985\n",
      "Epoch [234/300], Step [91/172], Loss: 26.2215\n",
      "Epoch [234/300], Step [92/172], Loss: 19.0310\n",
      "Epoch [234/300], Step [93/172], Loss: 18.6236\n",
      "Epoch [234/300], Step [94/172], Loss: 25.6290\n",
      "Epoch [234/300], Step [95/172], Loss: 21.4344\n",
      "Epoch [234/300], Step [96/172], Loss: 20.6527\n",
      "Epoch [234/300], Step [97/172], Loss: 29.7439\n",
      "Epoch [234/300], Step [98/172], Loss: 20.2619\n",
      "Epoch [234/300], Step [99/172], Loss: 20.2209\n",
      "Epoch [234/300], Step [100/172], Loss: 19.2111\n",
      "Epoch [234/300], Step [101/172], Loss: 20.8503\n",
      "Epoch [234/300], Step [102/172], Loss: 18.7414\n",
      "Epoch [234/300], Step [103/172], Loss: 14.2393\n",
      "Epoch [234/300], Step [104/172], Loss: 20.9190\n",
      "Epoch [234/300], Step [105/172], Loss: 25.3601\n",
      "Epoch [234/300], Step [106/172], Loss: 17.4403\n",
      "Epoch [234/300], Step [107/172], Loss: 17.5216\n",
      "Epoch [234/300], Step [108/172], Loss: 17.6438\n",
      "Epoch [234/300], Step [109/172], Loss: 16.2093\n",
      "Epoch [234/300], Step [110/172], Loss: 19.2478\n",
      "Epoch [234/300], Step [111/172], Loss: 19.3542\n",
      "Epoch [234/300], Step [112/172], Loss: 17.3894\n",
      "Epoch [234/300], Step [113/172], Loss: 15.4205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [234/300], Step [114/172], Loss: 15.7080\n",
      "Epoch [234/300], Step [115/172], Loss: 19.0109\n",
      "Epoch [234/300], Step [116/172], Loss: 15.1130\n",
      "Epoch [234/300], Step [117/172], Loss: 13.1786\n",
      "Epoch [234/300], Step [118/172], Loss: 12.9778\n",
      "Epoch [234/300], Step [119/172], Loss: 19.0051\n",
      "Epoch [234/300], Step [120/172], Loss: 11.7183\n",
      "Epoch [234/300], Step [121/172], Loss: 10.6642\n",
      "Epoch [234/300], Step [122/172], Loss: 14.3264\n",
      "Epoch [234/300], Step [123/172], Loss: 12.9984\n",
      "Epoch [234/300], Step [124/172], Loss: 8.0517\n",
      "Epoch [234/300], Step [125/172], Loss: 13.5579\n",
      "Epoch [234/300], Step [126/172], Loss: 13.4234\n",
      "Epoch [234/300], Step [127/172], Loss: 11.7675\n",
      "Epoch [234/300], Step [128/172], Loss: 11.1962\n",
      "Epoch [234/300], Step [129/172], Loss: 9.8208\n",
      "Epoch [234/300], Step [130/172], Loss: 13.4520\n",
      "Epoch [234/300], Step [131/172], Loss: 9.0647\n",
      "Epoch [234/300], Step [132/172], Loss: 10.9630\n",
      "Epoch [234/300], Step [133/172], Loss: 11.1060\n",
      "Epoch [234/300], Step [134/172], Loss: 11.2114\n",
      "Epoch [234/300], Step [135/172], Loss: 10.1126\n",
      "Epoch [234/300], Step [136/172], Loss: 9.6507\n",
      "Epoch [234/300], Step [137/172], Loss: 9.5702\n",
      "Epoch [234/300], Step [138/172], Loss: 9.5840\n",
      "Epoch [234/300], Step [139/172], Loss: 10.7698\n",
      "Epoch [234/300], Step [140/172], Loss: 11.7961\n",
      "Epoch [234/300], Step [141/172], Loss: 10.2735\n",
      "Epoch [234/300], Step [142/172], Loss: 14.4397\n",
      "Epoch [234/300], Step [143/172], Loss: 11.9556\n",
      "Epoch [234/300], Step [144/172], Loss: 9.9452\n",
      "Epoch [234/300], Step [145/172], Loss: 11.3820\n",
      "Epoch [234/300], Step [146/172], Loss: 11.3888\n",
      "Epoch [234/300], Step [147/172], Loss: 6.4982\n",
      "Epoch [234/300], Step [148/172], Loss: 7.7441\n",
      "Epoch [234/300], Step [149/172], Loss: 8.2314\n",
      "Epoch [234/300], Step [150/172], Loss: 6.6430\n",
      "Epoch [234/300], Step [151/172], Loss: 6.4247\n",
      "Epoch [234/300], Step [152/172], Loss: 9.6565\n",
      "Epoch [234/300], Step [153/172], Loss: 6.9907\n",
      "Epoch [234/300], Step [154/172], Loss: 7.3424\n",
      "Epoch [234/300], Step [155/172], Loss: 7.3839\n",
      "Epoch [234/300], Step [156/172], Loss: 14.7341\n",
      "Epoch [234/300], Step [157/172], Loss: 8.7818\n",
      "Epoch [234/300], Step [158/172], Loss: 7.9142\n",
      "Epoch [234/300], Step [159/172], Loss: 9.7274\n",
      "Epoch [234/300], Step [160/172], Loss: 9.5109\n",
      "Epoch [234/300], Step [161/172], Loss: 9.7942\n",
      "Epoch [234/300], Step [162/172], Loss: 5.7349\n",
      "Epoch [234/300], Step [163/172], Loss: 7.0338\n",
      "Epoch [234/300], Step [164/172], Loss: 9.9213\n",
      "Epoch [234/300], Step [165/172], Loss: 7.7547\n",
      "Epoch [234/300], Step [166/172], Loss: 6.5989\n",
      "Epoch [234/300], Step [167/172], Loss: 10.5268\n",
      "Epoch [234/300], Step [168/172], Loss: 7.2932\n",
      "Epoch [234/300], Step [169/172], Loss: 7.5450\n",
      "Epoch [234/300], Step [170/172], Loss: 6.5113\n",
      "Epoch [234/300], Step [171/172], Loss: 8.4094\n",
      "Epoch [234/300], Step [172/172], Loss: 6.3558\n",
      "Epoch [235/300], Step [1/172], Loss: 44.5570\n",
      "Epoch [235/300], Step [2/172], Loss: 49.8405\n",
      "Epoch [235/300], Step [3/172], Loss: 46.0476\n",
      "Epoch [235/300], Step [4/172], Loss: 21.8953\n",
      "Epoch [235/300], Step [5/172], Loss: 41.0956\n",
      "Epoch [235/300], Step [6/172], Loss: 20.7213\n",
      "Epoch [235/300], Step [7/172], Loss: 27.7349\n",
      "Epoch [235/300], Step [8/172], Loss: 4.3439\n",
      "Epoch [235/300], Step [9/172], Loss: 27.1100\n",
      "Epoch [235/300], Step [10/172], Loss: 40.1797\n",
      "Epoch [235/300], Step [11/172], Loss: 51.3138\n",
      "Epoch [235/300], Step [12/172], Loss: 53.1064\n",
      "Epoch [235/300], Step [13/172], Loss: 32.8723\n",
      "Epoch [235/300], Step [14/172], Loss: 57.2227\n",
      "Epoch [235/300], Step [15/172], Loss: 46.7564\n",
      "Epoch [235/300], Step [16/172], Loss: 9.5568\n",
      "Epoch [235/300], Step [17/172], Loss: 39.0576\n",
      "Epoch [235/300], Step [18/172], Loss: 50.6072\n",
      "Epoch [235/300], Step [19/172], Loss: 70.1783\n",
      "Epoch [235/300], Step [20/172], Loss: 25.9993\n",
      "Epoch [235/300], Step [21/172], Loss: 79.0312\n",
      "Epoch [235/300], Step [22/172], Loss: 51.3329\n",
      "Epoch [235/300], Step [23/172], Loss: 1.8299\n",
      "Epoch [235/300], Step [24/172], Loss: 51.1535\n",
      "Epoch [235/300], Step [25/172], Loss: 36.6730\n",
      "Epoch [235/300], Step [26/172], Loss: 44.1313\n",
      "Epoch [235/300], Step [27/172], Loss: 53.5585\n",
      "Epoch [235/300], Step [28/172], Loss: 21.5710\n",
      "Epoch [235/300], Step [29/172], Loss: 14.1116\n",
      "Epoch [235/300], Step [30/172], Loss: 55.1846\n",
      "Epoch [235/300], Step [31/172], Loss: 35.2199\n",
      "Epoch [235/300], Step [32/172], Loss: 43.6514\n",
      "Epoch [235/300], Step [33/172], Loss: 68.8642\n",
      "Epoch [235/300], Step [34/172], Loss: 2.4425\n",
      "Epoch [235/300], Step [35/172], Loss: 14.5435\n",
      "Epoch [235/300], Step [36/172], Loss: 16.8330\n",
      "Epoch [235/300], Step [37/172], Loss: 16.9539\n",
      "Epoch [235/300], Step [38/172], Loss: 32.9105\n",
      "Epoch [235/300], Step [39/172], Loss: 37.0983\n",
      "Epoch [235/300], Step [40/172], Loss: 24.7672\n",
      "Epoch [235/300], Step [41/172], Loss: 37.4309\n",
      "Epoch [235/300], Step [42/172], Loss: 43.4066\n",
      "Epoch [235/300], Step [43/172], Loss: 31.1212\n",
      "Epoch [235/300], Step [44/172], Loss: 24.5054\n",
      "Epoch [235/300], Step [45/172], Loss: 36.6724\n",
      "Epoch [235/300], Step [46/172], Loss: 18.8230\n",
      "Epoch [235/300], Step [47/172], Loss: 54.5051\n",
      "Epoch [235/300], Step [48/172], Loss: 61.3130\n",
      "Epoch [235/300], Step [49/172], Loss: 28.0176\n",
      "Epoch [235/300], Step [50/172], Loss: 47.5737\n",
      "Epoch [235/300], Step [51/172], Loss: 11.9068\n",
      "Epoch [235/300], Step [52/172], Loss: 27.2708\n",
      "Epoch [235/300], Step [53/172], Loss: 28.6307\n",
      "Epoch [235/300], Step [54/172], Loss: 22.8114\n",
      "Epoch [235/300], Step [55/172], Loss: 22.0262\n",
      "Epoch [235/300], Step [56/172], Loss: 20.2994\n",
      "Epoch [235/300], Step [57/172], Loss: 18.9458\n",
      "Epoch [235/300], Step [58/172], Loss: 18.4142\n",
      "Epoch [235/300], Step [59/172], Loss: 31.8470\n",
      "Epoch [235/300], Step [60/172], Loss: 19.4105\n",
      "Epoch [235/300], Step [61/172], Loss: 9.2309\n",
      "Epoch [235/300], Step [62/172], Loss: 17.3227\n",
      "Epoch [235/300], Step [63/172], Loss: 14.2226\n",
      "Epoch [235/300], Step [64/172], Loss: 15.4745\n",
      "Epoch [235/300], Step [65/172], Loss: 21.2921\n",
      "Epoch [235/300], Step [66/172], Loss: 8.7341\n",
      "Epoch [235/300], Step [67/172], Loss: 25.8049\n",
      "Epoch [235/300], Step [68/172], Loss: 6.3986\n",
      "Epoch [235/300], Step [69/172], Loss: 28.7810\n",
      "Epoch [235/300], Step [70/172], Loss: 32.6074\n",
      "Epoch [235/300], Step [71/172], Loss: 33.3120\n",
      "Epoch [235/300], Step [72/172], Loss: 29.9067\n",
      "Epoch [235/300], Step [73/172], Loss: 38.1352\n",
      "Epoch [235/300], Step [74/172], Loss: 20.8093\n",
      "Epoch [235/300], Step [75/172], Loss: 19.3336\n",
      "Epoch [235/300], Step [76/172], Loss: 27.0705\n",
      "Epoch [235/300], Step [77/172], Loss: 41.1388\n",
      "Epoch [235/300], Step [78/172], Loss: 31.0973\n",
      "Epoch [235/300], Step [79/172], Loss: 30.1216\n",
      "Epoch [235/300], Step [80/172], Loss: 46.6176\n",
      "Epoch [235/300], Step [81/172], Loss: 27.4159\n",
      "Epoch [235/300], Step [82/172], Loss: 35.9781\n",
      "Epoch [235/300], Step [83/172], Loss: 40.4044\n",
      "Epoch [235/300], Step [84/172], Loss: 32.2886\n",
      "Epoch [235/300], Step [85/172], Loss: 36.5103\n",
      "Epoch [235/300], Step [86/172], Loss: 33.6353\n",
      "Epoch [235/300], Step [87/172], Loss: 23.6574\n",
      "Epoch [235/300], Step [88/172], Loss: 21.8738\n",
      "Epoch [235/300], Step [89/172], Loss: 28.6095\n",
      "Epoch [235/300], Step [90/172], Loss: 21.3248\n",
      "Epoch [235/300], Step [91/172], Loss: 26.3294\n",
      "Epoch [235/300], Step [92/172], Loss: 19.2172\n",
      "Epoch [235/300], Step [93/172], Loss: 18.8683\n",
      "Epoch [235/300], Step [94/172], Loss: 25.7652\n",
      "Epoch [235/300], Step [95/172], Loss: 21.7946\n",
      "Epoch [235/300], Step [96/172], Loss: 20.6124\n",
      "Epoch [235/300], Step [97/172], Loss: 29.7748\n",
      "Epoch [235/300], Step [98/172], Loss: 20.3059\n",
      "Epoch [235/300], Step [99/172], Loss: 20.3020\n",
      "Epoch [235/300], Step [100/172], Loss: 19.3538\n",
      "Epoch [235/300], Step [101/172], Loss: 20.9194\n",
      "Epoch [235/300], Step [102/172], Loss: 18.8129\n",
      "Epoch [235/300], Step [103/172], Loss: 14.2860\n",
      "Epoch [235/300], Step [104/172], Loss: 20.9703\n",
      "Epoch [235/300], Step [105/172], Loss: 25.5274\n",
      "Epoch [235/300], Step [106/172], Loss: 17.6239\n",
      "Epoch [235/300], Step [107/172], Loss: 17.6078\n",
      "Epoch [235/300], Step [108/172], Loss: 17.8630\n",
      "Epoch [235/300], Step [109/172], Loss: 16.4303\n",
      "Epoch [235/300], Step [110/172], Loss: 19.6092\n",
      "Epoch [235/300], Step [111/172], Loss: 19.4260\n",
      "Epoch [235/300], Step [112/172], Loss: 17.3382\n",
      "Epoch [235/300], Step [113/172], Loss: 15.5613\n",
      "Epoch [235/300], Step [114/172], Loss: 15.8069\n",
      "Epoch [235/300], Step [115/172], Loss: 19.2554\n",
      "Epoch [235/300], Step [116/172], Loss: 15.2803\n",
      "Epoch [235/300], Step [117/172], Loss: 13.2586\n",
      "Epoch [235/300], Step [118/172], Loss: 13.1119\n",
      "Epoch [235/300], Step [119/172], Loss: 19.2586\n",
      "Epoch [235/300], Step [120/172], Loss: 11.7639\n",
      "Epoch [235/300], Step [121/172], Loss: 10.6536\n",
      "Epoch [235/300], Step [122/172], Loss: 14.3713\n",
      "Epoch [235/300], Step [123/172], Loss: 13.1302\n",
      "Epoch [235/300], Step [124/172], Loss: 8.0281\n",
      "Epoch [235/300], Step [125/172], Loss: 13.6427\n",
      "Epoch [235/300], Step [126/172], Loss: 13.5175\n",
      "Epoch [235/300], Step [127/172], Loss: 11.7107\n",
      "Epoch [235/300], Step [128/172], Loss: 11.2744\n",
      "Epoch [235/300], Step [129/172], Loss: 9.9208\n",
      "Epoch [235/300], Step [130/172], Loss: 13.4863\n",
      "Epoch [235/300], Step [131/172], Loss: 9.0795\n",
      "Epoch [235/300], Step [132/172], Loss: 11.0082\n",
      "Epoch [235/300], Step [133/172], Loss: 11.1144\n",
      "Epoch [235/300], Step [134/172], Loss: 11.2477\n",
      "Epoch [235/300], Step [135/172], Loss: 10.0925\n",
      "Epoch [235/300], Step [136/172], Loss: 9.6376\n",
      "Epoch [235/300], Step [137/172], Loss: 9.7447\n",
      "Epoch [235/300], Step [138/172], Loss: 9.8288\n",
      "Epoch [235/300], Step [139/172], Loss: 10.9511\n",
      "Epoch [235/300], Step [140/172], Loss: 11.7453\n",
      "Epoch [235/300], Step [141/172], Loss: 10.3547\n",
      "Epoch [235/300], Step [142/172], Loss: 14.5853\n",
      "Epoch [235/300], Step [143/172], Loss: 12.1200\n",
      "Epoch [235/300], Step [144/172], Loss: 10.0270\n",
      "Epoch [235/300], Step [145/172], Loss: 11.3268\n",
      "Epoch [235/300], Step [146/172], Loss: 11.4969\n",
      "Epoch [235/300], Step [147/172], Loss: 6.5003\n",
      "Epoch [235/300], Step [148/172], Loss: 7.7461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [235/300], Step [149/172], Loss: 8.2961\n",
      "Epoch [235/300], Step [150/172], Loss: 6.6664\n",
      "Epoch [235/300], Step [151/172], Loss: 6.4639\n",
      "Epoch [235/300], Step [152/172], Loss: 9.7509\n",
      "Epoch [235/300], Step [153/172], Loss: 6.9879\n",
      "Epoch [235/300], Step [154/172], Loss: 7.4168\n",
      "Epoch [235/300], Step [155/172], Loss: 7.3616\n",
      "Epoch [235/300], Step [156/172], Loss: 14.7612\n",
      "Epoch [235/300], Step [157/172], Loss: 8.9395\n",
      "Epoch [235/300], Step [158/172], Loss: 7.9107\n",
      "Epoch [235/300], Step [159/172], Loss: 9.7127\n",
      "Epoch [235/300], Step [160/172], Loss: 9.5319\n",
      "Epoch [235/300], Step [161/172], Loss: 9.8774\n",
      "Epoch [235/300], Step [162/172], Loss: 5.7901\n",
      "Epoch [235/300], Step [163/172], Loss: 7.1520\n",
      "Epoch [235/300], Step [164/172], Loss: 9.9019\n",
      "Epoch [235/300], Step [165/172], Loss: 7.7568\n",
      "Epoch [235/300], Step [166/172], Loss: 6.8006\n",
      "Epoch [235/300], Step [167/172], Loss: 10.6912\n",
      "Epoch [235/300], Step [168/172], Loss: 7.4321\n",
      "Epoch [235/300], Step [169/172], Loss: 7.5608\n",
      "Epoch [235/300], Step [170/172], Loss: 6.5325\n",
      "Epoch [235/300], Step [171/172], Loss: 8.6681\n",
      "Epoch [235/300], Step [172/172], Loss: 6.3049\n",
      "Epoch [236/300], Step [1/172], Loss: 44.5401\n",
      "Epoch [236/300], Step [2/172], Loss: 48.3752\n",
      "Epoch [236/300], Step [3/172], Loss: 46.7353\n",
      "Epoch [236/300], Step [4/172], Loss: 21.9087\n",
      "Epoch [236/300], Step [5/172], Loss: 41.5013\n",
      "Epoch [236/300], Step [6/172], Loss: 21.0173\n",
      "Epoch [236/300], Step [7/172], Loss: 26.6152\n",
      "Epoch [236/300], Step [8/172], Loss: 4.6997\n",
      "Epoch [236/300], Step [9/172], Loss: 26.9678\n",
      "Epoch [236/300], Step [10/172], Loss: 38.5363\n",
      "Epoch [236/300], Step [11/172], Loss: 51.2402\n",
      "Epoch [236/300], Step [12/172], Loss: 52.8217\n",
      "Epoch [236/300], Step [13/172], Loss: 32.7583\n",
      "Epoch [236/300], Step [14/172], Loss: 57.2549\n",
      "Epoch [236/300], Step [15/172], Loss: 46.9306\n",
      "Epoch [236/300], Step [16/172], Loss: 9.7002\n",
      "Epoch [236/300], Step [17/172], Loss: 38.7178\n",
      "Epoch [236/300], Step [18/172], Loss: 50.9278\n",
      "Epoch [236/300], Step [19/172], Loss: 69.5110\n",
      "Epoch [236/300], Step [20/172], Loss: 27.9992\n",
      "Epoch [236/300], Step [21/172], Loss: 78.3731\n",
      "Epoch [236/300], Step [22/172], Loss: 50.8254\n",
      "Epoch [236/300], Step [23/172], Loss: 1.4459\n",
      "Epoch [236/300], Step [24/172], Loss: 50.9636\n",
      "Epoch [236/300], Step [25/172], Loss: 35.9016\n",
      "Epoch [236/300], Step [26/172], Loss: 44.0799\n",
      "Epoch [236/300], Step [27/172], Loss: 53.5780\n",
      "Epoch [236/300], Step [28/172], Loss: 21.6864\n",
      "Epoch [236/300], Step [29/172], Loss: 14.6886\n",
      "Epoch [236/300], Step [30/172], Loss: 55.3436\n",
      "Epoch [236/300], Step [31/172], Loss: 35.2618\n",
      "Epoch [236/300], Step [32/172], Loss: 43.6176\n",
      "Epoch [236/300], Step [33/172], Loss: 69.6650\n",
      "Epoch [236/300], Step [34/172], Loss: 2.5465\n",
      "Epoch [236/300], Step [35/172], Loss: 14.6808\n",
      "Epoch [236/300], Step [36/172], Loss: 17.8358\n",
      "Epoch [236/300], Step [37/172], Loss: 17.1626\n",
      "Epoch [236/300], Step [38/172], Loss: 33.2583\n",
      "Epoch [236/300], Step [39/172], Loss: 38.0124\n",
      "Epoch [236/300], Step [40/172], Loss: 25.0203\n",
      "Epoch [236/300], Step [41/172], Loss: 36.9930\n",
      "Epoch [236/300], Step [42/172], Loss: 43.4022\n",
      "Epoch [236/300], Step [43/172], Loss: 31.3498\n",
      "Epoch [236/300], Step [44/172], Loss: 24.9181\n",
      "Epoch [236/300], Step [45/172], Loss: 36.6113\n",
      "Epoch [236/300], Step [46/172], Loss: 18.7401\n",
      "Epoch [236/300], Step [47/172], Loss: 54.6719\n",
      "Epoch [236/300], Step [48/172], Loss: 62.5583\n",
      "Epoch [236/300], Step [49/172], Loss: 28.5206\n",
      "Epoch [236/300], Step [50/172], Loss: 48.2400\n",
      "Epoch [236/300], Step [51/172], Loss: 12.0101\n",
      "Epoch [236/300], Step [52/172], Loss: 27.6774\n",
      "Epoch [236/300], Step [53/172], Loss: 29.4929\n",
      "Epoch [236/300], Step [54/172], Loss: 22.5287\n",
      "Epoch [236/300], Step [55/172], Loss: 22.1541\n",
      "Epoch [236/300], Step [56/172], Loss: 20.5167\n",
      "Epoch [236/300], Step [57/172], Loss: 22.2778\n",
      "Epoch [236/300], Step [58/172], Loss: 18.3555\n",
      "Epoch [236/300], Step [59/172], Loss: 31.7353\n",
      "Epoch [236/300], Step [60/172], Loss: 21.7488\n",
      "Epoch [236/300], Step [61/172], Loss: 9.2722\n",
      "Epoch [236/300], Step [62/172], Loss: 17.5370\n",
      "Epoch [236/300], Step [63/172], Loss: 14.3910\n",
      "Epoch [236/300], Step [64/172], Loss: 15.8110\n",
      "Epoch [236/300], Step [65/172], Loss: 21.5281\n",
      "Epoch [236/300], Step [66/172], Loss: 8.6076\n",
      "Epoch [236/300], Step [67/172], Loss: 25.8176\n",
      "Epoch [236/300], Step [68/172], Loss: 8.3163\n",
      "Epoch [236/300], Step [69/172], Loss: 28.3829\n",
      "Epoch [236/300], Step [70/172], Loss: 32.6727\n",
      "Epoch [236/300], Step [71/172], Loss: 33.2081\n",
      "Epoch [236/300], Step [72/172], Loss: 29.6562\n",
      "Epoch [236/300], Step [73/172], Loss: 38.1895\n",
      "Epoch [236/300], Step [74/172], Loss: 20.8307\n",
      "Epoch [236/300], Step [75/172], Loss: 19.6563\n",
      "Epoch [236/300], Step [76/172], Loss: 27.5118\n",
      "Epoch [236/300], Step [77/172], Loss: 41.1128\n",
      "Epoch [236/300], Step [78/172], Loss: 31.0569\n",
      "Epoch [236/300], Step [79/172], Loss: 30.1889\n",
      "Epoch [236/300], Step [80/172], Loss: 46.3117\n",
      "Epoch [236/300], Step [81/172], Loss: 27.2921\n",
      "Epoch [236/300], Step [82/172], Loss: 36.1495\n",
      "Epoch [236/300], Step [83/172], Loss: 40.1795\n",
      "Epoch [236/300], Step [84/172], Loss: 31.9656\n",
      "Epoch [236/300], Step [85/172], Loss: 36.3549\n",
      "Epoch [236/300], Step [86/172], Loss: 33.4124\n",
      "Epoch [236/300], Step [87/172], Loss: 23.8675\n",
      "Epoch [236/300], Step [88/172], Loss: 22.2265\n",
      "Epoch [236/300], Step [89/172], Loss: 28.8620\n",
      "Epoch [236/300], Step [90/172], Loss: 21.4768\n",
      "Epoch [236/300], Step [91/172], Loss: 26.4434\n",
      "Epoch [236/300], Step [92/172], Loss: 19.2809\n",
      "Epoch [236/300], Step [93/172], Loss: 18.8235\n",
      "Epoch [236/300], Step [94/172], Loss: 25.7061\n",
      "Epoch [236/300], Step [95/172], Loss: 22.0607\n",
      "Epoch [236/300], Step [96/172], Loss: 20.7442\n",
      "Epoch [236/300], Step [97/172], Loss: 30.1366\n",
      "Epoch [236/300], Step [98/172], Loss: 20.5316\n",
      "Epoch [236/300], Step [99/172], Loss: 20.5463\n",
      "Epoch [236/300], Step [100/172], Loss: 19.5664\n",
      "Epoch [236/300], Step [101/172], Loss: 21.1783\n",
      "Epoch [236/300], Step [102/172], Loss: 18.9356\n",
      "Epoch [236/300], Step [103/172], Loss: 14.4297\n",
      "Epoch [236/300], Step [104/172], Loss: 21.3691\n",
      "Epoch [236/300], Step [105/172], Loss: 25.6271\n",
      "Epoch [236/300], Step [106/172], Loss: 17.6021\n",
      "Epoch [236/300], Step [107/172], Loss: 17.5429\n",
      "Epoch [236/300], Step [108/172], Loss: 17.8678\n",
      "Epoch [236/300], Step [109/172], Loss: 16.4042\n",
      "Epoch [236/300], Step [110/172], Loss: 19.6125\n",
      "Epoch [236/300], Step [111/172], Loss: 19.4957\n",
      "Epoch [236/300], Step [112/172], Loss: 17.4860\n",
      "Epoch [236/300], Step [113/172], Loss: 15.7495\n",
      "Epoch [236/300], Step [114/172], Loss: 15.9542\n",
      "Epoch [236/300], Step [115/172], Loss: 19.1430\n",
      "Epoch [236/300], Step [116/172], Loss: 15.3461\n",
      "Epoch [236/300], Step [117/172], Loss: 13.3364\n",
      "Epoch [236/300], Step [118/172], Loss: 12.9833\n",
      "Epoch [236/300], Step [119/172], Loss: 19.2351\n",
      "Epoch [236/300], Step [120/172], Loss: 11.7595\n",
      "Epoch [236/300], Step [121/172], Loss: 10.7200\n",
      "Epoch [236/300], Step [122/172], Loss: 13.9289\n",
      "Epoch [236/300], Step [123/172], Loss: 12.8967\n",
      "Epoch [236/300], Step [124/172], Loss: 8.2946\n",
      "Epoch [236/300], Step [125/172], Loss: 13.8893\n",
      "Epoch [236/300], Step [126/172], Loss: 13.9924\n",
      "Epoch [236/300], Step [127/172], Loss: 11.8080\n",
      "Epoch [236/300], Step [128/172], Loss: 11.3834\n",
      "Epoch [236/300], Step [129/172], Loss: 9.9677\n",
      "Epoch [236/300], Step [130/172], Loss: 13.5305\n",
      "Epoch [236/300], Step [131/172], Loss: 9.1346\n",
      "Epoch [236/300], Step [132/172], Loss: 11.2485\n",
      "Epoch [236/300], Step [133/172], Loss: 10.8840\n",
      "Epoch [236/300], Step [134/172], Loss: 11.0092\n",
      "Epoch [236/300], Step [135/172], Loss: 10.3178\n",
      "Epoch [236/300], Step [136/172], Loss: 9.8787\n",
      "Epoch [236/300], Step [137/172], Loss: 9.7684\n",
      "Epoch [236/300], Step [138/172], Loss: 10.0178\n",
      "Epoch [236/300], Step [139/172], Loss: 11.2816\n",
      "Epoch [236/300], Step [140/172], Loss: 12.0436\n",
      "Epoch [236/300], Step [141/172], Loss: 10.4891\n",
      "Epoch [236/300], Step [142/172], Loss: 14.3315\n",
      "Epoch [236/300], Step [143/172], Loss: 12.0485\n",
      "Epoch [236/300], Step [144/172], Loss: 10.0800\n",
      "Epoch [236/300], Step [145/172], Loss: 11.3983\n",
      "Epoch [236/300], Step [146/172], Loss: 11.5412\n",
      "Epoch [236/300], Step [147/172], Loss: 6.6504\n",
      "Epoch [236/300], Step [148/172], Loss: 7.8989\n",
      "Epoch [236/300], Step [149/172], Loss: 8.3967\n",
      "Epoch [236/300], Step [150/172], Loss: 6.8212\n",
      "Epoch [236/300], Step [151/172], Loss: 6.6391\n",
      "Epoch [236/300], Step [152/172], Loss: 9.8704\n",
      "Epoch [236/300], Step [153/172], Loss: 7.1610\n",
      "Epoch [236/300], Step [154/172], Loss: 7.4327\n",
      "Epoch [236/300], Step [155/172], Loss: 7.7015\n",
      "Epoch [236/300], Step [156/172], Loss: 14.6779\n",
      "Epoch [236/300], Step [157/172], Loss: 8.7979\n",
      "Epoch [236/300], Step [158/172], Loss: 7.9727\n",
      "Epoch [236/300], Step [159/172], Loss: 9.7120\n",
      "Epoch [236/300], Step [160/172], Loss: 9.4969\n",
      "Epoch [236/300], Step [161/172], Loss: 10.0710\n",
      "Epoch [236/300], Step [162/172], Loss: 5.8481\n",
      "Epoch [236/300], Step [163/172], Loss: 7.0637\n",
      "Epoch [236/300], Step [164/172], Loss: 10.1106\n",
      "Epoch [236/300], Step [165/172], Loss: 7.9182\n",
      "Epoch [236/300], Step [166/172], Loss: 6.7813\n",
      "Epoch [236/300], Step [167/172], Loss: 10.7413\n",
      "Epoch [236/300], Step [168/172], Loss: 7.5717\n",
      "Epoch [236/300], Step [169/172], Loss: 7.6919\n",
      "Epoch [236/300], Step [170/172], Loss: 6.6565\n",
      "Epoch [236/300], Step [171/172], Loss: 8.7348\n",
      "Epoch [236/300], Step [172/172], Loss: 6.5506\n",
      "Epoch [237/300], Step [1/172], Loss: 44.0837\n",
      "Epoch [237/300], Step [2/172], Loss: 49.4947\n",
      "Epoch [237/300], Step [3/172], Loss: 44.9596\n",
      "Epoch [237/300], Step [4/172], Loss: 21.5254\n",
      "Epoch [237/300], Step [5/172], Loss: 40.7136\n",
      "Epoch [237/300], Step [6/172], Loss: 21.0703\n",
      "Epoch [237/300], Step [7/172], Loss: 28.4871\n",
      "Epoch [237/300], Step [8/172], Loss: 4.0066\n",
      "Epoch [237/300], Step [9/172], Loss: 26.7595\n",
      "Epoch [237/300], Step [10/172], Loss: 40.2901\n",
      "Epoch [237/300], Step [11/172], Loss: 51.1778\n",
      "Epoch [237/300], Step [12/172], Loss: 52.1450\n",
      "Epoch [237/300], Step [13/172], Loss: 32.4904\n",
      "Epoch [237/300], Step [14/172], Loss: 56.7856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [237/300], Step [15/172], Loss: 46.3177\n",
      "Epoch [237/300], Step [16/172], Loss: 10.0451\n",
      "Epoch [237/300], Step [17/172], Loss: 38.6933\n",
      "Epoch [237/300], Step [18/172], Loss: 50.7594\n",
      "Epoch [237/300], Step [19/172], Loss: 69.1556\n",
      "Epoch [237/300], Step [20/172], Loss: 27.0173\n",
      "Epoch [237/300], Step [21/172], Loss: 78.2624\n",
      "Epoch [237/300], Step [22/172], Loss: 51.6079\n",
      "Epoch [237/300], Step [23/172], Loss: 1.7964\n",
      "Epoch [237/300], Step [24/172], Loss: 50.3198\n",
      "Epoch [237/300], Step [25/172], Loss: 36.4920\n",
      "Epoch [237/300], Step [26/172], Loss: 44.1901\n",
      "Epoch [237/300], Step [27/172], Loss: 53.7174\n",
      "Epoch [237/300], Step [28/172], Loss: 21.1390\n",
      "Epoch [237/300], Step [29/172], Loss: 14.0387\n",
      "Epoch [237/300], Step [30/172], Loss: 55.5644\n",
      "Epoch [237/300], Step [31/172], Loss: 35.5589\n",
      "Epoch [237/300], Step [32/172], Loss: 43.0254\n",
      "Epoch [237/300], Step [33/172], Loss: 67.8798\n",
      "Epoch [237/300], Step [34/172], Loss: 2.3403\n",
      "Epoch [237/300], Step [35/172], Loss: 14.6800\n",
      "Epoch [237/300], Step [36/172], Loss: 16.8290\n",
      "Epoch [237/300], Step [37/172], Loss: 17.0388\n",
      "Epoch [237/300], Step [38/172], Loss: 33.0689\n",
      "Epoch [237/300], Step [39/172], Loss: 37.3194\n",
      "Epoch [237/300], Step [40/172], Loss: 24.9527\n",
      "Epoch [237/300], Step [41/172], Loss: 37.8918\n",
      "Epoch [237/300], Step [42/172], Loss: 43.5804\n",
      "Epoch [237/300], Step [43/172], Loss: 31.3426\n",
      "Epoch [237/300], Step [44/172], Loss: 24.8539\n",
      "Epoch [237/300], Step [45/172], Loss: 37.5413\n",
      "Epoch [237/300], Step [46/172], Loss: 19.1128\n",
      "Epoch [237/300], Step [47/172], Loss: 54.5475\n",
      "Epoch [237/300], Step [48/172], Loss: 61.6875\n",
      "Epoch [237/300], Step [49/172], Loss: 28.2764\n",
      "Epoch [237/300], Step [50/172], Loss: 47.7172\n",
      "Epoch [237/300], Step [51/172], Loss: 11.7246\n",
      "Epoch [237/300], Step [52/172], Loss: 27.2989\n",
      "Epoch [237/300], Step [53/172], Loss: 29.0764\n",
      "Epoch [237/300], Step [54/172], Loss: 22.6936\n",
      "Epoch [237/300], Step [55/172], Loss: 22.2395\n",
      "Epoch [237/300], Step [56/172], Loss: 19.6075\n",
      "Epoch [237/300], Step [57/172], Loss: 19.3509\n",
      "Epoch [237/300], Step [58/172], Loss: 18.1957\n",
      "Epoch [237/300], Step [59/172], Loss: 31.9346\n",
      "Epoch [237/300], Step [60/172], Loss: 19.8058\n",
      "Epoch [237/300], Step [61/172], Loss: 9.3733\n",
      "Epoch [237/300], Step [62/172], Loss: 17.4526\n",
      "Epoch [237/300], Step [63/172], Loss: 13.8522\n",
      "Epoch [237/300], Step [64/172], Loss: 15.3157\n",
      "Epoch [237/300], Step [65/172], Loss: 21.3159\n",
      "Epoch [237/300], Step [66/172], Loss: 8.8629\n",
      "Epoch [237/300], Step [67/172], Loss: 25.5491\n",
      "Epoch [237/300], Step [68/172], Loss: 5.8998\n",
      "Epoch [237/300], Step [69/172], Loss: 28.6887\n",
      "Epoch [237/300], Step [70/172], Loss: 32.8948\n",
      "Epoch [237/300], Step [71/172], Loss: 33.3486\n",
      "Epoch [237/300], Step [72/172], Loss: 29.6207\n",
      "Epoch [237/300], Step [73/172], Loss: 37.7205\n",
      "Epoch [237/300], Step [74/172], Loss: 20.5326\n",
      "Epoch [237/300], Step [75/172], Loss: 19.3599\n",
      "Epoch [237/300], Step [76/172], Loss: 27.5076\n",
      "Epoch [237/300], Step [77/172], Loss: 40.9092\n",
      "Epoch [237/300], Step [78/172], Loss: 31.0474\n",
      "Epoch [237/300], Step [79/172], Loss: 29.8049\n",
      "Epoch [237/300], Step [80/172], Loss: 45.7025\n",
      "Epoch [237/300], Step [81/172], Loss: 26.8079\n",
      "Epoch [237/300], Step [82/172], Loss: 35.1700\n",
      "Epoch [237/300], Step [83/172], Loss: 40.0557\n",
      "Epoch [237/300], Step [84/172], Loss: 31.8201\n",
      "Epoch [237/300], Step [85/172], Loss: 35.7755\n",
      "Epoch [237/300], Step [86/172], Loss: 33.3886\n",
      "Epoch [237/300], Step [87/172], Loss: 23.4553\n",
      "Epoch [237/300], Step [88/172], Loss: 21.8963\n",
      "Epoch [237/300], Step [89/172], Loss: 28.5874\n",
      "Epoch [237/300], Step [90/172], Loss: 21.4174\n",
      "Epoch [237/300], Step [91/172], Loss: 26.2339\n",
      "Epoch [237/300], Step [92/172], Loss: 19.1565\n",
      "Epoch [237/300], Step [93/172], Loss: 18.6616\n",
      "Epoch [237/300], Step [94/172], Loss: 25.5538\n",
      "Epoch [237/300], Step [95/172], Loss: 22.1289\n",
      "Epoch [237/300], Step [96/172], Loss: 20.7422\n",
      "Epoch [237/300], Step [97/172], Loss: 30.0050\n",
      "Epoch [237/300], Step [98/172], Loss: 20.3299\n",
      "Epoch [237/300], Step [99/172], Loss: 20.4609\n",
      "Epoch [237/300], Step [100/172], Loss: 19.5085\n",
      "Epoch [237/300], Step [101/172], Loss: 21.1023\n",
      "Epoch [237/300], Step [102/172], Loss: 18.8775\n",
      "Epoch [237/300], Step [103/172], Loss: 14.3085\n",
      "Epoch [237/300], Step [104/172], Loss: 21.1678\n",
      "Epoch [237/300], Step [105/172], Loss: 25.4222\n",
      "Epoch [237/300], Step [106/172], Loss: 17.6093\n",
      "Epoch [237/300], Step [107/172], Loss: 17.5605\n",
      "Epoch [237/300], Step [108/172], Loss: 18.0719\n",
      "Epoch [237/300], Step [109/172], Loss: 16.5464\n",
      "Epoch [237/300], Step [110/172], Loss: 19.8427\n",
      "Epoch [237/300], Step [111/172], Loss: 19.5608\n",
      "Epoch [237/300], Step [112/172], Loss: 17.4063\n",
      "Epoch [237/300], Step [113/172], Loss: 15.7548\n",
      "Epoch [237/300], Step [114/172], Loss: 16.0006\n",
      "Epoch [237/300], Step [115/172], Loss: 19.4279\n",
      "Epoch [237/300], Step [116/172], Loss: 15.4190\n",
      "Epoch [237/300], Step [117/172], Loss: 13.3862\n",
      "Epoch [237/300], Step [118/172], Loss: 13.2565\n",
      "Epoch [237/300], Step [119/172], Loss: 19.3179\n",
      "Epoch [237/300], Step [120/172], Loss: 11.9029\n",
      "Epoch [237/300], Step [121/172], Loss: 10.6866\n",
      "Epoch [237/300], Step [122/172], Loss: 14.5481\n",
      "Epoch [237/300], Step [123/172], Loss: 13.1069\n",
      "Epoch [237/300], Step [124/172], Loss: 8.1222\n",
      "Epoch [237/300], Step [125/172], Loss: 13.7511\n",
      "Epoch [237/300], Step [126/172], Loss: 13.8636\n",
      "Epoch [237/300], Step [127/172], Loss: 11.8020\n",
      "Epoch [237/300], Step [128/172], Loss: 11.2447\n",
      "Epoch [237/300], Step [129/172], Loss: 9.8231\n",
      "Epoch [237/300], Step [130/172], Loss: 13.7595\n",
      "Epoch [237/300], Step [131/172], Loss: 9.1486\n",
      "Epoch [237/300], Step [132/172], Loss: 11.1751\n",
      "Epoch [237/300], Step [133/172], Loss: 11.2224\n",
      "Epoch [237/300], Step [134/172], Loss: 11.1658\n",
      "Epoch [237/300], Step [135/172], Loss: 10.2467\n",
      "Epoch [237/300], Step [136/172], Loss: 9.7338\n",
      "Epoch [237/300], Step [137/172], Loss: 9.8051\n",
      "Epoch [237/300], Step [138/172], Loss: 10.0387\n",
      "Epoch [237/300], Step [139/172], Loss: 11.2759\n",
      "Epoch [237/300], Step [140/172], Loss: 11.9902\n",
      "Epoch [237/300], Step [141/172], Loss: 10.4676\n",
      "Epoch [237/300], Step [142/172], Loss: 14.6860\n",
      "Epoch [237/300], Step [143/172], Loss: 12.2791\n",
      "Epoch [237/300], Step [144/172], Loss: 10.1442\n",
      "Epoch [237/300], Step [145/172], Loss: 11.4135\n",
      "Epoch [237/300], Step [146/172], Loss: 11.7601\n",
      "Epoch [237/300], Step [147/172], Loss: 6.6435\n",
      "Epoch [237/300], Step [148/172], Loss: 7.9005\n",
      "Epoch [237/300], Step [149/172], Loss: 8.3847\n",
      "Epoch [237/300], Step [150/172], Loss: 6.7918\n",
      "Epoch [237/300], Step [151/172], Loss: 6.6451\n",
      "Epoch [237/300], Step [152/172], Loss: 9.8509\n",
      "Epoch [237/300], Step [153/172], Loss: 7.1090\n",
      "Epoch [237/300], Step [154/172], Loss: 7.4280\n",
      "Epoch [237/300], Step [155/172], Loss: 7.5697\n",
      "Epoch [237/300], Step [156/172], Loss: 14.9255\n",
      "Epoch [237/300], Step [157/172], Loss: 9.1049\n",
      "Epoch [237/300], Step [158/172], Loss: 8.0683\n",
      "Epoch [237/300], Step [159/172], Loss: 9.9608\n",
      "Epoch [237/300], Step [160/172], Loss: 9.7760\n",
      "Epoch [237/300], Step [161/172], Loss: 9.8614\n",
      "Epoch [237/300], Step [162/172], Loss: 5.9114\n",
      "Epoch [237/300], Step [163/172], Loss: 7.2471\n",
      "Epoch [237/300], Step [164/172], Loss: 9.7421\n",
      "Epoch [237/300], Step [165/172], Loss: 7.9365\n",
      "Epoch [237/300], Step [166/172], Loss: 6.8525\n",
      "Epoch [237/300], Step [167/172], Loss: 10.8615\n",
      "Epoch [237/300], Step [168/172], Loss: 7.5454\n",
      "Epoch [237/300], Step [169/172], Loss: 7.7021\n",
      "Epoch [237/300], Step [170/172], Loss: 6.6585\n",
      "Epoch [237/300], Step [171/172], Loss: 9.0925\n",
      "Epoch [237/300], Step [172/172], Loss: 6.5212\n",
      "Epoch [238/300], Step [1/172], Loss: 43.9329\n",
      "Epoch [238/300], Step [2/172], Loss: 47.7209\n",
      "Epoch [238/300], Step [3/172], Loss: 45.3598\n",
      "Epoch [238/300], Step [4/172], Loss: 21.3995\n",
      "Epoch [238/300], Step [5/172], Loss: 40.9119\n",
      "Epoch [238/300], Step [6/172], Loss: 20.5137\n",
      "Epoch [238/300], Step [7/172], Loss: 27.7033\n",
      "Epoch [238/300], Step [8/172], Loss: 5.5768\n",
      "Epoch [238/300], Step [9/172], Loss: 26.7466\n",
      "Epoch [238/300], Step [10/172], Loss: 37.8015\n",
      "Epoch [238/300], Step [11/172], Loss: 50.3275\n",
      "Epoch [238/300], Step [12/172], Loss: 52.1470\n",
      "Epoch [238/300], Step [13/172], Loss: 32.3711\n",
      "Epoch [238/300], Step [14/172], Loss: 56.2741\n",
      "Epoch [238/300], Step [15/172], Loss: 46.1189\n",
      "Epoch [238/300], Step [16/172], Loss: 8.9614\n",
      "Epoch [238/300], Step [17/172], Loss: 38.8074\n",
      "Epoch [238/300], Step [18/172], Loss: 50.2967\n",
      "Epoch [238/300], Step [19/172], Loss: 69.1958\n",
      "Epoch [238/300], Step [20/172], Loss: 26.1181\n",
      "Epoch [238/300], Step [21/172], Loss: 78.1061\n",
      "Epoch [238/300], Step [22/172], Loss: 50.8047\n",
      "Epoch [238/300], Step [23/172], Loss: 2.0089\n",
      "Epoch [238/300], Step [24/172], Loss: 50.4096\n",
      "Epoch [238/300], Step [25/172], Loss: 35.8630\n",
      "Epoch [238/300], Step [26/172], Loss: 43.9289\n",
      "Epoch [238/300], Step [27/172], Loss: 53.3253\n",
      "Epoch [238/300], Step [28/172], Loss: 20.9623\n",
      "Epoch [238/300], Step [29/172], Loss: 14.1543\n",
      "Epoch [238/300], Step [30/172], Loss: 54.3326\n",
      "Epoch [238/300], Step [31/172], Loss: 34.5019\n",
      "Epoch [238/300], Step [32/172], Loss: 43.1160\n",
      "Epoch [238/300], Step [33/172], Loss: 68.4199\n",
      "Epoch [238/300], Step [34/172], Loss: 2.2274\n",
      "Epoch [238/300], Step [35/172], Loss: 14.7403\n",
      "Epoch [238/300], Step [36/172], Loss: 16.8985\n",
      "Epoch [238/300], Step [37/172], Loss: 16.8543\n",
      "Epoch [238/300], Step [38/172], Loss: 32.5739\n",
      "Epoch [238/300], Step [39/172], Loss: 36.9741\n",
      "Epoch [238/300], Step [40/172], Loss: 24.6729\n",
      "Epoch [238/300], Step [41/172], Loss: 36.6819\n",
      "Epoch [238/300], Step [42/172], Loss: 43.0064\n",
      "Epoch [238/300], Step [43/172], Loss: 30.7293\n",
      "Epoch [238/300], Step [44/172], Loss: 24.1563\n",
      "Epoch [238/300], Step [45/172], Loss: 36.9557\n",
      "Epoch [238/300], Step [46/172], Loss: 18.4615\n",
      "Epoch [238/300], Step [47/172], Loss: 53.8322\n",
      "Epoch [238/300], Step [48/172], Loss: 61.0518\n",
      "Epoch [238/300], Step [49/172], Loss: 27.8541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238/300], Step [50/172], Loss: 48.1709\n",
      "Epoch [238/300], Step [51/172], Loss: 11.7122\n",
      "Epoch [238/300], Step [52/172], Loss: 27.0489\n",
      "Epoch [238/300], Step [53/172], Loss: 28.4320\n",
      "Epoch [238/300], Step [54/172], Loss: 22.3854\n",
      "Epoch [238/300], Step [55/172], Loss: 21.9609\n",
      "Epoch [238/300], Step [56/172], Loss: 19.7994\n",
      "Epoch [238/300], Step [57/172], Loss: 19.4700\n",
      "Epoch [238/300], Step [58/172], Loss: 18.2190\n",
      "Epoch [238/300], Step [59/172], Loss: 32.3529\n",
      "Epoch [238/300], Step [60/172], Loss: 20.1278\n",
      "Epoch [238/300], Step [61/172], Loss: 9.1423\n",
      "Epoch [238/300], Step [62/172], Loss: 17.1603\n",
      "Epoch [238/300], Step [63/172], Loss: 14.0691\n",
      "Epoch [238/300], Step [64/172], Loss: 15.4559\n",
      "Epoch [238/300], Step [65/172], Loss: 22.1364\n",
      "Epoch [238/300], Step [66/172], Loss: 8.9453\n",
      "Epoch [238/300], Step [67/172], Loss: 26.2483\n",
      "Epoch [238/300], Step [68/172], Loss: 6.8791\n",
      "Epoch [238/300], Step [69/172], Loss: 28.7038\n",
      "Epoch [238/300], Step [70/172], Loss: 31.4681\n",
      "Epoch [238/300], Step [71/172], Loss: 32.7360\n",
      "Epoch [238/300], Step [72/172], Loss: 29.2704\n",
      "Epoch [238/300], Step [73/172], Loss: 37.3248\n",
      "Epoch [238/300], Step [74/172], Loss: 20.4072\n",
      "Epoch [238/300], Step [75/172], Loss: 19.3709\n",
      "Epoch [238/300], Step [76/172], Loss: 27.0280\n",
      "Epoch [238/300], Step [77/172], Loss: 40.4582\n",
      "Epoch [238/300], Step [78/172], Loss: 30.6340\n",
      "Epoch [238/300], Step [79/172], Loss: 29.5946\n",
      "Epoch [238/300], Step [80/172], Loss: 46.4372\n",
      "Epoch [238/300], Step [81/172], Loss: 26.8206\n",
      "Epoch [238/300], Step [82/172], Loss: 36.3499\n",
      "Epoch [238/300], Step [83/172], Loss: 39.8337\n",
      "Epoch [238/300], Step [84/172], Loss: 31.9636\n",
      "Epoch [238/300], Step [85/172], Loss: 35.7095\n",
      "Epoch [238/300], Step [86/172], Loss: 33.2429\n",
      "Epoch [238/300], Step [87/172], Loss: 23.5774\n",
      "Epoch [238/300], Step [88/172], Loss: 21.6992\n",
      "Epoch [238/300], Step [89/172], Loss: 28.8040\n",
      "Epoch [238/300], Step [90/172], Loss: 21.2110\n",
      "Epoch [238/300], Step [91/172], Loss: 26.4355\n",
      "Epoch [238/300], Step [92/172], Loss: 19.1167\n",
      "Epoch [238/300], Step [93/172], Loss: 18.5361\n",
      "Epoch [238/300], Step [94/172], Loss: 25.4784\n",
      "Epoch [238/300], Step [95/172], Loss: 21.8509\n",
      "Epoch [238/300], Step [96/172], Loss: 20.7132\n",
      "Epoch [238/300], Step [97/172], Loss: 30.3253\n",
      "Epoch [238/300], Step [98/172], Loss: 20.3749\n",
      "Epoch [238/300], Step [99/172], Loss: 20.4682\n",
      "Epoch [238/300], Step [100/172], Loss: 19.5678\n",
      "Epoch [238/300], Step [101/172], Loss: 21.1151\n",
      "Epoch [238/300], Step [102/172], Loss: 19.3464\n",
      "Epoch [238/300], Step [103/172], Loss: 14.2839\n",
      "Epoch [238/300], Step [104/172], Loss: 21.2701\n",
      "Epoch [238/300], Step [105/172], Loss: 25.6972\n",
      "Epoch [238/300], Step [106/172], Loss: 17.4268\n",
      "Epoch [238/300], Step [107/172], Loss: 17.5516\n",
      "Epoch [238/300], Step [108/172], Loss: 17.9765\n",
      "Epoch [238/300], Step [109/172], Loss: 16.4656\n",
      "Epoch [238/300], Step [110/172], Loss: 19.6776\n",
      "Epoch [238/300], Step [111/172], Loss: 19.4357\n",
      "Epoch [238/300], Step [112/172], Loss: 17.3083\n",
      "Epoch [238/300], Step [113/172], Loss: 15.6877\n",
      "Epoch [238/300], Step [114/172], Loss: 15.9114\n",
      "Epoch [238/300], Step [115/172], Loss: 19.1773\n",
      "Epoch [238/300], Step [116/172], Loss: 15.2529\n",
      "Epoch [238/300], Step [117/172], Loss: 13.3789\n",
      "Epoch [238/300], Step [118/172], Loss: 13.1219\n",
      "Epoch [238/300], Step [119/172], Loss: 19.2845\n",
      "Epoch [238/300], Step [120/172], Loss: 11.7737\n",
      "Epoch [238/300], Step [121/172], Loss: 10.5459\n",
      "Epoch [238/300], Step [122/172], Loss: 14.6913\n",
      "Epoch [238/300], Step [123/172], Loss: 13.0018\n",
      "Epoch [238/300], Step [124/172], Loss: 8.1425\n",
      "Epoch [238/300], Step [125/172], Loss: 13.7025\n",
      "Epoch [238/300], Step [126/172], Loss: 13.7120\n",
      "Epoch [238/300], Step [127/172], Loss: 11.8340\n",
      "Epoch [238/300], Step [128/172], Loss: 11.0735\n",
      "Epoch [238/300], Step [129/172], Loss: 9.7839\n",
      "Epoch [238/300], Step [130/172], Loss: 13.5142\n",
      "Epoch [238/300], Step [131/172], Loss: 9.2153\n",
      "Epoch [238/300], Step [132/172], Loss: 11.1816\n",
      "Epoch [238/300], Step [133/172], Loss: 11.2532\n",
      "Epoch [238/300], Step [134/172], Loss: 11.1265\n",
      "Epoch [238/300], Step [135/172], Loss: 10.1790\n",
      "Epoch [238/300], Step [136/172], Loss: 9.6979\n",
      "Epoch [238/300], Step [137/172], Loss: 9.7564\n",
      "Epoch [238/300], Step [138/172], Loss: 10.0125\n",
      "Epoch [238/300], Step [139/172], Loss: 11.1831\n",
      "Epoch [238/300], Step [140/172], Loss: 11.9858\n",
      "Epoch [238/300], Step [141/172], Loss: 10.3039\n",
      "Epoch [238/300], Step [142/172], Loss: 14.6398\n",
      "Epoch [238/300], Step [143/172], Loss: 12.1245\n",
      "Epoch [238/300], Step [144/172], Loss: 10.0330\n",
      "Epoch [238/300], Step [145/172], Loss: 11.4800\n",
      "Epoch [238/300], Step [146/172], Loss: 11.5707\n",
      "Epoch [238/300], Step [147/172], Loss: 6.5500\n",
      "Epoch [238/300], Step [148/172], Loss: 7.8660\n",
      "Epoch [238/300], Step [149/172], Loss: 8.2250\n",
      "Epoch [238/300], Step [150/172], Loss: 6.6869\n",
      "Epoch [238/300], Step [151/172], Loss: 6.5169\n",
      "Epoch [238/300], Step [152/172], Loss: 9.6988\n",
      "Epoch [238/300], Step [153/172], Loss: 7.0632\n",
      "Epoch [238/300], Step [154/172], Loss: 7.2791\n",
      "Epoch [238/300], Step [155/172], Loss: 7.5628\n",
      "Epoch [238/300], Step [156/172], Loss: 14.9898\n",
      "Epoch [238/300], Step [157/172], Loss: 8.9719\n",
      "Epoch [238/300], Step [158/172], Loss: 8.0720\n",
      "Epoch [238/300], Step [159/172], Loss: 9.9168\n",
      "Epoch [238/300], Step [160/172], Loss: 9.7875\n",
      "Epoch [238/300], Step [161/172], Loss: 9.7574\n",
      "Epoch [238/300], Step [162/172], Loss: 5.8041\n",
      "Epoch [238/300], Step [163/172], Loss: 6.9944\n",
      "Epoch [238/300], Step [164/172], Loss: 10.0496\n",
      "Epoch [238/300], Step [165/172], Loss: 7.8559\n",
      "Epoch [238/300], Step [166/172], Loss: 6.7231\n",
      "Epoch [238/300], Step [167/172], Loss: 10.7926\n",
      "Epoch [238/300], Step [168/172], Loss: 7.4860\n",
      "Epoch [238/300], Step [169/172], Loss: 7.7417\n",
      "Epoch [238/300], Step [170/172], Loss: 6.6134\n",
      "Epoch [238/300], Step [171/172], Loss: 8.6198\n",
      "Epoch [238/300], Step [172/172], Loss: 6.4313\n",
      "Epoch [239/300], Step [1/172], Loss: 44.0849\n",
      "Epoch [239/300], Step [2/172], Loss: 47.8125\n",
      "Epoch [239/300], Step [3/172], Loss: 45.5349\n",
      "Epoch [239/300], Step [4/172], Loss: 21.2780\n",
      "Epoch [239/300], Step [5/172], Loss: 40.1930\n",
      "Epoch [239/300], Step [6/172], Loss: 20.1111\n",
      "Epoch [239/300], Step [7/172], Loss: 26.7749\n",
      "Epoch [239/300], Step [8/172], Loss: 4.1106\n",
      "Epoch [239/300], Step [9/172], Loss: 26.5166\n",
      "Epoch [239/300], Step [10/172], Loss: 38.9188\n",
      "Epoch [239/300], Step [11/172], Loss: 50.7476\n",
      "Epoch [239/300], Step [12/172], Loss: 51.9252\n",
      "Epoch [239/300], Step [13/172], Loss: 32.2140\n",
      "Epoch [239/300], Step [14/172], Loss: 55.9762\n",
      "Epoch [239/300], Step [15/172], Loss: 46.6432\n",
      "Epoch [239/300], Step [16/172], Loss: 10.5075\n",
      "Epoch [239/300], Step [17/172], Loss: 38.3071\n",
      "Epoch [239/300], Step [18/172], Loss: 50.5072\n",
      "Epoch [239/300], Step [19/172], Loss: 69.1627\n",
      "Epoch [239/300], Step [20/172], Loss: 27.3782\n",
      "Epoch [239/300], Step [21/172], Loss: 77.8448\n",
      "Epoch [239/300], Step [22/172], Loss: 50.9245\n",
      "Epoch [239/300], Step [23/172], Loss: 2.0640\n",
      "Epoch [239/300], Step [24/172], Loss: 49.6275\n",
      "Epoch [239/300], Step [25/172], Loss: 36.0817\n",
      "Epoch [239/300], Step [26/172], Loss: 43.7474\n",
      "Epoch [239/300], Step [27/172], Loss: 53.5863\n",
      "Epoch [239/300], Step [28/172], Loss: 20.6106\n",
      "Epoch [239/300], Step [29/172], Loss: 13.8083\n",
      "Epoch [239/300], Step [30/172], Loss: 54.6166\n",
      "Epoch [239/300], Step [31/172], Loss: 34.6634\n",
      "Epoch [239/300], Step [32/172], Loss: 42.7816\n",
      "Epoch [239/300], Step [33/172], Loss: 67.2452\n",
      "Epoch [239/300], Step [34/172], Loss: 2.2506\n",
      "Epoch [239/300], Step [35/172], Loss: 14.6568\n",
      "Epoch [239/300], Step [36/172], Loss: 16.5540\n",
      "Epoch [239/300], Step [37/172], Loss: 16.8138\n",
      "Epoch [239/300], Step [38/172], Loss: 32.7466\n",
      "Epoch [239/300], Step [39/172], Loss: 36.8224\n",
      "Epoch [239/300], Step [40/172], Loss: 24.6116\n",
      "Epoch [239/300], Step [41/172], Loss: 37.0710\n",
      "Epoch [239/300], Step [42/172], Loss: 43.1166\n",
      "Epoch [239/300], Step [43/172], Loss: 30.7396\n",
      "Epoch [239/300], Step [44/172], Loss: 24.5573\n",
      "Epoch [239/300], Step [45/172], Loss: 36.7657\n",
      "Epoch [239/300], Step [46/172], Loss: 19.2490\n",
      "Epoch [239/300], Step [47/172], Loss: 54.1961\n",
      "Epoch [239/300], Step [48/172], Loss: 62.8341\n",
      "Epoch [239/300], Step [49/172], Loss: 27.6578\n",
      "Epoch [239/300], Step [50/172], Loss: 48.1254\n",
      "Epoch [239/300], Step [51/172], Loss: 11.5726\n",
      "Epoch [239/300], Step [52/172], Loss: 27.1582\n",
      "Epoch [239/300], Step [53/172], Loss: 28.3111\n",
      "Epoch [239/300], Step [54/172], Loss: 22.6164\n",
      "Epoch [239/300], Step [55/172], Loss: 22.2518\n",
      "Epoch [239/300], Step [56/172], Loss: 19.7321\n",
      "Epoch [239/300], Step [57/172], Loss: 18.8258\n",
      "Epoch [239/300], Step [58/172], Loss: 18.1550\n",
      "Epoch [239/300], Step [59/172], Loss: 32.0754\n",
      "Epoch [239/300], Step [60/172], Loss: 20.0130\n",
      "Epoch [239/300], Step [61/172], Loss: 9.2502\n",
      "Epoch [239/300], Step [62/172], Loss: 17.1786\n",
      "Epoch [239/300], Step [63/172], Loss: 14.2015\n",
      "Epoch [239/300], Step [64/172], Loss: 15.7398\n",
      "Epoch [239/300], Step [65/172], Loss: 22.3045\n",
      "Epoch [239/300], Step [66/172], Loss: 8.8701\n",
      "Epoch [239/300], Step [67/172], Loss: 26.6250\n",
      "Epoch [239/300], Step [68/172], Loss: 7.3265\n",
      "Epoch [239/300], Step [69/172], Loss: 28.7819\n",
      "Epoch [239/300], Step [70/172], Loss: 31.6978\n",
      "Epoch [239/300], Step [71/172], Loss: 32.7715\n",
      "Epoch [239/300], Step [72/172], Loss: 29.1796\n",
      "Epoch [239/300], Step [73/172], Loss: 37.2168\n",
      "Epoch [239/300], Step [74/172], Loss: 20.5394\n",
      "Epoch [239/300], Step [75/172], Loss: 19.2166\n",
      "Epoch [239/300], Step [76/172], Loss: 26.9439\n",
      "Epoch [239/300], Step [77/172], Loss: 40.6032\n",
      "Epoch [239/300], Step [78/172], Loss: 30.7660\n",
      "Epoch [239/300], Step [79/172], Loss: 29.5712\n",
      "Epoch [239/300], Step [80/172], Loss: 46.1943\n",
      "Epoch [239/300], Step [81/172], Loss: 26.6332\n",
      "Epoch [239/300], Step [82/172], Loss: 36.1442\n",
      "Epoch [239/300], Step [83/172], Loss: 39.9103\n",
      "Epoch [239/300], Step [84/172], Loss: 31.8868\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [239/300], Step [85/172], Loss: 36.0669\n",
      "Epoch [239/300], Step [86/172], Loss: 33.5876\n",
      "Epoch [239/300], Step [87/172], Loss: 23.4364\n",
      "Epoch [239/300], Step [88/172], Loss: 21.9343\n",
      "Epoch [239/300], Step [89/172], Loss: 29.0316\n",
      "Epoch [239/300], Step [90/172], Loss: 21.4682\n",
      "Epoch [239/300], Step [91/172], Loss: 26.5033\n",
      "Epoch [239/300], Step [92/172], Loss: 19.2546\n",
      "Epoch [239/300], Step [93/172], Loss: 18.7027\n",
      "Epoch [239/300], Step [94/172], Loss: 25.4077\n",
      "Epoch [239/300], Step [95/172], Loss: 22.0578\n",
      "Epoch [239/300], Step [96/172], Loss: 20.7084\n",
      "Epoch [239/300], Step [97/172], Loss: 30.2831\n",
      "Epoch [239/300], Step [98/172], Loss: 20.3690\n",
      "Epoch [239/300], Step [99/172], Loss: 20.5960\n",
      "Epoch [239/300], Step [100/172], Loss: 19.6311\n",
      "Epoch [239/300], Step [101/172], Loss: 21.0598\n",
      "Epoch [239/300], Step [102/172], Loss: 19.3641\n",
      "Epoch [239/300], Step [103/172], Loss: 14.3024\n",
      "Epoch [239/300], Step [104/172], Loss: 21.2194\n",
      "Epoch [239/300], Step [105/172], Loss: 25.7770\n",
      "Epoch [239/300], Step [106/172], Loss: 17.4962\n",
      "Epoch [239/300], Step [107/172], Loss: 17.6540\n",
      "Epoch [239/300], Step [108/172], Loss: 18.2024\n",
      "Epoch [239/300], Step [109/172], Loss: 16.5393\n",
      "Epoch [239/300], Step [110/172], Loss: 19.9589\n",
      "Epoch [239/300], Step [111/172], Loss: 19.5546\n",
      "Epoch [239/300], Step [112/172], Loss: 17.3749\n",
      "Epoch [239/300], Step [113/172], Loss: 15.9991\n",
      "Epoch [239/300], Step [114/172], Loss: 16.0089\n",
      "Epoch [239/300], Step [115/172], Loss: 19.4826\n",
      "Epoch [239/300], Step [116/172], Loss: 15.5195\n",
      "Epoch [239/300], Step [117/172], Loss: 13.5189\n",
      "Epoch [239/300], Step [118/172], Loss: 13.2198\n",
      "Epoch [239/300], Step [119/172], Loss: 19.6082\n",
      "Epoch [239/300], Step [120/172], Loss: 11.8110\n",
      "Epoch [239/300], Step [121/172], Loss: 10.5408\n",
      "Epoch [239/300], Step [122/172], Loss: 14.5670\n",
      "Epoch [239/300], Step [123/172], Loss: 13.5355\n",
      "Epoch [239/300], Step [124/172], Loss: 8.0876\n",
      "Epoch [239/300], Step [125/172], Loss: 13.6394\n",
      "Epoch [239/300], Step [126/172], Loss: 13.9211\n",
      "Epoch [239/300], Step [127/172], Loss: 11.8782\n",
      "Epoch [239/300], Step [128/172], Loss: 11.2731\n",
      "Epoch [239/300], Step [129/172], Loss: 9.7390\n",
      "Epoch [239/300], Step [130/172], Loss: 13.7143\n",
      "Epoch [239/300], Step [131/172], Loss: 9.1759\n",
      "Epoch [239/300], Step [132/172], Loss: 11.2597\n",
      "Epoch [239/300], Step [133/172], Loss: 11.1932\n",
      "Epoch [239/300], Step [134/172], Loss: 11.3052\n",
      "Epoch [239/300], Step [135/172], Loss: 10.2692\n",
      "Epoch [239/300], Step [136/172], Loss: 9.7096\n",
      "Epoch [239/300], Step [137/172], Loss: 9.6947\n",
      "Epoch [239/300], Step [138/172], Loss: 10.1819\n",
      "Epoch [239/300], Step [139/172], Loss: 11.3103\n",
      "Epoch [239/300], Step [140/172], Loss: 12.0631\n",
      "Epoch [239/300], Step [141/172], Loss: 10.3648\n",
      "Epoch [239/300], Step [142/172], Loss: 15.0815\n",
      "Epoch [239/300], Step [143/172], Loss: 12.2511\n",
      "Epoch [239/300], Step [144/172], Loss: 10.1794\n",
      "Epoch [239/300], Step [145/172], Loss: 11.6208\n",
      "Epoch [239/300], Step [146/172], Loss: 11.9041\n",
      "Epoch [239/300], Step [147/172], Loss: 6.5475\n",
      "Epoch [239/300], Step [148/172], Loss: 7.8501\n",
      "Epoch [239/300], Step [149/172], Loss: 8.2621\n",
      "Epoch [239/300], Step [150/172], Loss: 6.7497\n",
      "Epoch [239/300], Step [151/172], Loss: 6.6439\n",
      "Epoch [239/300], Step [152/172], Loss: 9.8559\n",
      "Epoch [239/300], Step [153/172], Loss: 6.9943\n",
      "Epoch [239/300], Step [154/172], Loss: 7.2876\n",
      "Epoch [239/300], Step [155/172], Loss: 7.4371\n",
      "Epoch [239/300], Step [156/172], Loss: 15.0067\n",
      "Epoch [239/300], Step [157/172], Loss: 9.0609\n",
      "Epoch [239/300], Step [158/172], Loss: 8.1038\n",
      "Epoch [239/300], Step [159/172], Loss: 10.2159\n",
      "Epoch [239/300], Step [160/172], Loss: 9.9488\n",
      "Epoch [239/300], Step [161/172], Loss: 9.7631\n",
      "Epoch [239/300], Step [162/172], Loss: 5.8322\n",
      "Epoch [239/300], Step [163/172], Loss: 7.2412\n",
      "Epoch [239/300], Step [164/172], Loss: 9.9162\n",
      "Epoch [239/300], Step [165/172], Loss: 7.8981\n",
      "Epoch [239/300], Step [166/172], Loss: 6.8476\n",
      "Epoch [239/300], Step [167/172], Loss: 11.1166\n",
      "Epoch [239/300], Step [168/172], Loss: 7.5036\n",
      "Epoch [239/300], Step [169/172], Loss: 7.7254\n",
      "Epoch [239/300], Step [170/172], Loss: 6.6560\n",
      "Epoch [239/300], Step [171/172], Loss: 9.3173\n",
      "Epoch [239/300], Step [172/172], Loss: 6.3569\n",
      "Epoch [240/300], Step [1/172], Loss: 43.7024\n",
      "Epoch [240/300], Step [2/172], Loss: 47.3766\n",
      "Epoch [240/300], Step [3/172], Loss: 44.2650\n",
      "Epoch [240/300], Step [4/172], Loss: 20.9957\n",
      "Epoch [240/300], Step [5/172], Loss: 40.0770\n",
      "Epoch [240/300], Step [6/172], Loss: 20.5569\n",
      "Epoch [240/300], Step [7/172], Loss: 27.7124\n",
      "Epoch [240/300], Step [8/172], Loss: 4.6879\n",
      "Epoch [240/300], Step [9/172], Loss: 26.2929\n",
      "Epoch [240/300], Step [10/172], Loss: 38.2339\n",
      "Epoch [240/300], Step [11/172], Loss: 50.2853\n",
      "Epoch [240/300], Step [12/172], Loss: 51.7335\n",
      "Epoch [240/300], Step [13/172], Loss: 32.1017\n",
      "Epoch [240/300], Step [14/172], Loss: 55.6951\n",
      "Epoch [240/300], Step [15/172], Loss: 46.0141\n",
      "Epoch [240/300], Step [16/172], Loss: 8.9726\n",
      "Epoch [240/300], Step [17/172], Loss: 38.3938\n",
      "Epoch [240/300], Step [18/172], Loss: 50.0385\n",
      "Epoch [240/300], Step [19/172], Loss: 68.9869\n",
      "Epoch [240/300], Step [20/172], Loss: 26.6680\n",
      "Epoch [240/300], Step [21/172], Loss: 76.8993\n",
      "Epoch [240/300], Step [22/172], Loss: 50.2971\n",
      "Epoch [240/300], Step [23/172], Loss: 1.8921\n",
      "Epoch [240/300], Step [24/172], Loss: 50.0082\n",
      "Epoch [240/300], Step [25/172], Loss: 35.6524\n",
      "Epoch [240/300], Step [26/172], Loss: 43.7602\n",
      "Epoch [240/300], Step [27/172], Loss: 53.1549\n",
      "Epoch [240/300], Step [28/172], Loss: 20.5161\n",
      "Epoch [240/300], Step [29/172], Loss: 14.0897\n",
      "Epoch [240/300], Step [30/172], Loss: 53.5632\n",
      "Epoch [240/300], Step [31/172], Loss: 34.0657\n",
      "Epoch [240/300], Step [32/172], Loss: 42.8464\n",
      "Epoch [240/300], Step [33/172], Loss: 67.5338\n",
      "Epoch [240/300], Step [34/172], Loss: 2.2283\n",
      "Epoch [240/300], Step [35/172], Loss: 14.7242\n",
      "Epoch [240/300], Step [36/172], Loss: 16.8319\n",
      "Epoch [240/300], Step [37/172], Loss: 16.9291\n",
      "Epoch [240/300], Step [38/172], Loss: 32.9634\n",
      "Epoch [240/300], Step [39/172], Loss: 37.0723\n",
      "Epoch [240/300], Step [40/172], Loss: 24.8363\n",
      "Epoch [240/300], Step [41/172], Loss: 37.0411\n",
      "Epoch [240/300], Step [42/172], Loss: 43.2224\n",
      "Epoch [240/300], Step [43/172], Loss: 30.9334\n",
      "Epoch [240/300], Step [44/172], Loss: 24.2574\n",
      "Epoch [240/300], Step [45/172], Loss: 37.1048\n",
      "Epoch [240/300], Step [46/172], Loss: 18.6552\n",
      "Epoch [240/300], Step [47/172], Loss: 53.9876\n",
      "Epoch [240/300], Step [48/172], Loss: 61.5524\n",
      "Epoch [240/300], Step [49/172], Loss: 27.7803\n",
      "Epoch [240/300], Step [50/172], Loss: 48.6438\n",
      "Epoch [240/300], Step [51/172], Loss: 11.4525\n",
      "Epoch [240/300], Step [52/172], Loss: 26.9949\n",
      "Epoch [240/300], Step [53/172], Loss: 28.3870\n",
      "Epoch [240/300], Step [54/172], Loss: 22.5791\n",
      "Epoch [240/300], Step [55/172], Loss: 21.8018\n",
      "Epoch [240/300], Step [56/172], Loss: 19.2741\n",
      "Epoch [240/300], Step [57/172], Loss: 19.5310\n",
      "Epoch [240/300], Step [58/172], Loss: 18.1187\n",
      "Epoch [240/300], Step [59/172], Loss: 32.3664\n",
      "Epoch [240/300], Step [60/172], Loss: 20.4522\n",
      "Epoch [240/300], Step [61/172], Loss: 9.1379\n",
      "Epoch [240/300], Step [62/172], Loss: 17.2701\n",
      "Epoch [240/300], Step [63/172], Loss: 13.8794\n",
      "Epoch [240/300], Step [64/172], Loss: 15.4010\n",
      "Epoch [240/300], Step [65/172], Loss: 22.3164\n",
      "Epoch [240/300], Step [66/172], Loss: 8.9115\n",
      "Epoch [240/300], Step [67/172], Loss: 26.6608\n",
      "Epoch [240/300], Step [68/172], Loss: 6.7463\n",
      "Epoch [240/300], Step [69/172], Loss: 28.6452\n",
      "Epoch [240/300], Step [70/172], Loss: 31.5551\n",
      "Epoch [240/300], Step [71/172], Loss: 32.5259\n",
      "Epoch [240/300], Step [72/172], Loss: 29.1008\n",
      "Epoch [240/300], Step [73/172], Loss: 37.1872\n",
      "Epoch [240/300], Step [74/172], Loss: 20.2397\n",
      "Epoch [240/300], Step [75/172], Loss: 19.5140\n",
      "Epoch [240/300], Step [76/172], Loss: 26.8996\n",
      "Epoch [240/300], Step [77/172], Loss: 40.7777\n",
      "Epoch [240/300], Step [78/172], Loss: 30.6796\n",
      "Epoch [240/300], Step [79/172], Loss: 29.4282\n",
      "Epoch [240/300], Step [80/172], Loss: 46.2107\n",
      "Epoch [240/300], Step [81/172], Loss: 26.6436\n",
      "Epoch [240/300], Step [82/172], Loss: 36.3016\n",
      "Epoch [240/300], Step [83/172], Loss: 39.9479\n",
      "Epoch [240/300], Step [84/172], Loss: 31.7774\n",
      "Epoch [240/300], Step [85/172], Loss: 35.6151\n",
      "Epoch [240/300], Step [86/172], Loss: 33.3660\n",
      "Epoch [240/300], Step [87/172], Loss: 23.4405\n",
      "Epoch [240/300], Step [88/172], Loss: 21.6672\n",
      "Epoch [240/300], Step [89/172], Loss: 28.9812\n",
      "Epoch [240/300], Step [90/172], Loss: 21.2633\n",
      "Epoch [240/300], Step [91/172], Loss: 26.4832\n",
      "Epoch [240/300], Step [92/172], Loss: 19.2347\n",
      "Epoch [240/300], Step [93/172], Loss: 18.5521\n",
      "Epoch [240/300], Step [94/172], Loss: 25.3764\n",
      "Epoch [240/300], Step [95/172], Loss: 22.0494\n",
      "Epoch [240/300], Step [96/172], Loss: 20.6257\n",
      "Epoch [240/300], Step [97/172], Loss: 30.4745\n",
      "Epoch [240/300], Step [98/172], Loss: 20.3965\n",
      "Epoch [240/300], Step [99/172], Loss: 20.5467\n",
      "Epoch [240/300], Step [100/172], Loss: 19.5841\n",
      "Epoch [240/300], Step [101/172], Loss: 21.0357\n",
      "Epoch [240/300], Step [102/172], Loss: 19.5851\n",
      "Epoch [240/300], Step [103/172], Loss: 14.2502\n",
      "Epoch [240/300], Step [104/172], Loss: 21.2493\n",
      "Epoch [240/300], Step [105/172], Loss: 25.7945\n",
      "Epoch [240/300], Step [106/172], Loss: 17.3803\n",
      "Epoch [240/300], Step [107/172], Loss: 17.7058\n",
      "Epoch [240/300], Step [108/172], Loss: 18.1040\n",
      "Epoch [240/300], Step [109/172], Loss: 16.5745\n",
      "Epoch [240/300], Step [110/172], Loss: 19.9436\n",
      "Epoch [240/300], Step [111/172], Loss: 19.4639\n",
      "Epoch [240/300], Step [112/172], Loss: 17.3534\n",
      "Epoch [240/300], Step [113/172], Loss: 15.8109\n",
      "Epoch [240/300], Step [114/172], Loss: 16.0517\n",
      "Epoch [240/300], Step [115/172], Loss: 19.4513\n",
      "Epoch [240/300], Step [116/172], Loss: 15.4566\n",
      "Epoch [240/300], Step [117/172], Loss: 13.6716\n",
      "Epoch [240/300], Step [118/172], Loss: 13.3621\n",
      "Epoch [240/300], Step [119/172], Loss: 19.5274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/300], Step [120/172], Loss: 11.8919\n",
      "Epoch [240/300], Step [121/172], Loss: 10.4691\n",
      "Epoch [240/300], Step [122/172], Loss: 14.9110\n",
      "Epoch [240/300], Step [123/172], Loss: 13.4021\n",
      "Epoch [240/300], Step [124/172], Loss: 8.1221\n",
      "Epoch [240/300], Step [125/172], Loss: 13.6192\n",
      "Epoch [240/300], Step [126/172], Loss: 13.6466\n",
      "Epoch [240/300], Step [127/172], Loss: 11.9092\n",
      "Epoch [240/300], Step [128/172], Loss: 11.1300\n",
      "Epoch [240/300], Step [129/172], Loss: 9.7211\n",
      "Epoch [240/300], Step [130/172], Loss: 13.6362\n",
      "Epoch [240/300], Step [131/172], Loss: 9.2483\n",
      "Epoch [240/300], Step [132/172], Loss: 11.1719\n",
      "Epoch [240/300], Step [133/172], Loss: 11.4672\n",
      "Epoch [240/300], Step [134/172], Loss: 11.4069\n",
      "Epoch [240/300], Step [135/172], Loss: 10.2319\n",
      "Epoch [240/300], Step [136/172], Loss: 9.6481\n",
      "Epoch [240/300], Step [137/172], Loss: 9.7769\n",
      "Epoch [240/300], Step [138/172], Loss: 10.0742\n",
      "Epoch [240/300], Step [139/172], Loss: 11.2624\n",
      "Epoch [240/300], Step [140/172], Loss: 11.9672\n",
      "Epoch [240/300], Step [141/172], Loss: 10.1675\n",
      "Epoch [240/300], Step [142/172], Loss: 15.2202\n",
      "Epoch [240/300], Step [143/172], Loss: 12.2906\n",
      "Epoch [240/300], Step [144/172], Loss: 10.1061\n",
      "Epoch [240/300], Step [145/172], Loss: 11.6469\n",
      "Epoch [240/300], Step [146/172], Loss: 11.6956\n",
      "Epoch [240/300], Step [147/172], Loss: 6.4740\n",
      "Epoch [240/300], Step [148/172], Loss: 7.7793\n",
      "Epoch [240/300], Step [149/172], Loss: 8.1390\n",
      "Epoch [240/300], Step [150/172], Loss: 6.6466\n",
      "Epoch [240/300], Step [151/172], Loss: 6.5085\n",
      "Epoch [240/300], Step [152/172], Loss: 9.6687\n",
      "Epoch [240/300], Step [153/172], Loss: 7.0365\n",
      "Epoch [240/300], Step [154/172], Loss: 7.1811\n",
      "Epoch [240/300], Step [155/172], Loss: 7.4590\n",
      "Epoch [240/300], Step [156/172], Loss: 15.1988\n",
      "Epoch [240/300], Step [157/172], Loss: 9.1959\n",
      "Epoch [240/300], Step [158/172], Loss: 8.1948\n",
      "Epoch [240/300], Step [159/172], Loss: 10.1697\n",
      "Epoch [240/300], Step [160/172], Loss: 10.1202\n",
      "Epoch [240/300], Step [161/172], Loss: 9.6763\n",
      "Epoch [240/300], Step [162/172], Loss: 5.7379\n",
      "Epoch [240/300], Step [163/172], Loss: 6.9366\n",
      "Epoch [240/300], Step [164/172], Loss: 10.0423\n",
      "Epoch [240/300], Step [165/172], Loss: 7.8140\n",
      "Epoch [240/300], Step [166/172], Loss: 6.6565\n",
      "Epoch [240/300], Step [167/172], Loss: 11.0781\n",
      "Epoch [240/300], Step [168/172], Loss: 7.3986\n",
      "Epoch [240/300], Step [169/172], Loss: 7.8203\n",
      "Epoch [240/300], Step [170/172], Loss: 6.6272\n",
      "Epoch [240/300], Step [171/172], Loss: 8.7868\n",
      "Epoch [240/300], Step [172/172], Loss: 6.4768\n",
      "Epoch [241/300], Step [1/172], Loss: 43.9336\n",
      "Epoch [241/300], Step [2/172], Loss: 46.3859\n",
      "Epoch [241/300], Step [3/172], Loss: 45.8502\n",
      "Epoch [241/300], Step [4/172], Loss: 20.9839\n",
      "Epoch [241/300], Step [5/172], Loss: 40.3452\n",
      "Epoch [241/300], Step [6/172], Loss: 19.1634\n",
      "Epoch [241/300], Step [7/172], Loss: 23.8333\n",
      "Epoch [241/300], Step [8/172], Loss: 4.0678\n",
      "Epoch [241/300], Step [9/172], Loss: 26.4323\n",
      "Epoch [241/300], Step [10/172], Loss: 38.8837\n",
      "Epoch [241/300], Step [11/172], Loss: 50.2668\n",
      "Epoch [241/300], Step [12/172], Loss: 51.4633\n",
      "Epoch [241/300], Step [13/172], Loss: 31.6663\n",
      "Epoch [241/300], Step [14/172], Loss: 55.3735\n",
      "Epoch [241/300], Step [15/172], Loss: 46.6249\n",
      "Epoch [241/300], Step [16/172], Loss: 10.9234\n",
      "Epoch [241/300], Step [17/172], Loss: 37.7053\n",
      "Epoch [241/300], Step [18/172], Loss: 50.3890\n",
      "Epoch [241/300], Step [19/172], Loss: 68.9586\n",
      "Epoch [241/300], Step [20/172], Loss: 27.4843\n",
      "Epoch [241/300], Step [21/172], Loss: 77.5540\n",
      "Epoch [241/300], Step [22/172], Loss: 51.4116\n",
      "Epoch [241/300], Step [23/172], Loss: 2.2224\n",
      "Epoch [241/300], Step [24/172], Loss: 49.1035\n",
      "Epoch [241/300], Step [25/172], Loss: 36.3565\n",
      "Epoch [241/300], Step [26/172], Loss: 43.5973\n",
      "Epoch [241/300], Step [27/172], Loss: 53.1575\n",
      "Epoch [241/300], Step [28/172], Loss: 20.0173\n",
      "Epoch [241/300], Step [29/172], Loss: 13.8809\n",
      "Epoch [241/300], Step [30/172], Loss: 54.2399\n",
      "Epoch [241/300], Step [31/172], Loss: 34.5095\n",
      "Epoch [241/300], Step [32/172], Loss: 42.4469\n",
      "Epoch [241/300], Step [33/172], Loss: 66.3063\n",
      "Epoch [241/300], Step [34/172], Loss: 2.1541\n",
      "Epoch [241/300], Step [35/172], Loss: 14.6046\n",
      "Epoch [241/300], Step [36/172], Loss: 16.3236\n",
      "Epoch [241/300], Step [37/172], Loss: 16.7292\n",
      "Epoch [241/300], Step [38/172], Loss: 32.9633\n",
      "Epoch [241/300], Step [39/172], Loss: 36.6692\n",
      "Epoch [241/300], Step [40/172], Loss: 24.4473\n",
      "Epoch [241/300], Step [41/172], Loss: 36.8733\n",
      "Epoch [241/300], Step [42/172], Loss: 42.6217\n",
      "Epoch [241/300], Step [43/172], Loss: 30.6198\n",
      "Epoch [241/300], Step [44/172], Loss: 24.3896\n",
      "Epoch [241/300], Step [45/172], Loss: 36.6864\n",
      "Epoch [241/300], Step [46/172], Loss: 18.9119\n",
      "Epoch [241/300], Step [47/172], Loss: 54.2096\n",
      "Epoch [241/300], Step [48/172], Loss: 62.3052\n",
      "Epoch [241/300], Step [49/172], Loss: 27.6076\n",
      "Epoch [241/300], Step [50/172], Loss: 48.6289\n",
      "Epoch [241/300], Step [51/172], Loss: 11.5852\n",
      "Epoch [241/300], Step [52/172], Loss: 27.2938\n",
      "Epoch [241/300], Step [53/172], Loss: 28.6640\n",
      "Epoch [241/300], Step [54/172], Loss: 22.6054\n",
      "Epoch [241/300], Step [55/172], Loss: 22.1268\n",
      "Epoch [241/300], Step [56/172], Loss: 19.6639\n",
      "Epoch [241/300], Step [57/172], Loss: 19.2851\n",
      "Epoch [241/300], Step [58/172], Loss: 18.0138\n",
      "Epoch [241/300], Step [59/172], Loss: 32.4765\n",
      "Epoch [241/300], Step [60/172], Loss: 20.5836\n",
      "Epoch [241/300], Step [61/172], Loss: 9.1505\n",
      "Epoch [241/300], Step [62/172], Loss: 17.5020\n",
      "Epoch [241/300], Step [63/172], Loss: 14.1566\n",
      "Epoch [241/300], Step [64/172], Loss: 15.7692\n",
      "Epoch [241/300], Step [65/172], Loss: 22.2737\n",
      "Epoch [241/300], Step [66/172], Loss: 9.1629\n",
      "Epoch [241/300], Step [67/172], Loss: 27.4546\n",
      "Epoch [241/300], Step [68/172], Loss: 6.9864\n",
      "Epoch [241/300], Step [69/172], Loss: 28.9924\n",
      "Epoch [241/300], Step [70/172], Loss: 31.1934\n",
      "Epoch [241/300], Step [71/172], Loss: 32.5086\n",
      "Epoch [241/300], Step [72/172], Loss: 28.9615\n",
      "Epoch [241/300], Step [73/172], Loss: 36.8805\n",
      "Epoch [241/300], Step [74/172], Loss: 20.5166\n",
      "Epoch [241/300], Step [75/172], Loss: 19.4904\n",
      "Epoch [241/300], Step [76/172], Loss: 26.2845\n",
      "Epoch [241/300], Step [77/172], Loss: 40.6615\n",
      "Epoch [241/300], Step [78/172], Loss: 30.4928\n",
      "Epoch [241/300], Step [79/172], Loss: 29.1153\n",
      "Epoch [241/300], Step [80/172], Loss: 45.9779\n",
      "Epoch [241/300], Step [81/172], Loss: 26.3629\n",
      "Epoch [241/300], Step [82/172], Loss: 36.0914\n",
      "Epoch [241/300], Step [83/172], Loss: 39.5491\n",
      "Epoch [241/300], Step [84/172], Loss: 31.4875\n",
      "Epoch [241/300], Step [85/172], Loss: 35.7192\n",
      "Epoch [241/300], Step [86/172], Loss: 33.0239\n",
      "Epoch [241/300], Step [87/172], Loss: 23.1423\n",
      "Epoch [241/300], Step [88/172], Loss: 21.6805\n",
      "Epoch [241/300], Step [89/172], Loss: 28.8214\n",
      "Epoch [241/300], Step [90/172], Loss: 21.0054\n",
      "Epoch [241/300], Step [91/172], Loss: 26.3931\n",
      "Epoch [241/300], Step [92/172], Loss: 19.1393\n",
      "Epoch [241/300], Step [93/172], Loss: 18.5948\n",
      "Epoch [241/300], Step [94/172], Loss: 25.2563\n",
      "Epoch [241/300], Step [95/172], Loss: 22.1556\n",
      "Epoch [241/300], Step [96/172], Loss: 20.4967\n",
      "Epoch [241/300], Step [97/172], Loss: 30.3320\n",
      "Epoch [241/300], Step [98/172], Loss: 20.1892\n",
      "Epoch [241/300], Step [99/172], Loss: 20.5558\n",
      "Epoch [241/300], Step [100/172], Loss: 19.5682\n",
      "Epoch [241/300], Step [101/172], Loss: 20.9944\n",
      "Epoch [241/300], Step [102/172], Loss: 19.3648\n",
      "Epoch [241/300], Step [103/172], Loss: 14.1901\n",
      "Epoch [241/300], Step [104/172], Loss: 21.1675\n",
      "Epoch [241/300], Step [105/172], Loss: 25.6610\n",
      "Epoch [241/300], Step [106/172], Loss: 17.4748\n",
      "Epoch [241/300], Step [107/172], Loss: 17.7523\n",
      "Epoch [241/300], Step [108/172], Loss: 18.1507\n",
      "Epoch [241/300], Step [109/172], Loss: 16.5114\n",
      "Epoch [241/300], Step [110/172], Loss: 19.9821\n",
      "Epoch [241/300], Step [111/172], Loss: 19.4794\n",
      "Epoch [241/300], Step [112/172], Loss: 17.1874\n",
      "Epoch [241/300], Step [113/172], Loss: 15.9293\n",
      "Epoch [241/300], Step [114/172], Loss: 15.9852\n",
      "Epoch [241/300], Step [115/172], Loss: 19.6651\n",
      "Epoch [241/300], Step [116/172], Loss: 15.5708\n",
      "Epoch [241/300], Step [117/172], Loss: 13.6817\n",
      "Epoch [241/300], Step [118/172], Loss: 13.4606\n",
      "Epoch [241/300], Step [119/172], Loss: 19.6622\n",
      "Epoch [241/300], Step [120/172], Loss: 11.8710\n",
      "Epoch [241/300], Step [121/172], Loss: 10.4860\n",
      "Epoch [241/300], Step [122/172], Loss: 14.3451\n",
      "Epoch [241/300], Step [123/172], Loss: 13.5283\n",
      "Epoch [241/300], Step [124/172], Loss: 8.0897\n",
      "Epoch [241/300], Step [125/172], Loss: 13.6692\n",
      "Epoch [241/300], Step [126/172], Loss: 13.8522\n",
      "Epoch [241/300], Step [127/172], Loss: 11.7050\n",
      "Epoch [241/300], Step [128/172], Loss: 11.1717\n",
      "Epoch [241/300], Step [129/172], Loss: 9.6433\n",
      "Epoch [241/300], Step [130/172], Loss: 13.8062\n",
      "Epoch [241/300], Step [131/172], Loss: 9.0449\n",
      "Epoch [241/300], Step [132/172], Loss: 11.2339\n",
      "Epoch [241/300], Step [133/172], Loss: 11.0825\n",
      "Epoch [241/300], Step [134/172], Loss: 11.2621\n",
      "Epoch [241/300], Step [135/172], Loss: 10.2586\n",
      "Epoch [241/300], Step [136/172], Loss: 9.7275\n",
      "Epoch [241/300], Step [137/172], Loss: 9.7265\n",
      "Epoch [241/300], Step [138/172], Loss: 10.1526\n",
      "Epoch [241/300], Step [139/172], Loss: 11.4442\n",
      "Epoch [241/300], Step [140/172], Loss: 12.0793\n",
      "Epoch [241/300], Step [141/172], Loss: 10.2970\n",
      "Epoch [241/300], Step [142/172], Loss: 15.1635\n",
      "Epoch [241/300], Step [143/172], Loss: 12.4149\n",
      "Epoch [241/300], Step [144/172], Loss: 10.2101\n",
      "Epoch [241/300], Step [145/172], Loss: 11.7521\n",
      "Epoch [241/300], Step [146/172], Loss: 11.9602\n",
      "Epoch [241/300], Step [147/172], Loss: 6.5595\n",
      "Epoch [241/300], Step [148/172], Loss: 7.8468\n",
      "Epoch [241/300], Step [149/172], Loss: 8.2103\n",
      "Epoch [241/300], Step [150/172], Loss: 6.7908\n",
      "Epoch [241/300], Step [151/172], Loss: 6.6412\n",
      "Epoch [241/300], Step [152/172], Loss: 9.8106\n",
      "Epoch [241/300], Step [153/172], Loss: 7.0884\n",
      "Epoch [241/300], Step [154/172], Loss: 7.2395\n",
      "Epoch [241/300], Step [155/172], Loss: 7.4677\n",
      "Epoch [241/300], Step [156/172], Loss: 14.8433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [241/300], Step [157/172], Loss: 9.1135\n",
      "Epoch [241/300], Step [158/172], Loss: 8.0351\n",
      "Epoch [241/300], Step [159/172], Loss: 10.4135\n",
      "Epoch [241/300], Step [160/172], Loss: 9.9365\n",
      "Epoch [241/300], Step [161/172], Loss: 9.6129\n",
      "Epoch [241/300], Step [162/172], Loss: 5.8309\n",
      "Epoch [241/300], Step [163/172], Loss: 7.1971\n",
      "Epoch [241/300], Step [164/172], Loss: 9.8708\n",
      "Epoch [241/300], Step [165/172], Loss: 7.8877\n",
      "Epoch [241/300], Step [166/172], Loss: 6.8060\n",
      "Epoch [241/300], Step [167/172], Loss: 11.3656\n",
      "Epoch [241/300], Step [168/172], Loss: 7.5075\n",
      "Epoch [241/300], Step [169/172], Loss: 7.7834\n",
      "Epoch [241/300], Step [170/172], Loss: 6.6738\n",
      "Epoch [241/300], Step [171/172], Loss: 9.5279\n",
      "Epoch [241/300], Step [172/172], Loss: 6.4051\n",
      "Epoch [242/300], Step [1/172], Loss: 43.4711\n",
      "Epoch [242/300], Step [2/172], Loss: 46.7289\n",
      "Epoch [242/300], Step [3/172], Loss: 43.1999\n",
      "Epoch [242/300], Step [4/172], Loss: 20.7863\n",
      "Epoch [242/300], Step [5/172], Loss: 39.9339\n",
      "Epoch [242/300], Step [6/172], Loss: 20.6207\n",
      "Epoch [242/300], Step [7/172], Loss: 28.7303\n",
      "Epoch [242/300], Step [8/172], Loss: 4.8374\n",
      "Epoch [242/300], Step [9/172], Loss: 26.3517\n",
      "Epoch [242/300], Step [10/172], Loss: 37.6891\n",
      "Epoch [242/300], Step [11/172], Loss: 49.8967\n",
      "Epoch [242/300], Step [12/172], Loss: 51.7424\n",
      "Epoch [242/300], Step [13/172], Loss: 31.5550\n",
      "Epoch [242/300], Step [14/172], Loss: 55.7242\n",
      "Epoch [242/300], Step [15/172], Loss: 46.1119\n",
      "Epoch [242/300], Step [16/172], Loss: 8.4727\n",
      "Epoch [242/300], Step [17/172], Loss: 38.4184\n",
      "Epoch [242/300], Step [18/172], Loss: 50.2002\n",
      "Epoch [242/300], Step [19/172], Loss: 68.8334\n",
      "Epoch [242/300], Step [20/172], Loss: 26.7963\n",
      "Epoch [242/300], Step [21/172], Loss: 76.6421\n",
      "Epoch [242/300], Step [22/172], Loss: 50.2351\n",
      "Epoch [242/300], Step [23/172], Loss: 1.7003\n",
      "Epoch [242/300], Step [24/172], Loss: 49.6568\n",
      "Epoch [242/300], Step [25/172], Loss: 35.2163\n",
      "Epoch [242/300], Step [26/172], Loss: 43.8350\n",
      "Epoch [242/300], Step [27/172], Loss: 53.3082\n",
      "Epoch [242/300], Step [28/172], Loss: 20.1472\n",
      "Epoch [242/300], Step [29/172], Loss: 14.3207\n",
      "Epoch [242/300], Step [30/172], Loss: 53.1987\n",
      "Epoch [242/300], Step [31/172], Loss: 33.9609\n",
      "Epoch [242/300], Step [32/172], Loss: 42.7748\n",
      "Epoch [242/300], Step [33/172], Loss: 67.2629\n",
      "Epoch [242/300], Step [34/172], Loss: 2.4121\n",
      "Epoch [242/300], Step [35/172], Loss: 14.7839\n",
      "Epoch [242/300], Step [36/172], Loss: 16.9608\n",
      "Epoch [242/300], Step [37/172], Loss: 16.8538\n",
      "Epoch [242/300], Step [38/172], Loss: 32.9694\n",
      "Epoch [242/300], Step [39/172], Loss: 36.8514\n",
      "Epoch [242/300], Step [40/172], Loss: 24.6446\n",
      "Epoch [242/300], Step [41/172], Loss: 36.4643\n",
      "Epoch [242/300], Step [42/172], Loss: 42.9348\n",
      "Epoch [242/300], Step [43/172], Loss: 30.8832\n",
      "Epoch [242/300], Step [44/172], Loss: 24.3902\n",
      "Epoch [242/300], Step [45/172], Loss: 37.5063\n",
      "Epoch [242/300], Step [46/172], Loss: 18.5273\n",
      "Epoch [242/300], Step [47/172], Loss: 53.8938\n",
      "Epoch [242/300], Step [48/172], Loss: 61.5597\n",
      "Epoch [242/300], Step [49/172], Loss: 28.0482\n",
      "Epoch [242/300], Step [50/172], Loss: 49.1821\n",
      "Epoch [242/300], Step [51/172], Loss: 11.3743\n",
      "Epoch [242/300], Step [52/172], Loss: 27.3449\n",
      "Epoch [242/300], Step [53/172], Loss: 28.5927\n",
      "Epoch [242/300], Step [54/172], Loss: 22.4796\n",
      "Epoch [242/300], Step [55/172], Loss: 21.9583\n",
      "Epoch [242/300], Step [56/172], Loss: 19.4369\n",
      "Epoch [242/300], Step [57/172], Loss: 20.0847\n",
      "Epoch [242/300], Step [58/172], Loss: 18.0391\n",
      "Epoch [242/300], Step [59/172], Loss: 32.1343\n",
      "Epoch [242/300], Step [60/172], Loss: 20.9451\n",
      "Epoch [242/300], Step [61/172], Loss: 9.2190\n",
      "Epoch [242/300], Step [62/172], Loss: 17.2492\n",
      "Epoch [242/300], Step [63/172], Loss: 13.8060\n",
      "Epoch [242/300], Step [64/172], Loss: 15.3321\n",
      "Epoch [242/300], Step [65/172], Loss: 22.4297\n",
      "Epoch [242/300], Step [66/172], Loss: 9.1198\n",
      "Epoch [242/300], Step [67/172], Loss: 26.9903\n",
      "Epoch [242/300], Step [68/172], Loss: 7.0759\n",
      "Epoch [242/300], Step [69/172], Loss: 28.8324\n",
      "Epoch [242/300], Step [70/172], Loss: 30.8442\n",
      "Epoch [242/300], Step [71/172], Loss: 32.3823\n",
      "Epoch [242/300], Step [72/172], Loss: 28.8920\n",
      "Epoch [242/300], Step [73/172], Loss: 36.9879\n",
      "Epoch [242/300], Step [74/172], Loss: 20.1247\n",
      "Epoch [242/300], Step [75/172], Loss: 19.4355\n",
      "Epoch [242/300], Step [76/172], Loss: 26.2656\n",
      "Epoch [242/300], Step [77/172], Loss: 40.8576\n",
      "Epoch [242/300], Step [78/172], Loss: 30.4748\n",
      "Epoch [242/300], Step [79/172], Loss: 29.1941\n",
      "Epoch [242/300], Step [80/172], Loss: 46.0423\n",
      "Epoch [242/300], Step [81/172], Loss: 26.2991\n",
      "Epoch [242/300], Step [82/172], Loss: 36.3650\n",
      "Epoch [242/300], Step [83/172], Loss: 39.7041\n",
      "Epoch [242/300], Step [84/172], Loss: 31.6501\n",
      "Epoch [242/300], Step [85/172], Loss: 35.1307\n",
      "Epoch [242/300], Step [86/172], Loss: 32.6556\n",
      "Epoch [242/300], Step [87/172], Loss: 23.1678\n",
      "Epoch [242/300], Step [88/172], Loss: 21.3920\n",
      "Epoch [242/300], Step [89/172], Loss: 28.5803\n",
      "Epoch [242/300], Step [90/172], Loss: 20.7575\n",
      "Epoch [242/300], Step [91/172], Loss: 26.4910\n",
      "Epoch [242/300], Step [92/172], Loss: 19.1622\n",
      "Epoch [242/300], Step [93/172], Loss: 18.4180\n",
      "Epoch [242/300], Step [94/172], Loss: 25.1545\n",
      "Epoch [242/300], Step [95/172], Loss: 21.8478\n",
      "Epoch [242/300], Step [96/172], Loss: 20.6457\n",
      "Epoch [242/300], Step [97/172], Loss: 30.5945\n",
      "Epoch [242/300], Step [98/172], Loss: 20.2907\n",
      "Epoch [242/300], Step [99/172], Loss: 20.4961\n",
      "Epoch [242/300], Step [100/172], Loss: 19.6813\n",
      "Epoch [242/300], Step [101/172], Loss: 20.9940\n",
      "Epoch [242/300], Step [102/172], Loss: 19.7493\n",
      "Epoch [242/300], Step [103/172], Loss: 14.1288\n",
      "Epoch [242/300], Step [104/172], Loss: 21.1713\n",
      "Epoch [242/300], Step [105/172], Loss: 25.8744\n",
      "Epoch [242/300], Step [106/172], Loss: 17.2376\n",
      "Epoch [242/300], Step [107/172], Loss: 17.6703\n",
      "Epoch [242/300], Step [108/172], Loss: 18.0231\n",
      "Epoch [242/300], Step [109/172], Loss: 16.4974\n",
      "Epoch [242/300], Step [110/172], Loss: 19.8319\n",
      "Epoch [242/300], Step [111/172], Loss: 19.4145\n",
      "Epoch [242/300], Step [112/172], Loss: 17.2405\n",
      "Epoch [242/300], Step [113/172], Loss: 15.9408\n",
      "Epoch [242/300], Step [114/172], Loss: 16.1087\n",
      "Epoch [242/300], Step [115/172], Loss: 19.3493\n",
      "Epoch [242/300], Step [116/172], Loss: 15.5445\n",
      "Epoch [242/300], Step [117/172], Loss: 13.7513\n",
      "Epoch [242/300], Step [118/172], Loss: 13.3489\n",
      "Epoch [242/300], Step [119/172], Loss: 19.5367\n",
      "Epoch [242/300], Step [120/172], Loss: 11.7681\n",
      "Epoch [242/300], Step [121/172], Loss: 10.2847\n",
      "Epoch [242/300], Step [122/172], Loss: 14.4076\n",
      "Epoch [242/300], Step [123/172], Loss: 13.4059\n",
      "Epoch [242/300], Step [124/172], Loss: 8.0676\n",
      "Epoch [242/300], Step [125/172], Loss: 13.6128\n",
      "Epoch [242/300], Step [126/172], Loss: 13.7777\n",
      "Epoch [242/300], Step [127/172], Loss: 11.7425\n",
      "Epoch [242/300], Step [128/172], Loss: 11.0468\n",
      "Epoch [242/300], Step [129/172], Loss: 9.5846\n",
      "Epoch [242/300], Step [130/172], Loss: 13.6985\n",
      "Epoch [242/300], Step [131/172], Loss: 9.0554\n",
      "Epoch [242/300], Step [132/172], Loss: 11.2241\n",
      "Epoch [242/300], Step [133/172], Loss: 11.0814\n",
      "Epoch [242/300], Step [134/172], Loss: 11.0998\n",
      "Epoch [242/300], Step [135/172], Loss: 10.1595\n",
      "Epoch [242/300], Step [136/172], Loss: 9.6854\n",
      "Epoch [242/300], Step [137/172], Loss: 9.6271\n",
      "Epoch [242/300], Step [138/172], Loss: 9.9320\n",
      "Epoch [242/300], Step [139/172], Loss: 11.2648\n",
      "Epoch [242/300], Step [140/172], Loss: 12.0400\n",
      "Epoch [242/300], Step [141/172], Loss: 10.0409\n",
      "Epoch [242/300], Step [142/172], Loss: 15.1189\n",
      "Epoch [242/300], Step [143/172], Loss: 12.3555\n",
      "Epoch [242/300], Step [144/172], Loss: 10.0530\n",
      "Epoch [242/300], Step [145/172], Loss: 11.6834\n",
      "Epoch [242/300], Step [146/172], Loss: 11.8920\n",
      "Epoch [242/300], Step [147/172], Loss: 6.3844\n",
      "Epoch [242/300], Step [148/172], Loss: 7.6850\n",
      "Epoch [242/300], Step [149/172], Loss: 8.0182\n",
      "Epoch [242/300], Step [150/172], Loss: 6.5767\n",
      "Epoch [242/300], Step [151/172], Loss: 6.4498\n",
      "Epoch [242/300], Step [152/172], Loss: 9.4089\n",
      "Epoch [242/300], Step [153/172], Loss: 6.9099\n",
      "Epoch [242/300], Step [154/172], Loss: 7.1265\n",
      "Epoch [242/300], Step [155/172], Loss: 7.2676\n",
      "Epoch [242/300], Step [156/172], Loss: 15.0064\n",
      "Epoch [242/300], Step [157/172], Loss: 9.1086\n",
      "Epoch [242/300], Step [158/172], Loss: 8.0707\n",
      "Epoch [242/300], Step [159/172], Loss: 10.4096\n",
      "Epoch [242/300], Step [160/172], Loss: 9.9912\n",
      "Epoch [242/300], Step [161/172], Loss: 9.4432\n",
      "Epoch [242/300], Step [162/172], Loss: 5.6750\n",
      "Epoch [242/300], Step [163/172], Loss: 6.9369\n",
      "Epoch [242/300], Step [164/172], Loss: 10.0084\n",
      "Epoch [242/300], Step [165/172], Loss: 7.7451\n",
      "Epoch [242/300], Step [166/172], Loss: 6.4964\n",
      "Epoch [242/300], Step [167/172], Loss: 11.1340\n",
      "Epoch [242/300], Step [168/172], Loss: 7.3532\n",
      "Epoch [242/300], Step [169/172], Loss: 7.6210\n",
      "Epoch [242/300], Step [170/172], Loss: 6.5426\n",
      "Epoch [242/300], Step [171/172], Loss: 8.9485\n",
      "Epoch [242/300], Step [172/172], Loss: 6.1068\n",
      "Epoch [243/300], Step [1/172], Loss: 43.4913\n",
      "Epoch [243/300], Step [2/172], Loss: 46.3451\n",
      "Epoch [243/300], Step [3/172], Loss: 44.7023\n",
      "Epoch [243/300], Step [4/172], Loss: 20.8001\n",
      "Epoch [243/300], Step [5/172], Loss: 40.0415\n",
      "Epoch [243/300], Step [6/172], Loss: 19.4876\n",
      "Epoch [243/300], Step [7/172], Loss: 26.7267\n",
      "Epoch [243/300], Step [8/172], Loss: 3.7329\n",
      "Epoch [243/300], Step [9/172], Loss: 26.2921\n",
      "Epoch [243/300], Step [10/172], Loss: 38.1686\n",
      "Epoch [243/300], Step [11/172], Loss: 50.5024\n",
      "Epoch [243/300], Step [12/172], Loss: 51.9834\n",
      "Epoch [243/300], Step [13/172], Loss: 30.9304\n",
      "Epoch [243/300], Step [14/172], Loss: 56.2585\n",
      "Epoch [243/300], Step [15/172], Loss: 47.0864\n",
      "Epoch [243/300], Step [16/172], Loss: 10.0659\n",
      "Epoch [243/300], Step [17/172], Loss: 38.0755\n",
      "Epoch [243/300], Step [18/172], Loss: 50.7743\n",
      "Epoch [243/300], Step [19/172], Loss: 69.8032\n",
      "Epoch [243/300], Step [20/172], Loss: 27.2301\n",
      "Epoch [243/300], Step [21/172], Loss: 76.7005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [243/300], Step [22/172], Loss: 50.8523\n",
      "Epoch [243/300], Step [23/172], Loss: 1.8820\n",
      "Epoch [243/300], Step [24/172], Loss: 48.7157\n",
      "Epoch [243/300], Step [25/172], Loss: 35.2639\n",
      "Epoch [243/300], Step [26/172], Loss: 43.7944\n",
      "Epoch [243/300], Step [27/172], Loss: 53.2819\n",
      "Epoch [243/300], Step [28/172], Loss: 19.5359\n",
      "Epoch [243/300], Step [29/172], Loss: 13.9394\n",
      "Epoch [243/300], Step [30/172], Loss: 53.3445\n",
      "Epoch [243/300], Step [31/172], Loss: 33.7723\n",
      "Epoch [243/300], Step [32/172], Loss: 42.4515\n",
      "Epoch [243/300], Step [33/172], Loss: 66.8739\n",
      "Epoch [243/300], Step [34/172], Loss: 2.0763\n",
      "Epoch [243/300], Step [35/172], Loss: 14.5566\n",
      "Epoch [243/300], Step [36/172], Loss: 16.2923\n",
      "Epoch [243/300], Step [37/172], Loss: 16.6343\n",
      "Epoch [243/300], Step [38/172], Loss: 32.3795\n",
      "Epoch [243/300], Step [39/172], Loss: 36.2511\n",
      "Epoch [243/300], Step [40/172], Loss: 24.1956\n",
      "Epoch [243/300], Step [41/172], Loss: 36.0706\n",
      "Epoch [243/300], Step [42/172], Loss: 42.3197\n",
      "Epoch [243/300], Step [43/172], Loss: 30.3024\n",
      "Epoch [243/300], Step [44/172], Loss: 24.4941\n",
      "Epoch [243/300], Step [45/172], Loss: 36.1821\n",
      "Epoch [243/300], Step [46/172], Loss: 19.6131\n",
      "Epoch [243/300], Step [47/172], Loss: 53.9200\n",
      "Epoch [243/300], Step [48/172], Loss: 63.9068\n",
      "Epoch [243/300], Step [49/172], Loss: 27.7124\n",
      "Epoch [243/300], Step [50/172], Loss: 49.4131\n",
      "Epoch [243/300], Step [51/172], Loss: 11.2427\n",
      "Epoch [243/300], Step [52/172], Loss: 27.5961\n",
      "Epoch [243/300], Step [53/172], Loss: 28.4812\n",
      "Epoch [243/300], Step [54/172], Loss: 22.7641\n",
      "Epoch [243/300], Step [55/172], Loss: 22.3809\n",
      "Epoch [243/300], Step [56/172], Loss: 19.5568\n",
      "Epoch [243/300], Step [57/172], Loss: 19.6990\n",
      "Epoch [243/300], Step [58/172], Loss: 18.3741\n",
      "Epoch [243/300], Step [59/172], Loss: 32.1119\n",
      "Epoch [243/300], Step [60/172], Loss: 20.6978\n",
      "Epoch [243/300], Step [61/172], Loss: 9.6644\n",
      "Epoch [243/300], Step [62/172], Loss: 16.9875\n",
      "Epoch [243/300], Step [63/172], Loss: 14.3405\n",
      "Epoch [243/300], Step [64/172], Loss: 15.8571\n",
      "Epoch [243/300], Step [65/172], Loss: 22.8682\n",
      "Epoch [243/300], Step [66/172], Loss: 9.2872\n",
      "Epoch [243/300], Step [67/172], Loss: 27.1381\n",
      "Epoch [243/300], Step [68/172], Loss: 7.9407\n",
      "Epoch [243/300], Step [69/172], Loss: 28.5926\n",
      "Epoch [243/300], Step [70/172], Loss: 30.4083\n",
      "Epoch [243/300], Step [71/172], Loss: 32.5104\n",
      "Epoch [243/300], Step [72/172], Loss: 28.7618\n",
      "Epoch [243/300], Step [73/172], Loss: 37.1093\n",
      "Epoch [243/300], Step [74/172], Loss: 20.5836\n",
      "Epoch [243/300], Step [75/172], Loss: 19.8673\n",
      "Epoch [243/300], Step [76/172], Loss: 26.3624\n",
      "Epoch [243/300], Step [77/172], Loss: 41.0850\n",
      "Epoch [243/300], Step [78/172], Loss: 30.5062\n",
      "Epoch [243/300], Step [79/172], Loss: 29.2709\n",
      "Epoch [243/300], Step [80/172], Loss: 45.8011\n",
      "Epoch [243/300], Step [81/172], Loss: 26.4701\n",
      "Epoch [243/300], Step [82/172], Loss: 36.1291\n",
      "Epoch [243/300], Step [83/172], Loss: 39.7894\n",
      "Epoch [243/300], Step [84/172], Loss: 31.3447\n",
      "Epoch [243/300], Step [85/172], Loss: 35.2957\n",
      "Epoch [243/300], Step [86/172], Loss: 32.7973\n",
      "Epoch [243/300], Step [87/172], Loss: 23.2196\n",
      "Epoch [243/300], Step [88/172], Loss: 21.4254\n",
      "Epoch [243/300], Step [89/172], Loss: 28.3925\n",
      "Epoch [243/300], Step [90/172], Loss: 21.1292\n",
      "Epoch [243/300], Step [91/172], Loss: 26.4559\n",
      "Epoch [243/300], Step [92/172], Loss: 19.3220\n",
      "Epoch [243/300], Step [93/172], Loss: 18.6119\n",
      "Epoch [243/300], Step [94/172], Loss: 25.1591\n",
      "Epoch [243/300], Step [95/172], Loss: 21.9353\n",
      "Epoch [243/300], Step [96/172], Loss: 20.5001\n",
      "Epoch [243/300], Step [97/172], Loss: 30.4772\n",
      "Epoch [243/300], Step [98/172], Loss: 20.2436\n",
      "Epoch [243/300], Step [99/172], Loss: 20.5044\n",
      "Epoch [243/300], Step [100/172], Loss: 19.6657\n",
      "Epoch [243/300], Step [101/172], Loss: 21.0044\n",
      "Epoch [243/300], Step [102/172], Loss: 19.5081\n",
      "Epoch [243/300], Step [103/172], Loss: 14.0551\n",
      "Epoch [243/300], Step [104/172], Loss: 21.2357\n",
      "Epoch [243/300], Step [105/172], Loss: 25.6396\n",
      "Epoch [243/300], Step [106/172], Loss: 17.3156\n",
      "Epoch [243/300], Step [107/172], Loss: 17.7477\n",
      "Epoch [243/300], Step [108/172], Loss: 18.1813\n",
      "Epoch [243/300], Step [109/172], Loss: 16.5415\n",
      "Epoch [243/300], Step [110/172], Loss: 20.0972\n",
      "Epoch [243/300], Step [111/172], Loss: 19.4230\n",
      "Epoch [243/300], Step [112/172], Loss: 17.4025\n",
      "Epoch [243/300], Step [113/172], Loss: 16.2455\n",
      "Epoch [243/300], Step [114/172], Loss: 16.1226\n",
      "Epoch [243/300], Step [115/172], Loss: 19.5693\n",
      "Epoch [243/300], Step [116/172], Loss: 15.7754\n",
      "Epoch [243/300], Step [117/172], Loss: 13.7423\n",
      "Epoch [243/300], Step [118/172], Loss: 13.3648\n",
      "Epoch [243/300], Step [119/172], Loss: 19.7666\n",
      "Epoch [243/300], Step [120/172], Loss: 11.8002\n",
      "Epoch [243/300], Step [121/172], Loss: 10.2479\n",
      "Epoch [243/300], Step [122/172], Loss: 14.4465\n",
      "Epoch [243/300], Step [123/172], Loss: 13.7181\n",
      "Epoch [243/300], Step [124/172], Loss: 8.0533\n",
      "Epoch [243/300], Step [125/172], Loss: 13.5940\n",
      "Epoch [243/300], Step [126/172], Loss: 14.0356\n",
      "Epoch [243/300], Step [127/172], Loss: 11.8482\n",
      "Epoch [243/300], Step [128/172], Loss: 11.2117\n",
      "Epoch [243/300], Step [129/172], Loss: 9.5807\n",
      "Epoch [243/300], Step [130/172], Loss: 13.7131\n",
      "Epoch [243/300], Step [131/172], Loss: 9.0775\n",
      "Epoch [243/300], Step [132/172], Loss: 11.2283\n",
      "Epoch [243/300], Step [133/172], Loss: 11.0355\n",
      "Epoch [243/300], Step [134/172], Loss: 11.1269\n",
      "Epoch [243/300], Step [135/172], Loss: 10.1635\n",
      "Epoch [243/300], Step [136/172], Loss: 9.8385\n",
      "Epoch [243/300], Step [137/172], Loss: 9.7855\n",
      "Epoch [243/300], Step [138/172], Loss: 10.1611\n",
      "Epoch [243/300], Step [139/172], Loss: 11.5106\n",
      "Epoch [243/300], Step [140/172], Loss: 12.1561\n",
      "Epoch [243/300], Step [141/172], Loss: 10.2686\n",
      "Epoch [243/300], Step [142/172], Loss: 15.3762\n",
      "Epoch [243/300], Step [143/172], Loss: 12.3988\n",
      "Epoch [243/300], Step [144/172], Loss: 10.2586\n",
      "Epoch [243/300], Step [145/172], Loss: 11.7005\n",
      "Epoch [243/300], Step [146/172], Loss: 12.0907\n",
      "Epoch [243/300], Step [147/172], Loss: 6.4327\n",
      "Epoch [243/300], Step [148/172], Loss: 7.7120\n",
      "Epoch [243/300], Step [149/172], Loss: 8.0784\n",
      "Epoch [243/300], Step [150/172], Loss: 6.6122\n",
      "Epoch [243/300], Step [151/172], Loss: 6.4719\n",
      "Epoch [243/300], Step [152/172], Loss: 9.5052\n",
      "Epoch [243/300], Step [153/172], Loss: 7.0466\n",
      "Epoch [243/300], Step [154/172], Loss: 7.1932\n",
      "Epoch [243/300], Step [155/172], Loss: 7.4870\n",
      "Epoch [243/300], Step [156/172], Loss: 15.0925\n",
      "Epoch [243/300], Step [157/172], Loss: 9.1068\n",
      "Epoch [243/300], Step [158/172], Loss: 8.0748\n",
      "Epoch [243/300], Step [159/172], Loss: 10.6693\n",
      "Epoch [243/300], Step [160/172], Loss: 10.1022\n",
      "Epoch [243/300], Step [161/172], Loss: 9.5205\n",
      "Epoch [243/300], Step [162/172], Loss: 5.7374\n",
      "Epoch [243/300], Step [163/172], Loss: 7.1020\n",
      "Epoch [243/300], Step [164/172], Loss: 9.7632\n",
      "Epoch [243/300], Step [165/172], Loss: 7.8284\n",
      "Epoch [243/300], Step [166/172], Loss: 6.7583\n",
      "Epoch [243/300], Step [167/172], Loss: 11.3487\n",
      "Epoch [243/300], Step [168/172], Loss: 7.5244\n",
      "Epoch [243/300], Step [169/172], Loss: 7.6279\n",
      "Epoch [243/300], Step [170/172], Loss: 6.5697\n",
      "Epoch [243/300], Step [171/172], Loss: 9.4332\n",
      "Epoch [243/300], Step [172/172], Loss: 6.2841\n",
      "Epoch [244/300], Step [1/172], Loss: 43.5066\n",
      "Epoch [244/300], Step [2/172], Loss: 46.7977\n",
      "Epoch [244/300], Step [3/172], Loss: 42.9223\n",
      "Epoch [244/300], Step [4/172], Loss: 20.6481\n",
      "Epoch [244/300], Step [5/172], Loss: 39.5528\n",
      "Epoch [244/300], Step [6/172], Loss: 19.5472\n",
      "Epoch [244/300], Step [7/172], Loss: 27.3631\n",
      "Epoch [244/300], Step [8/172], Loss: 4.7589\n",
      "Epoch [244/300], Step [9/172], Loss: 26.2571\n",
      "Epoch [244/300], Step [10/172], Loss: 37.8568\n",
      "Epoch [244/300], Step [11/172], Loss: 50.0832\n",
      "Epoch [244/300], Step [12/172], Loss: 51.4706\n",
      "Epoch [244/300], Step [13/172], Loss: 31.2813\n",
      "Epoch [244/300], Step [14/172], Loss: 54.2288\n",
      "Epoch [244/300], Step [15/172], Loss: 46.5497\n",
      "Epoch [244/300], Step [16/172], Loss: 8.7406\n",
      "Epoch [244/300], Step [17/172], Loss: 37.8478\n",
      "Epoch [244/300], Step [18/172], Loss: 50.7301\n",
      "Epoch [244/300], Step [19/172], Loss: 69.4711\n",
      "Epoch [244/300], Step [20/172], Loss: 26.7864\n",
      "Epoch [244/300], Step [21/172], Loss: 75.9103\n",
      "Epoch [244/300], Step [22/172], Loss: 50.0919\n",
      "Epoch [244/300], Step [23/172], Loss: 1.8141\n",
      "Epoch [244/300], Step [24/172], Loss: 48.7252\n",
      "Epoch [244/300], Step [25/172], Loss: 34.8352\n",
      "Epoch [244/300], Step [26/172], Loss: 43.4719\n",
      "Epoch [244/300], Step [27/172], Loss: 53.5761\n",
      "Epoch [244/300], Step [28/172], Loss: 19.3519\n",
      "Epoch [244/300], Step [29/172], Loss: 13.9667\n",
      "Epoch [244/300], Step [30/172], Loss: 52.8295\n",
      "Epoch [244/300], Step [31/172], Loss: 33.3733\n",
      "Epoch [244/300], Step [32/172], Loss: 42.0469\n",
      "Epoch [244/300], Step [33/172], Loss: 66.1443\n",
      "Epoch [244/300], Step [34/172], Loss: 2.1800\n",
      "Epoch [244/300], Step [35/172], Loss: 14.5387\n",
      "Epoch [244/300], Step [36/172], Loss: 16.4870\n",
      "Epoch [244/300], Step [37/172], Loss: 16.5818\n",
      "Epoch [244/300], Step [38/172], Loss: 32.5060\n",
      "Epoch [244/300], Step [39/172], Loss: 36.0472\n",
      "Epoch [244/300], Step [40/172], Loss: 23.8786\n",
      "Epoch [244/300], Step [41/172], Loss: 35.4600\n",
      "Epoch [244/300], Step [42/172], Loss: 41.8201\n",
      "Epoch [244/300], Step [43/172], Loss: 29.9120\n",
      "Epoch [244/300], Step [44/172], Loss: 24.0851\n",
      "Epoch [244/300], Step [45/172], Loss: 36.3277\n",
      "Epoch [244/300], Step [46/172], Loss: 18.6546\n",
      "Epoch [244/300], Step [47/172], Loss: 53.0425\n",
      "Epoch [244/300], Step [48/172], Loss: 62.1137\n",
      "Epoch [244/300], Step [49/172], Loss: 27.6495\n",
      "Epoch [244/300], Step [50/172], Loss: 49.9334\n",
      "Epoch [244/300], Step [51/172], Loss: 11.1518\n",
      "Epoch [244/300], Step [52/172], Loss: 27.2402\n",
      "Epoch [244/300], Step [53/172], Loss: 28.4847\n",
      "Epoch [244/300], Step [54/172], Loss: 21.9802\n",
      "Epoch [244/300], Step [55/172], Loss: 21.9312\n",
      "Epoch [244/300], Step [56/172], Loss: 19.2366\n",
      "Epoch [244/300], Step [57/172], Loss: 20.0084\n",
      "Epoch [244/300], Step [58/172], Loss: 17.9978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [244/300], Step [59/172], Loss: 32.2127\n",
      "Epoch [244/300], Step [60/172], Loss: 21.2363\n",
      "Epoch [244/300], Step [61/172], Loss: 9.3449\n",
      "Epoch [244/300], Step [62/172], Loss: 17.1550\n",
      "Epoch [244/300], Step [63/172], Loss: 14.1254\n",
      "Epoch [244/300], Step [64/172], Loss: 15.7558\n",
      "Epoch [244/300], Step [65/172], Loss: 22.8632\n",
      "Epoch [244/300], Step [66/172], Loss: 9.3523\n",
      "Epoch [244/300], Step [67/172], Loss: 27.6582\n",
      "Epoch [244/300], Step [68/172], Loss: 7.4521\n",
      "Epoch [244/300], Step [69/172], Loss: 29.0023\n",
      "Epoch [244/300], Step [70/172], Loss: 30.3620\n",
      "Epoch [244/300], Step [71/172], Loss: 32.0496\n",
      "Epoch [244/300], Step [72/172], Loss: 28.6778\n",
      "Epoch [244/300], Step [73/172], Loss: 36.5963\n",
      "Epoch [244/300], Step [74/172], Loss: 20.3911\n",
      "Epoch [244/300], Step [75/172], Loss: 19.6170\n",
      "Epoch [244/300], Step [76/172], Loss: 25.9086\n",
      "Epoch [244/300], Step [77/172], Loss: 40.5019\n",
      "Epoch [244/300], Step [78/172], Loss: 30.2671\n",
      "Epoch [244/300], Step [79/172], Loss: 28.6947\n",
      "Epoch [244/300], Step [80/172], Loss: 45.4436\n",
      "Epoch [244/300], Step [81/172], Loss: 25.9414\n",
      "Epoch [244/300], Step [82/172], Loss: 36.1903\n",
      "Epoch [244/300], Step [83/172], Loss: 39.6891\n",
      "Epoch [244/300], Step [84/172], Loss: 31.2550\n",
      "Epoch [244/300], Step [85/172], Loss: 35.1259\n",
      "Epoch [244/300], Step [86/172], Loss: 32.6218\n",
      "Epoch [244/300], Step [87/172], Loss: 23.0280\n",
      "Epoch [244/300], Step [88/172], Loss: 21.3781\n",
      "Epoch [244/300], Step [89/172], Loss: 28.5441\n",
      "Epoch [244/300], Step [90/172], Loss: 20.8921\n",
      "Epoch [244/300], Step [91/172], Loss: 26.3596\n",
      "Epoch [244/300], Step [92/172], Loss: 19.3022\n",
      "Epoch [244/300], Step [93/172], Loss: 18.7277\n",
      "Epoch [244/300], Step [94/172], Loss: 25.2413\n",
      "Epoch [244/300], Step [95/172], Loss: 22.2652\n",
      "Epoch [244/300], Step [96/172], Loss: 20.4941\n",
      "Epoch [244/300], Step [97/172], Loss: 30.5277\n",
      "Epoch [244/300], Step [98/172], Loss: 20.1583\n",
      "Epoch [244/300], Step [99/172], Loss: 20.4574\n",
      "Epoch [244/300], Step [100/172], Loss: 19.4071\n",
      "Epoch [244/300], Step [101/172], Loss: 20.9604\n",
      "Epoch [244/300], Step [102/172], Loss: 19.5706\n",
      "Epoch [244/300], Step [103/172], Loss: 13.9408\n",
      "Epoch [244/300], Step [104/172], Loss: 21.1763\n",
      "Epoch [244/300], Step [105/172], Loss: 25.6688\n",
      "Epoch [244/300], Step [106/172], Loss: 17.1756\n",
      "Epoch [244/300], Step [107/172], Loss: 17.6033\n",
      "Epoch [244/300], Step [108/172], Loss: 17.9845\n",
      "Epoch [244/300], Step [109/172], Loss: 16.4507\n",
      "Epoch [244/300], Step [110/172], Loss: 19.9897\n",
      "Epoch [244/300], Step [111/172], Loss: 19.2919\n",
      "Epoch [244/300], Step [112/172], Loss: 17.4316\n",
      "Epoch [244/300], Step [113/172], Loss: 16.1255\n",
      "Epoch [244/300], Step [114/172], Loss: 16.1807\n",
      "Epoch [244/300], Step [115/172], Loss: 19.8088\n",
      "Epoch [244/300], Step [116/172], Loss: 15.8669\n",
      "Epoch [244/300], Step [117/172], Loss: 13.8637\n",
      "Epoch [244/300], Step [118/172], Loss: 13.4327\n",
      "Epoch [244/300], Step [119/172], Loss: 19.6727\n",
      "Epoch [244/300], Step [120/172], Loss: 11.7282\n",
      "Epoch [244/300], Step [121/172], Loss: 10.0946\n",
      "Epoch [244/300], Step [122/172], Loss: 14.5440\n",
      "Epoch [244/300], Step [123/172], Loss: 13.6097\n",
      "Epoch [244/300], Step [124/172], Loss: 8.0083\n",
      "Epoch [244/300], Step [125/172], Loss: 13.4980\n",
      "Epoch [244/300], Step [126/172], Loss: 14.0048\n",
      "Epoch [244/300], Step [127/172], Loss: 11.8281\n",
      "Epoch [244/300], Step [128/172], Loss: 11.2160\n",
      "Epoch [244/300], Step [129/172], Loss: 9.4942\n",
      "Epoch [244/300], Step [130/172], Loss: 13.7328\n",
      "Epoch [244/300], Step [131/172], Loss: 8.9849\n",
      "Epoch [244/300], Step [132/172], Loss: 11.2420\n",
      "Epoch [244/300], Step [133/172], Loss: 11.0674\n",
      "Epoch [244/300], Step [134/172], Loss: 11.1901\n",
      "Epoch [244/300], Step [135/172], Loss: 10.0890\n",
      "Epoch [244/300], Step [136/172], Loss: 9.6898\n",
      "Epoch [244/300], Step [137/172], Loss: 9.6874\n",
      "Epoch [244/300], Step [138/172], Loss: 10.0246\n",
      "Epoch [244/300], Step [139/172], Loss: 11.4994\n",
      "Epoch [244/300], Step [140/172], Loss: 12.1386\n",
      "Epoch [244/300], Step [141/172], Loss: 10.1201\n",
      "Epoch [244/300], Step [142/172], Loss: 15.4980\n",
      "Epoch [244/300], Step [143/172], Loss: 12.4599\n",
      "Epoch [244/300], Step [144/172], Loss: 10.2296\n",
      "Epoch [244/300], Step [145/172], Loss: 11.6585\n",
      "Epoch [244/300], Step [146/172], Loss: 12.1568\n",
      "Epoch [244/300], Step [147/172], Loss: 6.3506\n",
      "Epoch [244/300], Step [148/172], Loss: 7.5917\n",
      "Epoch [244/300], Step [149/172], Loss: 7.9817\n",
      "Epoch [244/300], Step [150/172], Loss: 6.6186\n",
      "Epoch [244/300], Step [151/172], Loss: 6.5257\n",
      "Epoch [244/300], Step [152/172], Loss: 9.2920\n",
      "Epoch [244/300], Step [153/172], Loss: 6.9190\n",
      "Epoch [244/300], Step [154/172], Loss: 7.1063\n",
      "Epoch [244/300], Step [155/172], Loss: 7.2460\n",
      "Epoch [244/300], Step [156/172], Loss: 15.2035\n",
      "Epoch [244/300], Step [157/172], Loss: 9.2241\n",
      "Epoch [244/300], Step [158/172], Loss: 8.1334\n",
      "Epoch [244/300], Step [159/172], Loss: 10.7287\n",
      "Epoch [244/300], Step [160/172], Loss: 10.2143\n",
      "Epoch [244/300], Step [161/172], Loss: 9.5065\n",
      "Epoch [244/300], Step [162/172], Loss: 5.6229\n",
      "Epoch [244/300], Step [163/172], Loss: 7.1141\n",
      "Epoch [244/300], Step [164/172], Loss: 9.8364\n",
      "Epoch [244/300], Step [165/172], Loss: 7.7206\n",
      "Epoch [244/300], Step [166/172], Loss: 6.6634\n",
      "Epoch [244/300], Step [167/172], Loss: 11.3573\n",
      "Epoch [244/300], Step [168/172], Loss: 7.4908\n",
      "Epoch [244/300], Step [169/172], Loss: 7.5349\n",
      "Epoch [244/300], Step [170/172], Loss: 6.5594\n",
      "Epoch [244/300], Step [171/172], Loss: 9.5202\n",
      "Epoch [244/300], Step [172/172], Loss: 6.0678\n",
      "Epoch [245/300], Step [1/172], Loss: 43.1813\n",
      "Epoch [245/300], Step [2/172], Loss: 45.8170\n",
      "Epoch [245/300], Step [3/172], Loss: 42.9559\n",
      "Epoch [245/300], Step [4/172], Loss: 20.3204\n",
      "Epoch [245/300], Step [5/172], Loss: 39.8482\n",
      "Epoch [245/300], Step [6/172], Loss: 19.1715\n",
      "Epoch [245/300], Step [7/172], Loss: 26.7029\n",
      "Epoch [245/300], Step [8/172], Loss: 4.1508\n",
      "Epoch [245/300], Step [9/172], Loss: 25.9255\n",
      "Epoch [245/300], Step [10/172], Loss: 37.1070\n",
      "Epoch [245/300], Step [11/172], Loss: 50.1168\n",
      "Epoch [245/300], Step [12/172], Loss: 51.4985\n",
      "Epoch [245/300], Step [13/172], Loss: 30.8776\n",
      "Epoch [245/300], Step [14/172], Loss: 54.9030\n",
      "Epoch [245/300], Step [15/172], Loss: 46.8653\n",
      "Epoch [245/300], Step [16/172], Loss: 9.0568\n",
      "Epoch [245/300], Step [17/172], Loss: 37.5450\n",
      "Epoch [245/300], Step [18/172], Loss: 50.6317\n",
      "Epoch [245/300], Step [19/172], Loss: 69.1165\n",
      "Epoch [245/300], Step [20/172], Loss: 27.3945\n",
      "Epoch [245/300], Step [21/172], Loss: 75.5024\n",
      "Epoch [245/300], Step [22/172], Loss: 50.0991\n",
      "Epoch [245/300], Step [23/172], Loss: 1.9322\n",
      "Epoch [245/300], Step [24/172], Loss: 48.3641\n",
      "Epoch [245/300], Step [25/172], Loss: 34.4634\n",
      "Epoch [245/300], Step [26/172], Loss: 43.4324\n",
      "Epoch [245/300], Step [27/172], Loss: 53.0890\n",
      "Epoch [245/300], Step [28/172], Loss: 19.2465\n",
      "Epoch [245/300], Step [29/172], Loss: 13.9587\n",
      "Epoch [245/300], Step [30/172], Loss: 52.5389\n",
      "Epoch [245/300], Step [31/172], Loss: 32.8470\n",
      "Epoch [245/300], Step [32/172], Loss: 42.0025\n",
      "Epoch [245/300], Step [33/172], Loss: 65.9762\n",
      "Epoch [245/300], Step [34/172], Loss: 2.0228\n",
      "Epoch [245/300], Step [35/172], Loss: 14.6648\n",
      "Epoch [245/300], Step [36/172], Loss: 16.1221\n",
      "Epoch [245/300], Step [37/172], Loss: 16.3723\n",
      "Epoch [245/300], Step [38/172], Loss: 32.3509\n",
      "Epoch [245/300], Step [39/172], Loss: 35.9361\n",
      "Epoch [245/300], Step [40/172], Loss: 23.7811\n",
      "Epoch [245/300], Step [41/172], Loss: 35.3053\n",
      "Epoch [245/300], Step [42/172], Loss: 41.7411\n",
      "Epoch [245/300], Step [43/172], Loss: 29.8289\n",
      "Epoch [245/300], Step [44/172], Loss: 24.0364\n",
      "Epoch [245/300], Step [45/172], Loss: 35.7940\n",
      "Epoch [245/300], Step [46/172], Loss: 18.3571\n",
      "Epoch [245/300], Step [47/172], Loss: 52.7381\n",
      "Epoch [245/300], Step [48/172], Loss: 62.9501\n",
      "Epoch [245/300], Step [49/172], Loss: 27.2798\n",
      "Epoch [245/300], Step [50/172], Loss: 50.1690\n",
      "Epoch [245/300], Step [51/172], Loss: 10.9026\n",
      "Epoch [245/300], Step [52/172], Loss: 26.9350\n",
      "Epoch [245/300], Step [53/172], Loss: 28.1066\n",
      "Epoch [245/300], Step [54/172], Loss: 22.0395\n",
      "Epoch [245/300], Step [55/172], Loss: 21.5945\n",
      "Epoch [245/300], Step [56/172], Loss: 18.7770\n",
      "Epoch [245/300], Step [57/172], Loss: 20.0375\n",
      "Epoch [245/300], Step [58/172], Loss: 17.9904\n",
      "Epoch [245/300], Step [59/172], Loss: 31.7889\n",
      "Epoch [245/300], Step [60/172], Loss: 21.2264\n",
      "Epoch [245/300], Step [61/172], Loss: 9.2110\n",
      "Epoch [245/300], Step [62/172], Loss: 16.9864\n",
      "Epoch [245/300], Step [63/172], Loss: 13.5664\n",
      "Epoch [245/300], Step [64/172], Loss: 15.4042\n",
      "Epoch [245/300], Step [65/172], Loss: 23.0437\n",
      "Epoch [245/300], Step [66/172], Loss: 9.2169\n",
      "Epoch [245/300], Step [67/172], Loss: 27.3959\n",
      "Epoch [245/300], Step [68/172], Loss: 7.4857\n",
      "Epoch [245/300], Step [69/172], Loss: 28.7451\n",
      "Epoch [245/300], Step [70/172], Loss: 30.2120\n",
      "Epoch [245/300], Step [71/172], Loss: 32.3238\n",
      "Epoch [245/300], Step [72/172], Loss: 28.5979\n",
      "Epoch [245/300], Step [73/172], Loss: 36.8630\n",
      "Epoch [245/300], Step [74/172], Loss: 20.2840\n",
      "Epoch [245/300], Step [75/172], Loss: 19.9472\n",
      "Epoch [245/300], Step [76/172], Loss: 25.9583\n",
      "Epoch [245/300], Step [77/172], Loss: 40.7518\n",
      "Epoch [245/300], Step [78/172], Loss: 30.2185\n",
      "Epoch [245/300], Step [79/172], Loss: 28.9149\n",
      "Epoch [245/300], Step [80/172], Loss: 45.5418\n",
      "Epoch [245/300], Step [81/172], Loss: 26.0771\n",
      "Epoch [245/300], Step [82/172], Loss: 36.0100\n",
      "Epoch [245/300], Step [83/172], Loss: 39.5845\n",
      "Epoch [245/300], Step [84/172], Loss: 31.1559\n",
      "Epoch [245/300], Step [85/172], Loss: 34.7723\n",
      "Epoch [245/300], Step [86/172], Loss: 32.3504\n",
      "Epoch [245/300], Step [87/172], Loss: 23.0973\n",
      "Epoch [245/300], Step [88/172], Loss: 21.2513\n",
      "Epoch [245/300], Step [89/172], Loss: 28.2325\n",
      "Epoch [245/300], Step [90/172], Loss: 20.8285\n",
      "Epoch [245/300], Step [91/172], Loss: 26.5098\n",
      "Epoch [245/300], Step [92/172], Loss: 19.3635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [245/300], Step [93/172], Loss: 18.6213\n",
      "Epoch [245/300], Step [94/172], Loss: 25.2882\n",
      "Epoch [245/300], Step [95/172], Loss: 21.9509\n",
      "Epoch [245/300], Step [96/172], Loss: 20.5889\n",
      "Epoch [245/300], Step [97/172], Loss: 30.7717\n",
      "Epoch [245/300], Step [98/172], Loss: 20.2995\n",
      "Epoch [245/300], Step [99/172], Loss: 20.6195\n",
      "Epoch [245/300], Step [100/172], Loss: 19.5907\n",
      "Epoch [245/300], Step [101/172], Loss: 21.1043\n",
      "Epoch [245/300], Step [102/172], Loss: 19.7306\n",
      "Epoch [245/300], Step [103/172], Loss: 14.0010\n",
      "Epoch [245/300], Step [104/172], Loss: 21.3803\n",
      "Epoch [245/300], Step [105/172], Loss: 25.7762\n",
      "Epoch [245/300], Step [106/172], Loss: 17.2358\n",
      "Epoch [245/300], Step [107/172], Loss: 17.6013\n",
      "Epoch [245/300], Step [108/172], Loss: 18.1012\n",
      "Epoch [245/300], Step [109/172], Loss: 16.4076\n",
      "Epoch [245/300], Step [110/172], Loss: 19.8778\n",
      "Epoch [245/300], Step [111/172], Loss: 19.4662\n",
      "Epoch [245/300], Step [112/172], Loss: 17.4605\n",
      "Epoch [245/300], Step [113/172], Loss: 16.0601\n",
      "Epoch [245/300], Step [114/172], Loss: 16.2386\n",
      "Epoch [245/300], Step [115/172], Loss: 19.4921\n",
      "Epoch [245/300], Step [116/172], Loss: 15.6638\n",
      "Epoch [245/300], Step [117/172], Loss: 13.8939\n",
      "Epoch [245/300], Step [118/172], Loss: 13.3001\n",
      "Epoch [245/300], Step [119/172], Loss: 19.5954\n",
      "Epoch [245/300], Step [120/172], Loss: 11.6838\n",
      "Epoch [245/300], Step [121/172], Loss: 10.0256\n",
      "Epoch [245/300], Step [122/172], Loss: 14.2610\n",
      "Epoch [245/300], Step [123/172], Loss: 13.7817\n",
      "Epoch [245/300], Step [124/172], Loss: 8.0246\n",
      "Epoch [245/300], Step [125/172], Loss: 13.5160\n",
      "Epoch [245/300], Step [126/172], Loss: 14.0988\n",
      "Epoch [245/300], Step [127/172], Loss: 11.7984\n",
      "Epoch [245/300], Step [128/172], Loss: 11.0850\n",
      "Epoch [245/300], Step [129/172], Loss: 9.4875\n",
      "Epoch [245/300], Step [130/172], Loss: 13.6919\n",
      "Epoch [245/300], Step [131/172], Loss: 8.9111\n",
      "Epoch [245/300], Step [132/172], Loss: 11.2550\n",
      "Epoch [245/300], Step [133/172], Loss: 10.8511\n",
      "Epoch [245/300], Step [134/172], Loss: 10.9327\n",
      "Epoch [245/300], Step [135/172], Loss: 10.0199\n",
      "Epoch [245/300], Step [136/172], Loss: 9.8947\n",
      "Epoch [245/300], Step [137/172], Loss: 9.6013\n",
      "Epoch [245/300], Step [138/172], Loss: 9.9146\n",
      "Epoch [245/300], Step [139/172], Loss: 11.4785\n",
      "Epoch [245/300], Step [140/172], Loss: 12.1505\n",
      "Epoch [245/300], Step [141/172], Loss: 10.2256\n",
      "Epoch [245/300], Step [142/172], Loss: 15.3729\n",
      "Epoch [245/300], Step [143/172], Loss: 12.3771\n",
      "Epoch [245/300], Step [144/172], Loss: 10.2141\n",
      "Epoch [245/300], Step [145/172], Loss: 11.7051\n",
      "Epoch [245/300], Step [146/172], Loss: 12.0750\n",
      "Epoch [245/300], Step [147/172], Loss: 6.3494\n",
      "Epoch [245/300], Step [148/172], Loss: 7.4896\n",
      "Epoch [245/300], Step [149/172], Loss: 8.0058\n",
      "Epoch [245/300], Step [150/172], Loss: 6.5460\n",
      "Epoch [245/300], Step [151/172], Loss: 6.3679\n",
      "Epoch [245/300], Step [152/172], Loss: 9.1877\n",
      "Epoch [245/300], Step [153/172], Loss: 6.9454\n",
      "Epoch [245/300], Step [154/172], Loss: 7.1168\n",
      "Epoch [245/300], Step [155/172], Loss: 7.3541\n",
      "Epoch [245/300], Step [156/172], Loss: 15.1626\n",
      "Epoch [245/300], Step [157/172], Loss: 9.2092\n",
      "Epoch [245/300], Step [158/172], Loss: 8.0799\n",
      "Epoch [245/300], Step [159/172], Loss: 10.6821\n",
      "Epoch [245/300], Step [160/172], Loss: 10.1899\n",
      "Epoch [245/300], Step [161/172], Loss: 9.4975\n",
      "Epoch [245/300], Step [162/172], Loss: 5.5850\n",
      "Epoch [245/300], Step [163/172], Loss: 7.1523\n",
      "Epoch [245/300], Step [164/172], Loss: 9.5616\n",
      "Epoch [245/300], Step [165/172], Loss: 7.6779\n",
      "Epoch [245/300], Step [166/172], Loss: 6.5852\n",
      "Epoch [245/300], Step [167/172], Loss: 11.3137\n",
      "Epoch [245/300], Step [168/172], Loss: 7.4245\n",
      "Epoch [245/300], Step [169/172], Loss: 7.4668\n",
      "Epoch [245/300], Step [170/172], Loss: 6.4806\n",
      "Epoch [245/300], Step [171/172], Loss: 9.3183\n",
      "Epoch [245/300], Step [172/172], Loss: 6.0762\n",
      "Epoch [246/300], Step [1/172], Loss: 43.2318\n",
      "Epoch [246/300], Step [2/172], Loss: 45.9237\n",
      "Epoch [246/300], Step [3/172], Loss: 41.9652\n",
      "Epoch [246/300], Step [4/172], Loss: 20.3564\n",
      "Epoch [246/300], Step [5/172], Loss: 39.4446\n",
      "Epoch [246/300], Step [6/172], Loss: 19.2340\n",
      "Epoch [246/300], Step [7/172], Loss: 27.6163\n",
      "Epoch [246/300], Step [8/172], Loss: 4.6568\n",
      "Epoch [246/300], Step [9/172], Loss: 25.9251\n",
      "Epoch [246/300], Step [10/172], Loss: 37.2246\n",
      "Epoch [246/300], Step [11/172], Loss: 50.5213\n",
      "Epoch [246/300], Step [12/172], Loss: 51.4465\n",
      "Epoch [246/300], Step [13/172], Loss: 31.0067\n",
      "Epoch [246/300], Step [14/172], Loss: 53.9146\n",
      "Epoch [246/300], Step [15/172], Loss: 46.9646\n",
      "Epoch [246/300], Step [16/172], Loss: 8.6860\n",
      "Epoch [246/300], Step [17/172], Loss: 37.5009\n",
      "Epoch [246/300], Step [18/172], Loss: 50.8402\n",
      "Epoch [246/300], Step [19/172], Loss: 69.4480\n",
      "Epoch [246/300], Step [20/172], Loss: 27.0305\n",
      "Epoch [246/300], Step [21/172], Loss: 74.9908\n",
      "Epoch [246/300], Step [22/172], Loss: 50.5165\n",
      "Epoch [246/300], Step [23/172], Loss: 1.6267\n",
      "Epoch [246/300], Step [24/172], Loss: 48.0904\n",
      "Epoch [246/300], Step [25/172], Loss: 34.0562\n",
      "Epoch [246/300], Step [26/172], Loss: 43.1075\n",
      "Epoch [246/300], Step [27/172], Loss: 53.7882\n",
      "Epoch [246/300], Step [28/172], Loss: 18.8248\n",
      "Epoch [246/300], Step [29/172], Loss: 13.8884\n",
      "Epoch [246/300], Step [30/172], Loss: 52.2388\n",
      "Epoch [246/300], Step [31/172], Loss: 32.4615\n",
      "Epoch [246/300], Step [32/172], Loss: 41.5245\n",
      "Epoch [246/300], Step [33/172], Loss: 65.5242\n",
      "Epoch [246/300], Step [34/172], Loss: 2.1324\n",
      "Epoch [246/300], Step [35/172], Loss: 14.6579\n",
      "Epoch [246/300], Step [36/172], Loss: 16.1766\n",
      "Epoch [246/300], Step [37/172], Loss: 16.2462\n",
      "Epoch [246/300], Step [38/172], Loss: 32.1833\n",
      "Epoch [246/300], Step [39/172], Loss: 35.6517\n",
      "Epoch [246/300], Step [40/172], Loss: 23.3116\n",
      "Epoch [246/300], Step [41/172], Loss: 34.8094\n",
      "Epoch [246/300], Step [42/172], Loss: 41.1799\n",
      "Epoch [246/300], Step [43/172], Loss: 29.3653\n",
      "Epoch [246/300], Step [44/172], Loss: 24.0248\n",
      "Epoch [246/300], Step [45/172], Loss: 35.3901\n",
      "Epoch [246/300], Step [46/172], Loss: 18.7437\n",
      "Epoch [246/300], Step [47/172], Loss: 52.7532\n",
      "Epoch [246/300], Step [48/172], Loss: 64.2170\n",
      "Epoch [246/300], Step [49/172], Loss: 27.3308\n",
      "Epoch [246/300], Step [50/172], Loss: 49.9488\n",
      "Epoch [246/300], Step [51/172], Loss: 10.5975\n",
      "Epoch [246/300], Step [52/172], Loss: 26.9018\n",
      "Epoch [246/300], Step [53/172], Loss: 28.0087\n",
      "Epoch [246/300], Step [54/172], Loss: 22.0147\n",
      "Epoch [246/300], Step [55/172], Loss: 21.6675\n",
      "Epoch [246/300], Step [56/172], Loss: 18.6923\n",
      "Epoch [246/300], Step [57/172], Loss: 19.6920\n",
      "Epoch [246/300], Step [58/172], Loss: 18.0160\n",
      "Epoch [246/300], Step [59/172], Loss: 31.8704\n",
      "Epoch [246/300], Step [60/172], Loss: 21.8008\n",
      "Epoch [246/300], Step [61/172], Loss: 9.1729\n",
      "Epoch [246/300], Step [62/172], Loss: 17.1287\n",
      "Epoch [246/300], Step [63/172], Loss: 13.5992\n",
      "Epoch [246/300], Step [64/172], Loss: 15.5010\n",
      "Epoch [246/300], Step [65/172], Loss: 23.0955\n",
      "Epoch [246/300], Step [66/172], Loss: 9.4045\n",
      "Epoch [246/300], Step [67/172], Loss: 27.9041\n",
      "Epoch [246/300], Step [68/172], Loss: 8.1795\n",
      "Epoch [246/300], Step [69/172], Loss: 29.0719\n",
      "Epoch [246/300], Step [70/172], Loss: 29.8561\n",
      "Epoch [246/300], Step [71/172], Loss: 32.1596\n",
      "Epoch [246/300], Step [72/172], Loss: 28.6666\n",
      "Epoch [246/300], Step [73/172], Loss: 36.9059\n",
      "Epoch [246/300], Step [74/172], Loss: 20.5727\n",
      "Epoch [246/300], Step [75/172], Loss: 19.9708\n",
      "Epoch [246/300], Step [76/172], Loss: 25.8861\n",
      "Epoch [246/300], Step [77/172], Loss: 40.7022\n",
      "Epoch [246/300], Step [78/172], Loss: 30.2420\n",
      "Epoch [246/300], Step [79/172], Loss: 28.7893\n",
      "Epoch [246/300], Step [80/172], Loss: 45.7366\n",
      "Epoch [246/300], Step [81/172], Loss: 26.1196\n",
      "Epoch [246/300], Step [82/172], Loss: 36.4417\n",
      "Epoch [246/300], Step [83/172], Loss: 39.7622\n",
      "Epoch [246/300], Step [84/172], Loss: 31.0842\n",
      "Epoch [246/300], Step [85/172], Loss: 34.8206\n",
      "Epoch [246/300], Step [86/172], Loss: 32.2630\n",
      "Epoch [246/300], Step [87/172], Loss: 23.0564\n",
      "Epoch [246/300], Step [88/172], Loss: 21.2239\n",
      "Epoch [246/300], Step [89/172], Loss: 28.0558\n",
      "Epoch [246/300], Step [90/172], Loss: 20.8361\n",
      "Epoch [246/300], Step [91/172], Loss: 26.4234\n",
      "Epoch [246/300], Step [92/172], Loss: 19.3929\n",
      "Epoch [246/300], Step [93/172], Loss: 18.7574\n",
      "Epoch [246/300], Step [94/172], Loss: 25.3457\n",
      "Epoch [246/300], Step [95/172], Loss: 22.1184\n",
      "Epoch [246/300], Step [96/172], Loss: 20.5723\n",
      "Epoch [246/300], Step [97/172], Loss: 30.6811\n",
      "Epoch [246/300], Step [98/172], Loss: 20.1458\n",
      "Epoch [246/300], Step [99/172], Loss: 20.5273\n",
      "Epoch [246/300], Step [100/172], Loss: 19.3480\n",
      "Epoch [246/300], Step [101/172], Loss: 21.1131\n",
      "Epoch [246/300], Step [102/172], Loss: 19.8751\n",
      "Epoch [246/300], Step [103/172], Loss: 13.9097\n",
      "Epoch [246/300], Step [104/172], Loss: 21.2260\n",
      "Epoch [246/300], Step [105/172], Loss: 25.9165\n",
      "Epoch [246/300], Step [106/172], Loss: 17.1208\n",
      "Epoch [246/300], Step [107/172], Loss: 17.5658\n",
      "Epoch [246/300], Step [108/172], Loss: 17.9941\n",
      "Epoch [246/300], Step [109/172], Loss: 16.4567\n",
      "Epoch [246/300], Step [110/172], Loss: 19.9451\n",
      "Epoch [246/300], Step [111/172], Loss: 19.3432\n",
      "Epoch [246/300], Step [112/172], Loss: 17.5647\n",
      "Epoch [246/300], Step [113/172], Loss: 16.0781\n",
      "Epoch [246/300], Step [114/172], Loss: 16.2857\n",
      "Epoch [246/300], Step [115/172], Loss: 19.6802\n",
      "Epoch [246/300], Step [116/172], Loss: 15.7610\n",
      "Epoch [246/300], Step [117/172], Loss: 13.9637\n",
      "Epoch [246/300], Step [118/172], Loss: 13.4259\n",
      "Epoch [246/300], Step [119/172], Loss: 19.6827\n",
      "Epoch [246/300], Step [120/172], Loss: 11.6352\n",
      "Epoch [246/300], Step [121/172], Loss: 9.8852\n",
      "Epoch [246/300], Step [122/172], Loss: 14.5737\n",
      "Epoch [246/300], Step [123/172], Loss: 13.8504\n",
      "Epoch [246/300], Step [124/172], Loss: 7.9297\n",
      "Epoch [246/300], Step [125/172], Loss: 13.4060\n",
      "Epoch [246/300], Step [126/172], Loss: 14.0676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [246/300], Step [127/172], Loss: 11.9809\n",
      "Epoch [246/300], Step [128/172], Loss: 11.2120\n",
      "Epoch [246/300], Step [129/172], Loss: 9.3794\n",
      "Epoch [246/300], Step [130/172], Loss: 13.7262\n",
      "Epoch [246/300], Step [131/172], Loss: 8.9521\n",
      "Epoch [246/300], Step [132/172], Loss: 11.2311\n",
      "Epoch [246/300], Step [133/172], Loss: 11.0803\n",
      "Epoch [246/300], Step [134/172], Loss: 11.1727\n",
      "Epoch [246/300], Step [135/172], Loss: 10.0069\n",
      "Epoch [246/300], Step [136/172], Loss: 9.8268\n",
      "Epoch [246/300], Step [137/172], Loss: 9.5129\n",
      "Epoch [246/300], Step [138/172], Loss: 9.6725\n",
      "Epoch [246/300], Step [139/172], Loss: 11.5054\n",
      "Epoch [246/300], Step [140/172], Loss: 12.1214\n",
      "Epoch [246/300], Step [141/172], Loss: 10.1343\n",
      "Epoch [246/300], Step [142/172], Loss: 15.8876\n",
      "Epoch [246/300], Step [143/172], Loss: 12.3049\n",
      "Epoch [246/300], Step [144/172], Loss: 10.2057\n",
      "Epoch [246/300], Step [145/172], Loss: 11.7543\n",
      "Epoch [246/300], Step [146/172], Loss: 12.2386\n",
      "Epoch [246/300], Step [147/172], Loss: 6.2755\n",
      "Epoch [246/300], Step [148/172], Loss: 7.4085\n",
      "Epoch [246/300], Step [149/172], Loss: 7.9154\n",
      "Epoch [246/300], Step [150/172], Loss: 6.4624\n",
      "Epoch [246/300], Step [151/172], Loss: 6.3867\n",
      "Epoch [246/300], Step [152/172], Loss: 9.1510\n",
      "Epoch [246/300], Step [153/172], Loss: 6.8635\n",
      "Epoch [246/300], Step [154/172], Loss: 7.0881\n",
      "Epoch [246/300], Step [155/172], Loss: 7.1550\n",
      "Epoch [246/300], Step [156/172], Loss: 15.3748\n",
      "Epoch [246/300], Step [157/172], Loss: 9.2737\n",
      "Epoch [246/300], Step [158/172], Loss: 8.1656\n",
      "Epoch [246/300], Step [159/172], Loss: 10.8018\n",
      "Epoch [246/300], Step [160/172], Loss: 10.4988\n",
      "Epoch [246/300], Step [161/172], Loss: 9.4500\n",
      "Epoch [246/300], Step [162/172], Loss: 5.5653\n",
      "Epoch [246/300], Step [163/172], Loss: 7.2208\n",
      "Epoch [246/300], Step [164/172], Loss: 9.7891\n",
      "Epoch [246/300], Step [165/172], Loss: 7.6669\n",
      "Epoch [246/300], Step [166/172], Loss: 6.5817\n",
      "Epoch [246/300], Step [167/172], Loss: 11.5505\n",
      "Epoch [246/300], Step [168/172], Loss: 7.4209\n",
      "Epoch [246/300], Step [169/172], Loss: 7.5543\n",
      "Epoch [246/300], Step [170/172], Loss: 6.4655\n",
      "Epoch [246/300], Step [171/172], Loss: 9.7188\n",
      "Epoch [246/300], Step [172/172], Loss: 5.9172\n",
      "Epoch [247/300], Step [1/172], Loss: 43.0571\n",
      "Epoch [247/300], Step [2/172], Loss: 45.0193\n",
      "Epoch [247/300], Step [3/172], Loss: 41.9905\n",
      "Epoch [247/300], Step [4/172], Loss: 20.2041\n",
      "Epoch [247/300], Step [5/172], Loss: 39.4661\n",
      "Epoch [247/300], Step [6/172], Loss: 18.2266\n",
      "Epoch [247/300], Step [7/172], Loss: 24.1035\n",
      "Epoch [247/300], Step [8/172], Loss: 3.6594\n",
      "Epoch [247/300], Step [9/172], Loss: 25.7892\n",
      "Epoch [247/300], Step [10/172], Loss: 37.9736\n",
      "Epoch [247/300], Step [11/172], Loss: 50.8434\n",
      "Epoch [247/300], Step [12/172], Loss: 51.2204\n",
      "Epoch [247/300], Step [13/172], Loss: 30.6414\n",
      "Epoch [247/300], Step [14/172], Loss: 53.7630\n",
      "Epoch [247/300], Step [15/172], Loss: 47.0652\n",
      "Epoch [247/300], Step [16/172], Loss: 10.5121\n",
      "Epoch [247/300], Step [17/172], Loss: 36.7063\n",
      "Epoch [247/300], Step [18/172], Loss: 50.3298\n",
      "Epoch [247/300], Step [19/172], Loss: 69.4618\n",
      "Epoch [247/300], Step [20/172], Loss: 26.0827\n",
      "Epoch [247/300], Step [21/172], Loss: 74.5770\n",
      "Epoch [247/300], Step [22/172], Loss: 51.2829\n",
      "Epoch [247/300], Step [23/172], Loss: 2.0102\n",
      "Epoch [247/300], Step [24/172], Loss: 47.4094\n",
      "Epoch [247/300], Step [25/172], Loss: 34.6079\n",
      "Epoch [247/300], Step [26/172], Loss: 43.3653\n",
      "Epoch [247/300], Step [27/172], Loss: 53.5528\n",
      "Epoch [247/300], Step [28/172], Loss: 18.3284\n",
      "Epoch [247/300], Step [29/172], Loss: 13.5825\n",
      "Epoch [247/300], Step [30/172], Loss: 51.5526\n",
      "Epoch [247/300], Step [31/172], Loss: 31.7160\n",
      "Epoch [247/300], Step [32/172], Loss: 41.5406\n",
      "Epoch [247/300], Step [33/172], Loss: 65.7221\n",
      "Epoch [247/300], Step [34/172], Loss: 1.8920\n",
      "Epoch [247/300], Step [35/172], Loss: 14.6445\n",
      "Epoch [247/300], Step [36/172], Loss: 15.7927\n",
      "Epoch [247/300], Step [37/172], Loss: 16.2411\n",
      "Epoch [247/300], Step [38/172], Loss: 32.2403\n",
      "Epoch [247/300], Step [39/172], Loss: 35.4430\n",
      "Epoch [247/300], Step [40/172], Loss: 23.0651\n",
      "Epoch [247/300], Step [41/172], Loss: 34.7508\n",
      "Epoch [247/300], Step [42/172], Loss: 41.1123\n",
      "Epoch [247/300], Step [43/172], Loss: 29.3339\n",
      "Epoch [247/300], Step [44/172], Loss: 23.6902\n",
      "Epoch [247/300], Step [45/172], Loss: 34.5809\n",
      "Epoch [247/300], Step [46/172], Loss: 18.2517\n",
      "Epoch [247/300], Step [47/172], Loss: 52.0498\n",
      "Epoch [247/300], Step [48/172], Loss: 62.3377\n",
      "Epoch [247/300], Step [49/172], Loss: 26.9633\n",
      "Epoch [247/300], Step [50/172], Loss: 50.7621\n",
      "Epoch [247/300], Step [51/172], Loss: 10.3490\n",
      "Epoch [247/300], Step [52/172], Loss: 26.2961\n",
      "Epoch [247/300], Step [53/172], Loss: 27.4576\n",
      "Epoch [247/300], Step [54/172], Loss: 21.3825\n",
      "Epoch [247/300], Step [55/172], Loss: 21.0155\n",
      "Epoch [247/300], Step [56/172], Loss: 18.6418\n",
      "Epoch [247/300], Step [57/172], Loss: 20.1812\n",
      "Epoch [247/300], Step [58/172], Loss: 17.8597\n",
      "Epoch [247/300], Step [59/172], Loss: 32.3351\n",
      "Epoch [247/300], Step [60/172], Loss: 22.1967\n",
      "Epoch [247/300], Step [61/172], Loss: 9.2012\n",
      "Epoch [247/300], Step [62/172], Loss: 17.0774\n",
      "Epoch [247/300], Step [63/172], Loss: 13.2777\n",
      "Epoch [247/300], Step [64/172], Loss: 15.3012\n",
      "Epoch [247/300], Step [65/172], Loss: 23.1777\n",
      "Epoch [247/300], Step [66/172], Loss: 9.4695\n",
      "Epoch [247/300], Step [67/172], Loss: 27.6579\n",
      "Epoch [247/300], Step [68/172], Loss: 7.3415\n",
      "Epoch [247/300], Step [69/172], Loss: 29.2884\n",
      "Epoch [247/300], Step [70/172], Loss: 29.7207\n",
      "Epoch [247/300], Step [71/172], Loss: 32.1103\n",
      "Epoch [247/300], Step [72/172], Loss: 28.7187\n",
      "Epoch [247/300], Step [73/172], Loss: 36.8022\n",
      "Epoch [247/300], Step [74/172], Loss: 20.6882\n",
      "Epoch [247/300], Step [75/172], Loss: 20.2846\n",
      "Epoch [247/300], Step [76/172], Loss: 25.6475\n",
      "Epoch [247/300], Step [77/172], Loss: 41.0356\n",
      "Epoch [247/300], Step [78/172], Loss: 30.1331\n",
      "Epoch [247/300], Step [79/172], Loss: 28.7870\n",
      "Epoch [247/300], Step [80/172], Loss: 45.4635\n",
      "Epoch [247/300], Step [81/172], Loss: 25.8761\n",
      "Epoch [247/300], Step [82/172], Loss: 35.8649\n",
      "Epoch [247/300], Step [83/172], Loss: 39.6365\n",
      "Epoch [247/300], Step [84/172], Loss: 31.0147\n",
      "Epoch [247/300], Step [85/172], Loss: 34.6450\n",
      "Epoch [247/300], Step [86/172], Loss: 32.0277\n",
      "Epoch [247/300], Step [87/172], Loss: 22.9378\n",
      "Epoch [247/300], Step [88/172], Loss: 21.1266\n",
      "Epoch [247/300], Step [89/172], Loss: 27.9944\n",
      "Epoch [247/300], Step [90/172], Loss: 20.4880\n",
      "Epoch [247/300], Step [91/172], Loss: 26.4232\n",
      "Epoch [247/300], Step [92/172], Loss: 19.3851\n",
      "Epoch [247/300], Step [93/172], Loss: 18.7360\n",
      "Epoch [247/300], Step [94/172], Loss: 25.2883\n",
      "Epoch [247/300], Step [95/172], Loss: 21.8982\n",
      "Epoch [247/300], Step [96/172], Loss: 20.5058\n",
      "Epoch [247/300], Step [97/172], Loss: 30.6480\n",
      "Epoch [247/300], Step [98/172], Loss: 19.9919\n",
      "Epoch [247/300], Step [99/172], Loss: 20.4498\n",
      "Epoch [247/300], Step [100/172], Loss: 19.1401\n",
      "Epoch [247/300], Step [101/172], Loss: 20.9585\n",
      "Epoch [247/300], Step [102/172], Loss: 19.6276\n",
      "Epoch [247/300], Step [103/172], Loss: 13.7409\n",
      "Epoch [247/300], Step [104/172], Loss: 21.1386\n",
      "Epoch [247/300], Step [105/172], Loss: 25.4094\n",
      "Epoch [247/300], Step [106/172], Loss: 17.0112\n",
      "Epoch [247/300], Step [107/172], Loss: 17.5310\n",
      "Epoch [247/300], Step [108/172], Loss: 17.8320\n",
      "Epoch [247/300], Step [109/172], Loss: 16.3069\n",
      "Epoch [247/300], Step [110/172], Loss: 19.8062\n",
      "Epoch [247/300], Step [111/172], Loss: 19.2532\n",
      "Epoch [247/300], Step [112/172], Loss: 17.4760\n",
      "Epoch [247/300], Step [113/172], Loss: 16.0921\n",
      "Epoch [247/300], Step [114/172], Loss: 16.2196\n",
      "Epoch [247/300], Step [115/172], Loss: 19.9365\n",
      "Epoch [247/300], Step [116/172], Loss: 15.7634\n",
      "Epoch [247/300], Step [117/172], Loss: 13.9471\n",
      "Epoch [247/300], Step [118/172], Loss: 13.5862\n",
      "Epoch [247/300], Step [119/172], Loss: 19.7212\n",
      "Epoch [247/300], Step [120/172], Loss: 11.5233\n",
      "Epoch [247/300], Step [121/172], Loss: 9.8722\n",
      "Epoch [247/300], Step [122/172], Loss: 14.3475\n",
      "Epoch [247/300], Step [123/172], Loss: 13.6195\n",
      "Epoch [247/300], Step [124/172], Loss: 7.9200\n",
      "Epoch [247/300], Step [125/172], Loss: 13.4284\n",
      "Epoch [247/300], Step [126/172], Loss: 13.9231\n",
      "Epoch [247/300], Step [127/172], Loss: 11.9638\n",
      "Epoch [247/300], Step [128/172], Loss: 11.1322\n",
      "Epoch [247/300], Step [129/172], Loss: 9.3990\n",
      "Epoch [247/300], Step [130/172], Loss: 13.8022\n",
      "Epoch [247/300], Step [131/172], Loss: 8.8900\n",
      "Epoch [247/300], Step [132/172], Loss: 11.2512\n",
      "Epoch [247/300], Step [133/172], Loss: 10.9839\n",
      "Epoch [247/300], Step [134/172], Loss: 11.1863\n",
      "Epoch [247/300], Step [135/172], Loss: 9.9953\n",
      "Epoch [247/300], Step [136/172], Loss: 9.7642\n",
      "Epoch [247/300], Step [137/172], Loss: 9.4486\n",
      "Epoch [247/300], Step [138/172], Loss: 9.7296\n",
      "Epoch [247/300], Step [139/172], Loss: 11.5149\n",
      "Epoch [247/300], Step [140/172], Loss: 12.1449\n",
      "Epoch [247/300], Step [141/172], Loss: 10.0786\n",
      "Epoch [247/300], Step [142/172], Loss: 15.6982\n",
      "Epoch [247/300], Step [143/172], Loss: 12.4268\n",
      "Epoch [247/300], Step [144/172], Loss: 10.3023\n",
      "Epoch [247/300], Step [145/172], Loss: 11.7733\n",
      "Epoch [247/300], Step [146/172], Loss: 12.1713\n",
      "Epoch [247/300], Step [147/172], Loss: 6.2723\n",
      "Epoch [247/300], Step [148/172], Loss: 7.3611\n",
      "Epoch [247/300], Step [149/172], Loss: 7.8327\n",
      "Epoch [247/300], Step [150/172], Loss: 6.4951\n",
      "Epoch [247/300], Step [151/172], Loss: 6.4329\n",
      "Epoch [247/300], Step [152/172], Loss: 9.1312\n",
      "Epoch [247/300], Step [153/172], Loss: 6.8832\n",
      "Epoch [247/300], Step [154/172], Loss: 7.0452\n",
      "Epoch [247/300], Step [155/172], Loss: 7.1329\n",
      "Epoch [247/300], Step [156/172], Loss: 15.3376\n",
      "Epoch [247/300], Step [157/172], Loss: 9.2992\n",
      "Epoch [247/300], Step [158/172], Loss: 8.0966\n",
      "Epoch [247/300], Step [159/172], Loss: 10.7442\n",
      "Epoch [247/300], Step [160/172], Loss: 10.5663\n",
      "Epoch [247/300], Step [161/172], Loss: 9.2207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [247/300], Step [162/172], Loss: 5.5326\n",
      "Epoch [247/300], Step [163/172], Loss: 7.2951\n",
      "Epoch [247/300], Step [164/172], Loss: 9.5363\n",
      "Epoch [247/300], Step [165/172], Loss: 7.6182\n",
      "Epoch [247/300], Step [166/172], Loss: 6.5832\n",
      "Epoch [247/300], Step [167/172], Loss: 11.6539\n",
      "Epoch [247/300], Step [168/172], Loss: 7.4084\n",
      "Epoch [247/300], Step [169/172], Loss: 7.5508\n",
      "Epoch [247/300], Step [170/172], Loss: 6.4285\n",
      "Epoch [247/300], Step [171/172], Loss: 9.6134\n",
      "Epoch [247/300], Step [172/172], Loss: 5.8720\n",
      "Epoch [248/300], Step [1/172], Loss: 42.7835\n",
      "Epoch [248/300], Step [2/172], Loss: 44.8834\n",
      "Epoch [248/300], Step [3/172], Loss: 41.4426\n",
      "Epoch [248/300], Step [4/172], Loss: 20.0355\n",
      "Epoch [248/300], Step [5/172], Loss: 38.5922\n",
      "Epoch [248/300], Step [6/172], Loss: 18.1153\n",
      "Epoch [248/300], Step [7/172], Loss: 24.6369\n",
      "Epoch [248/300], Step [8/172], Loss: 4.5918\n",
      "Epoch [248/300], Step [9/172], Loss: 25.9203\n",
      "Epoch [248/300], Step [10/172], Loss: 37.2611\n",
      "Epoch [248/300], Step [11/172], Loss: 50.6347\n",
      "Epoch [248/300], Step [12/172], Loss: 51.0428\n",
      "Epoch [248/300], Step [13/172], Loss: 30.3774\n",
      "Epoch [248/300], Step [14/172], Loss: 53.1566\n",
      "Epoch [248/300], Step [15/172], Loss: 46.8667\n",
      "Epoch [248/300], Step [16/172], Loss: 8.7592\n",
      "Epoch [248/300], Step [17/172], Loss: 36.7258\n",
      "Epoch [248/300], Step [18/172], Loss: 50.3805\n",
      "Epoch [248/300], Step [19/172], Loss: 69.2002\n",
      "Epoch [248/300], Step [20/172], Loss: 26.4328\n",
      "Epoch [248/300], Step [21/172], Loss: 73.9842\n",
      "Epoch [248/300], Step [22/172], Loss: 50.0964\n",
      "Epoch [248/300], Step [23/172], Loss: 1.8014\n",
      "Epoch [248/300], Step [24/172], Loss: 47.3548\n",
      "Epoch [248/300], Step [25/172], Loss: 34.1855\n",
      "Epoch [248/300], Step [26/172], Loss: 43.1964\n",
      "Epoch [248/300], Step [27/172], Loss: 53.7581\n",
      "Epoch [248/300], Step [28/172], Loss: 18.1150\n",
      "Epoch [248/300], Step [29/172], Loss: 13.7740\n",
      "Epoch [248/300], Step [30/172], Loss: 51.2914\n",
      "Epoch [248/300], Step [31/172], Loss: 31.6564\n",
      "Epoch [248/300], Step [32/172], Loss: 41.4859\n",
      "Epoch [248/300], Step [33/172], Loss: 65.4888\n",
      "Epoch [248/300], Step [34/172], Loss: 2.2944\n",
      "Epoch [248/300], Step [35/172], Loss: 14.7136\n",
      "Epoch [248/300], Step [36/172], Loss: 16.0101\n",
      "Epoch [248/300], Step [37/172], Loss: 16.0787\n",
      "Epoch [248/300], Step [38/172], Loss: 31.8818\n",
      "Epoch [248/300], Step [39/172], Loss: 35.3536\n",
      "Epoch [248/300], Step [40/172], Loss: 23.0053\n",
      "Epoch [248/300], Step [41/172], Loss: 34.3486\n",
      "Epoch [248/300], Step [42/172], Loss: 40.7670\n",
      "Epoch [248/300], Step [43/172], Loss: 29.0623\n",
      "Epoch [248/300], Step [44/172], Loss: 23.8577\n",
      "Epoch [248/300], Step [45/172], Loss: 34.7190\n",
      "Epoch [248/300], Step [46/172], Loss: 17.9861\n",
      "Epoch [248/300], Step [47/172], Loss: 52.1678\n",
      "Epoch [248/300], Step [48/172], Loss: 62.9059\n",
      "Epoch [248/300], Step [49/172], Loss: 27.0965\n",
      "Epoch [248/300], Step [50/172], Loss: 50.8023\n",
      "Epoch [248/300], Step [51/172], Loss: 10.3488\n",
      "Epoch [248/300], Step [52/172], Loss: 26.4754\n",
      "Epoch [248/300], Step [53/172], Loss: 27.5066\n",
      "Epoch [248/300], Step [54/172], Loss: 21.2009\n",
      "Epoch [248/300], Step [55/172], Loss: 20.8493\n",
      "Epoch [248/300], Step [56/172], Loss: 18.2004\n",
      "Epoch [248/300], Step [57/172], Loss: 20.5939\n",
      "Epoch [248/300], Step [58/172], Loss: 17.7454\n",
      "Epoch [248/300], Step [59/172], Loss: 31.8200\n",
      "Epoch [248/300], Step [60/172], Loss: 22.1292\n",
      "Epoch [248/300], Step [61/172], Loss: 9.1012\n",
      "Epoch [248/300], Step [62/172], Loss: 16.6627\n",
      "Epoch [248/300], Step [63/172], Loss: 13.1923\n",
      "Epoch [248/300], Step [64/172], Loss: 15.2081\n",
      "Epoch [248/300], Step [65/172], Loss: 23.0988\n",
      "Epoch [248/300], Step [66/172], Loss: 9.5403\n",
      "Epoch [248/300], Step [67/172], Loss: 27.6188\n",
      "Epoch [248/300], Step [68/172], Loss: 7.2914\n",
      "Epoch [248/300], Step [69/172], Loss: 29.2605\n",
      "Epoch [248/300], Step [70/172], Loss: 29.7549\n",
      "Epoch [248/300], Step [71/172], Loss: 32.3502\n",
      "Epoch [248/300], Step [72/172], Loss: 28.8540\n",
      "Epoch [248/300], Step [73/172], Loss: 36.9803\n",
      "Epoch [248/300], Step [74/172], Loss: 20.5861\n",
      "Epoch [248/300], Step [75/172], Loss: 20.3343\n",
      "Epoch [248/300], Step [76/172], Loss: 25.4157\n",
      "Epoch [248/300], Step [77/172], Loss: 41.1685\n",
      "Epoch [248/300], Step [78/172], Loss: 30.1367\n",
      "Epoch [248/300], Step [79/172], Loss: 28.8801\n",
      "Epoch [248/300], Step [80/172], Loss: 45.8408\n",
      "Epoch [248/300], Step [81/172], Loss: 26.0396\n",
      "Epoch [248/300], Step [82/172], Loss: 36.6374\n",
      "Epoch [248/300], Step [83/172], Loss: 39.4099\n",
      "Epoch [248/300], Step [84/172], Loss: 30.8681\n",
      "Epoch [248/300], Step [85/172], Loss: 34.4639\n",
      "Epoch [248/300], Step [86/172], Loss: 31.5198\n",
      "Epoch [248/300], Step [87/172], Loss: 22.9161\n",
      "Epoch [248/300], Step [88/172], Loss: 20.9132\n",
      "Epoch [248/300], Step [89/172], Loss: 27.5655\n",
      "Epoch [248/300], Step [90/172], Loss: 20.2847\n",
      "Epoch [248/300], Step [91/172], Loss: 26.3837\n",
      "Epoch [248/300], Step [92/172], Loss: 19.2524\n",
      "Epoch [248/300], Step [93/172], Loss: 18.5643\n",
      "Epoch [248/300], Step [94/172], Loss: 25.0997\n",
      "Epoch [248/300], Step [95/172], Loss: 21.4058\n",
      "Epoch [248/300], Step [96/172], Loss: 20.5173\n",
      "Epoch [248/300], Step [97/172], Loss: 30.8016\n",
      "Epoch [248/300], Step [98/172], Loss: 19.8721\n",
      "Epoch [248/300], Step [99/172], Loss: 20.2802\n",
      "Epoch [248/300], Step [100/172], Loss: 19.0592\n",
      "Epoch [248/300], Step [101/172], Loss: 20.8387\n",
      "Epoch [248/300], Step [102/172], Loss: 19.7940\n",
      "Epoch [248/300], Step [103/172], Loss: 13.5694\n",
      "Epoch [248/300], Step [104/172], Loss: 21.1290\n",
      "Epoch [248/300], Step [105/172], Loss: 25.5100\n",
      "Epoch [248/300], Step [106/172], Loss: 16.9380\n",
      "Epoch [248/300], Step [107/172], Loss: 17.5372\n",
      "Epoch [248/300], Step [108/172], Loss: 17.6342\n",
      "Epoch [248/300], Step [109/172], Loss: 16.2424\n",
      "Epoch [248/300], Step [110/172], Loss: 19.5963\n",
      "Epoch [248/300], Step [111/172], Loss: 19.1804\n",
      "Epoch [248/300], Step [112/172], Loss: 17.4848\n",
      "Epoch [248/300], Step [113/172], Loss: 15.9767\n",
      "Epoch [248/300], Step [114/172], Loss: 16.2329\n",
      "Epoch [248/300], Step [115/172], Loss: 19.9571\n",
      "Epoch [248/300], Step [116/172], Loss: 15.6173\n",
      "Epoch [248/300], Step [117/172], Loss: 13.9096\n",
      "Epoch [248/300], Step [118/172], Loss: 13.6493\n",
      "Epoch [248/300], Step [119/172], Loss: 19.6430\n",
      "Epoch [248/300], Step [120/172], Loss: 11.3916\n",
      "Epoch [248/300], Step [121/172], Loss: 9.8373\n",
      "Epoch [248/300], Step [122/172], Loss: 14.0671\n",
      "Epoch [248/300], Step [123/172], Loss: 13.4245\n",
      "Epoch [248/300], Step [124/172], Loss: 7.9131\n",
      "Epoch [248/300], Step [125/172], Loss: 13.3630\n",
      "Epoch [248/300], Step [126/172], Loss: 13.8229\n",
      "Epoch [248/300], Step [127/172], Loss: 11.9306\n",
      "Epoch [248/300], Step [128/172], Loss: 11.1195\n",
      "Epoch [248/300], Step [129/172], Loss: 9.4595\n",
      "Epoch [248/300], Step [130/172], Loss: 13.8490\n",
      "Epoch [248/300], Step [131/172], Loss: 8.7753\n",
      "Epoch [248/300], Step [132/172], Loss: 11.1621\n",
      "Epoch [248/300], Step [133/172], Loss: 10.7266\n",
      "Epoch [248/300], Step [134/172], Loss: 10.9896\n",
      "Epoch [248/300], Step [135/172], Loss: 9.9400\n",
      "Epoch [248/300], Step [136/172], Loss: 9.7040\n",
      "Epoch [248/300], Step [137/172], Loss: 9.2421\n",
      "Epoch [248/300], Step [138/172], Loss: 9.5052\n",
      "Epoch [248/300], Step [139/172], Loss: 11.4486\n",
      "Epoch [248/300], Step [140/172], Loss: 12.0804\n",
      "Epoch [248/300], Step [141/172], Loss: 9.9898\n",
      "Epoch [248/300], Step [142/172], Loss: 15.3879\n",
      "Epoch [248/300], Step [143/172], Loss: 12.4041\n",
      "Epoch [248/300], Step [144/172], Loss: 10.1457\n",
      "Epoch [248/300], Step [145/172], Loss: 11.6630\n",
      "Epoch [248/300], Step [146/172], Loss: 12.0605\n",
      "Epoch [248/300], Step [147/172], Loss: 6.2833\n",
      "Epoch [248/300], Step [148/172], Loss: 7.3208\n",
      "Epoch [248/300], Step [149/172], Loss: 7.8538\n",
      "Epoch [248/300], Step [150/172], Loss: 6.3734\n",
      "Epoch [248/300], Step [151/172], Loss: 6.2544\n",
      "Epoch [248/300], Step [152/172], Loss: 9.0691\n",
      "Epoch [248/300], Step [153/172], Loss: 6.8428\n",
      "Epoch [248/300], Step [154/172], Loss: 6.9924\n",
      "Epoch [248/300], Step [155/172], Loss: 7.1943\n",
      "Epoch [248/300], Step [156/172], Loss: 15.1092\n",
      "Epoch [248/300], Step [157/172], Loss: 9.2336\n",
      "Epoch [248/300], Step [158/172], Loss: 7.9927\n",
      "Epoch [248/300], Step [159/172], Loss: 10.6236\n",
      "Epoch [248/300], Step [160/172], Loss: 10.3452\n",
      "Epoch [248/300], Step [161/172], Loss: 9.3957\n",
      "Epoch [248/300], Step [162/172], Loss: 5.4784\n",
      "Epoch [248/300], Step [163/172], Loss: 7.1306\n",
      "Epoch [248/300], Step [164/172], Loss: 9.6150\n",
      "Epoch [248/300], Step [165/172], Loss: 7.5897\n",
      "Epoch [248/300], Step [166/172], Loss: 6.3394\n",
      "Epoch [248/300], Step [167/172], Loss: 11.6274\n",
      "Epoch [248/300], Step [168/172], Loss: 7.3360\n",
      "Epoch [248/300], Step [169/172], Loss: 7.4960\n",
      "Epoch [248/300], Step [170/172], Loss: 6.3613\n",
      "Epoch [248/300], Step [171/172], Loss: 9.6391\n",
      "Epoch [248/300], Step [172/172], Loss: 5.9854\n",
      "Epoch [249/300], Step [1/172], Loss: 42.3297\n",
      "Epoch [249/300], Step [2/172], Loss: 45.2227\n",
      "Epoch [249/300], Step [3/172], Loss: 41.4197\n",
      "Epoch [249/300], Step [4/172], Loss: 19.8057\n",
      "Epoch [249/300], Step [5/172], Loss: 37.7564\n",
      "Epoch [249/300], Step [6/172], Loss: 18.3158\n",
      "Epoch [249/300], Step [7/172], Loss: 27.0631\n",
      "Epoch [249/300], Step [8/172], Loss: 4.0245\n",
      "Epoch [249/300], Step [9/172], Loss: 25.6930\n",
      "Epoch [249/300], Step [10/172], Loss: 37.3040\n",
      "Epoch [249/300], Step [11/172], Loss: 50.5250\n",
      "Epoch [249/300], Step [12/172], Loss: 50.8377\n",
      "Epoch [249/300], Step [13/172], Loss: 30.0031\n",
      "Epoch [249/300], Step [14/172], Loss: 53.4718\n",
      "Epoch [249/300], Step [15/172], Loss: 47.0407\n",
      "Epoch [249/300], Step [16/172], Loss: 8.4997\n",
      "Epoch [249/300], Step [17/172], Loss: 36.8563\n",
      "Epoch [249/300], Step [18/172], Loss: 50.6517\n",
      "Epoch [249/300], Step [19/172], Loss: 69.0712\n",
      "Epoch [249/300], Step [20/172], Loss: 26.5679\n",
      "Epoch [249/300], Step [21/172], Loss: 73.6140\n",
      "Epoch [249/300], Step [22/172], Loss: 50.5712\n",
      "Epoch [249/300], Step [23/172], Loss: 1.5357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [249/300], Step [24/172], Loss: 47.0803\n",
      "Epoch [249/300], Step [25/172], Loss: 33.3515\n",
      "Epoch [249/300], Step [26/172], Loss: 43.1418\n",
      "Epoch [249/300], Step [27/172], Loss: 54.6265\n",
      "Epoch [249/300], Step [28/172], Loss: 18.0520\n",
      "Epoch [249/300], Step [29/172], Loss: 13.6867\n",
      "Epoch [249/300], Step [30/172], Loss: 50.4801\n",
      "Epoch [249/300], Step [31/172], Loss: 31.5323\n",
      "Epoch [249/300], Step [32/172], Loss: 41.1668\n",
      "Epoch [249/300], Step [33/172], Loss: 64.3855\n",
      "Epoch [249/300], Step [34/172], Loss: 2.1414\n",
      "Epoch [249/300], Step [35/172], Loss: 14.7696\n",
      "Epoch [249/300], Step [36/172], Loss: 16.0388\n",
      "Epoch [249/300], Step [37/172], Loss: 15.8561\n",
      "Epoch [249/300], Step [38/172], Loss: 31.6115\n",
      "Epoch [249/300], Step [39/172], Loss: 35.4068\n",
      "Epoch [249/300], Step [40/172], Loss: 22.9611\n",
      "Epoch [249/300], Step [41/172], Loss: 34.0884\n",
      "Epoch [249/300], Step [42/172], Loss: 40.4808\n",
      "Epoch [249/300], Step [43/172], Loss: 28.8990\n",
      "Epoch [249/300], Step [44/172], Loss: 23.6939\n",
      "Epoch [249/300], Step [45/172], Loss: 34.6862\n",
      "Epoch [249/300], Step [46/172], Loss: 18.1700\n",
      "Epoch [249/300], Step [47/172], Loss: 52.2165\n",
      "Epoch [249/300], Step [48/172], Loss: 63.4039\n",
      "Epoch [249/300], Step [49/172], Loss: 26.7786\n",
      "Epoch [249/300], Step [50/172], Loss: 51.5135\n",
      "Epoch [249/300], Step [51/172], Loss: 10.3049\n",
      "Epoch [249/300], Step [52/172], Loss: 26.4746\n",
      "Epoch [249/300], Step [53/172], Loss: 27.3954\n",
      "Epoch [249/300], Step [54/172], Loss: 21.0745\n",
      "Epoch [249/300], Step [55/172], Loss: 20.7078\n",
      "Epoch [249/300], Step [56/172], Loss: 18.1942\n",
      "Epoch [249/300], Step [57/172], Loss: 20.2153\n",
      "Epoch [249/300], Step [58/172], Loss: 17.8136\n",
      "Epoch [249/300], Step [59/172], Loss: 32.3207\n",
      "Epoch [249/300], Step [60/172], Loss: 22.5084\n",
      "Epoch [249/300], Step [61/172], Loss: 9.1778\n",
      "Epoch [249/300], Step [62/172], Loss: 16.8037\n",
      "Epoch [249/300], Step [63/172], Loss: 13.3404\n",
      "Epoch [249/300], Step [64/172], Loss: 15.2623\n",
      "Epoch [249/300], Step [65/172], Loss: 23.2515\n",
      "Epoch [249/300], Step [66/172], Loss: 9.6238\n",
      "Epoch [249/300], Step [67/172], Loss: 28.0665\n",
      "Epoch [249/300], Step [68/172], Loss: 7.6874\n",
      "Epoch [249/300], Step [69/172], Loss: 29.5439\n",
      "Epoch [249/300], Step [70/172], Loss: 29.6323\n",
      "Epoch [249/300], Step [71/172], Loss: 32.2435\n",
      "Epoch [249/300], Step [72/172], Loss: 28.7563\n",
      "Epoch [249/300], Step [73/172], Loss: 36.9047\n",
      "Epoch [249/300], Step [74/172], Loss: 20.5696\n",
      "Epoch [249/300], Step [75/172], Loss: 20.2013\n",
      "Epoch [249/300], Step [76/172], Loss: 25.4635\n",
      "Epoch [249/300], Step [77/172], Loss: 41.0267\n",
      "Epoch [249/300], Step [78/172], Loss: 30.1133\n",
      "Epoch [249/300], Step [79/172], Loss: 28.8103\n",
      "Epoch [249/300], Step [80/172], Loss: 45.6405\n",
      "Epoch [249/300], Step [81/172], Loss: 26.0764\n",
      "Epoch [249/300], Step [82/172], Loss: 36.5334\n",
      "Epoch [249/300], Step [83/172], Loss: 39.4658\n",
      "Epoch [249/300], Step [84/172], Loss: 30.8478\n",
      "Epoch [249/300], Step [85/172], Loss: 34.5386\n",
      "Epoch [249/300], Step [86/172], Loss: 31.6211\n",
      "Epoch [249/300], Step [87/172], Loss: 22.8436\n",
      "Epoch [249/300], Step [88/172], Loss: 21.0494\n",
      "Epoch [249/300], Step [89/172], Loss: 27.6724\n",
      "Epoch [249/300], Step [90/172], Loss: 20.2266\n",
      "Epoch [249/300], Step [91/172], Loss: 26.2818\n",
      "Epoch [249/300], Step [92/172], Loss: 19.2674\n",
      "Epoch [249/300], Step [93/172], Loss: 18.6386\n",
      "Epoch [249/300], Step [94/172], Loss: 25.1438\n",
      "Epoch [249/300], Step [95/172], Loss: 21.2957\n",
      "Epoch [249/300], Step [96/172], Loss: 20.5646\n",
      "Epoch [249/300], Step [97/172], Loss: 30.7433\n",
      "Epoch [249/300], Step [98/172], Loss: 19.9155\n",
      "Epoch [249/300], Step [99/172], Loss: 20.3590\n",
      "Epoch [249/300], Step [100/172], Loss: 19.1349\n",
      "Epoch [249/300], Step [101/172], Loss: 20.8710\n",
      "Epoch [249/300], Step [102/172], Loss: 19.7138\n",
      "Epoch [249/300], Step [103/172], Loss: 13.6173\n",
      "Epoch [249/300], Step [104/172], Loss: 21.0856\n",
      "Epoch [249/300], Step [105/172], Loss: 25.4210\n",
      "Epoch [249/300], Step [106/172], Loss: 17.0240\n",
      "Epoch [249/300], Step [107/172], Loss: 17.4204\n",
      "Epoch [249/300], Step [108/172], Loss: 17.6090\n",
      "Epoch [249/300], Step [109/172], Loss: 16.2857\n",
      "Epoch [249/300], Step [110/172], Loss: 19.5988\n",
      "Epoch [249/300], Step [111/172], Loss: 19.2060\n",
      "Epoch [249/300], Step [112/172], Loss: 17.3867\n",
      "Epoch [249/300], Step [113/172], Loss: 15.8672\n",
      "Epoch [249/300], Step [114/172], Loss: 16.2554\n",
      "Epoch [249/300], Step [115/172], Loss: 20.0718\n",
      "Epoch [249/300], Step [116/172], Loss: 15.5808\n",
      "Epoch [249/300], Step [117/172], Loss: 13.9040\n",
      "Epoch [249/300], Step [118/172], Loss: 13.5676\n",
      "Epoch [249/300], Step [119/172], Loss: 19.6501\n",
      "Epoch [249/300], Step [120/172], Loss: 11.3620\n",
      "Epoch [249/300], Step [121/172], Loss: 9.8110\n",
      "Epoch [249/300], Step [122/172], Loss: 14.0636\n",
      "Epoch [249/300], Step [123/172], Loss: 13.5089\n",
      "Epoch [249/300], Step [124/172], Loss: 7.8567\n",
      "Epoch [249/300], Step [125/172], Loss: 13.3603\n",
      "Epoch [249/300], Step [126/172], Loss: 13.8267\n",
      "Epoch [249/300], Step [127/172], Loss: 11.8966\n",
      "Epoch [249/300], Step [128/172], Loss: 11.0709\n",
      "Epoch [249/300], Step [129/172], Loss: 9.4863\n",
      "Epoch [249/300], Step [130/172], Loss: 13.7895\n",
      "Epoch [249/300], Step [131/172], Loss: 8.7817\n",
      "Epoch [249/300], Step [132/172], Loss: 11.2536\n",
      "Epoch [249/300], Step [133/172], Loss: 10.6879\n",
      "Epoch [249/300], Step [134/172], Loss: 10.9405\n",
      "Epoch [249/300], Step [135/172], Loss: 9.9859\n",
      "Epoch [249/300], Step [136/172], Loss: 9.6976\n",
      "Epoch [249/300], Step [137/172], Loss: 9.3671\n",
      "Epoch [249/300], Step [138/172], Loss: 9.5957\n",
      "Epoch [249/300], Step [139/172], Loss: 11.6083\n",
      "Epoch [249/300], Step [140/172], Loss: 12.1395\n",
      "Epoch [249/300], Step [141/172], Loss: 10.0692\n",
      "Epoch [249/300], Step [142/172], Loss: 15.4198\n",
      "Epoch [249/300], Step [143/172], Loss: 12.4436\n",
      "Epoch [249/300], Step [144/172], Loss: 10.2331\n",
      "Epoch [249/300], Step [145/172], Loss: 11.7791\n",
      "Epoch [249/300], Step [146/172], Loss: 12.0631\n",
      "Epoch [249/300], Step [147/172], Loss: 6.2644\n",
      "Epoch [249/300], Step [148/172], Loss: 7.3397\n",
      "Epoch [249/300], Step [149/172], Loss: 7.8256\n",
      "Epoch [249/300], Step [150/172], Loss: 6.4092\n",
      "Epoch [249/300], Step [151/172], Loss: 6.2867\n",
      "Epoch [249/300], Step [152/172], Loss: 9.1185\n",
      "Epoch [249/300], Step [153/172], Loss: 6.7869\n",
      "Epoch [249/300], Step [154/172], Loss: 7.0359\n",
      "Epoch [249/300], Step [155/172], Loss: 7.2021\n",
      "Epoch [249/300], Step [156/172], Loss: 15.2778\n",
      "Epoch [249/300], Step [157/172], Loss: 9.2106\n",
      "Epoch [249/300], Step [158/172], Loss: 8.0241\n",
      "Epoch [249/300], Step [159/172], Loss: 10.4976\n",
      "Epoch [249/300], Step [160/172], Loss: 10.4100\n",
      "Epoch [249/300], Step [161/172], Loss: 9.3846\n",
      "Epoch [249/300], Step [162/172], Loss: 5.4969\n",
      "Epoch [249/300], Step [163/172], Loss: 7.3941\n",
      "Epoch [249/300], Step [164/172], Loss: 9.5444\n",
      "Epoch [249/300], Step [165/172], Loss: 7.6321\n",
      "Epoch [249/300], Step [166/172], Loss: 6.5401\n",
      "Epoch [249/300], Step [167/172], Loss: 11.6433\n",
      "Epoch [249/300], Step [168/172], Loss: 7.3686\n",
      "Epoch [249/300], Step [169/172], Loss: 7.6363\n",
      "Epoch [249/300], Step [170/172], Loss: 6.3583\n",
      "Epoch [249/300], Step [171/172], Loss: 10.0371\n",
      "Epoch [249/300], Step [172/172], Loss: 5.9199\n",
      "Epoch [250/300], Step [1/172], Loss: 42.2676\n",
      "Epoch [250/300], Step [2/172], Loss: 45.1903\n",
      "Epoch [250/300], Step [3/172], Loss: 40.3592\n",
      "Epoch [250/300], Step [4/172], Loss: 19.6792\n",
      "Epoch [250/300], Step [5/172], Loss: 37.8027\n",
      "Epoch [250/300], Step [6/172], Loss: 18.4110\n",
      "Epoch [250/300], Step [7/172], Loss: 26.9561\n",
      "Epoch [250/300], Step [8/172], Loss: 4.1362\n",
      "Epoch [250/300], Step [9/172], Loss: 25.5299\n",
      "Epoch [250/300], Step [10/172], Loss: 36.7466\n",
      "Epoch [250/300], Step [11/172], Loss: 50.3031\n",
      "Epoch [250/300], Step [12/172], Loss: 50.3318\n",
      "Epoch [250/300], Step [13/172], Loss: 30.3343\n",
      "Epoch [250/300], Step [14/172], Loss: 52.6344\n",
      "Epoch [250/300], Step [15/172], Loss: 46.6973\n",
      "Epoch [250/300], Step [16/172], Loss: 8.4172\n",
      "Epoch [250/300], Step [17/172], Loss: 36.6965\n",
      "Epoch [250/300], Step [18/172], Loss: 50.2557\n",
      "Epoch [250/300], Step [19/172], Loss: 69.1971\n",
      "Epoch [250/300], Step [20/172], Loss: 24.9624\n",
      "Epoch [250/300], Step [21/172], Loss: 73.1312\n",
      "Epoch [250/300], Step [22/172], Loss: 50.1902\n",
      "Epoch [250/300], Step [23/172], Loss: 1.5429\n",
      "Epoch [250/300], Step [24/172], Loss: 46.7949\n",
      "Epoch [250/300], Step [25/172], Loss: 33.2329\n",
      "Epoch [250/300], Step [26/172], Loss: 42.8553\n",
      "Epoch [250/300], Step [27/172], Loss: 54.4765\n",
      "Epoch [250/300], Step [28/172], Loss: 17.8264\n",
      "Epoch [250/300], Step [29/172], Loss: 13.6207\n",
      "Epoch [250/300], Step [30/172], Loss: 49.5061\n",
      "Epoch [250/300], Step [31/172], Loss: 30.6752\n",
      "Epoch [250/300], Step [32/172], Loss: 41.0087\n",
      "Epoch [250/300], Step [33/172], Loss: 64.8115\n",
      "Epoch [250/300], Step [34/172], Loss: 1.9343\n",
      "Epoch [250/300], Step [35/172], Loss: 14.6739\n",
      "Epoch [250/300], Step [36/172], Loss: 15.8434\n",
      "Epoch [250/300], Step [37/172], Loss: 15.8000\n",
      "Epoch [250/300], Step [38/172], Loss: 31.6173\n",
      "Epoch [250/300], Step [39/172], Loss: 35.1745\n",
      "Epoch [250/300], Step [40/172], Loss: 22.5085\n",
      "Epoch [250/300], Step [41/172], Loss: 33.8189\n",
      "Epoch [250/300], Step [42/172], Loss: 39.9724\n",
      "Epoch [250/300], Step [43/172], Loss: 28.6325\n",
      "Epoch [250/300], Step [44/172], Loss: 23.3204\n",
      "Epoch [250/300], Step [45/172], Loss: 33.7144\n",
      "Epoch [250/300], Step [46/172], Loss: 18.4687\n",
      "Epoch [250/300], Step [47/172], Loss: 51.7041\n",
      "Epoch [250/300], Step [48/172], Loss: 63.7041\n",
      "Epoch [250/300], Step [49/172], Loss: 26.5625\n",
      "Epoch [250/300], Step [50/172], Loss: 50.6002\n",
      "Epoch [250/300], Step [51/172], Loss: 9.9869\n",
      "Epoch [250/300], Step [52/172], Loss: 26.0870\n",
      "Epoch [250/300], Step [53/172], Loss: 27.0222\n",
      "Epoch [250/300], Step [54/172], Loss: 20.7713\n",
      "Epoch [250/300], Step [55/172], Loss: 20.3872\n",
      "Epoch [250/300], Step [56/172], Loss: 18.0586\n",
      "Epoch [250/300], Step [57/172], Loss: 20.2503\n",
      "Epoch [250/300], Step [58/172], Loss: 17.6017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/300], Step [59/172], Loss: 31.9077\n",
      "Epoch [250/300], Step [60/172], Loss: 22.2120\n",
      "Epoch [250/300], Step [61/172], Loss: 8.9034\n",
      "Epoch [250/300], Step [62/172], Loss: 16.7815\n",
      "Epoch [250/300], Step [63/172], Loss: 12.9140\n",
      "Epoch [250/300], Step [64/172], Loss: 15.0522\n",
      "Epoch [250/300], Step [65/172], Loss: 23.0501\n",
      "Epoch [250/300], Step [66/172], Loss: 9.4791\n",
      "Epoch [250/300], Step [67/172], Loss: 27.9034\n",
      "Epoch [250/300], Step [68/172], Loss: 7.1323\n",
      "Epoch [250/300], Step [69/172], Loss: 29.5429\n",
      "Epoch [250/300], Step [70/172], Loss: 29.4679\n",
      "Epoch [250/300], Step [71/172], Loss: 32.2351\n",
      "Epoch [250/300], Step [72/172], Loss: 28.9209\n",
      "Epoch [250/300], Step [73/172], Loss: 37.1333\n",
      "Epoch [250/300], Step [74/172], Loss: 20.5484\n",
      "Epoch [250/300], Step [75/172], Loss: 20.5509\n",
      "Epoch [250/300], Step [76/172], Loss: 25.5471\n",
      "Epoch [250/300], Step [77/172], Loss: 41.3990\n",
      "Epoch [250/300], Step [78/172], Loss: 30.2256\n",
      "Epoch [250/300], Step [79/172], Loss: 28.7136\n",
      "Epoch [250/300], Step [80/172], Loss: 45.8086\n",
      "Epoch [250/300], Step [81/172], Loss: 26.0309\n",
      "Epoch [250/300], Step [82/172], Loss: 36.5794\n",
      "Epoch [250/300], Step [83/172], Loss: 39.4094\n",
      "Epoch [250/300], Step [84/172], Loss: 30.7413\n",
      "Epoch [250/300], Step [85/172], Loss: 34.5572\n",
      "Epoch [250/300], Step [86/172], Loss: 31.6182\n",
      "Epoch [250/300], Step [87/172], Loss: 22.7966\n",
      "Epoch [250/300], Step [88/172], Loss: 20.8938\n",
      "Epoch [250/300], Step [89/172], Loss: 27.6264\n",
      "Epoch [250/300], Step [90/172], Loss: 20.0835\n",
      "Epoch [250/300], Step [91/172], Loss: 26.2886\n",
      "Epoch [250/300], Step [92/172], Loss: 19.1870\n",
      "Epoch [250/300], Step [93/172], Loss: 18.6364\n",
      "Epoch [250/300], Step [94/172], Loss: 25.1086\n",
      "Epoch [250/300], Step [95/172], Loss: 21.0003\n",
      "Epoch [250/300], Step [96/172], Loss: 20.3960\n",
      "Epoch [250/300], Step [97/172], Loss: 30.7320\n",
      "Epoch [250/300], Step [98/172], Loss: 19.7007\n",
      "Epoch [250/300], Step [99/172], Loss: 20.1739\n",
      "Epoch [250/300], Step [100/172], Loss: 18.9487\n",
      "Epoch [250/300], Step [101/172], Loss: 20.6374\n",
      "Epoch [250/300], Step [102/172], Loss: 19.6394\n",
      "Epoch [250/300], Step [103/172], Loss: 13.4275\n",
      "Epoch [250/300], Step [104/172], Loss: 20.8423\n",
      "Epoch [250/300], Step [105/172], Loss: 25.1016\n",
      "Epoch [250/300], Step [106/172], Loss: 16.8418\n",
      "Epoch [250/300], Step [107/172], Loss: 17.2924\n",
      "Epoch [250/300], Step [108/172], Loss: 17.5336\n",
      "Epoch [250/300], Step [109/172], Loss: 16.1433\n",
      "Epoch [250/300], Step [110/172], Loss: 19.5421\n",
      "Epoch [250/300], Step [111/172], Loss: 19.2198\n",
      "Epoch [250/300], Step [112/172], Loss: 17.5173\n",
      "Epoch [250/300], Step [113/172], Loss: 15.9501\n",
      "Epoch [250/300], Step [114/172], Loss: 16.1646\n",
      "Epoch [250/300], Step [115/172], Loss: 20.0404\n",
      "Epoch [250/300], Step [116/172], Loss: 15.5243\n",
      "Epoch [250/300], Step [117/172], Loss: 13.8348\n",
      "Epoch [250/300], Step [118/172], Loss: 13.7209\n",
      "Epoch [250/300], Step [119/172], Loss: 19.6262\n",
      "Epoch [250/300], Step [120/172], Loss: 11.0947\n",
      "Epoch [250/300], Step [121/172], Loss: 9.7199\n",
      "Epoch [250/300], Step [122/172], Loss: 14.0590\n",
      "Epoch [250/300], Step [123/172], Loss: 13.3552\n",
      "Epoch [250/300], Step [124/172], Loss: 7.8004\n",
      "Epoch [250/300], Step [125/172], Loss: 13.2261\n",
      "Epoch [250/300], Step [126/172], Loss: 13.5909\n",
      "Epoch [250/300], Step [127/172], Loss: 12.0702\n",
      "Epoch [250/300], Step [128/172], Loss: 11.3001\n",
      "Epoch [250/300], Step [129/172], Loss: 9.5350\n",
      "Epoch [250/300], Step [130/172], Loss: 13.9263\n",
      "Epoch [250/300], Step [131/172], Loss: 8.7902\n",
      "Epoch [250/300], Step [132/172], Loss: 11.1997\n",
      "Epoch [250/300], Step [133/172], Loss: 10.6856\n",
      "Epoch [250/300], Step [134/172], Loss: 10.9754\n",
      "Epoch [250/300], Step [135/172], Loss: 9.9223\n",
      "Epoch [250/300], Step [136/172], Loss: 9.6584\n",
      "Epoch [250/300], Step [137/172], Loss: 9.3383\n",
      "Epoch [250/300], Step [138/172], Loss: 9.4978\n",
      "Epoch [250/300], Step [139/172], Loss: 11.5582\n",
      "Epoch [250/300], Step [140/172], Loss: 12.1853\n",
      "Epoch [250/300], Step [141/172], Loss: 9.9833\n",
      "Epoch [250/300], Step [142/172], Loss: 15.5699\n",
      "Epoch [250/300], Step [143/172], Loss: 12.5263\n",
      "Epoch [250/300], Step [144/172], Loss: 10.1236\n",
      "Epoch [250/300], Step [145/172], Loss: 11.8478\n",
      "Epoch [250/300], Step [146/172], Loss: 12.0549\n",
      "Epoch [250/300], Step [147/172], Loss: 6.2259\n",
      "Epoch [250/300], Step [148/172], Loss: 7.2786\n",
      "Epoch [250/300], Step [149/172], Loss: 7.7445\n",
      "Epoch [250/300], Step [150/172], Loss: 6.4007\n",
      "Epoch [250/300], Step [151/172], Loss: 6.2408\n",
      "Epoch [250/300], Step [152/172], Loss: 9.0463\n",
      "Epoch [250/300], Step [153/172], Loss: 6.8088\n",
      "Epoch [250/300], Step [154/172], Loss: 7.0325\n",
      "Epoch [250/300], Step [155/172], Loss: 7.2006\n",
      "Epoch [250/300], Step [156/172], Loss: 15.3555\n",
      "Epoch [250/300], Step [157/172], Loss: 9.2627\n",
      "Epoch [250/300], Step [158/172], Loss: 8.0807\n",
      "Epoch [250/300], Step [159/172], Loss: 10.5320\n",
      "Epoch [250/300], Step [160/172], Loss: 10.5027\n",
      "Epoch [250/300], Step [161/172], Loss: 9.2169\n",
      "Epoch [250/300], Step [162/172], Loss: 5.4624\n",
      "Epoch [250/300], Step [163/172], Loss: 7.3746\n",
      "Epoch [250/300], Step [164/172], Loss: 9.5255\n",
      "Epoch [250/300], Step [165/172], Loss: 7.5916\n",
      "Epoch [250/300], Step [166/172], Loss: 6.4948\n",
      "Epoch [250/300], Step [167/172], Loss: 11.5612\n",
      "Epoch [250/300], Step [168/172], Loss: 7.2531\n",
      "Epoch [250/300], Step [169/172], Loss: 7.5847\n",
      "Epoch [250/300], Step [170/172], Loss: 6.3959\n",
      "Epoch [250/300], Step [171/172], Loss: 9.7146\n",
      "Epoch [250/300], Step [172/172], Loss: 5.9274\n",
      "Epoch [251/300], Step [1/172], Loss: 42.1510\n",
      "Epoch [251/300], Step [2/172], Loss: 45.1894\n",
      "Epoch [251/300], Step [3/172], Loss: 41.7182\n",
      "Epoch [251/300], Step [4/172], Loss: 19.5696\n",
      "Epoch [251/300], Step [5/172], Loss: 37.7422\n",
      "Epoch [251/300], Step [6/172], Loss: 17.8734\n",
      "Epoch [251/300], Step [7/172], Loss: 25.7957\n",
      "Epoch [251/300], Step [8/172], Loss: 3.9653\n",
      "Epoch [251/300], Step [9/172], Loss: 25.4107\n",
      "Epoch [251/300], Step [10/172], Loss: 37.0041\n",
      "Epoch [251/300], Step [11/172], Loss: 50.6969\n",
      "Epoch [251/300], Step [12/172], Loss: 50.2681\n",
      "Epoch [251/300], Step [13/172], Loss: 30.1822\n",
      "Epoch [251/300], Step [14/172], Loss: 52.8258\n",
      "Epoch [251/300], Step [15/172], Loss: 47.0251\n",
      "Epoch [251/300], Step [16/172], Loss: 8.5423\n",
      "Epoch [251/300], Step [17/172], Loss: 36.6135\n",
      "Epoch [251/300], Step [18/172], Loss: 50.4162\n",
      "Epoch [251/300], Step [19/172], Loss: 69.2636\n",
      "Epoch [251/300], Step [20/172], Loss: 24.9270\n",
      "Epoch [251/300], Step [21/172], Loss: 73.9657\n",
      "Epoch [251/300], Step [22/172], Loss: 49.7853\n",
      "Epoch [251/300], Step [23/172], Loss: 1.5229\n",
      "Epoch [251/300], Step [24/172], Loss: 46.9090\n",
      "Epoch [251/300], Step [25/172], Loss: 33.0069\n",
      "Epoch [251/300], Step [26/172], Loss: 42.7201\n",
      "Epoch [251/300], Step [27/172], Loss: 54.6126\n",
      "Epoch [251/300], Step [28/172], Loss: 17.7539\n",
      "Epoch [251/300], Step [29/172], Loss: 13.4763\n",
      "Epoch [251/300], Step [30/172], Loss: 49.2981\n",
      "Epoch [251/300], Step [31/172], Loss: 30.6586\n",
      "Epoch [251/300], Step [32/172], Loss: 41.1194\n",
      "Epoch [251/300], Step [33/172], Loss: 65.0165\n",
      "Epoch [251/300], Step [34/172], Loss: 1.9793\n",
      "Epoch [251/300], Step [35/172], Loss: 14.6835\n",
      "Epoch [251/300], Step [36/172], Loss: 15.5902\n",
      "Epoch [251/300], Step [37/172], Loss: 15.7600\n",
      "Epoch [251/300], Step [38/172], Loss: 31.7250\n",
      "Epoch [251/300], Step [39/172], Loss: 34.9624\n",
      "Epoch [251/300], Step [40/172], Loss: 22.3914\n",
      "Epoch [251/300], Step [41/172], Loss: 33.4926\n",
      "Epoch [251/300], Step [42/172], Loss: 39.9113\n",
      "Epoch [251/300], Step [43/172], Loss: 28.4332\n",
      "Epoch [251/300], Step [44/172], Loss: 23.3435\n",
      "Epoch [251/300], Step [45/172], Loss: 33.8748\n",
      "Epoch [251/300], Step [46/172], Loss: 18.3497\n",
      "Epoch [251/300], Step [47/172], Loss: 51.8085\n",
      "Epoch [251/300], Step [48/172], Loss: 64.1095\n",
      "Epoch [251/300], Step [49/172], Loss: 26.4333\n",
      "Epoch [251/300], Step [50/172], Loss: 51.3874\n",
      "Epoch [251/300], Step [51/172], Loss: 9.9300\n",
      "Epoch [251/300], Step [52/172], Loss: 26.1594\n",
      "Epoch [251/300], Step [53/172], Loss: 27.0798\n",
      "Epoch [251/300], Step [54/172], Loss: 20.4672\n",
      "Epoch [251/300], Step [55/172], Loss: 20.4437\n",
      "Epoch [251/300], Step [56/172], Loss: 18.1799\n",
      "Epoch [251/300], Step [57/172], Loss: 19.9947\n",
      "Epoch [251/300], Step [58/172], Loss: 17.6269\n",
      "Epoch [251/300], Step [59/172], Loss: 32.4415\n",
      "Epoch [251/300], Step [60/172], Loss: 22.2469\n",
      "Epoch [251/300], Step [61/172], Loss: 9.0213\n",
      "Epoch [251/300], Step [62/172], Loss: 16.8379\n",
      "Epoch [251/300], Step [63/172], Loss: 13.0764\n",
      "Epoch [251/300], Step [64/172], Loss: 15.1094\n",
      "Epoch [251/300], Step [65/172], Loss: 23.1031\n",
      "Epoch [251/300], Step [66/172], Loss: 9.5057\n",
      "Epoch [251/300], Step [67/172], Loss: 28.2124\n",
      "Epoch [251/300], Step [68/172], Loss: 6.9534\n",
      "Epoch [251/300], Step [69/172], Loss: 29.4023\n",
      "Epoch [251/300], Step [70/172], Loss: 29.3554\n",
      "Epoch [251/300], Step [71/172], Loss: 32.1108\n",
      "Epoch [251/300], Step [72/172], Loss: 28.9090\n",
      "Epoch [251/300], Step [73/172], Loss: 36.9737\n",
      "Epoch [251/300], Step [74/172], Loss: 20.7871\n",
      "Epoch [251/300], Step [75/172], Loss: 20.6770\n",
      "Epoch [251/300], Step [76/172], Loss: 25.5088\n",
      "Epoch [251/300], Step [77/172], Loss: 41.2801\n",
      "Epoch [251/300], Step [78/172], Loss: 30.1785\n",
      "Epoch [251/300], Step [79/172], Loss: 28.5316\n",
      "Epoch [251/300], Step [80/172], Loss: 45.6084\n",
      "Epoch [251/300], Step [81/172], Loss: 25.7582\n",
      "Epoch [251/300], Step [82/172], Loss: 36.4821\n",
      "Epoch [251/300], Step [83/172], Loss: 39.3261\n",
      "Epoch [251/300], Step [84/172], Loss: 30.5460\n",
      "Epoch [251/300], Step [85/172], Loss: 34.4411\n",
      "Epoch [251/300], Step [86/172], Loss: 31.3925\n",
      "Epoch [251/300], Step [87/172], Loss: 22.6528\n",
      "Epoch [251/300], Step [88/172], Loss: 20.7699\n",
      "Epoch [251/300], Step [89/172], Loss: 27.3762\n",
      "Epoch [251/300], Step [90/172], Loss: 20.0259\n",
      "Epoch [251/300], Step [91/172], Loss: 26.1983\n",
      "Epoch [251/300], Step [92/172], Loss: 19.2412\n",
      "Epoch [251/300], Step [93/172], Loss: 18.7288\n",
      "Epoch [251/300], Step [94/172], Loss: 25.2576\n",
      "Epoch [251/300], Step [95/172], Loss: 21.1484\n",
      "Epoch [251/300], Step [96/172], Loss: 20.4285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [251/300], Step [97/172], Loss: 30.7518\n",
      "Epoch [251/300], Step [98/172], Loss: 19.5450\n",
      "Epoch [251/300], Step [99/172], Loss: 20.1786\n",
      "Epoch [251/300], Step [100/172], Loss: 18.8320\n",
      "Epoch [251/300], Step [101/172], Loss: 20.6085\n",
      "Epoch [251/300], Step [102/172], Loss: 19.7083\n",
      "Epoch [251/300], Step [103/172], Loss: 13.3444\n",
      "Epoch [251/300], Step [104/172], Loss: 20.8934\n",
      "Epoch [251/300], Step [105/172], Loss: 25.1932\n",
      "Epoch [251/300], Step [106/172], Loss: 16.7875\n",
      "Epoch [251/300], Step [107/172], Loss: 17.3083\n",
      "Epoch [251/300], Step [108/172], Loss: 17.4328\n",
      "Epoch [251/300], Step [109/172], Loss: 16.2181\n",
      "Epoch [251/300], Step [110/172], Loss: 19.5747\n",
      "Epoch [251/300], Step [111/172], Loss: 19.0890\n",
      "Epoch [251/300], Step [112/172], Loss: 17.7018\n",
      "Epoch [251/300], Step [113/172], Loss: 15.7145\n",
      "Epoch [251/300], Step [114/172], Loss: 16.1887\n",
      "Epoch [251/300], Step [115/172], Loss: 20.3151\n",
      "Epoch [251/300], Step [116/172], Loss: 15.6683\n",
      "Epoch [251/300], Step [117/172], Loss: 13.8497\n",
      "Epoch [251/300], Step [118/172], Loss: 13.7253\n",
      "Epoch [251/300], Step [119/172], Loss: 19.6371\n",
      "Epoch [251/300], Step [120/172], Loss: 11.1463\n",
      "Epoch [251/300], Step [121/172], Loss: 9.7437\n",
      "Epoch [251/300], Step [122/172], Loss: 14.1585\n",
      "Epoch [251/300], Step [123/172], Loss: 13.1953\n",
      "Epoch [251/300], Step [124/172], Loss: 7.8295\n",
      "Epoch [251/300], Step [125/172], Loss: 13.2289\n",
      "Epoch [251/300], Step [126/172], Loss: 13.6233\n",
      "Epoch [251/300], Step [127/172], Loss: 11.9780\n",
      "Epoch [251/300], Step [128/172], Loss: 11.0856\n",
      "Epoch [251/300], Step [129/172], Loss: 9.4817\n",
      "Epoch [251/300], Step [130/172], Loss: 13.9273\n",
      "Epoch [251/300], Step [131/172], Loss: 8.7770\n",
      "Epoch [251/300], Step [132/172], Loss: 11.1807\n",
      "Epoch [251/300], Step [133/172], Loss: 10.7651\n",
      "Epoch [251/300], Step [134/172], Loss: 11.0631\n",
      "Epoch [251/300], Step [135/172], Loss: 9.8757\n",
      "Epoch [251/300], Step [136/172], Loss: 9.4266\n",
      "Epoch [251/300], Step [137/172], Loss: 9.3103\n",
      "Epoch [251/300], Step [138/172], Loss: 9.5049\n",
      "Epoch [251/300], Step [139/172], Loss: 11.5657\n",
      "Epoch [251/300], Step [140/172], Loss: 12.0702\n",
      "Epoch [251/300], Step [141/172], Loss: 9.9276\n",
      "Epoch [251/300], Step [142/172], Loss: 15.8117\n",
      "Epoch [251/300], Step [143/172], Loss: 12.5972\n",
      "Epoch [251/300], Step [144/172], Loss: 10.1882\n",
      "Epoch [251/300], Step [145/172], Loss: 11.9367\n",
      "Epoch [251/300], Step [146/172], Loss: 12.1624\n",
      "Epoch [251/300], Step [147/172], Loss: 6.2399\n",
      "Epoch [251/300], Step [148/172], Loss: 7.2391\n",
      "Epoch [251/300], Step [149/172], Loss: 7.6428\n",
      "Epoch [251/300], Step [150/172], Loss: 6.3127\n",
      "Epoch [251/300], Step [151/172], Loss: 6.1772\n",
      "Epoch [251/300], Step [152/172], Loss: 8.9378\n",
      "Epoch [251/300], Step [153/172], Loss: 6.7826\n",
      "Epoch [251/300], Step [154/172], Loss: 7.0125\n",
      "Epoch [251/300], Step [155/172], Loss: 7.2094\n",
      "Epoch [251/300], Step [156/172], Loss: 15.4399\n",
      "Epoch [251/300], Step [157/172], Loss: 9.4328\n",
      "Epoch [251/300], Step [158/172], Loss: 8.0218\n",
      "Epoch [251/300], Step [159/172], Loss: 10.6647\n",
      "Epoch [251/300], Step [160/172], Loss: 10.7316\n",
      "Epoch [251/300], Step [161/172], Loss: 9.0486\n",
      "Epoch [251/300], Step [162/172], Loss: 5.3982\n",
      "Epoch [251/300], Step [163/172], Loss: 7.3879\n",
      "Epoch [251/300], Step [164/172], Loss: 9.5774\n",
      "Epoch [251/300], Step [165/172], Loss: 7.4957\n",
      "Epoch [251/300], Step [166/172], Loss: 6.4753\n",
      "Epoch [251/300], Step [167/172], Loss: 11.7338\n",
      "Epoch [251/300], Step [168/172], Loss: 7.2872\n",
      "Epoch [251/300], Step [169/172], Loss: 7.6117\n",
      "Epoch [251/300], Step [170/172], Loss: 6.2151\n",
      "Epoch [251/300], Step [171/172], Loss: 9.7407\n",
      "Epoch [251/300], Step [172/172], Loss: 5.9421\n",
      "Epoch [252/300], Step [1/172], Loss: 41.9062\n",
      "Epoch [252/300], Step [2/172], Loss: 44.7093\n",
      "Epoch [252/300], Step [3/172], Loss: 41.3444\n",
      "Epoch [252/300], Step [4/172], Loss: 19.5213\n",
      "Epoch [252/300], Step [5/172], Loss: 37.6423\n",
      "Epoch [252/300], Step [6/172], Loss: 17.2496\n",
      "Epoch [252/300], Step [7/172], Loss: 23.9009\n",
      "Epoch [252/300], Step [8/172], Loss: 3.8340\n",
      "Epoch [252/300], Step [9/172], Loss: 25.5174\n",
      "Epoch [252/300], Step [10/172], Loss: 37.1670\n",
      "Epoch [252/300], Step [11/172], Loss: 50.4731\n",
      "Epoch [252/300], Step [12/172], Loss: 49.9700\n",
      "Epoch [252/300], Step [13/172], Loss: 29.7999\n",
      "Epoch [252/300], Step [14/172], Loss: 52.2396\n",
      "Epoch [252/300], Step [15/172], Loss: 47.0117\n",
      "Epoch [252/300], Step [16/172], Loss: 9.5932\n",
      "Epoch [252/300], Step [17/172], Loss: 36.0047\n",
      "Epoch [252/300], Step [18/172], Loss: 49.7048\n",
      "Epoch [252/300], Step [19/172], Loss: 68.6917\n",
      "Epoch [252/300], Step [20/172], Loss: 25.4829\n",
      "Epoch [252/300], Step [21/172], Loss: 72.9006\n",
      "Epoch [252/300], Step [22/172], Loss: 50.2064\n",
      "Epoch [252/300], Step [23/172], Loss: 1.7440\n",
      "Epoch [252/300], Step [24/172], Loss: 46.9970\n",
      "Epoch [252/300], Step [25/172], Loss: 33.7070\n",
      "Epoch [252/300], Step [26/172], Loss: 42.9978\n",
      "Epoch [252/300], Step [27/172], Loss: 54.6873\n",
      "Epoch [252/300], Step [28/172], Loss: 17.4712\n",
      "Epoch [252/300], Step [29/172], Loss: 13.4026\n",
      "Epoch [252/300], Step [30/172], Loss: 49.2367\n",
      "Epoch [252/300], Step [31/172], Loss: 30.6903\n",
      "Epoch [252/300], Step [32/172], Loss: 40.9558\n",
      "Epoch [252/300], Step [33/172], Loss: 63.7154\n",
      "Epoch [252/300], Step [34/172], Loss: 1.7528\n",
      "Epoch [252/300], Step [35/172], Loss: 14.3328\n",
      "Epoch [252/300], Step [36/172], Loss: 15.3311\n",
      "Epoch [252/300], Step [37/172], Loss: 15.7613\n",
      "Epoch [252/300], Step [38/172], Loss: 31.4900\n",
      "Epoch [252/300], Step [39/172], Loss: 34.9422\n",
      "Epoch [252/300], Step [40/172], Loss: 22.3075\n",
      "Epoch [252/300], Step [41/172], Loss: 33.3800\n",
      "Epoch [252/300], Step [42/172], Loss: 39.4741\n",
      "Epoch [252/300], Step [43/172], Loss: 28.2318\n",
      "Epoch [252/300], Step [44/172], Loss: 22.9604\n",
      "Epoch [252/300], Step [45/172], Loss: 32.9291\n",
      "Epoch [252/300], Step [46/172], Loss: 17.5305\n",
      "Epoch [252/300], Step [47/172], Loss: 51.1996\n",
      "Epoch [252/300], Step [48/172], Loss: 63.1701\n",
      "Epoch [252/300], Step [49/172], Loss: 26.1895\n",
      "Epoch [252/300], Step [50/172], Loss: 52.0544\n",
      "Epoch [252/300], Step [51/172], Loss: 9.8231\n",
      "Epoch [252/300], Step [52/172], Loss: 25.5608\n",
      "Epoch [252/300], Step [53/172], Loss: 26.7089\n",
      "Epoch [252/300], Step [54/172], Loss: 20.1542\n",
      "Epoch [252/300], Step [55/172], Loss: 19.7885\n",
      "Epoch [252/300], Step [56/172], Loss: 17.7998\n",
      "Epoch [252/300], Step [57/172], Loss: 20.1863\n",
      "Epoch [252/300], Step [58/172], Loss: 17.5045\n",
      "Epoch [252/300], Step [59/172], Loss: 31.8295\n",
      "Epoch [252/300], Step [60/172], Loss: 22.4999\n",
      "Epoch [252/300], Step [61/172], Loss: 8.7569\n",
      "Epoch [252/300], Step [62/172], Loss: 16.5302\n",
      "Epoch [252/300], Step [63/172], Loss: 12.5957\n",
      "Epoch [252/300], Step [64/172], Loss: 15.0854\n",
      "Epoch [252/300], Step [65/172], Loss: 23.1790\n",
      "Epoch [252/300], Step [66/172], Loss: 9.5856\n",
      "Epoch [252/300], Step [67/172], Loss: 27.8049\n",
      "Epoch [252/300], Step [68/172], Loss: 6.9736\n",
      "Epoch [252/300], Step [69/172], Loss: 29.4535\n",
      "Epoch [252/300], Step [70/172], Loss: 29.3681\n",
      "Epoch [252/300], Step [71/172], Loss: 32.0669\n",
      "Epoch [252/300], Step [72/172], Loss: 28.7623\n",
      "Epoch [252/300], Step [73/172], Loss: 36.7902\n",
      "Epoch [252/300], Step [74/172], Loss: 20.6981\n",
      "Epoch [252/300], Step [75/172], Loss: 20.5027\n",
      "Epoch [252/300], Step [76/172], Loss: 25.1727\n",
      "Epoch [252/300], Step [77/172], Loss: 41.7476\n",
      "Epoch [252/300], Step [78/172], Loss: 30.0560\n",
      "Epoch [252/300], Step [79/172], Loss: 28.5089\n",
      "Epoch [252/300], Step [80/172], Loss: 45.6923\n",
      "Epoch [252/300], Step [81/172], Loss: 25.7814\n",
      "Epoch [252/300], Step [82/172], Loss: 36.5306\n",
      "Epoch [252/300], Step [83/172], Loss: 39.2696\n",
      "Epoch [252/300], Step [84/172], Loss: 30.5452\n",
      "Epoch [252/300], Step [85/172], Loss: 34.1889\n",
      "Epoch [252/300], Step [86/172], Loss: 31.1689\n",
      "Epoch [252/300], Step [87/172], Loss: 22.5383\n",
      "Epoch [252/300], Step [88/172], Loss: 20.5587\n",
      "Epoch [252/300], Step [89/172], Loss: 27.2037\n",
      "Epoch [252/300], Step [90/172], Loss: 19.8075\n",
      "Epoch [252/300], Step [91/172], Loss: 26.0785\n",
      "Epoch [252/300], Step [92/172], Loss: 19.1448\n",
      "Epoch [252/300], Step [93/172], Loss: 18.6120\n",
      "Epoch [252/300], Step [94/172], Loss: 25.0359\n",
      "Epoch [252/300], Step [95/172], Loss: 20.8576\n",
      "Epoch [252/300], Step [96/172], Loss: 20.2393\n",
      "Epoch [252/300], Step [97/172], Loss: 30.5462\n",
      "Epoch [252/300], Step [98/172], Loss: 19.3178\n",
      "Epoch [252/300], Step [99/172], Loss: 19.8984\n",
      "Epoch [252/300], Step [100/172], Loss: 18.6154\n",
      "Epoch [252/300], Step [101/172], Loss: 20.3803\n",
      "Epoch [252/300], Step [102/172], Loss: 19.5536\n",
      "Epoch [252/300], Step [103/172], Loss: 13.1119\n",
      "Epoch [252/300], Step [104/172], Loss: 20.6523\n",
      "Epoch [252/300], Step [105/172], Loss: 24.9224\n",
      "Epoch [252/300], Step [106/172], Loss: 16.6934\n",
      "Epoch [252/300], Step [107/172], Loss: 17.2333\n",
      "Epoch [252/300], Step [108/172], Loss: 17.3109\n",
      "Epoch [252/300], Step [109/172], Loss: 16.0553\n",
      "Epoch [252/300], Step [110/172], Loss: 19.3962\n",
      "Epoch [252/300], Step [111/172], Loss: 18.9641\n",
      "Epoch [252/300], Step [112/172], Loss: 17.4335\n",
      "Epoch [252/300], Step [113/172], Loss: 15.6656\n",
      "Epoch [252/300], Step [114/172], Loss: 16.0762\n",
      "Epoch [252/300], Step [115/172], Loss: 20.2662\n",
      "Epoch [252/300], Step [116/172], Loss: 15.5742\n",
      "Epoch [252/300], Step [117/172], Loss: 13.7640\n",
      "Epoch [252/300], Step [118/172], Loss: 13.5963\n",
      "Epoch [252/300], Step [119/172], Loss: 19.6256\n",
      "Epoch [252/300], Step [120/172], Loss: 10.9581\n",
      "Epoch [252/300], Step [121/172], Loss: 9.6369\n",
      "Epoch [252/300], Step [122/172], Loss: 13.8292\n",
      "Epoch [252/300], Step [123/172], Loss: 12.8900\n",
      "Epoch [252/300], Step [124/172], Loss: 7.7320\n",
      "Epoch [252/300], Step [125/172], Loss: 13.1206\n",
      "Epoch [252/300], Step [126/172], Loss: 13.3629\n",
      "Epoch [252/300], Step [127/172], Loss: 11.8548\n",
      "Epoch [252/300], Step [128/172], Loss: 11.1166\n",
      "Epoch [252/300], Step [129/172], Loss: 9.4395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [252/300], Step [130/172], Loss: 13.8177\n",
      "Epoch [252/300], Step [131/172], Loss: 8.6207\n",
      "Epoch [252/300], Step [132/172], Loss: 11.0844\n",
      "Epoch [252/300], Step [133/172], Loss: 10.5342\n",
      "Epoch [252/300], Step [134/172], Loss: 11.0764\n",
      "Epoch [252/300], Step [135/172], Loss: 9.8756\n",
      "Epoch [252/300], Step [136/172], Loss: 9.2877\n",
      "Epoch [252/300], Step [137/172], Loss: 9.2852\n",
      "Epoch [252/300], Step [138/172], Loss: 9.3180\n",
      "Epoch [252/300], Step [139/172], Loss: 11.6193\n",
      "Epoch [252/300], Step [140/172], Loss: 11.9830\n",
      "Epoch [252/300], Step [141/172], Loss: 9.7976\n",
      "Epoch [252/300], Step [142/172], Loss: 15.6211\n",
      "Epoch [252/300], Step [143/172], Loss: 12.6056\n",
      "Epoch [252/300], Step [144/172], Loss: 10.0561\n",
      "Epoch [252/300], Step [145/172], Loss: 11.8159\n",
      "Epoch [252/300], Step [146/172], Loss: 12.0095\n",
      "Epoch [252/300], Step [147/172], Loss: 6.1283\n",
      "Epoch [252/300], Step [148/172], Loss: 7.1005\n",
      "Epoch [252/300], Step [149/172], Loss: 7.4497\n",
      "Epoch [252/300], Step [150/172], Loss: 6.1747\n",
      "Epoch [252/300], Step [151/172], Loss: 5.9926\n",
      "Epoch [252/300], Step [152/172], Loss: 8.8166\n",
      "Epoch [252/300], Step [153/172], Loss: 6.7002\n",
      "Epoch [252/300], Step [154/172], Loss: 6.9537\n",
      "Epoch [252/300], Step [155/172], Loss: 7.0798\n",
      "Epoch [252/300], Step [156/172], Loss: 15.3300\n",
      "Epoch [252/300], Step [157/172], Loss: 9.4000\n",
      "Epoch [252/300], Step [158/172], Loss: 7.8847\n",
      "Epoch [252/300], Step [159/172], Loss: 10.5767\n",
      "Epoch [252/300], Step [160/172], Loss: 10.6392\n",
      "Epoch [252/300], Step [161/172], Loss: 8.7035\n",
      "Epoch [252/300], Step [162/172], Loss: 5.2813\n",
      "Epoch [252/300], Step [163/172], Loss: 7.3358\n",
      "Epoch [252/300], Step [164/172], Loss: 9.3717\n",
      "Epoch [252/300], Step [165/172], Loss: 7.3320\n",
      "Epoch [252/300], Step [166/172], Loss: 6.5295\n",
      "Epoch [252/300], Step [167/172], Loss: 11.4676\n",
      "Epoch [252/300], Step [168/172], Loss: 7.3499\n",
      "Epoch [252/300], Step [169/172], Loss: 7.4932\n",
      "Epoch [252/300], Step [170/172], Loss: 6.0139\n",
      "Epoch [252/300], Step [171/172], Loss: 9.4308\n",
      "Epoch [252/300], Step [172/172], Loss: 5.7685\n",
      "Epoch [253/300], Step [1/172], Loss: 41.9752\n",
      "Epoch [253/300], Step [2/172], Loss: 45.2677\n",
      "Epoch [253/300], Step [3/172], Loss: 41.3682\n",
      "Epoch [253/300], Step [4/172], Loss: 19.4992\n",
      "Epoch [253/300], Step [5/172], Loss: 37.7579\n",
      "Epoch [253/300], Step [6/172], Loss: 17.4975\n",
      "Epoch [253/300], Step [7/172], Loss: 25.5107\n",
      "Epoch [253/300], Step [8/172], Loss: 3.9887\n",
      "Epoch [253/300], Step [9/172], Loss: 25.5221\n",
      "Epoch [253/300], Step [10/172], Loss: 36.3373\n",
      "Epoch [253/300], Step [11/172], Loss: 50.4895\n",
      "Epoch [253/300], Step [12/172], Loss: 50.3855\n",
      "Epoch [253/300], Step [13/172], Loss: 30.2469\n",
      "Epoch [253/300], Step [14/172], Loss: 51.9323\n",
      "Epoch [253/300], Step [15/172], Loss: 47.0142\n",
      "Epoch [253/300], Step [16/172], Loss: 8.2564\n",
      "Epoch [253/300], Step [17/172], Loss: 36.3552\n",
      "Epoch [253/300], Step [18/172], Loss: 50.0265\n",
      "Epoch [253/300], Step [19/172], Loss: 69.1280\n",
      "Epoch [253/300], Step [20/172], Loss: 26.1679\n",
      "Epoch [253/300], Step [21/172], Loss: 72.7518\n",
      "Epoch [253/300], Step [22/172], Loss: 49.8142\n",
      "Epoch [253/300], Step [23/172], Loss: 1.4437\n",
      "Epoch [253/300], Step [24/172], Loss: 46.7076\n",
      "Epoch [253/300], Step [25/172], Loss: 32.9891\n",
      "Epoch [253/300], Step [26/172], Loss: 42.5200\n",
      "Epoch [253/300], Step [27/172], Loss: 53.7275\n",
      "Epoch [253/300], Step [28/172], Loss: 17.6420\n",
      "Epoch [253/300], Step [29/172], Loss: 13.6998\n",
      "Epoch [253/300], Step [30/172], Loss: 48.7147\n",
      "Epoch [253/300], Step [31/172], Loss: 30.3758\n",
      "Epoch [253/300], Step [32/172], Loss: 41.0986\n",
      "Epoch [253/300], Step [33/172], Loss: 64.0531\n",
      "Epoch [253/300], Step [34/172], Loss: 2.2065\n",
      "Epoch [253/300], Step [35/172], Loss: 14.7523\n",
      "Epoch [253/300], Step [36/172], Loss: 15.8356\n",
      "Epoch [253/300], Step [37/172], Loss: 15.6749\n",
      "Epoch [253/300], Step [38/172], Loss: 31.3830\n",
      "Epoch [253/300], Step [39/172], Loss: 34.9191\n",
      "Epoch [253/300], Step [40/172], Loss: 22.4676\n",
      "Epoch [253/300], Step [41/172], Loss: 32.7382\n",
      "Epoch [253/300], Step [42/172], Loss: 39.3010\n",
      "Epoch [253/300], Step [43/172], Loss: 28.0142\n",
      "Epoch [253/300], Step [44/172], Loss: 23.2017\n",
      "Epoch [253/300], Step [45/172], Loss: 33.2740\n",
      "Epoch [253/300], Step [46/172], Loss: 17.5908\n",
      "Epoch [253/300], Step [47/172], Loss: 51.5996\n",
      "Epoch [253/300], Step [48/172], Loss: 64.0161\n",
      "Epoch [253/300], Step [49/172], Loss: 26.1289\n",
      "Epoch [253/300], Step [50/172], Loss: 52.2640\n",
      "Epoch [253/300], Step [51/172], Loss: 9.7922\n",
      "Epoch [253/300], Step [52/172], Loss: 25.7826\n",
      "Epoch [253/300], Step [53/172], Loss: 26.6900\n",
      "Epoch [253/300], Step [54/172], Loss: 19.8242\n",
      "Epoch [253/300], Step [55/172], Loss: 19.8306\n",
      "Epoch [253/300], Step [56/172], Loss: 17.8966\n",
      "Epoch [253/300], Step [57/172], Loss: 20.4852\n",
      "Epoch [253/300], Step [58/172], Loss: 17.1710\n",
      "Epoch [253/300], Step [59/172], Loss: 31.6696\n",
      "Epoch [253/300], Step [60/172], Loss: 22.9988\n",
      "Epoch [253/300], Step [61/172], Loss: 8.6993\n",
      "Epoch [253/300], Step [62/172], Loss: 16.8076\n",
      "Epoch [253/300], Step [63/172], Loss: 12.4010\n",
      "Epoch [253/300], Step [64/172], Loss: 14.9079\n",
      "Epoch [253/300], Step [65/172], Loss: 22.9156\n",
      "Epoch [253/300], Step [66/172], Loss: 9.7171\n",
      "Epoch [253/300], Step [67/172], Loss: 28.0444\n",
      "Epoch [253/300], Step [68/172], Loss: 7.1277\n",
      "Epoch [253/300], Step [69/172], Loss: 29.5807\n",
      "Epoch [253/300], Step [70/172], Loss: 29.4142\n",
      "Epoch [253/300], Step [71/172], Loss: 32.0718\n",
      "Epoch [253/300], Step [72/172], Loss: 28.6049\n",
      "Epoch [253/300], Step [73/172], Loss: 36.8207\n",
      "Epoch [253/300], Step [74/172], Loss: 20.5402\n",
      "Epoch [253/300], Step [75/172], Loss: 20.5057\n",
      "Epoch [253/300], Step [76/172], Loss: 25.0340\n",
      "Epoch [253/300], Step [77/172], Loss: 41.4042\n",
      "Epoch [253/300], Step [78/172], Loss: 30.0170\n",
      "Epoch [253/300], Step [79/172], Loss: 28.3124\n",
      "Epoch [253/300], Step [80/172], Loss: 45.3946\n",
      "Epoch [253/300], Step [81/172], Loss: 25.5557\n",
      "Epoch [253/300], Step [82/172], Loss: 36.3941\n",
      "Epoch [253/300], Step [83/172], Loss: 38.9265\n",
      "Epoch [253/300], Step [84/172], Loss: 30.3697\n",
      "Epoch [253/300], Step [85/172], Loss: 33.9348\n",
      "Epoch [253/300], Step [86/172], Loss: 30.9543\n",
      "Epoch [253/300], Step [87/172], Loss: 22.4026\n",
      "Epoch [253/300], Step [88/172], Loss: 20.5376\n",
      "Epoch [253/300], Step [89/172], Loss: 27.1084\n",
      "Epoch [253/300], Step [90/172], Loss: 19.7398\n",
      "Epoch [253/300], Step [91/172], Loss: 25.9890\n",
      "Epoch [253/300], Step [92/172], Loss: 19.0428\n",
      "Epoch [253/300], Step [93/172], Loss: 18.4915\n",
      "Epoch [253/300], Step [94/172], Loss: 25.0138\n",
      "Epoch [253/300], Step [95/172], Loss: 20.8101\n",
      "Epoch [253/300], Step [96/172], Loss: 20.3193\n",
      "Epoch [253/300], Step [97/172], Loss: 30.4794\n",
      "Epoch [253/300], Step [98/172], Loss: 19.1991\n",
      "Epoch [253/300], Step [99/172], Loss: 19.7514\n",
      "Epoch [253/300], Step [100/172], Loss: 18.5506\n",
      "Epoch [253/300], Step [101/172], Loss: 20.2794\n",
      "Epoch [253/300], Step [102/172], Loss: 19.5088\n",
      "Epoch [253/300], Step [103/172], Loss: 13.0324\n",
      "Epoch [253/300], Step [104/172], Loss: 20.5781\n",
      "Epoch [253/300], Step [105/172], Loss: 24.7599\n",
      "Epoch [253/300], Step [106/172], Loss: 16.5785\n",
      "Epoch [253/300], Step [107/172], Loss: 17.1351\n",
      "Epoch [253/300], Step [108/172], Loss: 17.2502\n",
      "Epoch [253/300], Step [109/172], Loss: 16.0132\n",
      "Epoch [253/300], Step [110/172], Loss: 19.2563\n",
      "Epoch [253/300], Step [111/172], Loss: 18.9469\n",
      "Epoch [253/300], Step [112/172], Loss: 17.5380\n",
      "Epoch [253/300], Step [113/172], Loss: 15.5378\n",
      "Epoch [253/300], Step [114/172], Loss: 16.0605\n",
      "Epoch [253/300], Step [115/172], Loss: 20.2902\n",
      "Epoch [253/300], Step [116/172], Loss: 15.4558\n",
      "Epoch [253/300], Step [117/172], Loss: 13.7686\n",
      "Epoch [253/300], Step [118/172], Loss: 13.7643\n",
      "Epoch [253/300], Step [119/172], Loss: 19.5515\n",
      "Epoch [253/300], Step [120/172], Loss: 10.9788\n",
      "Epoch [253/300], Step [121/172], Loss: 9.5944\n",
      "Epoch [253/300], Step [122/172], Loss: 13.7636\n",
      "Epoch [253/300], Step [123/172], Loss: 12.8958\n",
      "Epoch [253/300], Step [124/172], Loss: 7.7514\n",
      "Epoch [253/300], Step [125/172], Loss: 13.1199\n",
      "Epoch [253/300], Step [126/172], Loss: 13.3551\n",
      "Epoch [253/300], Step [127/172], Loss: 11.8601\n",
      "Epoch [253/300], Step [128/172], Loss: 11.0666\n",
      "Epoch [253/300], Step [129/172], Loss: 9.3852\n",
      "Epoch [253/300], Step [130/172], Loss: 13.8379\n",
      "Epoch [253/300], Step [131/172], Loss: 8.6116\n",
      "Epoch [253/300], Step [132/172], Loss: 11.0661\n",
      "Epoch [253/300], Step [133/172], Loss: 10.5872\n",
      "Epoch [253/300], Step [134/172], Loss: 11.0638\n",
      "Epoch [253/300], Step [135/172], Loss: 9.8012\n",
      "Epoch [253/300], Step [136/172], Loss: 9.2160\n",
      "Epoch [253/300], Step [137/172], Loss: 9.3235\n",
      "Epoch [253/300], Step [138/172], Loss: 9.1005\n",
      "Epoch [253/300], Step [139/172], Loss: 11.5944\n",
      "Epoch [253/300], Step [140/172], Loss: 11.8990\n",
      "Epoch [253/300], Step [141/172], Loss: 9.7403\n",
      "Epoch [253/300], Step [142/172], Loss: 15.6329\n",
      "Epoch [253/300], Step [143/172], Loss: 12.5885\n",
      "Epoch [253/300], Step [144/172], Loss: 9.9656\n",
      "Epoch [253/300], Step [145/172], Loss: 11.7734\n",
      "Epoch [253/300], Step [146/172], Loss: 11.8276\n",
      "Epoch [253/300], Step [147/172], Loss: 6.1066\n",
      "Epoch [253/300], Step [148/172], Loss: 7.0306\n",
      "Epoch [253/300], Step [149/172], Loss: 7.3703\n",
      "Epoch [253/300], Step [150/172], Loss: 6.0971\n",
      "Epoch [253/300], Step [151/172], Loss: 5.9580\n",
      "Epoch [253/300], Step [152/172], Loss: 8.7280\n",
      "Epoch [253/300], Step [153/172], Loss: 6.6724\n",
      "Epoch [253/300], Step [154/172], Loss: 6.9364\n",
      "Epoch [253/300], Step [155/172], Loss: 6.9959\n",
      "Epoch [253/300], Step [156/172], Loss: 15.3352\n",
      "Epoch [253/300], Step [157/172], Loss: 9.5245\n",
      "Epoch [253/300], Step [158/172], Loss: 7.9025\n",
      "Epoch [253/300], Step [159/172], Loss: 10.5432\n",
      "Epoch [253/300], Step [160/172], Loss: 10.7169\n",
      "Epoch [253/300], Step [161/172], Loss: 8.4293\n",
      "Epoch [253/300], Step [162/172], Loss: 5.2353\n",
      "Epoch [253/300], Step [163/172], Loss: 7.3353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [253/300], Step [164/172], Loss: 9.3016\n",
      "Epoch [253/300], Step [165/172], Loss: 7.3207\n",
      "Epoch [253/300], Step [166/172], Loss: 6.4808\n",
      "Epoch [253/300], Step [167/172], Loss: 11.4831\n",
      "Epoch [253/300], Step [168/172], Loss: 7.1504\n",
      "Epoch [253/300], Step [169/172], Loss: 7.4769\n",
      "Epoch [253/300], Step [170/172], Loss: 6.0308\n",
      "Epoch [253/300], Step [171/172], Loss: 9.5151\n",
      "Epoch [253/300], Step [172/172], Loss: 5.7639\n",
      "Epoch [254/300], Step [1/172], Loss: 41.7892\n",
      "Epoch [254/300], Step [2/172], Loss: 43.9480\n",
      "Epoch [254/300], Step [3/172], Loss: 41.6671\n",
      "Epoch [254/300], Step [4/172], Loss: 19.5287\n",
      "Epoch [254/300], Step [5/172], Loss: 37.5799\n",
      "Epoch [254/300], Step [6/172], Loss: 16.9870\n",
      "Epoch [254/300], Step [7/172], Loss: 23.8210\n",
      "Epoch [254/300], Step [8/172], Loss: 3.8202\n",
      "Epoch [254/300], Step [9/172], Loss: 25.4111\n",
      "Epoch [254/300], Step [10/172], Loss: 36.5207\n",
      "Epoch [254/300], Step [11/172], Loss: 50.6940\n",
      "Epoch [254/300], Step [12/172], Loss: 50.1788\n",
      "Epoch [254/300], Step [13/172], Loss: 30.2415\n",
      "Epoch [254/300], Step [14/172], Loss: 51.7347\n",
      "Epoch [254/300], Step [15/172], Loss: 47.3057\n",
      "Epoch [254/300], Step [16/172], Loss: 9.4727\n",
      "Epoch [254/300], Step [17/172], Loss: 35.6228\n",
      "Epoch [254/300], Step [18/172], Loss: 49.7682\n",
      "Epoch [254/300], Step [19/172], Loss: 68.9400\n",
      "Epoch [254/300], Step [20/172], Loss: 25.1858\n",
      "Epoch [254/300], Step [21/172], Loss: 72.2749\n",
      "Epoch [254/300], Step [22/172], Loss: 49.6995\n",
      "Epoch [254/300], Step [23/172], Loss: 1.7563\n",
      "Epoch [254/300], Step [24/172], Loss: 46.1187\n",
      "Epoch [254/300], Step [25/172], Loss: 33.3829\n",
      "Epoch [254/300], Step [26/172], Loss: 42.6705\n",
      "Epoch [254/300], Step [27/172], Loss: 53.8237\n",
      "Epoch [254/300], Step [28/172], Loss: 17.1409\n",
      "Epoch [254/300], Step [29/172], Loss: 13.4238\n",
      "Epoch [254/300], Step [30/172], Loss: 47.7758\n",
      "Epoch [254/300], Step [31/172], Loss: 29.6695\n",
      "Epoch [254/300], Step [32/172], Loss: 40.8171\n",
      "Epoch [254/300], Step [33/172], Loss: 63.6444\n",
      "Epoch [254/300], Step [34/172], Loss: 1.8990\n",
      "Epoch [254/300], Step [35/172], Loss: 14.5851\n",
      "Epoch [254/300], Step [36/172], Loss: 15.2362\n",
      "Epoch [254/300], Step [37/172], Loss: 15.4492\n",
      "Epoch [254/300], Step [38/172], Loss: 31.3270\n",
      "Epoch [254/300], Step [39/172], Loss: 34.6306\n",
      "Epoch [254/300], Step [40/172], Loss: 21.9932\n",
      "Epoch [254/300], Step [41/172], Loss: 32.6940\n",
      "Epoch [254/300], Step [42/172], Loss: 38.7511\n",
      "Epoch [254/300], Step [43/172], Loss: 27.7644\n",
      "Epoch [254/300], Step [44/172], Loss: 22.7525\n",
      "Epoch [254/300], Step [45/172], Loss: 32.4965\n",
      "Epoch [254/300], Step [46/172], Loss: 17.4080\n",
      "Epoch [254/300], Step [47/172], Loss: 50.9747\n",
      "Epoch [254/300], Step [48/172], Loss: 64.0652\n",
      "Epoch [254/300], Step [49/172], Loss: 25.8724\n",
      "Epoch [254/300], Step [50/172], Loss: 51.8430\n",
      "Epoch [254/300], Step [51/172], Loss: 9.5793\n",
      "Epoch [254/300], Step [52/172], Loss: 25.1187\n",
      "Epoch [254/300], Step [53/172], Loss: 26.2979\n",
      "Epoch [254/300], Step [54/172], Loss: 19.3751\n",
      "Epoch [254/300], Step [55/172], Loss: 19.3007\n",
      "Epoch [254/300], Step [56/172], Loss: 17.6719\n",
      "Epoch [254/300], Step [57/172], Loss: 20.2182\n",
      "Epoch [254/300], Step [58/172], Loss: 17.0512\n",
      "Epoch [254/300], Step [59/172], Loss: 31.4158\n",
      "Epoch [254/300], Step [60/172], Loss: 22.4906\n",
      "Epoch [254/300], Step [61/172], Loss: 8.4219\n",
      "Epoch [254/300], Step [62/172], Loss: 16.5351\n",
      "Epoch [254/300], Step [63/172], Loss: 12.2009\n",
      "Epoch [254/300], Step [64/172], Loss: 14.8284\n",
      "Epoch [254/300], Step [65/172], Loss: 22.4316\n",
      "Epoch [254/300], Step [66/172], Loss: 9.5190\n",
      "Epoch [254/300], Step [67/172], Loss: 27.7323\n",
      "Epoch [254/300], Step [68/172], Loss: 6.5955\n",
      "Epoch [254/300], Step [69/172], Loss: 29.5722\n",
      "Epoch [254/300], Step [70/172], Loss: 29.1540\n",
      "Epoch [254/300], Step [71/172], Loss: 32.1191\n",
      "Epoch [254/300], Step [72/172], Loss: 28.7936\n",
      "Epoch [254/300], Step [73/172], Loss: 36.9593\n",
      "Epoch [254/300], Step [74/172], Loss: 20.5670\n",
      "Epoch [254/300], Step [75/172], Loss: 20.6585\n",
      "Epoch [254/300], Step [76/172], Loss: 24.8449\n",
      "Epoch [254/300], Step [77/172], Loss: 41.7162\n",
      "Epoch [254/300], Step [78/172], Loss: 29.9423\n",
      "Epoch [254/300], Step [79/172], Loss: 28.2576\n",
      "Epoch [254/300], Step [80/172], Loss: 45.2999\n",
      "Epoch [254/300], Step [81/172], Loss: 25.5746\n",
      "Epoch [254/300], Step [82/172], Loss: 36.1694\n",
      "Epoch [254/300], Step [83/172], Loss: 38.8785\n",
      "Epoch [254/300], Step [84/172], Loss: 30.1783\n",
      "Epoch [254/300], Step [85/172], Loss: 33.8355\n",
      "Epoch [254/300], Step [86/172], Loss: 30.9202\n",
      "Epoch [254/300], Step [87/172], Loss: 22.3532\n",
      "Epoch [254/300], Step [88/172], Loss: 20.4446\n",
      "Epoch [254/300], Step [89/172], Loss: 26.9293\n",
      "Epoch [254/300], Step [90/172], Loss: 19.5956\n",
      "Epoch [254/300], Step [91/172], Loss: 25.9217\n",
      "Epoch [254/300], Step [92/172], Loss: 18.9476\n",
      "Epoch [254/300], Step [93/172], Loss: 18.4529\n",
      "Epoch [254/300], Step [94/172], Loss: 25.0962\n",
      "Epoch [254/300], Step [95/172], Loss: 20.4612\n",
      "Epoch [254/300], Step [96/172], Loss: 20.1509\n",
      "Epoch [254/300], Step [97/172], Loss: 30.3708\n",
      "Epoch [254/300], Step [98/172], Loss: 19.0585\n",
      "Epoch [254/300], Step [99/172], Loss: 19.7124\n",
      "Epoch [254/300], Step [100/172], Loss: 18.4571\n",
      "Epoch [254/300], Step [101/172], Loss: 20.0620\n",
      "Epoch [254/300], Step [102/172], Loss: 19.2924\n",
      "Epoch [254/300], Step [103/172], Loss: 12.9241\n",
      "Epoch [254/300], Step [104/172], Loss: 20.4923\n",
      "Epoch [254/300], Step [105/172], Loss: 24.4674\n",
      "Epoch [254/300], Step [106/172], Loss: 16.4723\n",
      "Epoch [254/300], Step [107/172], Loss: 17.0544\n",
      "Epoch [254/300], Step [108/172], Loss: 17.1517\n",
      "Epoch [254/300], Step [109/172], Loss: 15.9182\n",
      "Epoch [254/300], Step [110/172], Loss: 19.0846\n",
      "Epoch [254/300], Step [111/172], Loss: 18.7921\n",
      "Epoch [254/300], Step [112/172], Loss: 17.5315\n",
      "Epoch [254/300], Step [113/172], Loss: 15.4868\n",
      "Epoch [254/300], Step [114/172], Loss: 16.0299\n",
      "Epoch [254/300], Step [115/172], Loss: 20.3672\n",
      "Epoch [254/300], Step [116/172], Loss: 15.3366\n",
      "Epoch [254/300], Step [117/172], Loss: 13.7291\n",
      "Epoch [254/300], Step [118/172], Loss: 13.7936\n",
      "Epoch [254/300], Step [119/172], Loss: 19.5007\n",
      "Epoch [254/300], Step [120/172], Loss: 10.8448\n",
      "Epoch [254/300], Step [121/172], Loss: 9.5451\n",
      "Epoch [254/300], Step [122/172], Loss: 13.5457\n",
      "Epoch [254/300], Step [123/172], Loss: 12.5999\n",
      "Epoch [254/300], Step [124/172], Loss: 7.7086\n",
      "Epoch [254/300], Step [125/172], Loss: 13.0345\n",
      "Epoch [254/300], Step [126/172], Loss: 13.2121\n",
      "Epoch [254/300], Step [127/172], Loss: 11.7209\n",
      "Epoch [254/300], Step [128/172], Loss: 10.9254\n",
      "Epoch [254/300], Step [129/172], Loss: 9.3974\n",
      "Epoch [254/300], Step [130/172], Loss: 13.7872\n",
      "Epoch [254/300], Step [131/172], Loss: 8.5293\n",
      "Epoch [254/300], Step [132/172], Loss: 10.9749\n",
      "Epoch [254/300], Step [133/172], Loss: 10.4120\n",
      "Epoch [254/300], Step [134/172], Loss: 11.0308\n",
      "Epoch [254/300], Step [135/172], Loss: 9.7985\n",
      "Epoch [254/300], Step [136/172], Loss: 9.0992\n",
      "Epoch [254/300], Step [137/172], Loss: 9.2559\n",
      "Epoch [254/300], Step [138/172], Loss: 9.0774\n",
      "Epoch [254/300], Step [139/172], Loss: 11.5914\n",
      "Epoch [254/300], Step [140/172], Loss: 11.8432\n",
      "Epoch [254/300], Step [141/172], Loss: 9.6524\n",
      "Epoch [254/300], Step [142/172], Loss: 15.4593\n",
      "Epoch [254/300], Step [143/172], Loss: 12.6316\n",
      "Epoch [254/300], Step [144/172], Loss: 9.9536\n",
      "Epoch [254/300], Step [145/172], Loss: 11.7392\n",
      "Epoch [254/300], Step [146/172], Loss: 11.7586\n",
      "Epoch [254/300], Step [147/172], Loss: 6.0611\n",
      "Epoch [254/300], Step [148/172], Loss: 6.9920\n",
      "Epoch [254/300], Step [149/172], Loss: 7.2867\n",
      "Epoch [254/300], Step [150/172], Loss: 6.0939\n",
      "Epoch [254/300], Step [151/172], Loss: 5.8968\n",
      "Epoch [254/300], Step [152/172], Loss: 8.7080\n",
      "Epoch [254/300], Step [153/172], Loss: 6.6811\n",
      "Epoch [254/300], Step [154/172], Loss: 6.9498\n",
      "Epoch [254/300], Step [155/172], Loss: 7.0756\n",
      "Epoch [254/300], Step [156/172], Loss: 15.1044\n",
      "Epoch [254/300], Step [157/172], Loss: 9.4076\n",
      "Epoch [254/300], Step [158/172], Loss: 7.7444\n",
      "Epoch [254/300], Step [159/172], Loss: 10.4578\n",
      "Epoch [254/300], Step [160/172], Loss: 10.5727\n",
      "Epoch [254/300], Step [161/172], Loss: 8.3848\n",
      "Epoch [254/300], Step [162/172], Loss: 5.1837\n",
      "Epoch [254/300], Step [163/172], Loss: 7.2846\n",
      "Epoch [254/300], Step [164/172], Loss: 9.1315\n",
      "Epoch [254/300], Step [165/172], Loss: 7.2710\n",
      "Epoch [254/300], Step [166/172], Loss: 6.5080\n",
      "Epoch [254/300], Step [167/172], Loss: 11.3891\n",
      "Epoch [254/300], Step [168/172], Loss: 7.1668\n",
      "Epoch [254/300], Step [169/172], Loss: 7.3584\n",
      "Epoch [254/300], Step [170/172], Loss: 5.9701\n",
      "Epoch [254/300], Step [171/172], Loss: 9.4471\n",
      "Epoch [254/300], Step [172/172], Loss: 5.8351\n",
      "Epoch [255/300], Step [1/172], Loss: 41.4928\n",
      "Epoch [255/300], Step [2/172], Loss: 44.9498\n",
      "Epoch [255/300], Step [3/172], Loss: 41.3025\n",
      "Epoch [255/300], Step [4/172], Loss: 19.7019\n",
      "Epoch [255/300], Step [5/172], Loss: 37.2436\n",
      "Epoch [255/300], Step [6/172], Loss: 17.7615\n",
      "Epoch [255/300], Step [7/172], Loss: 27.0969\n",
      "Epoch [255/300], Step [8/172], Loss: 4.4727\n",
      "Epoch [255/300], Step [9/172], Loss: 25.5751\n",
      "Epoch [255/300], Step [10/172], Loss: 36.1956\n",
      "Epoch [255/300], Step [11/172], Loss: 50.1994\n",
      "Epoch [255/300], Step [12/172], Loss: 50.3845\n",
      "Epoch [255/300], Step [13/172], Loss: 30.4413\n",
      "Epoch [255/300], Step [14/172], Loss: 52.1510\n",
      "Epoch [255/300], Step [15/172], Loss: 47.4043\n",
      "Epoch [255/300], Step [16/172], Loss: 7.6134\n",
      "Epoch [255/300], Step [17/172], Loss: 36.3018\n",
      "Epoch [255/300], Step [18/172], Loss: 50.0252\n",
      "Epoch [255/300], Step [19/172], Loss: 69.5851\n",
      "Epoch [255/300], Step [20/172], Loss: 25.8950\n",
      "Epoch [255/300], Step [21/172], Loss: 72.7719\n",
      "Epoch [255/300], Step [22/172], Loss: 49.4312\n",
      "Epoch [255/300], Step [23/172], Loss: 1.4978\n",
      "Epoch [255/300], Step [24/172], Loss: 46.3484\n",
      "Epoch [255/300], Step [25/172], Loss: 32.7403\n",
      "Epoch [255/300], Step [26/172], Loss: 42.6878\n",
      "Epoch [255/300], Step [27/172], Loss: 54.3199\n",
      "Epoch [255/300], Step [28/172], Loss: 16.9642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [255/300], Step [29/172], Loss: 13.5771\n",
      "Epoch [255/300], Step [30/172], Loss: 47.7225\n",
      "Epoch [255/300], Step [31/172], Loss: 29.5794\n",
      "Epoch [255/300], Step [32/172], Loss: 40.6992\n",
      "Epoch [255/300], Step [33/172], Loss: 63.1955\n",
      "Epoch [255/300], Step [34/172], Loss: 2.2180\n",
      "Epoch [255/300], Step [35/172], Loss: 14.7056\n",
      "Epoch [255/300], Step [36/172], Loss: 15.7484\n",
      "Epoch [255/300], Step [37/172], Loss: 15.3404\n",
      "Epoch [255/300], Step [38/172], Loss: 31.0670\n",
      "Epoch [255/300], Step [39/172], Loss: 34.1699\n",
      "Epoch [255/300], Step [40/172], Loss: 21.7319\n",
      "Epoch [255/300], Step [41/172], Loss: 31.8739\n",
      "Epoch [255/300], Step [42/172], Loss: 38.4108\n",
      "Epoch [255/300], Step [43/172], Loss: 27.5945\n",
      "Epoch [255/300], Step [44/172], Loss: 22.5890\n",
      "Epoch [255/300], Step [45/172], Loss: 32.3434\n",
      "Epoch [255/300], Step [46/172], Loss: 17.1800\n",
      "Epoch [255/300], Step [47/172], Loss: 50.5698\n",
      "Epoch [255/300], Step [48/172], Loss: 63.6624\n",
      "Epoch [255/300], Step [49/172], Loss: 25.8856\n",
      "Epoch [255/300], Step [50/172], Loss: 52.0849\n",
      "Epoch [255/300], Step [51/172], Loss: 9.5642\n",
      "Epoch [255/300], Step [52/172], Loss: 25.0185\n",
      "Epoch [255/300], Step [53/172], Loss: 26.1254\n",
      "Epoch [255/300], Step [54/172], Loss: 19.3497\n",
      "Epoch [255/300], Step [55/172], Loss: 19.0773\n",
      "Epoch [255/300], Step [56/172], Loss: 17.9051\n",
      "Epoch [255/300], Step [57/172], Loss: 20.2842\n",
      "Epoch [255/300], Step [58/172], Loss: 16.7847\n",
      "Epoch [255/300], Step [59/172], Loss: 31.1911\n",
      "Epoch [255/300], Step [60/172], Loss: 23.0516\n",
      "Epoch [255/300], Step [61/172], Loss: 8.5275\n",
      "Epoch [255/300], Step [62/172], Loss: 16.4218\n",
      "Epoch [255/300], Step [63/172], Loss: 12.2640\n",
      "Epoch [255/300], Step [64/172], Loss: 14.7133\n",
      "Epoch [255/300], Step [65/172], Loss: 22.5347\n",
      "Epoch [255/300], Step [66/172], Loss: 9.3632\n",
      "Epoch [255/300], Step [67/172], Loss: 27.6933\n",
      "Epoch [255/300], Step [68/172], Loss: 6.3419\n",
      "Epoch [255/300], Step [69/172], Loss: 29.5083\n",
      "Epoch [255/300], Step [70/172], Loss: 29.6682\n",
      "Epoch [255/300], Step [71/172], Loss: 32.2141\n",
      "Epoch [255/300], Step [72/172], Loss: 28.9000\n",
      "Epoch [255/300], Step [73/172], Loss: 37.3882\n",
      "Epoch [255/300], Step [74/172], Loss: 20.4886\n",
      "Epoch [255/300], Step [75/172], Loss: 20.6112\n",
      "Epoch [255/300], Step [76/172], Loss: 24.9559\n",
      "Epoch [255/300], Step [77/172], Loss: 41.9593\n",
      "Epoch [255/300], Step [78/172], Loss: 30.1642\n",
      "Epoch [255/300], Step [79/172], Loss: 28.1717\n",
      "Epoch [255/300], Step [80/172], Loss: 45.3089\n",
      "Epoch [255/300], Step [81/172], Loss: 25.5294\n",
      "Epoch [255/300], Step [82/172], Loss: 36.4415\n",
      "Epoch [255/300], Step [83/172], Loss: 38.8553\n",
      "Epoch [255/300], Step [84/172], Loss: 30.0306\n",
      "Epoch [255/300], Step [85/172], Loss: 33.6560\n",
      "Epoch [255/300], Step [86/172], Loss: 30.9162\n",
      "Epoch [255/300], Step [87/172], Loss: 22.1755\n",
      "Epoch [255/300], Step [88/172], Loss: 20.3703\n",
      "Epoch [255/300], Step [89/172], Loss: 26.7993\n",
      "Epoch [255/300], Step [90/172], Loss: 19.5901\n",
      "Epoch [255/300], Step [91/172], Loss: 25.7249\n",
      "Epoch [255/300], Step [92/172], Loss: 18.8444\n",
      "Epoch [255/300], Step [93/172], Loss: 18.3670\n",
      "Epoch [255/300], Step [94/172], Loss: 25.0824\n",
      "Epoch [255/300], Step [95/172], Loss: 20.4564\n",
      "Epoch [255/300], Step [96/172], Loss: 20.0446\n",
      "Epoch [255/300], Step [97/172], Loss: 30.1838\n",
      "Epoch [255/300], Step [98/172], Loss: 18.8537\n",
      "Epoch [255/300], Step [99/172], Loss: 19.5398\n",
      "Epoch [255/300], Step [100/172], Loss: 18.2208\n",
      "Epoch [255/300], Step [101/172], Loss: 19.8811\n",
      "Epoch [255/300], Step [102/172], Loss: 19.3362\n",
      "Epoch [255/300], Step [103/172], Loss: 12.8361\n",
      "Epoch [255/300], Step [104/172], Loss: 20.3212\n",
      "Epoch [255/300], Step [105/172], Loss: 24.4212\n",
      "Epoch [255/300], Step [106/172], Loss: 16.3411\n",
      "Epoch [255/300], Step [107/172], Loss: 16.9061\n",
      "Epoch [255/300], Step [108/172], Loss: 17.0211\n",
      "Epoch [255/300], Step [109/172], Loss: 15.9100\n",
      "Epoch [255/300], Step [110/172], Loss: 19.0258\n",
      "Epoch [255/300], Step [111/172], Loss: 18.8122\n",
      "Epoch [255/300], Step [112/172], Loss: 17.5347\n",
      "Epoch [255/300], Step [113/172], Loss: 15.3735\n",
      "Epoch [255/300], Step [114/172], Loss: 16.0668\n",
      "Epoch [255/300], Step [115/172], Loss: 20.5256\n",
      "Epoch [255/300], Step [116/172], Loss: 15.4961\n",
      "Epoch [255/300], Step [117/172], Loss: 13.7951\n",
      "Epoch [255/300], Step [118/172], Loss: 13.7879\n",
      "Epoch [255/300], Step [119/172], Loss: 19.4224\n",
      "Epoch [255/300], Step [120/172], Loss: 10.8017\n",
      "Epoch [255/300], Step [121/172], Loss: 9.5041\n",
      "Epoch [255/300], Step [122/172], Loss: 13.5789\n",
      "Epoch [255/300], Step [123/172], Loss: 12.8913\n",
      "Epoch [255/300], Step [124/172], Loss: 7.7440\n",
      "Epoch [255/300], Step [125/172], Loss: 13.0230\n",
      "Epoch [255/300], Step [126/172], Loss: 13.3113\n",
      "Epoch [255/300], Step [127/172], Loss: 11.8214\n",
      "Epoch [255/300], Step [128/172], Loss: 10.9942\n",
      "Epoch [255/300], Step [129/172], Loss: 9.3655\n",
      "Epoch [255/300], Step [130/172], Loss: 13.6964\n",
      "Epoch [255/300], Step [131/172], Loss: 8.5078\n",
      "Epoch [255/300], Step [132/172], Loss: 10.9043\n",
      "Epoch [255/300], Step [133/172], Loss: 10.4826\n",
      "Epoch [255/300], Step [134/172], Loss: 11.0003\n",
      "Epoch [255/300], Step [135/172], Loss: 9.7330\n",
      "Epoch [255/300], Step [136/172], Loss: 9.1060\n",
      "Epoch [255/300], Step [137/172], Loss: 9.2836\n",
      "Epoch [255/300], Step [138/172], Loss: 8.9350\n",
      "Epoch [255/300], Step [139/172], Loss: 11.5987\n",
      "Epoch [255/300], Step [140/172], Loss: 11.8103\n",
      "Epoch [255/300], Step [141/172], Loss: 9.6833\n",
      "Epoch [255/300], Step [142/172], Loss: 15.8371\n",
      "Epoch [255/300], Step [143/172], Loss: 12.5501\n",
      "Epoch [255/300], Step [144/172], Loss: 9.9808\n",
      "Epoch [255/300], Step [145/172], Loss: 11.7496\n",
      "Epoch [255/300], Step [146/172], Loss: 11.6807\n",
      "Epoch [255/300], Step [147/172], Loss: 6.0028\n",
      "Epoch [255/300], Step [148/172], Loss: 6.8980\n",
      "Epoch [255/300], Step [149/172], Loss: 7.1963\n",
      "Epoch [255/300], Step [150/172], Loss: 6.0471\n",
      "Epoch [255/300], Step [151/172], Loss: 5.8604\n",
      "Epoch [255/300], Step [152/172], Loss: 8.6021\n",
      "Epoch [255/300], Step [153/172], Loss: 6.6043\n",
      "Epoch [255/300], Step [154/172], Loss: 6.9102\n",
      "Epoch [255/300], Step [155/172], Loss: 7.0032\n",
      "Epoch [255/300], Step [156/172], Loss: 15.2828\n",
      "Epoch [255/300], Step [157/172], Loss: 9.4715\n",
      "Epoch [255/300], Step [158/172], Loss: 7.8929\n",
      "Epoch [255/300], Step [159/172], Loss: 10.4264\n",
      "Epoch [255/300], Step [160/172], Loss: 10.8372\n",
      "Epoch [255/300], Step [161/172], Loss: 8.1196\n",
      "Epoch [255/300], Step [162/172], Loss: 5.1333\n",
      "Epoch [255/300], Step [163/172], Loss: 7.2348\n",
      "Epoch [255/300], Step [164/172], Loss: 9.2278\n",
      "Epoch [255/300], Step [165/172], Loss: 7.2253\n",
      "Epoch [255/300], Step [166/172], Loss: 6.4254\n",
      "Epoch [255/300], Step [167/172], Loss: 11.4890\n",
      "Epoch [255/300], Step [168/172], Loss: 7.0671\n",
      "Epoch [255/300], Step [169/172], Loss: 7.3524\n",
      "Epoch [255/300], Step [170/172], Loss: 5.9415\n",
      "Epoch [255/300], Step [171/172], Loss: 9.6924\n",
      "Epoch [255/300], Step [172/172], Loss: 5.6382\n",
      "Epoch [256/300], Step [1/172], Loss: 41.4964\n",
      "Epoch [256/300], Step [2/172], Loss: 43.9056\n",
      "Epoch [256/300], Step [3/172], Loss: 39.8392\n",
      "Epoch [256/300], Step [4/172], Loss: 19.3812\n",
      "Epoch [256/300], Step [5/172], Loss: 37.3624\n",
      "Epoch [256/300], Step [6/172], Loss: 16.7877\n",
      "Epoch [256/300], Step [7/172], Loss: 23.1504\n",
      "Epoch [256/300], Step [8/172], Loss: 3.8010\n",
      "Epoch [256/300], Step [9/172], Loss: 25.4735\n",
      "Epoch [256/300], Step [10/172], Loss: 37.0016\n",
      "Epoch [256/300], Step [11/172], Loss: 50.4759\n",
      "Epoch [256/300], Step [12/172], Loss: 49.8953\n",
      "Epoch [256/300], Step [13/172], Loss: 30.1894\n",
      "Epoch [256/300], Step [14/172], Loss: 51.8496\n",
      "Epoch [256/300], Step [15/172], Loss: 47.2172\n",
      "Epoch [256/300], Step [16/172], Loss: 9.6034\n",
      "Epoch [256/300], Step [17/172], Loss: 35.2349\n",
      "Epoch [256/300], Step [18/172], Loss: 49.2648\n",
      "Epoch [256/300], Step [19/172], Loss: 68.1930\n",
      "Epoch [256/300], Step [20/172], Loss: 25.3736\n",
      "Epoch [256/300], Step [21/172], Loss: 71.6950\n",
      "Epoch [256/300], Step [22/172], Loss: 49.5261\n",
      "Epoch [256/300], Step [23/172], Loss: 1.7711\n",
      "Epoch [256/300], Step [24/172], Loss: 45.4841\n",
      "Epoch [256/300], Step [25/172], Loss: 33.2623\n",
      "Epoch [256/300], Step [26/172], Loss: 43.0383\n",
      "Epoch [256/300], Step [27/172], Loss: 53.8957\n",
      "Epoch [256/300], Step [28/172], Loss: 16.7657\n",
      "Epoch [256/300], Step [29/172], Loss: 13.3157\n",
      "Epoch [256/300], Step [30/172], Loss: 47.5636\n",
      "Epoch [256/300], Step [31/172], Loss: 29.5160\n",
      "Epoch [256/300], Step [32/172], Loss: 40.8776\n",
      "Epoch [256/300], Step [33/172], Loss: 62.9151\n",
      "Epoch [256/300], Step [34/172], Loss: 1.6959\n",
      "Epoch [256/300], Step [35/172], Loss: 14.5772\n",
      "Epoch [256/300], Step [36/172], Loss: 15.0750\n",
      "Epoch [256/300], Step [37/172], Loss: 15.4940\n",
      "Epoch [256/300], Step [38/172], Loss: 31.2922\n",
      "Epoch [256/300], Step [39/172], Loss: 34.6713\n",
      "Epoch [256/300], Step [40/172], Loss: 21.8637\n",
      "Epoch [256/300], Step [41/172], Loss: 32.4309\n",
      "Epoch [256/300], Step [42/172], Loss: 38.6084\n",
      "Epoch [256/300], Step [43/172], Loss: 27.8064\n",
      "Epoch [256/300], Step [44/172], Loss: 22.7833\n",
      "Epoch [256/300], Step [45/172], Loss: 32.0779\n",
      "Epoch [256/300], Step [46/172], Loss: 17.2779\n",
      "Epoch [256/300], Step [47/172], Loss: 50.5475\n",
      "Epoch [256/300], Step [48/172], Loss: 64.5846\n",
      "Epoch [256/300], Step [49/172], Loss: 25.5360\n",
      "Epoch [256/300], Step [50/172], Loss: 52.3540\n",
      "Epoch [256/300], Step [51/172], Loss: 9.3471\n",
      "Epoch [256/300], Step [52/172], Loss: 24.7309\n",
      "Epoch [256/300], Step [53/172], Loss: 26.0752\n",
      "Epoch [256/300], Step [54/172], Loss: 18.5628\n",
      "Epoch [256/300], Step [55/172], Loss: 18.5320\n",
      "Epoch [256/300], Step [56/172], Loss: 17.4646\n",
      "Epoch [256/300], Step [57/172], Loss: 20.4222\n",
      "Epoch [256/300], Step [58/172], Loss: 16.4104\n",
      "Epoch [256/300], Step [59/172], Loss: 30.9974\n",
      "Epoch [256/300], Step [60/172], Loss: 22.8641\n",
      "Epoch [256/300], Step [61/172], Loss: 8.1646\n",
      "Epoch [256/300], Step [62/172], Loss: 16.5796\n",
      "Epoch [256/300], Step [63/172], Loss: 11.7188\n",
      "Epoch [256/300], Step [64/172], Loss: 14.4037\n",
      "Epoch [256/300], Step [65/172], Loss: 22.0991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [256/300], Step [66/172], Loss: 9.4915\n",
      "Epoch [256/300], Step [67/172], Loss: 27.6220\n",
      "Epoch [256/300], Step [68/172], Loss: 6.2703\n",
      "Epoch [256/300], Step [69/172], Loss: 29.3258\n",
      "Epoch [256/300], Step [70/172], Loss: 29.1912\n",
      "Epoch [256/300], Step [71/172], Loss: 32.0278\n",
      "Epoch [256/300], Step [72/172], Loss: 28.8179\n",
      "Epoch [256/300], Step [73/172], Loss: 37.0378\n",
      "Epoch [256/300], Step [74/172], Loss: 20.5668\n",
      "Epoch [256/300], Step [75/172], Loss: 20.7403\n",
      "Epoch [256/300], Step [76/172], Loss: 24.5502\n",
      "Epoch [256/300], Step [77/172], Loss: 41.8401\n",
      "Epoch [256/300], Step [78/172], Loss: 29.7295\n",
      "Epoch [256/300], Step [79/172], Loss: 27.9811\n",
      "Epoch [256/300], Step [80/172], Loss: 45.2382\n",
      "Epoch [256/300], Step [81/172], Loss: 25.3996\n",
      "Epoch [256/300], Step [82/172], Loss: 35.8811\n",
      "Epoch [256/300], Step [83/172], Loss: 38.8677\n",
      "Epoch [256/300], Step [84/172], Loss: 30.0415\n",
      "Epoch [256/300], Step [85/172], Loss: 33.5563\n",
      "Epoch [256/300], Step [86/172], Loss: 30.5633\n",
      "Epoch [256/300], Step [87/172], Loss: 22.1366\n",
      "Epoch [256/300], Step [88/172], Loss: 20.2303\n",
      "Epoch [256/300], Step [89/172], Loss: 26.6100\n",
      "Epoch [256/300], Step [90/172], Loss: 19.2715\n",
      "Epoch [256/300], Step [91/172], Loss: 25.7098\n",
      "Epoch [256/300], Step [92/172], Loss: 18.7449\n",
      "Epoch [256/300], Step [93/172], Loss: 18.2676\n",
      "Epoch [256/300], Step [94/172], Loss: 24.9615\n",
      "Epoch [256/300], Step [95/172], Loss: 20.3840\n",
      "Epoch [256/300], Step [96/172], Loss: 19.9778\n",
      "Epoch [256/300], Step [97/172], Loss: 30.0072\n",
      "Epoch [256/300], Step [98/172], Loss: 18.6557\n",
      "Epoch [256/300], Step [99/172], Loss: 19.2534\n",
      "Epoch [256/300], Step [100/172], Loss: 17.9606\n",
      "Epoch [256/300], Step [101/172], Loss: 19.7191\n",
      "Epoch [256/300], Step [102/172], Loss: 19.0789\n",
      "Epoch [256/300], Step [103/172], Loss: 12.5804\n",
      "Epoch [256/300], Step [104/172], Loss: 20.2354\n",
      "Epoch [256/300], Step [105/172], Loss: 24.2043\n",
      "Epoch [256/300], Step [106/172], Loss: 16.2690\n",
      "Epoch [256/300], Step [107/172], Loss: 16.8543\n",
      "Epoch [256/300], Step [108/172], Loss: 16.9505\n",
      "Epoch [256/300], Step [109/172], Loss: 15.6546\n",
      "Epoch [256/300], Step [110/172], Loss: 18.6641\n",
      "Epoch [256/300], Step [111/172], Loss: 18.6365\n",
      "Epoch [256/300], Step [112/172], Loss: 17.4608\n",
      "Epoch [256/300], Step [113/172], Loss: 15.3144\n",
      "Epoch [256/300], Step [114/172], Loss: 15.9412\n",
      "Epoch [256/300], Step [115/172], Loss: 20.3448\n",
      "Epoch [256/300], Step [116/172], Loss: 15.2464\n",
      "Epoch [256/300], Step [117/172], Loss: 13.7886\n",
      "Epoch [256/300], Step [118/172], Loss: 13.8263\n",
      "Epoch [256/300], Step [119/172], Loss: 19.3503\n",
      "Epoch [256/300], Step [120/172], Loss: 10.7981\n",
      "Epoch [256/300], Step [121/172], Loss: 9.4185\n",
      "Epoch [256/300], Step [122/172], Loss: 13.3743\n",
      "Epoch [256/300], Step [123/172], Loss: 13.0461\n",
      "Epoch [256/300], Step [124/172], Loss: 7.6560\n",
      "Epoch [256/300], Step [125/172], Loss: 12.8526\n",
      "Epoch [256/300], Step [126/172], Loss: 13.1555\n",
      "Epoch [256/300], Step [127/172], Loss: 11.5459\n",
      "Epoch [256/300], Step [128/172], Loss: 10.5796\n",
      "Epoch [256/300], Step [129/172], Loss: 9.2568\n",
      "Epoch [256/300], Step [130/172], Loss: 13.6798\n",
      "Epoch [256/300], Step [131/172], Loss: 8.3890\n",
      "Epoch [256/300], Step [132/172], Loss: 10.7049\n",
      "Epoch [256/300], Step [133/172], Loss: 10.3289\n",
      "Epoch [256/300], Step [134/172], Loss: 10.9735\n",
      "Epoch [256/300], Step [135/172], Loss: 9.6790\n",
      "Epoch [256/300], Step [136/172], Loss: 9.1661\n",
      "Epoch [256/300], Step [137/172], Loss: 9.1994\n",
      "Epoch [256/300], Step [138/172], Loss: 8.7870\n",
      "Epoch [256/300], Step [139/172], Loss: 11.5417\n",
      "Epoch [256/300], Step [140/172], Loss: 11.6375\n",
      "Epoch [256/300], Step [141/172], Loss: 9.6102\n",
      "Epoch [256/300], Step [142/172], Loss: 15.7092\n",
      "Epoch [256/300], Step [143/172], Loss: 12.5700\n",
      "Epoch [256/300], Step [144/172], Loss: 9.8345\n",
      "Epoch [256/300], Step [145/172], Loss: 11.7628\n",
      "Epoch [256/300], Step [146/172], Loss: 11.5216\n",
      "Epoch [256/300], Step [147/172], Loss: 5.9635\n",
      "Epoch [256/300], Step [148/172], Loss: 6.7690\n",
      "Epoch [256/300], Step [149/172], Loss: 7.1319\n",
      "Epoch [256/300], Step [150/172], Loss: 5.9244\n",
      "Epoch [256/300], Step [151/172], Loss: 5.7796\n",
      "Epoch [256/300], Step [152/172], Loss: 8.5045\n",
      "Epoch [256/300], Step [153/172], Loss: 6.6379\n",
      "Epoch [256/300], Step [154/172], Loss: 6.8500\n",
      "Epoch [256/300], Step [155/172], Loss: 7.0348\n",
      "Epoch [256/300], Step [156/172], Loss: 15.0561\n",
      "Epoch [256/300], Step [157/172], Loss: 9.6528\n",
      "Epoch [256/300], Step [158/172], Loss: 7.7709\n",
      "Epoch [256/300], Step [159/172], Loss: 10.5064\n",
      "Epoch [256/300], Step [160/172], Loss: 10.8373\n",
      "Epoch [256/300], Step [161/172], Loss: 7.7916\n",
      "Epoch [256/300], Step [162/172], Loss: 5.0794\n",
      "Epoch [256/300], Step [163/172], Loss: 7.2851\n",
      "Epoch [256/300], Step [164/172], Loss: 9.0036\n",
      "Epoch [256/300], Step [165/172], Loss: 7.1664\n",
      "Epoch [256/300], Step [166/172], Loss: 6.4189\n",
      "Epoch [256/300], Step [167/172], Loss: 11.3406\n",
      "Epoch [256/300], Step [168/172], Loss: 6.9952\n",
      "Epoch [256/300], Step [169/172], Loss: 7.2416\n",
      "Epoch [256/300], Step [170/172], Loss: 5.9482\n",
      "Epoch [256/300], Step [171/172], Loss: 9.5524\n",
      "Epoch [256/300], Step [172/172], Loss: 5.8084\n",
      "Epoch [257/300], Step [1/172], Loss: 41.4632\n",
      "Epoch [257/300], Step [2/172], Loss: 44.3696\n",
      "Epoch [257/300], Step [3/172], Loss: 39.6977\n",
      "Epoch [257/300], Step [4/172], Loss: 19.4080\n",
      "Epoch [257/300], Step [5/172], Loss: 37.3307\n",
      "Epoch [257/300], Step [6/172], Loss: 16.9725\n",
      "Epoch [257/300], Step [7/172], Loss: 23.7417\n",
      "Epoch [257/300], Step [8/172], Loss: 4.1259\n",
      "Epoch [257/300], Step [9/172], Loss: 25.4079\n",
      "Epoch [257/300], Step [10/172], Loss: 35.9008\n",
      "Epoch [257/300], Step [11/172], Loss: 50.5090\n",
      "Epoch [257/300], Step [12/172], Loss: 50.0201\n",
      "Epoch [257/300], Step [13/172], Loss: 30.1201\n",
      "Epoch [257/300], Step [14/172], Loss: 52.2011\n",
      "Epoch [257/300], Step [15/172], Loss: 47.9045\n",
      "Epoch [257/300], Step [16/172], Loss: 8.7381\n",
      "Epoch [257/300], Step [17/172], Loss: 35.7068\n",
      "Epoch [257/300], Step [18/172], Loss: 50.1338\n",
      "Epoch [257/300], Step [19/172], Loss: 68.4768\n",
      "Epoch [257/300], Step [20/172], Loss: 25.4459\n",
      "Epoch [257/300], Step [21/172], Loss: 72.5948\n",
      "Epoch [257/300], Step [22/172], Loss: 49.4268\n",
      "Epoch [257/300], Step [23/172], Loss: 1.7014\n",
      "Epoch [257/300], Step [24/172], Loss: 45.6854\n",
      "Epoch [257/300], Step [25/172], Loss: 32.7189\n",
      "Epoch [257/300], Step [26/172], Loss: 42.8811\n",
      "Epoch [257/300], Step [27/172], Loss: 54.4499\n",
      "Epoch [257/300], Step [28/172], Loss: 16.6527\n",
      "Epoch [257/300], Step [29/172], Loss: 13.5473\n",
      "Epoch [257/300], Step [30/172], Loss: 47.4392\n",
      "Epoch [257/300], Step [31/172], Loss: 29.3536\n",
      "Epoch [257/300], Step [32/172], Loss: 40.2495\n",
      "Epoch [257/300], Step [33/172], Loss: 61.9816\n",
      "Epoch [257/300], Step [34/172], Loss: 2.0716\n",
      "Epoch [257/300], Step [35/172], Loss: 14.6932\n",
      "Epoch [257/300], Step [36/172], Loss: 15.8867\n",
      "Epoch [257/300], Step [37/172], Loss: 15.2799\n",
      "Epoch [257/300], Step [38/172], Loss: 30.9076\n",
      "Epoch [257/300], Step [39/172], Loss: 34.2127\n",
      "Epoch [257/300], Step [40/172], Loss: 21.6517\n",
      "Epoch [257/300], Step [41/172], Loss: 31.6980\n",
      "Epoch [257/300], Step [42/172], Loss: 38.1122\n",
      "Epoch [257/300], Step [43/172], Loss: 27.5186\n",
      "Epoch [257/300], Step [44/172], Loss: 22.9622\n",
      "Epoch [257/300], Step [45/172], Loss: 32.4150\n",
      "Epoch [257/300], Step [46/172], Loss: 16.8330\n",
      "Epoch [257/300], Step [47/172], Loss: 50.1848\n",
      "Epoch [257/300], Step [48/172], Loss: 64.1884\n",
      "Epoch [257/300], Step [49/172], Loss: 25.3137\n",
      "Epoch [257/300], Step [50/172], Loss: 52.2890\n",
      "Epoch [257/300], Step [51/172], Loss: 9.3064\n",
      "Epoch [257/300], Step [52/172], Loss: 24.3136\n",
      "Epoch [257/300], Step [53/172], Loss: 25.7115\n",
      "Epoch [257/300], Step [54/172], Loss: 18.3457\n",
      "Epoch [257/300], Step [55/172], Loss: 18.3933\n",
      "Epoch [257/300], Step [56/172], Loss: 17.4909\n",
      "Epoch [257/300], Step [57/172], Loss: 20.2598\n",
      "Epoch [257/300], Step [58/172], Loss: 16.2208\n",
      "Epoch [257/300], Step [59/172], Loss: 30.2826\n",
      "Epoch [257/300], Step [60/172], Loss: 23.1827\n",
      "Epoch [257/300], Step [61/172], Loss: 8.0025\n",
      "Epoch [257/300], Step [62/172], Loss: 16.6082\n",
      "Epoch [257/300], Step [63/172], Loss: 11.6561\n",
      "Epoch [257/300], Step [64/172], Loss: 14.1493\n",
      "Epoch [257/300], Step [65/172], Loss: 22.1151\n",
      "Epoch [257/300], Step [66/172], Loss: 9.4414\n",
      "Epoch [257/300], Step [67/172], Loss: 27.6770\n",
      "Epoch [257/300], Step [68/172], Loss: 6.6576\n",
      "Epoch [257/300], Step [69/172], Loss: 29.7337\n",
      "Epoch [257/300], Step [70/172], Loss: 29.0398\n",
      "Epoch [257/300], Step [71/172], Loss: 32.0283\n",
      "Epoch [257/300], Step [72/172], Loss: 29.0678\n",
      "Epoch [257/300], Step [73/172], Loss: 37.3660\n",
      "Epoch [257/300], Step [74/172], Loss: 20.5484\n",
      "Epoch [257/300], Step [75/172], Loss: 20.7332\n",
      "Epoch [257/300], Step [76/172], Loss: 24.3587\n",
      "Epoch [257/300], Step [77/172], Loss: 41.7626\n",
      "Epoch [257/300], Step [78/172], Loss: 29.8778\n",
      "Epoch [257/300], Step [79/172], Loss: 27.9267\n",
      "Epoch [257/300], Step [80/172], Loss: 45.3499\n",
      "Epoch [257/300], Step [81/172], Loss: 25.2492\n",
      "Epoch [257/300], Step [82/172], Loss: 36.1249\n",
      "Epoch [257/300], Step [83/172], Loss: 38.4451\n",
      "Epoch [257/300], Step [84/172], Loss: 29.7884\n",
      "Epoch [257/300], Step [85/172], Loss: 33.3108\n",
      "Epoch [257/300], Step [86/172], Loss: 30.2643\n",
      "Epoch [257/300], Step [87/172], Loss: 22.0266\n",
      "Epoch [257/300], Step [88/172], Loss: 20.1111\n",
      "Epoch [257/300], Step [89/172], Loss: 26.2880\n",
      "Epoch [257/300], Step [90/172], Loss: 19.0500\n",
      "Epoch [257/300], Step [91/172], Loss: 25.5129\n",
      "Epoch [257/300], Step [92/172], Loss: 18.5774\n",
      "Epoch [257/300], Step [93/172], Loss: 18.1393\n",
      "Epoch [257/300], Step [94/172], Loss: 24.6668\n",
      "Epoch [257/300], Step [95/172], Loss: 19.9250\n",
      "Epoch [257/300], Step [96/172], Loss: 19.8462\n",
      "Epoch [257/300], Step [97/172], Loss: 29.8852\n",
      "Epoch [257/300], Step [98/172], Loss: 18.3969\n",
      "Epoch [257/300], Step [99/172], Loss: 19.0998\n",
      "Epoch [257/300], Step [100/172], Loss: 17.7948\n",
      "Epoch [257/300], Step [101/172], Loss: 19.5477\n",
      "Epoch [257/300], Step [102/172], Loss: 18.9756\n",
      "Epoch [257/300], Step [103/172], Loss: 12.3930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [257/300], Step [104/172], Loss: 20.0690\n",
      "Epoch [257/300], Step [105/172], Loss: 24.0102\n",
      "Epoch [257/300], Step [106/172], Loss: 16.0075\n",
      "Epoch [257/300], Step [107/172], Loss: 16.6958\n",
      "Epoch [257/300], Step [108/172], Loss: 16.6179\n",
      "Epoch [257/300], Step [109/172], Loss: 15.3835\n",
      "Epoch [257/300], Step [110/172], Loss: 18.3656\n",
      "Epoch [257/300], Step [111/172], Loss: 18.4210\n",
      "Epoch [257/300], Step [112/172], Loss: 17.2736\n",
      "Epoch [257/300], Step [113/172], Loss: 15.0809\n",
      "Epoch [257/300], Step [114/172], Loss: 15.8540\n",
      "Epoch [257/300], Step [115/172], Loss: 20.3594\n",
      "Epoch [257/300], Step [116/172], Loss: 15.0783\n",
      "Epoch [257/300], Step [117/172], Loss: 13.5526\n",
      "Epoch [257/300], Step [118/172], Loss: 13.6661\n",
      "Epoch [257/300], Step [119/172], Loss: 19.1842\n",
      "Epoch [257/300], Step [120/172], Loss: 10.6137\n",
      "Epoch [257/300], Step [121/172], Loss: 9.3782\n",
      "Epoch [257/300], Step [122/172], Loss: 12.9925\n",
      "Epoch [257/300], Step [123/172], Loss: 12.7420\n",
      "Epoch [257/300], Step [124/172], Loss: 7.6302\n",
      "Epoch [257/300], Step [125/172], Loss: 12.8635\n",
      "Epoch [257/300], Step [126/172], Loss: 13.1087\n",
      "Epoch [257/300], Step [127/172], Loss: 11.4292\n",
      "Epoch [257/300], Step [128/172], Loss: 10.5663\n",
      "Epoch [257/300], Step [129/172], Loss: 9.2588\n",
      "Epoch [257/300], Step [130/172], Loss: 13.6205\n",
      "Epoch [257/300], Step [131/172], Loss: 8.2211\n",
      "Epoch [257/300], Step [132/172], Loss: 10.6404\n",
      "Epoch [257/300], Step [133/172], Loss: 10.0138\n",
      "Epoch [257/300], Step [134/172], Loss: 10.7265\n",
      "Epoch [257/300], Step [135/172], Loss: 9.6462\n",
      "Epoch [257/300], Step [136/172], Loss: 9.0701\n",
      "Epoch [257/300], Step [137/172], Loss: 9.1294\n",
      "Epoch [257/300], Step [138/172], Loss: 8.6352\n",
      "Epoch [257/300], Step [139/172], Loss: 11.5537\n",
      "Epoch [257/300], Step [140/172], Loss: 11.6400\n",
      "Epoch [257/300], Step [141/172], Loss: 9.6049\n",
      "Epoch [257/300], Step [142/172], Loss: 15.3669\n",
      "Epoch [257/300], Step [143/172], Loss: 12.5196\n",
      "Epoch [257/300], Step [144/172], Loss: 9.8085\n",
      "Epoch [257/300], Step [145/172], Loss: 11.5856\n",
      "Epoch [257/300], Step [146/172], Loss: 11.4224\n",
      "Epoch [257/300], Step [147/172], Loss: 5.9648\n",
      "Epoch [257/300], Step [148/172], Loss: 6.7550\n",
      "Epoch [257/300], Step [149/172], Loss: 7.0618\n",
      "Epoch [257/300], Step [150/172], Loss: 5.9126\n",
      "Epoch [257/300], Step [151/172], Loss: 5.7316\n",
      "Epoch [257/300], Step [152/172], Loss: 8.5019\n",
      "Epoch [257/300], Step [153/172], Loss: 6.5101\n",
      "Epoch [257/300], Step [154/172], Loss: 6.8988\n",
      "Epoch [257/300], Step [155/172], Loss: 6.9388\n",
      "Epoch [257/300], Step [156/172], Loss: 14.8051\n",
      "Epoch [257/300], Step [157/172], Loss: 9.3664\n",
      "Epoch [257/300], Step [158/172], Loss: 7.6246\n",
      "Epoch [257/300], Step [159/172], Loss: 10.2637\n",
      "Epoch [257/300], Step [160/172], Loss: 10.4459\n",
      "Epoch [257/300], Step [161/172], Loss: 7.8385\n",
      "Epoch [257/300], Step [162/172], Loss: 5.0487\n",
      "Epoch [257/300], Step [163/172], Loss: 7.2512\n",
      "Epoch [257/300], Step [164/172], Loss: 8.9866\n",
      "Epoch [257/300], Step [165/172], Loss: 7.1403\n",
      "Epoch [257/300], Step [166/172], Loss: 6.2982\n",
      "Epoch [257/300], Step [167/172], Loss: 11.3399\n",
      "Epoch [257/300], Step [168/172], Loss: 6.9397\n",
      "Epoch [257/300], Step [169/172], Loss: 7.1722\n",
      "Epoch [257/300], Step [170/172], Loss: 5.9029\n",
      "Epoch [257/300], Step [171/172], Loss: 9.5688\n",
      "Epoch [257/300], Step [172/172], Loss: 5.5994\n",
      "Epoch [258/300], Step [1/172], Loss: 41.1121\n",
      "Epoch [258/300], Step [2/172], Loss: 44.7814\n",
      "Epoch [258/300], Step [3/172], Loss: 39.7932\n",
      "Epoch [258/300], Step [4/172], Loss: 19.1864\n",
      "Epoch [258/300], Step [5/172], Loss: 37.1841\n",
      "Epoch [258/300], Step [6/172], Loss: 17.6029\n",
      "Epoch [258/300], Step [7/172], Loss: 27.5037\n",
      "Epoch [258/300], Step [8/172], Loss: 3.8722\n",
      "Epoch [258/300], Step [9/172], Loss: 25.0680\n",
      "Epoch [258/300], Step [10/172], Loss: 35.9629\n",
      "Epoch [258/300], Step [11/172], Loss: 50.4343\n",
      "Epoch [258/300], Step [12/172], Loss: 49.8446\n",
      "Epoch [258/300], Step [13/172], Loss: 30.5009\n",
      "Epoch [258/300], Step [14/172], Loss: 51.9035\n",
      "Epoch [258/300], Step [15/172], Loss: 47.4447\n",
      "Epoch [258/300], Step [16/172], Loss: 8.1561\n",
      "Epoch [258/300], Step [17/172], Loss: 35.5613\n",
      "Epoch [258/300], Step [18/172], Loss: 49.6940\n",
      "Epoch [258/300], Step [19/172], Loss: 68.2656\n",
      "Epoch [258/300], Step [20/172], Loss: 25.2637\n",
      "Epoch [258/300], Step [21/172], Loss: 71.9036\n",
      "Epoch [258/300], Step [22/172], Loss: 49.4259\n",
      "Epoch [258/300], Step [23/172], Loss: 1.4675\n",
      "Epoch [258/300], Step [24/172], Loss: 45.1021\n",
      "Epoch [258/300], Step [25/172], Loss: 32.0719\n",
      "Epoch [258/300], Step [26/172], Loss: 42.3581\n",
      "Epoch [258/300], Step [27/172], Loss: 54.1497\n",
      "Epoch [258/300], Step [28/172], Loss: 16.3775\n",
      "Epoch [258/300], Step [29/172], Loss: 13.4649\n",
      "Epoch [258/300], Step [30/172], Loss: 47.0275\n",
      "Epoch [258/300], Step [31/172], Loss: 28.9979\n",
      "Epoch [258/300], Step [32/172], Loss: 40.4368\n",
      "Epoch [258/300], Step [33/172], Loss: 61.9034\n",
      "Epoch [258/300], Step [34/172], Loss: 2.1708\n",
      "Epoch [258/300], Step [35/172], Loss: 14.8549\n",
      "Epoch [258/300], Step [36/172], Loss: 15.4665\n",
      "Epoch [258/300], Step [37/172], Loss: 14.9566\n",
      "Epoch [258/300], Step [38/172], Loss: 30.4298\n",
      "Epoch [258/300], Step [39/172], Loss: 33.7953\n",
      "Epoch [258/300], Step [40/172], Loss: 21.3677\n",
      "Epoch [258/300], Step [41/172], Loss: 31.0026\n",
      "Epoch [258/300], Step [42/172], Loss: 37.5436\n",
      "Epoch [258/300], Step [43/172], Loss: 26.9715\n",
      "Epoch [258/300], Step [44/172], Loss: 22.7235\n",
      "Epoch [258/300], Step [45/172], Loss: 31.6392\n",
      "Epoch [258/300], Step [46/172], Loss: 16.8889\n",
      "Epoch [258/300], Step [47/172], Loss: 49.8541\n",
      "Epoch [258/300], Step [48/172], Loss: 64.4423\n",
      "Epoch [258/300], Step [49/172], Loss: 25.0962\n",
      "Epoch [258/300], Step [50/172], Loss: 52.0093\n",
      "Epoch [258/300], Step [51/172], Loss: 9.0537\n",
      "Epoch [258/300], Step [52/172], Loss: 24.3111\n",
      "Epoch [258/300], Step [53/172], Loss: 25.4150\n",
      "Epoch [258/300], Step [54/172], Loss: 18.3493\n",
      "Epoch [258/300], Step [55/172], Loss: 18.2321\n",
      "Epoch [258/300], Step [56/172], Loss: 17.6803\n",
      "Epoch [258/300], Step [57/172], Loss: 20.0257\n",
      "Epoch [258/300], Step [58/172], Loss: 16.3566\n",
      "Epoch [258/300], Step [59/172], Loss: 30.4977\n",
      "Epoch [258/300], Step [60/172], Loss: 23.7099\n",
      "Epoch [258/300], Step [61/172], Loss: 7.9757\n",
      "Epoch [258/300], Step [62/172], Loss: 16.5628\n",
      "Epoch [258/300], Step [63/172], Loss: 11.7224\n",
      "Epoch [258/300], Step [64/172], Loss: 14.0368\n",
      "Epoch [258/300], Step [65/172], Loss: 21.9521\n",
      "Epoch [258/300], Step [66/172], Loss: 9.2431\n",
      "Epoch [258/300], Step [67/172], Loss: 27.7104\n",
      "Epoch [258/300], Step [68/172], Loss: 6.7809\n",
      "Epoch [258/300], Step [69/172], Loss: 29.7686\n",
      "Epoch [258/300], Step [70/172], Loss: 29.2426\n",
      "Epoch [258/300], Step [71/172], Loss: 32.0319\n",
      "Epoch [258/300], Step [72/172], Loss: 29.2457\n",
      "Epoch [258/300], Step [73/172], Loss: 37.5879\n",
      "Epoch [258/300], Step [74/172], Loss: 20.7384\n",
      "Epoch [258/300], Step [75/172], Loss: 20.7930\n",
      "Epoch [258/300], Step [76/172], Loss: 24.4462\n",
      "Epoch [258/300], Step [77/172], Loss: 41.9822\n",
      "Epoch [258/300], Step [78/172], Loss: 30.0615\n",
      "Epoch [258/300], Step [79/172], Loss: 27.7451\n",
      "Epoch [258/300], Step [80/172], Loss: 45.2642\n",
      "Epoch [258/300], Step [81/172], Loss: 25.2504\n",
      "Epoch [258/300], Step [82/172], Loss: 35.9411\n",
      "Epoch [258/300], Step [83/172], Loss: 38.5756\n",
      "Epoch [258/300], Step [84/172], Loss: 29.7415\n",
      "Epoch [258/300], Step [85/172], Loss: 33.1675\n",
      "Epoch [258/300], Step [86/172], Loss: 30.2153\n",
      "Epoch [258/300], Step [87/172], Loss: 21.8533\n",
      "Epoch [258/300], Step [88/172], Loss: 20.0849\n",
      "Epoch [258/300], Step [89/172], Loss: 26.1697\n",
      "Epoch [258/300], Step [90/172], Loss: 18.9997\n",
      "Epoch [258/300], Step [91/172], Loss: 25.3666\n",
      "Epoch [258/300], Step [92/172], Loss: 18.5041\n",
      "Epoch [258/300], Step [93/172], Loss: 18.1868\n",
      "Epoch [258/300], Step [94/172], Loss: 24.6915\n",
      "Epoch [258/300], Step [95/172], Loss: 19.9042\n",
      "Epoch [258/300], Step [96/172], Loss: 19.8499\n",
      "Epoch [258/300], Step [97/172], Loss: 29.7277\n",
      "Epoch [258/300], Step [98/172], Loss: 18.2348\n",
      "Epoch [258/300], Step [99/172], Loss: 18.9278\n",
      "Epoch [258/300], Step [100/172], Loss: 17.6101\n",
      "Epoch [258/300], Step [101/172], Loss: 19.3441\n",
      "Epoch [258/300], Step [102/172], Loss: 18.9219\n",
      "Epoch [258/300], Step [103/172], Loss: 12.2320\n",
      "Epoch [258/300], Step [104/172], Loss: 19.8500\n",
      "Epoch [258/300], Step [105/172], Loss: 23.7768\n",
      "Epoch [258/300], Step [106/172], Loss: 15.8851\n",
      "Epoch [258/300], Step [107/172], Loss: 16.6291\n",
      "Epoch [258/300], Step [108/172], Loss: 16.5726\n",
      "Epoch [258/300], Step [109/172], Loss: 15.4266\n",
      "Epoch [258/300], Step [110/172], Loss: 18.2276\n",
      "Epoch [258/300], Step [111/172], Loss: 18.3705\n",
      "Epoch [258/300], Step [112/172], Loss: 17.2767\n",
      "Epoch [258/300], Step [113/172], Loss: 14.9990\n",
      "Epoch [258/300], Step [114/172], Loss: 15.8714\n",
      "Epoch [258/300], Step [115/172], Loss: 20.5219\n",
      "Epoch [258/300], Step [116/172], Loss: 15.1221\n",
      "Epoch [258/300], Step [117/172], Loss: 13.5121\n",
      "Epoch [258/300], Step [118/172], Loss: 13.8358\n",
      "Epoch [258/300], Step [119/172], Loss: 19.1832\n",
      "Epoch [258/300], Step [120/172], Loss: 10.5170\n",
      "Epoch [258/300], Step [121/172], Loss: 9.3207\n",
      "Epoch [258/300], Step [122/172], Loss: 13.0783\n",
      "Epoch [258/300], Step [123/172], Loss: 12.7774\n",
      "Epoch [258/300], Step [124/172], Loss: 7.6184\n",
      "Epoch [258/300], Step [125/172], Loss: 12.7625\n",
      "Epoch [258/300], Step [126/172], Loss: 12.9724\n",
      "Epoch [258/300], Step [127/172], Loss: 11.4892\n",
      "Epoch [258/300], Step [128/172], Loss: 10.4549\n",
      "Epoch [258/300], Step [129/172], Loss: 9.1859\n",
      "Epoch [258/300], Step [130/172], Loss: 13.5974\n",
      "Epoch [258/300], Step [131/172], Loss: 8.2613\n",
      "Epoch [258/300], Step [132/172], Loss: 10.5051\n",
      "Epoch [258/300], Step [133/172], Loss: 10.1623\n",
      "Epoch [258/300], Step [134/172], Loss: 11.0185\n",
      "Epoch [258/300], Step [135/172], Loss: 9.6247\n",
      "Epoch [258/300], Step [136/172], Loss: 9.0417\n",
      "Epoch [258/300], Step [137/172], Loss: 9.2845\n",
      "Epoch [258/300], Step [138/172], Loss: 8.6276\n",
      "Epoch [258/300], Step [139/172], Loss: 11.5782\n",
      "Epoch [258/300], Step [140/172], Loss: 11.5141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [258/300], Step [141/172], Loss: 9.4583\n",
      "Epoch [258/300], Step [142/172], Loss: 15.6239\n",
      "Epoch [258/300], Step [143/172], Loss: 12.4301\n",
      "Epoch [258/300], Step [144/172], Loss: 9.7372\n",
      "Epoch [258/300], Step [145/172], Loss: 11.6028\n",
      "Epoch [258/300], Step [146/172], Loss: 11.2776\n",
      "Epoch [258/300], Step [147/172], Loss: 5.9008\n",
      "Epoch [258/300], Step [148/172], Loss: 6.6252\n",
      "Epoch [258/300], Step [149/172], Loss: 6.8815\n",
      "Epoch [258/300], Step [150/172], Loss: 5.8366\n",
      "Epoch [258/300], Step [151/172], Loss: 5.7160\n",
      "Epoch [258/300], Step [152/172], Loss: 8.3616\n",
      "Epoch [258/300], Step [153/172], Loss: 6.4759\n",
      "Epoch [258/300], Step [154/172], Loss: 6.8560\n",
      "Epoch [258/300], Step [155/172], Loss: 6.7744\n",
      "Epoch [258/300], Step [156/172], Loss: 15.0114\n",
      "Epoch [258/300], Step [157/172], Loss: 9.6694\n",
      "Epoch [258/300], Step [158/172], Loss: 7.7309\n",
      "Epoch [258/300], Step [159/172], Loss: 10.2082\n",
      "Epoch [258/300], Step [160/172], Loss: 10.7874\n",
      "Epoch [258/300], Step [161/172], Loss: 7.4861\n",
      "Epoch [258/300], Step [162/172], Loss: 5.0049\n",
      "Epoch [258/300], Step [163/172], Loss: 7.2111\n",
      "Epoch [258/300], Step [164/172], Loss: 8.9530\n",
      "Epoch [258/300], Step [165/172], Loss: 7.0980\n",
      "Epoch [258/300], Step [166/172], Loss: 6.3811\n",
      "Epoch [258/300], Step [167/172], Loss: 11.2925\n",
      "Epoch [258/300], Step [168/172], Loss: 6.8957\n",
      "Epoch [258/300], Step [169/172], Loss: 7.1341\n",
      "Epoch [258/300], Step [170/172], Loss: 5.8758\n",
      "Epoch [258/300], Step [171/172], Loss: 9.3201\n",
      "Epoch [258/300], Step [172/172], Loss: 5.5442\n",
      "Epoch [259/300], Step [1/172], Loss: 41.0758\n",
      "Epoch [259/300], Step [2/172], Loss: 43.9021\n",
      "Epoch [259/300], Step [3/172], Loss: 40.4247\n",
      "Epoch [259/300], Step [4/172], Loss: 19.2280\n",
      "Epoch [259/300], Step [5/172], Loss: 37.2800\n",
      "Epoch [259/300], Step [6/172], Loss: 16.6787\n",
      "Epoch [259/300], Step [7/172], Loss: 23.8646\n",
      "Epoch [259/300], Step [8/172], Loss: 3.7785\n",
      "Epoch [259/300], Step [9/172], Loss: 24.9497\n",
      "Epoch [259/300], Step [10/172], Loss: 35.6981\n",
      "Epoch [259/300], Step [11/172], Loss: 50.5076\n",
      "Epoch [259/300], Step [12/172], Loss: 49.3965\n",
      "Epoch [259/300], Step [13/172], Loss: 29.7839\n",
      "Epoch [259/300], Step [14/172], Loss: 51.2370\n",
      "Epoch [259/300], Step [15/172], Loss: 47.7410\n",
      "Epoch [259/300], Step [16/172], Loss: 9.1749\n",
      "Epoch [259/300], Step [17/172], Loss: 35.0470\n",
      "Epoch [259/300], Step [18/172], Loss: 49.6273\n",
      "Epoch [259/300], Step [19/172], Loss: 68.2509\n",
      "Epoch [259/300], Step [20/172], Loss: 25.8080\n",
      "Epoch [259/300], Step [21/172], Loss: 72.2818\n",
      "Epoch [259/300], Step [22/172], Loss: 49.0320\n",
      "Epoch [259/300], Step [23/172], Loss: 1.8051\n",
      "Epoch [259/300], Step [24/172], Loss: 45.1327\n",
      "Epoch [259/300], Step [25/172], Loss: 32.5072\n",
      "Epoch [259/300], Step [26/172], Loss: 42.4431\n",
      "Epoch [259/300], Step [27/172], Loss: 53.6960\n",
      "Epoch [259/300], Step [28/172], Loss: 16.2244\n",
      "Epoch [259/300], Step [29/172], Loss: 13.3803\n",
      "Epoch [259/300], Step [30/172], Loss: 47.2970\n",
      "Epoch [259/300], Step [31/172], Loss: 28.8967\n",
      "Epoch [259/300], Step [32/172], Loss: 40.4863\n",
      "Epoch [259/300], Step [33/172], Loss: 61.7492\n",
      "Epoch [259/300], Step [34/172], Loss: 1.8772\n",
      "Epoch [259/300], Step [35/172], Loss: 14.5500\n",
      "Epoch [259/300], Step [36/172], Loss: 14.9347\n",
      "Epoch [259/300], Step [37/172], Loss: 14.9646\n",
      "Epoch [259/300], Step [38/172], Loss: 30.5938\n",
      "Epoch [259/300], Step [39/172], Loss: 33.8339\n",
      "Epoch [259/300], Step [40/172], Loss: 21.2112\n",
      "Epoch [259/300], Step [41/172], Loss: 31.1107\n",
      "Epoch [259/300], Step [42/172], Loss: 37.3787\n",
      "Epoch [259/300], Step [43/172], Loss: 26.8963\n",
      "Epoch [259/300], Step [44/172], Loss: 22.4209\n",
      "Epoch [259/300], Step [45/172], Loss: 30.9406\n",
      "Epoch [259/300], Step [46/172], Loss: 16.7364\n",
      "Epoch [259/300], Step [47/172], Loss: 49.5831\n",
      "Epoch [259/300], Step [48/172], Loss: 64.8041\n",
      "Epoch [259/300], Step [49/172], Loss: 24.7531\n",
      "Epoch [259/300], Step [50/172], Loss: 52.1794\n",
      "Epoch [259/300], Step [51/172], Loss: 8.9642\n",
      "Epoch [259/300], Step [52/172], Loss: 23.8991\n",
      "Epoch [259/300], Step [53/172], Loss: 25.4209\n",
      "Epoch [259/300], Step [54/172], Loss: 17.6830\n",
      "Epoch [259/300], Step [55/172], Loss: 17.7992\n",
      "Epoch [259/300], Step [56/172], Loss: 17.3934\n",
      "Epoch [259/300], Step [57/172], Loss: 19.5517\n",
      "Epoch [259/300], Step [58/172], Loss: 16.0543\n",
      "Epoch [259/300], Step [59/172], Loss: 30.1622\n",
      "Epoch [259/300], Step [60/172], Loss: 23.3846\n",
      "Epoch [259/300], Step [61/172], Loss: 7.6258\n",
      "Epoch [259/300], Step [62/172], Loss: 16.3780\n",
      "Epoch [259/300], Step [63/172], Loss: 11.3233\n",
      "Epoch [259/300], Step [64/172], Loss: 13.9715\n",
      "Epoch [259/300], Step [65/172], Loss: 21.7692\n",
      "Epoch [259/300], Step [66/172], Loss: 9.2503\n",
      "Epoch [259/300], Step [67/172], Loss: 27.5168\n",
      "Epoch [259/300], Step [68/172], Loss: 6.8393\n",
      "Epoch [259/300], Step [69/172], Loss: 29.5806\n",
      "Epoch [259/300], Step [70/172], Loss: 29.2437\n",
      "Epoch [259/300], Step [71/172], Loss: 32.1961\n",
      "Epoch [259/300], Step [72/172], Loss: 29.4194\n",
      "Epoch [259/300], Step [73/172], Loss: 37.7188\n",
      "Epoch [259/300], Step [74/172], Loss: 20.7394\n",
      "Epoch [259/300], Step [75/172], Loss: 21.0660\n",
      "Epoch [259/300], Step [76/172], Loss: 24.4561\n",
      "Epoch [259/300], Step [77/172], Loss: 42.2045\n",
      "Epoch [259/300], Step [78/172], Loss: 30.1036\n",
      "Epoch [259/300], Step [79/172], Loss: 27.9608\n",
      "Epoch [259/300], Step [80/172], Loss: 45.5267\n",
      "Epoch [259/300], Step [81/172], Loss: 25.3987\n",
      "Epoch [259/300], Step [82/172], Loss: 36.0011\n",
      "Epoch [259/300], Step [83/172], Loss: 38.5092\n",
      "Epoch [259/300], Step [84/172], Loss: 29.6239\n",
      "Epoch [259/300], Step [85/172], Loss: 33.2178\n",
      "Epoch [259/300], Step [86/172], Loss: 30.1857\n",
      "Epoch [259/300], Step [87/172], Loss: 21.8947\n",
      "Epoch [259/300], Step [88/172], Loss: 20.0816\n",
      "Epoch [259/300], Step [89/172], Loss: 26.1157\n",
      "Epoch [259/300], Step [90/172], Loss: 19.0205\n",
      "Epoch [259/300], Step [91/172], Loss: 25.3716\n",
      "Epoch [259/300], Step [92/172], Loss: 18.5547\n",
      "Epoch [259/300], Step [93/172], Loss: 18.0021\n",
      "Epoch [259/300], Step [94/172], Loss: 24.5771\n",
      "Epoch [259/300], Step [95/172], Loss: 19.6644\n",
      "Epoch [259/300], Step [96/172], Loss: 19.7665\n",
      "Epoch [259/300], Step [97/172], Loss: 29.6535\n",
      "Epoch [259/300], Step [98/172], Loss: 18.1049\n",
      "Epoch [259/300], Step [99/172], Loss: 18.7986\n",
      "Epoch [259/300], Step [100/172], Loss: 17.4744\n",
      "Epoch [259/300], Step [101/172], Loss: 19.1886\n",
      "Epoch [259/300], Step [102/172], Loss: 18.8191\n",
      "Epoch [259/300], Step [103/172], Loss: 11.9965\n",
      "Epoch [259/300], Step [104/172], Loss: 19.7842\n",
      "Epoch [259/300], Step [105/172], Loss: 23.5165\n",
      "Epoch [259/300], Step [106/172], Loss: 15.7889\n",
      "Epoch [259/300], Step [107/172], Loss: 16.5397\n",
      "Epoch [259/300], Step [108/172], Loss: 16.3842\n",
      "Epoch [259/300], Step [109/172], Loss: 15.2163\n",
      "Epoch [259/300], Step [110/172], Loss: 18.0003\n",
      "Epoch [259/300], Step [111/172], Loss: 18.1016\n",
      "Epoch [259/300], Step [112/172], Loss: 17.1191\n",
      "Epoch [259/300], Step [113/172], Loss: 14.8798\n",
      "Epoch [259/300], Step [114/172], Loss: 15.6948\n",
      "Epoch [259/300], Step [115/172], Loss: 20.4592\n",
      "Epoch [259/300], Step [116/172], Loss: 14.9478\n",
      "Epoch [259/300], Step [117/172], Loss: 13.4547\n",
      "Epoch [259/300], Step [118/172], Loss: 13.7176\n",
      "Epoch [259/300], Step [119/172], Loss: 18.9903\n",
      "Epoch [259/300], Step [120/172], Loss: 10.4524\n",
      "Epoch [259/300], Step [121/172], Loss: 9.2285\n",
      "Epoch [259/300], Step [122/172], Loss: 12.9039\n",
      "Epoch [259/300], Step [123/172], Loss: 12.6040\n",
      "Epoch [259/300], Step [124/172], Loss: 7.6129\n",
      "Epoch [259/300], Step [125/172], Loss: 12.6761\n",
      "Epoch [259/300], Step [126/172], Loss: 12.9756\n",
      "Epoch [259/300], Step [127/172], Loss: 11.3727\n",
      "Epoch [259/300], Step [128/172], Loss: 10.3289\n",
      "Epoch [259/300], Step [129/172], Loss: 9.1147\n",
      "Epoch [259/300], Step [130/172], Loss: 13.6118\n",
      "Epoch [259/300], Step [131/172], Loss: 8.1279\n",
      "Epoch [259/300], Step [132/172], Loss: 10.2921\n",
      "Epoch [259/300], Step [133/172], Loss: 10.0308\n",
      "Epoch [259/300], Step [134/172], Loss: 10.8651\n",
      "Epoch [259/300], Step [135/172], Loss: 9.5316\n",
      "Epoch [259/300], Step [136/172], Loss: 9.0189\n",
      "Epoch [259/300], Step [137/172], Loss: 9.0859\n",
      "Epoch [259/300], Step [138/172], Loss: 8.6125\n",
      "Epoch [259/300], Step [139/172], Loss: 11.5330\n",
      "Epoch [259/300], Step [140/172], Loss: 11.4199\n",
      "Epoch [259/300], Step [141/172], Loss: 9.3981\n",
      "Epoch [259/300], Step [142/172], Loss: 15.6409\n",
      "Epoch [259/300], Step [143/172], Loss: 12.4104\n",
      "Epoch [259/300], Step [144/172], Loss: 9.7011\n",
      "Epoch [259/300], Step [145/172], Loss: 11.5384\n",
      "Epoch [259/300], Step [146/172], Loss: 11.1818\n",
      "Epoch [259/300], Step [147/172], Loss: 5.9191\n",
      "Epoch [259/300], Step [148/172], Loss: 6.5449\n",
      "Epoch [259/300], Step [149/172], Loss: 6.8639\n",
      "Epoch [259/300], Step [150/172], Loss: 5.7496\n",
      "Epoch [259/300], Step [151/172], Loss: 5.6097\n",
      "Epoch [259/300], Step [152/172], Loss: 8.2932\n",
      "Epoch [259/300], Step [153/172], Loss: 6.4699\n",
      "Epoch [259/300], Step [154/172], Loss: 6.7699\n",
      "Epoch [259/300], Step [155/172], Loss: 6.8917\n",
      "Epoch [259/300], Step [156/172], Loss: 14.8754\n",
      "Epoch [259/300], Step [157/172], Loss: 9.6097\n",
      "Epoch [259/300], Step [158/172], Loss: 7.6452\n",
      "Epoch [259/300], Step [159/172], Loss: 10.1821\n",
      "Epoch [259/300], Step [160/172], Loss: 10.6786\n",
      "Epoch [259/300], Step [161/172], Loss: 7.4273\n",
      "Epoch [259/300], Step [162/172], Loss: 4.9296\n",
      "Epoch [259/300], Step [163/172], Loss: 7.2429\n",
      "Epoch [259/300], Step [164/172], Loss: 8.8031\n",
      "Epoch [259/300], Step [165/172], Loss: 7.0500\n",
      "Epoch [259/300], Step [166/172], Loss: 6.2382\n",
      "Epoch [259/300], Step [167/172], Loss: 11.3081\n",
      "Epoch [259/300], Step [168/172], Loss: 6.8253\n",
      "Epoch [259/300], Step [169/172], Loss: 7.0041\n",
      "Epoch [259/300], Step [170/172], Loss: 5.8016\n",
      "Epoch [259/300], Step [171/172], Loss: 9.3557\n",
      "Epoch [259/300], Step [172/172], Loss: 5.6421\n",
      "Epoch [260/300], Step [1/172], Loss: 41.0930\n",
      "Epoch [260/300], Step [2/172], Loss: 43.8446\n",
      "Epoch [260/300], Step [3/172], Loss: 39.6910\n",
      "Epoch [260/300], Step [4/172], Loss: 18.9511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/300], Step [5/172], Loss: 37.0859\n",
      "Epoch [260/300], Step [6/172], Loss: 16.8106\n",
      "Epoch [260/300], Step [7/172], Loss: 25.1435\n",
      "Epoch [260/300], Step [8/172], Loss: 4.2251\n",
      "Epoch [260/300], Step [9/172], Loss: 24.8842\n",
      "Epoch [260/300], Step [10/172], Loss: 35.4929\n",
      "Epoch [260/300], Step [11/172], Loss: 50.0852\n",
      "Epoch [260/300], Step [12/172], Loss: 48.7054\n",
      "Epoch [260/300], Step [13/172], Loss: 29.6989\n",
      "Epoch [260/300], Step [14/172], Loss: 51.3908\n",
      "Epoch [260/300], Step [15/172], Loss: 47.5226\n",
      "Epoch [260/300], Step [16/172], Loss: 8.4209\n",
      "Epoch [260/300], Step [17/172], Loss: 34.9826\n",
      "Epoch [260/300], Step [18/172], Loss: 49.5020\n",
      "Epoch [260/300], Step [19/172], Loss: 67.9151\n",
      "Epoch [260/300], Step [20/172], Loss: 25.5037\n",
      "Epoch [260/300], Step [21/172], Loss: 72.0743\n",
      "Epoch [260/300], Step [22/172], Loss: 48.8290\n",
      "Epoch [260/300], Step [23/172], Loss: 1.5006\n",
      "Epoch [260/300], Step [24/172], Loss: 44.8132\n",
      "Epoch [260/300], Step [25/172], Loss: 32.3009\n",
      "Epoch [260/300], Step [26/172], Loss: 42.1548\n",
      "Epoch [260/300], Step [27/172], Loss: 54.0168\n",
      "Epoch [260/300], Step [28/172], Loss: 15.9944\n",
      "Epoch [260/300], Step [29/172], Loss: 13.7126\n",
      "Epoch [260/300], Step [30/172], Loss: 46.8157\n",
      "Epoch [260/300], Step [31/172], Loss: 28.4516\n",
      "Epoch [260/300], Step [32/172], Loss: 39.9530\n",
      "Epoch [260/300], Step [33/172], Loss: 60.9543\n",
      "Epoch [260/300], Step [34/172], Loss: 1.8835\n",
      "Epoch [260/300], Step [35/172], Loss: 14.5800\n",
      "Epoch [260/300], Step [36/172], Loss: 15.2010\n",
      "Epoch [260/300], Step [37/172], Loss: 14.6989\n",
      "Epoch [260/300], Step [38/172], Loss: 30.2652\n",
      "Epoch [260/300], Step [39/172], Loss: 33.5419\n",
      "Epoch [260/300], Step [40/172], Loss: 21.0028\n",
      "Epoch [260/300], Step [41/172], Loss: 30.9546\n",
      "Epoch [260/300], Step [42/172], Loss: 37.2860\n",
      "Epoch [260/300], Step [43/172], Loss: 26.8695\n",
      "Epoch [260/300], Step [44/172], Loss: 22.2382\n",
      "Epoch [260/300], Step [45/172], Loss: 30.8317\n",
      "Epoch [260/300], Step [46/172], Loss: 16.3615\n",
      "Epoch [260/300], Step [47/172], Loss: 49.0326\n",
      "Epoch [260/300], Step [48/172], Loss: 64.3011\n",
      "Epoch [260/300], Step [49/172], Loss: 24.9139\n",
      "Epoch [260/300], Step [50/172], Loss: 52.0636\n",
      "Epoch [260/300], Step [51/172], Loss: 8.8929\n",
      "Epoch [260/300], Step [52/172], Loss: 23.5709\n",
      "Epoch [260/300], Step [53/172], Loss: 25.3621\n",
      "Epoch [260/300], Step [54/172], Loss: 17.9159\n",
      "Epoch [260/300], Step [55/172], Loss: 17.6351\n",
      "Epoch [260/300], Step [56/172], Loss: 17.3512\n",
      "Epoch [260/300], Step [57/172], Loss: 19.6488\n",
      "Epoch [260/300], Step [58/172], Loss: 15.9760\n",
      "Epoch [260/300], Step [59/172], Loss: 29.8662\n",
      "Epoch [260/300], Step [60/172], Loss: 23.6452\n",
      "Epoch [260/300], Step [61/172], Loss: 7.6238\n",
      "Epoch [260/300], Step [62/172], Loss: 16.4357\n",
      "Epoch [260/300], Step [63/172], Loss: 11.3729\n",
      "Epoch [260/300], Step [64/172], Loss: 13.7988\n",
      "Epoch [260/300], Step [65/172], Loss: 21.7800\n",
      "Epoch [260/300], Step [66/172], Loss: 9.0991\n",
      "Epoch [260/300], Step [67/172], Loss: 27.6054\n",
      "Epoch [260/300], Step [68/172], Loss: 6.2624\n",
      "Epoch [260/300], Step [69/172], Loss: 29.8309\n",
      "Epoch [260/300], Step [70/172], Loss: 29.4150\n",
      "Epoch [260/300], Step [71/172], Loss: 32.0537\n",
      "Epoch [260/300], Step [72/172], Loss: 29.5144\n",
      "Epoch [260/300], Step [73/172], Loss: 37.5063\n",
      "Epoch [260/300], Step [74/172], Loss: 20.6982\n",
      "Epoch [260/300], Step [75/172], Loss: 20.9710\n",
      "Epoch [260/300], Step [76/172], Loss: 24.2436\n",
      "Epoch [260/300], Step [77/172], Loss: 42.2649\n",
      "Epoch [260/300], Step [78/172], Loss: 30.1488\n",
      "Epoch [260/300], Step [79/172], Loss: 27.6521\n",
      "Epoch [260/300], Step [80/172], Loss: 45.5088\n",
      "Epoch [260/300], Step [81/172], Loss: 25.1726\n",
      "Epoch [260/300], Step [82/172], Loss: 36.1965\n",
      "Epoch [260/300], Step [83/172], Loss: 38.5098\n",
      "Epoch [260/300], Step [84/172], Loss: 29.4577\n",
      "Epoch [260/300], Step [85/172], Loss: 33.1801\n",
      "Epoch [260/300], Step [86/172], Loss: 29.9903\n",
      "Epoch [260/300], Step [87/172], Loss: 21.7673\n",
      "Epoch [260/300], Step [88/172], Loss: 20.0267\n",
      "Epoch [260/300], Step [89/172], Loss: 26.0763\n",
      "Epoch [260/300], Step [90/172], Loss: 18.7861\n",
      "Epoch [260/300], Step [91/172], Loss: 25.2509\n",
      "Epoch [260/300], Step [92/172], Loss: 18.4981\n",
      "Epoch [260/300], Step [93/172], Loss: 18.0901\n",
      "Epoch [260/300], Step [94/172], Loss: 24.4496\n",
      "Epoch [260/300], Step [95/172], Loss: 19.5337\n",
      "Epoch [260/300], Step [96/172], Loss: 19.7701\n",
      "Epoch [260/300], Step [97/172], Loss: 29.5231\n",
      "Epoch [260/300], Step [98/172], Loss: 17.9266\n",
      "Epoch [260/300], Step [99/172], Loss: 18.7161\n",
      "Epoch [260/300], Step [100/172], Loss: 17.2503\n",
      "Epoch [260/300], Step [101/172], Loss: 19.0645\n",
      "Epoch [260/300], Step [102/172], Loss: 18.8510\n",
      "Epoch [260/300], Step [103/172], Loss: 11.9113\n",
      "Epoch [260/300], Step [104/172], Loss: 19.5924\n",
      "Epoch [260/300], Step [105/172], Loss: 23.5700\n",
      "Epoch [260/300], Step [106/172], Loss: 15.6708\n",
      "Epoch [260/300], Step [107/172], Loss: 16.5130\n",
      "Epoch [260/300], Step [108/172], Loss: 16.2474\n",
      "Epoch [260/300], Step [109/172], Loss: 15.1913\n",
      "Epoch [260/300], Step [110/172], Loss: 17.8712\n",
      "Epoch [260/300], Step [111/172], Loss: 17.9747\n",
      "Epoch [260/300], Step [112/172], Loss: 16.8614\n",
      "Epoch [260/300], Step [113/172], Loss: 14.6886\n",
      "Epoch [260/300], Step [114/172], Loss: 15.6533\n",
      "Epoch [260/300], Step [115/172], Loss: 20.6515\n",
      "Epoch [260/300], Step [116/172], Loss: 15.0254\n",
      "Epoch [260/300], Step [117/172], Loss: 13.2015\n",
      "Epoch [260/300], Step [118/172], Loss: 13.7421\n",
      "Epoch [260/300], Step [119/172], Loss: 19.0487\n",
      "Epoch [260/300], Step [120/172], Loss: 10.3326\n",
      "Epoch [260/300], Step [121/172], Loss: 9.2257\n",
      "Epoch [260/300], Step [122/172], Loss: 12.6561\n",
      "Epoch [260/300], Step [123/172], Loss: 12.3497\n",
      "Epoch [260/300], Step [124/172], Loss: 7.6506\n",
      "Epoch [260/300], Step [125/172], Loss: 12.6164\n",
      "Epoch [260/300], Step [126/172], Loss: 12.8424\n",
      "Epoch [260/300], Step [127/172], Loss: 11.3173\n",
      "Epoch [260/300], Step [128/172], Loss: 10.4317\n",
      "Epoch [260/300], Step [129/172], Loss: 9.1092\n",
      "Epoch [260/300], Step [130/172], Loss: 13.5835\n",
      "Epoch [260/300], Step [131/172], Loss: 8.0322\n",
      "Epoch [260/300], Step [132/172], Loss: 10.2069\n",
      "Epoch [260/300], Step [133/172], Loss: 9.9618\n",
      "Epoch [260/300], Step [134/172], Loss: 11.0772\n",
      "Epoch [260/300], Step [135/172], Loss: 9.5890\n",
      "Epoch [260/300], Step [136/172], Loss: 8.8909\n",
      "Epoch [260/300], Step [137/172], Loss: 9.1505\n",
      "Epoch [260/300], Step [138/172], Loss: 8.4881\n",
      "Epoch [260/300], Step [139/172], Loss: 11.6857\n",
      "Epoch [260/300], Step [140/172], Loss: 11.3424\n",
      "Epoch [260/300], Step [141/172], Loss: 9.2854\n",
      "Epoch [260/300], Step [142/172], Loss: 15.5405\n",
      "Epoch [260/300], Step [143/172], Loss: 12.3721\n",
      "Epoch [260/300], Step [144/172], Loss: 9.7409\n",
      "Epoch [260/300], Step [145/172], Loss: 11.4008\n",
      "Epoch [260/300], Step [146/172], Loss: 11.0287\n",
      "Epoch [260/300], Step [147/172], Loss: 5.8649\n",
      "Epoch [260/300], Step [148/172], Loss: 6.4620\n",
      "Epoch [260/300], Step [149/172], Loss: 6.6000\n",
      "Epoch [260/300], Step [150/172], Loss: 5.6839\n",
      "Epoch [260/300], Step [151/172], Loss: 5.5430\n",
      "Epoch [260/300], Step [152/172], Loss: 8.2505\n",
      "Epoch [260/300], Step [153/172], Loss: 6.4195\n",
      "Epoch [260/300], Step [154/172], Loss: 6.7645\n",
      "Epoch [260/300], Step [155/172], Loss: 6.7202\n",
      "Epoch [260/300], Step [156/172], Loss: 14.8832\n",
      "Epoch [260/300], Step [157/172], Loss: 9.7162\n",
      "Epoch [260/300], Step [158/172], Loss: 7.5732\n",
      "Epoch [260/300], Step [159/172], Loss: 10.1476\n",
      "Epoch [260/300], Step [160/172], Loss: 10.7182\n",
      "Epoch [260/300], Step [161/172], Loss: 7.0338\n",
      "Epoch [260/300], Step [162/172], Loss: 4.8528\n",
      "Epoch [260/300], Step [163/172], Loss: 7.1676\n",
      "Epoch [260/300], Step [164/172], Loss: 8.8438\n",
      "Epoch [260/300], Step [165/172], Loss: 6.9540\n",
      "Epoch [260/300], Step [166/172], Loss: 6.3359\n",
      "Epoch [260/300], Step [167/172], Loss: 11.3123\n",
      "Epoch [260/300], Step [168/172], Loss: 6.8200\n",
      "Epoch [260/300], Step [169/172], Loss: 6.9498\n",
      "Epoch [260/300], Step [170/172], Loss: 5.6739\n",
      "Epoch [260/300], Step [171/172], Loss: 9.2668\n",
      "Epoch [260/300], Step [172/172], Loss: 5.4991\n",
      "Epoch [261/300], Step [1/172], Loss: 40.8145\n",
      "Epoch [261/300], Step [2/172], Loss: 43.5471\n",
      "Epoch [261/300], Step [3/172], Loss: 39.6238\n",
      "Epoch [261/300], Step [4/172], Loss: 19.0443\n",
      "Epoch [261/300], Step [5/172], Loss: 37.0807\n",
      "Epoch [261/300], Step [6/172], Loss: 16.5064\n",
      "Epoch [261/300], Step [7/172], Loss: 24.0979\n",
      "Epoch [261/300], Step [8/172], Loss: 3.3591\n",
      "Epoch [261/300], Step [9/172], Loss: 24.5829\n",
      "Epoch [261/300], Step [10/172], Loss: 35.4660\n",
      "Epoch [261/300], Step [11/172], Loss: 50.1179\n",
      "Epoch [261/300], Step [12/172], Loss: 48.6321\n",
      "Epoch [261/300], Step [13/172], Loss: 30.0431\n",
      "Epoch [261/300], Step [14/172], Loss: 50.5538\n",
      "Epoch [261/300], Step [15/172], Loss: 47.2624\n",
      "Epoch [261/300], Step [16/172], Loss: 9.4257\n",
      "Epoch [261/300], Step [17/172], Loss: 34.0893\n",
      "Epoch [261/300], Step [18/172], Loss: 48.7663\n",
      "Epoch [261/300], Step [19/172], Loss: 66.7043\n",
      "Epoch [261/300], Step [20/172], Loss: 26.3136\n",
      "Epoch [261/300], Step [21/172], Loss: 70.2438\n",
      "Epoch [261/300], Step [22/172], Loss: 49.5589\n",
      "Epoch [261/300], Step [23/172], Loss: 1.8074\n",
      "Epoch [261/300], Step [24/172], Loss: 44.0951\n",
      "Epoch [261/300], Step [25/172], Loss: 32.1292\n",
      "Epoch [261/300], Step [26/172], Loss: 42.4816\n",
      "Epoch [261/300], Step [27/172], Loss: 53.6950\n",
      "Epoch [261/300], Step [28/172], Loss: 15.6631\n",
      "Epoch [261/300], Step [29/172], Loss: 13.5430\n",
      "Epoch [261/300], Step [30/172], Loss: 46.5290\n",
      "Epoch [261/300], Step [31/172], Loss: 28.2317\n",
      "Epoch [261/300], Step [32/172], Loss: 39.9272\n",
      "Epoch [261/300], Step [33/172], Loss: 60.6248\n",
      "Epoch [261/300], Step [34/172], Loss: 1.8743\n",
      "Epoch [261/300], Step [35/172], Loss: 14.7687\n",
      "Epoch [261/300], Step [36/172], Loss: 14.6740\n",
      "Epoch [261/300], Step [37/172], Loss: 14.6130\n",
      "Epoch [261/300], Step [38/172], Loss: 30.2838\n",
      "Epoch [261/300], Step [39/172], Loss: 33.4174\n",
      "Epoch [261/300], Step [40/172], Loss: 20.8165\n",
      "Epoch [261/300], Step [41/172], Loss: 30.4481\n",
      "Epoch [261/300], Step [42/172], Loss: 36.9699\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [261/300], Step [43/172], Loss: 26.4266\n",
      "Epoch [261/300], Step [44/172], Loss: 22.3418\n",
      "Epoch [261/300], Step [45/172], Loss: 30.4962\n",
      "Epoch [261/300], Step [46/172], Loss: 16.1274\n",
      "Epoch [261/300], Step [47/172], Loss: 48.6281\n",
      "Epoch [261/300], Step [48/172], Loss: 63.9500\n",
      "Epoch [261/300], Step [49/172], Loss: 24.3015\n",
      "Epoch [261/300], Step [50/172], Loss: 51.6450\n",
      "Epoch [261/300], Step [51/172], Loss: 8.6209\n",
      "Epoch [261/300], Step [52/172], Loss: 23.1819\n",
      "Epoch [261/300], Step [53/172], Loss: 24.7980\n",
      "Epoch [261/300], Step [54/172], Loss: 17.0314\n",
      "Epoch [261/300], Step [55/172], Loss: 17.0387\n",
      "Epoch [261/300], Step [56/172], Loss: 17.3457\n",
      "Epoch [261/300], Step [57/172], Loss: 19.5719\n",
      "Epoch [261/300], Step [58/172], Loss: 15.6981\n",
      "Epoch [261/300], Step [59/172], Loss: 29.5721\n",
      "Epoch [261/300], Step [60/172], Loss: 24.3940\n",
      "Epoch [261/300], Step [61/172], Loss: 7.2153\n",
      "Epoch [261/300], Step [62/172], Loss: 16.4865\n",
      "Epoch [261/300], Step [63/172], Loss: 10.8551\n",
      "Epoch [261/300], Step [64/172], Loss: 13.5284\n",
      "Epoch [261/300], Step [65/172], Loss: 21.4382\n",
      "Epoch [261/300], Step [66/172], Loss: 9.0925\n",
      "Epoch [261/300], Step [67/172], Loss: 27.5611\n",
      "Epoch [261/300], Step [68/172], Loss: 6.5825\n",
      "Epoch [261/300], Step [69/172], Loss: 29.9859\n",
      "Epoch [261/300], Step [70/172], Loss: 29.2631\n",
      "Epoch [261/300], Step [71/172], Loss: 32.1691\n",
      "Epoch [261/300], Step [72/172], Loss: 29.8023\n",
      "Epoch [261/300], Step [73/172], Loss: 37.9131\n",
      "Epoch [261/300], Step [74/172], Loss: 20.8124\n",
      "Epoch [261/300], Step [75/172], Loss: 21.0173\n",
      "Epoch [261/300], Step [76/172], Loss: 24.1112\n",
      "Epoch [261/300], Step [77/172], Loss: 42.5090\n",
      "Epoch [261/300], Step [78/172], Loss: 30.1957\n",
      "Epoch [261/300], Step [79/172], Loss: 27.9292\n",
      "Epoch [261/300], Step [80/172], Loss: 45.4429\n",
      "Epoch [261/300], Step [81/172], Loss: 25.3092\n",
      "Epoch [261/300], Step [82/172], Loss: 35.5932\n",
      "Epoch [261/300], Step [83/172], Loss: 38.3770\n",
      "Epoch [261/300], Step [84/172], Loss: 29.3700\n",
      "Epoch [261/300], Step [85/172], Loss: 32.9886\n",
      "Epoch [261/300], Step [86/172], Loss: 29.8360\n",
      "Epoch [261/300], Step [87/172], Loss: 21.7018\n",
      "Epoch [261/300], Step [88/172], Loss: 20.0712\n",
      "Epoch [261/300], Step [89/172], Loss: 25.9247\n",
      "Epoch [261/300], Step [90/172], Loss: 18.6308\n",
      "Epoch [261/300], Step [91/172], Loss: 25.1417\n",
      "Epoch [261/300], Step [92/172], Loss: 18.4417\n",
      "Epoch [261/300], Step [93/172], Loss: 17.8920\n",
      "Epoch [261/300], Step [94/172], Loss: 24.3576\n",
      "Epoch [261/300], Step [95/172], Loss: 19.2842\n",
      "Epoch [261/300], Step [96/172], Loss: 19.6333\n",
      "Epoch [261/300], Step [97/172], Loss: 29.3378\n",
      "Epoch [261/300], Step [98/172], Loss: 17.7406\n",
      "Epoch [261/300], Step [99/172], Loss: 18.4961\n",
      "Epoch [261/300], Step [100/172], Loss: 16.9681\n",
      "Epoch [261/300], Step [101/172], Loss: 18.8156\n",
      "Epoch [261/300], Step [102/172], Loss: 18.4999\n",
      "Epoch [261/300], Step [103/172], Loss: 11.7024\n",
      "Epoch [261/300], Step [104/172], Loss: 19.3132\n",
      "Epoch [261/300], Step [105/172], Loss: 22.9509\n",
      "Epoch [261/300], Step [106/172], Loss: 15.5301\n",
      "Epoch [261/300], Step [107/172], Loss: 16.3566\n",
      "Epoch [261/300], Step [108/172], Loss: 16.0970\n",
      "Epoch [261/300], Step [109/172], Loss: 14.9914\n",
      "Epoch [261/300], Step [110/172], Loss: 17.5389\n",
      "Epoch [261/300], Step [111/172], Loss: 17.7696\n",
      "Epoch [261/300], Step [112/172], Loss: 16.8077\n",
      "Epoch [261/300], Step [113/172], Loss: 14.6109\n",
      "Epoch [261/300], Step [114/172], Loss: 15.5395\n",
      "Epoch [261/300], Step [115/172], Loss: 20.5452\n",
      "Epoch [261/300], Step [116/172], Loss: 14.7737\n",
      "Epoch [261/300], Step [117/172], Loss: 13.1616\n",
      "Epoch [261/300], Step [118/172], Loss: 13.9006\n",
      "Epoch [261/300], Step [119/172], Loss: 18.8988\n",
      "Epoch [261/300], Step [120/172], Loss: 10.2666\n",
      "Epoch [261/300], Step [121/172], Loss: 9.0742\n",
      "Epoch [261/300], Step [122/172], Loss: 12.5978\n",
      "Epoch [261/300], Step [123/172], Loss: 12.1514\n",
      "Epoch [261/300], Step [124/172], Loss: 7.5651\n",
      "Epoch [261/300], Step [125/172], Loss: 12.5122\n",
      "Epoch [261/300], Step [126/172], Loss: 12.7698\n",
      "Epoch [261/300], Step [127/172], Loss: 11.2997\n",
      "Epoch [261/300], Step [128/172], Loss: 10.3096\n",
      "Epoch [261/300], Step [129/172], Loss: 9.0242\n",
      "Epoch [261/300], Step [130/172], Loss: 13.6379\n",
      "Epoch [261/300], Step [131/172], Loss: 7.9345\n",
      "Epoch [261/300], Step [132/172], Loss: 10.0318\n",
      "Epoch [261/300], Step [133/172], Loss: 9.9092\n",
      "Epoch [261/300], Step [134/172], Loss: 11.0911\n",
      "Epoch [261/300], Step [135/172], Loss: 9.4844\n",
      "Epoch [261/300], Step [136/172], Loss: 8.8425\n",
      "Epoch [261/300], Step [137/172], Loss: 9.0415\n",
      "Epoch [261/300], Step [138/172], Loss: 8.3299\n",
      "Epoch [261/300], Step [139/172], Loss: 11.6483\n",
      "Epoch [261/300], Step [140/172], Loss: 11.2752\n",
      "Epoch [261/300], Step [141/172], Loss: 9.2044\n",
      "Epoch [261/300], Step [142/172], Loss: 15.4379\n",
      "Epoch [261/300], Step [143/172], Loss: 12.3958\n",
      "Epoch [261/300], Step [144/172], Loss: 9.5946\n",
      "Epoch [261/300], Step [145/172], Loss: 11.2517\n",
      "Epoch [261/300], Step [146/172], Loss: 10.9278\n",
      "Epoch [261/300], Step [147/172], Loss: 5.8291\n",
      "Epoch [261/300], Step [148/172], Loss: 6.4031\n",
      "Epoch [261/300], Step [149/172], Loss: 6.5248\n",
      "Epoch [261/300], Step [150/172], Loss: 5.6374\n",
      "Epoch [261/300], Step [151/172], Loss: 5.5163\n",
      "Epoch [261/300], Step [152/172], Loss: 8.2111\n",
      "Epoch [261/300], Step [153/172], Loss: 6.3887\n",
      "Epoch [261/300], Step [154/172], Loss: 6.7423\n",
      "Epoch [261/300], Step [155/172], Loss: 6.6212\n",
      "Epoch [261/300], Step [156/172], Loss: 14.7695\n",
      "Epoch [261/300], Step [157/172], Loss: 9.6829\n",
      "Epoch [261/300], Step [158/172], Loss: 7.5297\n",
      "Epoch [261/300], Step [159/172], Loss: 10.0925\n",
      "Epoch [261/300], Step [160/172], Loss: 10.6466\n",
      "Epoch [261/300], Step [161/172], Loss: 6.8963\n",
      "Epoch [261/300], Step [162/172], Loss: 4.7945\n",
      "Epoch [261/300], Step [163/172], Loss: 7.2178\n",
      "Epoch [261/300], Step [164/172], Loss: 8.5508\n",
      "Epoch [261/300], Step [165/172], Loss: 6.9517\n",
      "Epoch [261/300], Step [166/172], Loss: 6.2586\n",
      "Epoch [261/300], Step [167/172], Loss: 11.3280\n",
      "Epoch [261/300], Step [168/172], Loss: 6.7418\n",
      "Epoch [261/300], Step [169/172], Loss: 6.8169\n",
      "Epoch [261/300], Step [170/172], Loss: 5.7257\n",
      "Epoch [261/300], Step [171/172], Loss: 9.2597\n",
      "Epoch [261/300], Step [172/172], Loss: 5.4943\n",
      "Epoch [262/300], Step [1/172], Loss: 40.8576\n",
      "Epoch [262/300], Step [2/172], Loss: 43.9913\n",
      "Epoch [262/300], Step [3/172], Loss: 39.6247\n",
      "Epoch [262/300], Step [4/172], Loss: 18.7061\n",
      "Epoch [262/300], Step [5/172], Loss: 36.7307\n",
      "Epoch [262/300], Step [6/172], Loss: 17.1731\n",
      "Epoch [262/300], Step [7/172], Loss: 26.8485\n",
      "Epoch [262/300], Step [8/172], Loss: 4.5975\n",
      "Epoch [262/300], Step [9/172], Loss: 24.6358\n",
      "Epoch [262/300], Step [10/172], Loss: 34.5952\n",
      "Epoch [262/300], Step [11/172], Loss: 49.8261\n",
      "Epoch [262/300], Step [12/172], Loss: 48.4030\n",
      "Epoch [262/300], Step [13/172], Loss: 29.6317\n",
      "Epoch [262/300], Step [14/172], Loss: 50.6904\n",
      "Epoch [262/300], Step [15/172], Loss: 47.1063\n",
      "Epoch [262/300], Step [16/172], Loss: 7.1297\n",
      "Epoch [262/300], Step [17/172], Loss: 34.7464\n",
      "Epoch [262/300], Step [18/172], Loss: 48.9429\n",
      "Epoch [262/300], Step [19/172], Loss: 66.5778\n",
      "Epoch [262/300], Step [20/172], Loss: 26.0619\n",
      "Epoch [262/300], Step [21/172], Loss: 69.6316\n",
      "Epoch [262/300], Step [22/172], Loss: 48.5547\n",
      "Epoch [262/300], Step [23/172], Loss: 1.4195\n",
      "Epoch [262/300], Step [24/172], Loss: 43.9230\n",
      "Epoch [262/300], Step [25/172], Loss: 30.9032\n",
      "Epoch [262/300], Step [26/172], Loss: 42.0046\n",
      "Epoch [262/300], Step [27/172], Loss: 53.7592\n",
      "Epoch [262/300], Step [28/172], Loss: 15.7158\n",
      "Epoch [262/300], Step [29/172], Loss: 13.7813\n",
      "Epoch [262/300], Step [30/172], Loss: 45.8961\n",
      "Epoch [262/300], Step [31/172], Loss: 27.8290\n",
      "Epoch [262/300], Step [32/172], Loss: 39.9384\n",
      "Epoch [262/300], Step [33/172], Loss: 60.5511\n",
      "Epoch [262/300], Step [34/172], Loss: 1.9258\n",
      "Epoch [262/300], Step [35/172], Loss: 14.7869\n",
      "Epoch [262/300], Step [36/172], Loss: 15.3285\n",
      "Epoch [262/300], Step [37/172], Loss: 14.6096\n",
      "Epoch [262/300], Step [38/172], Loss: 30.1024\n",
      "Epoch [262/300], Step [39/172], Loss: 33.6416\n",
      "Epoch [262/300], Step [40/172], Loss: 20.9465\n",
      "Epoch [262/300], Step [41/172], Loss: 30.4853\n",
      "Epoch [262/300], Step [42/172], Loss: 37.3340\n",
      "Epoch [262/300], Step [43/172], Loss: 26.7744\n",
      "Epoch [262/300], Step [44/172], Loss: 22.4409\n",
      "Epoch [262/300], Step [45/172], Loss: 30.6032\n",
      "Epoch [262/300], Step [46/172], Loss: 16.1726\n",
      "Epoch [262/300], Step [47/172], Loss: 48.7843\n",
      "Epoch [262/300], Step [48/172], Loss: 64.4484\n",
      "Epoch [262/300], Step [49/172], Loss: 24.3487\n",
      "Epoch [262/300], Step [50/172], Loss: 52.0252\n",
      "Epoch [262/300], Step [51/172], Loss: 8.5236\n",
      "Epoch [262/300], Step [52/172], Loss: 23.1781\n",
      "Epoch [262/300], Step [53/172], Loss: 24.4300\n",
      "Epoch [262/300], Step [54/172], Loss: 16.9470\n",
      "Epoch [262/300], Step [55/172], Loss: 16.8630\n",
      "Epoch [262/300], Step [56/172], Loss: 17.7104\n",
      "Epoch [262/300], Step [57/172], Loss: 19.4061\n",
      "Epoch [262/300], Step [58/172], Loss: 15.4870\n",
      "Epoch [262/300], Step [59/172], Loss: 29.3960\n",
      "Epoch [262/300], Step [60/172], Loss: 24.2216\n",
      "Epoch [262/300], Step [61/172], Loss: 7.1292\n",
      "Epoch [262/300], Step [62/172], Loss: 16.3429\n",
      "Epoch [262/300], Step [63/172], Loss: 10.8243\n",
      "Epoch [262/300], Step [64/172], Loss: 13.2287\n",
      "Epoch [262/300], Step [65/172], Loss: 21.1314\n",
      "Epoch [262/300], Step [66/172], Loss: 8.8606\n",
      "Epoch [262/300], Step [67/172], Loss: 27.5776\n",
      "Epoch [262/300], Step [68/172], Loss: 6.1197\n",
      "Epoch [262/300], Step [69/172], Loss: 30.0755\n",
      "Epoch [262/300], Step [70/172], Loss: 29.9095\n",
      "Epoch [262/300], Step [71/172], Loss: 32.3566\n",
      "Epoch [262/300], Step [72/172], Loss: 29.8626\n",
      "Epoch [262/300], Step [73/172], Loss: 38.1958\n",
      "Epoch [262/300], Step [74/172], Loss: 20.9446\n",
      "Epoch [262/300], Step [75/172], Loss: 21.2039\n",
      "Epoch [262/300], Step [76/172], Loss: 24.4351\n",
      "Epoch [262/300], Step [77/172], Loss: 42.4301\n",
      "Epoch [262/300], Step [78/172], Loss: 30.2959\n",
      "Epoch [262/300], Step [79/172], Loss: 27.7881\n",
      "Epoch [262/300], Step [80/172], Loss: 45.3293\n",
      "Epoch [262/300], Step [81/172], Loss: 25.1754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [262/300], Step [82/172], Loss: 35.9892\n",
      "Epoch [262/300], Step [83/172], Loss: 38.2359\n",
      "Epoch [262/300], Step [84/172], Loss: 29.0385\n",
      "Epoch [262/300], Step [85/172], Loss: 32.7691\n",
      "Epoch [262/300], Step [86/172], Loss: 29.7241\n",
      "Epoch [262/300], Step [87/172], Loss: 21.5038\n",
      "Epoch [262/300], Step [88/172], Loss: 20.0596\n",
      "Epoch [262/300], Step [89/172], Loss: 25.8893\n",
      "Epoch [262/300], Step [90/172], Loss: 18.8127\n",
      "Epoch [262/300], Step [91/172], Loss: 25.0310\n",
      "Epoch [262/300], Step [92/172], Loss: 18.4455\n",
      "Epoch [262/300], Step [93/172], Loss: 17.9803\n",
      "Epoch [262/300], Step [94/172], Loss: 24.4357\n",
      "Epoch [262/300], Step [95/172], Loss: 19.2590\n",
      "Epoch [262/300], Step [96/172], Loss: 19.6694\n",
      "Epoch [262/300], Step [97/172], Loss: 29.3540\n",
      "Epoch [262/300], Step [98/172], Loss: 17.6772\n",
      "Epoch [262/300], Step [99/172], Loss: 18.5189\n",
      "Epoch [262/300], Step [100/172], Loss: 16.8590\n",
      "Epoch [262/300], Step [101/172], Loss: 18.7772\n",
      "Epoch [262/300], Step [102/172], Loss: 18.7446\n",
      "Epoch [262/300], Step [103/172], Loss: 11.7630\n",
      "Epoch [262/300], Step [104/172], Loss: 19.3524\n",
      "Epoch [262/300], Step [105/172], Loss: 23.1810\n",
      "Epoch [262/300], Step [106/172], Loss: 15.6127\n",
      "Epoch [262/300], Step [107/172], Loss: 16.4039\n",
      "Epoch [262/300], Step [108/172], Loss: 16.1003\n",
      "Epoch [262/300], Step [109/172], Loss: 15.2252\n",
      "Epoch [262/300], Step [110/172], Loss: 17.6033\n",
      "Epoch [262/300], Step [111/172], Loss: 17.8580\n",
      "Epoch [262/300], Step [112/172], Loss: 16.8989\n",
      "Epoch [262/300], Step [113/172], Loss: 14.4169\n",
      "Epoch [262/300], Step [114/172], Loss: 15.6276\n",
      "Epoch [262/300], Step [115/172], Loss: 20.7000\n",
      "Epoch [262/300], Step [116/172], Loss: 14.9387\n",
      "Epoch [262/300], Step [117/172], Loss: 13.1450\n",
      "Epoch [262/300], Step [118/172], Loss: 14.0138\n",
      "Epoch [262/300], Step [119/172], Loss: 18.8854\n",
      "Epoch [262/300], Step [120/172], Loss: 10.3414\n",
      "Epoch [262/300], Step [121/172], Loss: 9.1885\n",
      "Epoch [262/300], Step [122/172], Loss: 12.7498\n",
      "Epoch [262/300], Step [123/172], Loss: 12.4746\n",
      "Epoch [262/300], Step [124/172], Loss: 7.6671\n",
      "Epoch [262/300], Step [125/172], Loss: 12.6088\n",
      "Epoch [262/300], Step [126/172], Loss: 12.7648\n",
      "Epoch [262/300], Step [127/172], Loss: 11.4000\n",
      "Epoch [262/300], Step [128/172], Loss: 10.3365\n",
      "Epoch [262/300], Step [129/172], Loss: 9.0738\n",
      "Epoch [262/300], Step [130/172], Loss: 13.6124\n",
      "Epoch [262/300], Step [131/172], Loss: 8.0208\n",
      "Epoch [262/300], Step [132/172], Loss: 10.0934\n",
      "Epoch [262/300], Step [133/172], Loss: 10.0053\n",
      "Epoch [262/300], Step [134/172], Loss: 11.1216\n",
      "Epoch [262/300], Step [135/172], Loss: 9.6121\n",
      "Epoch [262/300], Step [136/172], Loss: 8.8387\n",
      "Epoch [262/300], Step [137/172], Loss: 9.1514\n",
      "Epoch [262/300], Step [138/172], Loss: 8.3325\n",
      "Epoch [262/300], Step [139/172], Loss: 11.7888\n",
      "Epoch [262/300], Step [140/172], Loss: 11.2876\n",
      "Epoch [262/300], Step [141/172], Loss: 9.2300\n",
      "Epoch [262/300], Step [142/172], Loss: 15.6609\n",
      "Epoch [262/300], Step [143/172], Loss: 12.3672\n",
      "Epoch [262/300], Step [144/172], Loss: 9.6746\n",
      "Epoch [262/300], Step [145/172], Loss: 11.3999\n",
      "Epoch [262/300], Step [146/172], Loss: 10.9476\n",
      "Epoch [262/300], Step [147/172], Loss: 5.8236\n",
      "Epoch [262/300], Step [148/172], Loss: 6.4108\n",
      "Epoch [262/300], Step [149/172], Loss: 6.4874\n",
      "Epoch [262/300], Step [150/172], Loss: 5.6336\n",
      "Epoch [262/300], Step [151/172], Loss: 5.5292\n",
      "Epoch [262/300], Step [152/172], Loss: 8.1665\n",
      "Epoch [262/300], Step [153/172], Loss: 6.3979\n",
      "Epoch [262/300], Step [154/172], Loss: 6.8387\n",
      "Epoch [262/300], Step [155/172], Loss: 6.6884\n",
      "Epoch [262/300], Step [156/172], Loss: 14.9229\n",
      "Epoch [262/300], Step [157/172], Loss: 9.8563\n",
      "Epoch [262/300], Step [158/172], Loss: 7.5940\n",
      "Epoch [262/300], Step [159/172], Loss: 10.1847\n",
      "Epoch [262/300], Step [160/172], Loss: 10.8275\n",
      "Epoch [262/300], Step [161/172], Loss: 6.7842\n",
      "Epoch [262/300], Step [162/172], Loss: 4.7919\n",
      "Epoch [262/300], Step [163/172], Loss: 7.2832\n",
      "Epoch [262/300], Step [164/172], Loss: 8.7874\n",
      "Epoch [262/300], Step [165/172], Loss: 6.9789\n",
      "Epoch [262/300], Step [166/172], Loss: 6.3475\n",
      "Epoch [262/300], Step [167/172], Loss: 11.4707\n",
      "Epoch [262/300], Step [168/172], Loss: 6.7776\n",
      "Epoch [262/300], Step [169/172], Loss: 6.9077\n",
      "Epoch [262/300], Step [170/172], Loss: 5.6573\n",
      "Epoch [262/300], Step [171/172], Loss: 9.2823\n",
      "Epoch [262/300], Step [172/172], Loss: 5.5310\n",
      "Epoch [263/300], Step [1/172], Loss: 40.3357\n",
      "Epoch [263/300], Step [2/172], Loss: 43.1273\n",
      "Epoch [263/300], Step [3/172], Loss: 39.0527\n",
      "Epoch [263/300], Step [4/172], Loss: 18.8719\n",
      "Epoch [263/300], Step [5/172], Loss: 35.9298\n",
      "Epoch [263/300], Step [6/172], Loss: 16.2084\n",
      "Epoch [263/300], Step [7/172], Loss: 23.2434\n",
      "Epoch [263/300], Step [8/172], Loss: 3.4636\n",
      "Epoch [263/300], Step [9/172], Loss: 24.5711\n",
      "Epoch [263/300], Step [10/172], Loss: 35.2615\n",
      "Epoch [263/300], Step [11/172], Loss: 50.1818\n",
      "Epoch [263/300], Step [12/172], Loss: 48.7483\n",
      "Epoch [263/300], Step [13/172], Loss: 29.7055\n",
      "Epoch [263/300], Step [14/172], Loss: 49.8348\n",
      "Epoch [263/300], Step [15/172], Loss: 47.6225\n",
      "Epoch [263/300], Step [16/172], Loss: 9.4134\n",
      "Epoch [263/300], Step [17/172], Loss: 34.1378\n",
      "Epoch [263/300], Step [18/172], Loss: 48.7721\n",
      "Epoch [263/300], Step [19/172], Loss: 67.3385\n",
      "Epoch [263/300], Step [20/172], Loss: 26.0447\n",
      "Epoch [263/300], Step [21/172], Loss: 70.0874\n",
      "Epoch [263/300], Step [22/172], Loss: 49.1033\n",
      "Epoch [263/300], Step [23/172], Loss: 1.9125\n",
      "Epoch [263/300], Step [24/172], Loss: 44.1065\n",
      "Epoch [263/300], Step [25/172], Loss: 32.3188\n",
      "Epoch [263/300], Step [26/172], Loss: 42.4090\n",
      "Epoch [263/300], Step [27/172], Loss: 53.6874\n",
      "Epoch [263/300], Step [28/172], Loss: 15.5519\n",
      "Epoch [263/300], Step [29/172], Loss: 13.7191\n",
      "Epoch [263/300], Step [30/172], Loss: 45.1751\n",
      "Epoch [263/300], Step [31/172], Loss: 27.2963\n",
      "Epoch [263/300], Step [32/172], Loss: 39.7241\n",
      "Epoch [263/300], Step [33/172], Loss: 60.5538\n",
      "Epoch [263/300], Step [34/172], Loss: 1.7532\n",
      "Epoch [263/300], Step [35/172], Loss: 14.4935\n",
      "Epoch [263/300], Step [36/172], Loss: 14.7276\n",
      "Epoch [263/300], Step [37/172], Loss: 14.3686\n",
      "Epoch [263/300], Step [38/172], Loss: 29.8740\n",
      "Epoch [263/300], Step [39/172], Loss: 33.3067\n",
      "Epoch [263/300], Step [40/172], Loss: 20.4763\n",
      "Epoch [263/300], Step [41/172], Loss: 30.2476\n",
      "Epoch [263/300], Step [42/172], Loss: 36.4464\n",
      "Epoch [263/300], Step [43/172], Loss: 26.2367\n",
      "Epoch [263/300], Step [44/172], Loss: 22.0433\n",
      "Epoch [263/300], Step [45/172], Loss: 29.8801\n",
      "Epoch [263/300], Step [46/172], Loss: 16.1320\n",
      "Epoch [263/300], Step [47/172], Loss: 48.6878\n",
      "Epoch [263/300], Step [48/172], Loss: 63.8567\n",
      "Epoch [263/300], Step [49/172], Loss: 24.5523\n",
      "Epoch [263/300], Step [50/172], Loss: 51.5713\n",
      "Epoch [263/300], Step [51/172], Loss: 8.4829\n",
      "Epoch [263/300], Step [52/172], Loss: 23.1953\n",
      "Epoch [263/300], Step [53/172], Loss: 24.6866\n",
      "Epoch [263/300], Step [54/172], Loss: 16.5530\n",
      "Epoch [263/300], Step [55/172], Loss: 16.5520\n",
      "Epoch [263/300], Step [56/172], Loss: 17.1769\n",
      "Epoch [263/300], Step [57/172], Loss: 18.8051\n",
      "Epoch [263/300], Step [58/172], Loss: 15.2852\n",
      "Epoch [263/300], Step [59/172], Loss: 29.2734\n",
      "Epoch [263/300], Step [60/172], Loss: 24.3326\n",
      "Epoch [263/300], Step [61/172], Loss: 7.0446\n",
      "Epoch [263/300], Step [62/172], Loss: 16.5014\n",
      "Epoch [263/300], Step [63/172], Loss: 10.4924\n",
      "Epoch [263/300], Step [64/172], Loss: 13.1684\n",
      "Epoch [263/300], Step [65/172], Loss: 20.8892\n",
      "Epoch [263/300], Step [66/172], Loss: 8.8007\n",
      "Epoch [263/300], Step [67/172], Loss: 27.2015\n",
      "Epoch [263/300], Step [68/172], Loss: 5.6203\n",
      "Epoch [263/300], Step [69/172], Loss: 29.7400\n",
      "Epoch [263/300], Step [70/172], Loss: 29.4040\n",
      "Epoch [263/300], Step [71/172], Loss: 32.3090\n",
      "Epoch [263/300], Step [72/172], Loss: 29.9714\n",
      "Epoch [263/300], Step [73/172], Loss: 37.7125\n",
      "Epoch [263/300], Step [74/172], Loss: 20.8418\n",
      "Epoch [263/300], Step [75/172], Loss: 21.4180\n",
      "Epoch [263/300], Step [76/172], Loss: 24.0891\n",
      "Epoch [263/300], Step [77/172], Loss: 42.4956\n",
      "Epoch [263/300], Step [78/172], Loss: 30.0909\n",
      "Epoch [263/300], Step [79/172], Loss: 27.7920\n",
      "Epoch [263/300], Step [80/172], Loss: 45.2859\n",
      "Epoch [263/300], Step [81/172], Loss: 25.0975\n",
      "Epoch [263/300], Step [82/172], Loss: 35.3327\n",
      "Epoch [263/300], Step [83/172], Loss: 38.2096\n",
      "Epoch [263/300], Step [84/172], Loss: 29.1926\n",
      "Epoch [263/300], Step [85/172], Loss: 32.8747\n",
      "Epoch [263/300], Step [86/172], Loss: 29.4342\n",
      "Epoch [263/300], Step [87/172], Loss: 21.5064\n",
      "Epoch [263/300], Step [88/172], Loss: 19.9381\n",
      "Epoch [263/300], Step [89/172], Loss: 25.5688\n",
      "Epoch [263/300], Step [90/172], Loss: 18.4279\n",
      "Epoch [263/300], Step [91/172], Loss: 25.0957\n",
      "Epoch [263/300], Step [92/172], Loss: 18.3882\n",
      "Epoch [263/300], Step [93/172], Loss: 17.8781\n",
      "Epoch [263/300], Step [94/172], Loss: 24.2557\n",
      "Epoch [263/300], Step [95/172], Loss: 18.8297\n",
      "Epoch [263/300], Step [96/172], Loss: 19.6368\n",
      "Epoch [263/300], Step [97/172], Loss: 29.3142\n",
      "Epoch [263/300], Step [98/172], Loss: 17.6052\n",
      "Epoch [263/300], Step [99/172], Loss: 18.3558\n",
      "Epoch [263/300], Step [100/172], Loss: 16.8243\n",
      "Epoch [263/300], Step [101/172], Loss: 18.6970\n",
      "Epoch [263/300], Step [102/172], Loss: 18.4574\n",
      "Epoch [263/300], Step [103/172], Loss: 11.4908\n",
      "Epoch [263/300], Step [104/172], Loss: 19.1779\n",
      "Epoch [263/300], Step [105/172], Loss: 22.7736\n",
      "Epoch [263/300], Step [106/172], Loss: 15.3506\n",
      "Epoch [263/300], Step [107/172], Loss: 16.2776\n",
      "Epoch [263/300], Step [108/172], Loss: 15.8264\n",
      "Epoch [263/300], Step [109/172], Loss: 14.9384\n",
      "Epoch [263/300], Step [110/172], Loss: 17.3225\n",
      "Epoch [263/300], Step [111/172], Loss: 17.5229\n",
      "Epoch [263/300], Step [112/172], Loss: 16.7055\n",
      "Epoch [263/300], Step [113/172], Loss: 14.3321\n",
      "Epoch [263/300], Step [114/172], Loss: 15.4789\n",
      "Epoch [263/300], Step [115/172], Loss: 20.4196\n",
      "Epoch [263/300], Step [116/172], Loss: 14.6245\n",
      "Epoch [263/300], Step [117/172], Loss: 12.9217\n",
      "Epoch [263/300], Step [118/172], Loss: 13.8226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [263/300], Step [119/172], Loss: 18.7257\n",
      "Epoch [263/300], Step [120/172], Loss: 10.0960\n",
      "Epoch [263/300], Step [121/172], Loss: 9.0395\n",
      "Epoch [263/300], Step [122/172], Loss: 12.4291\n",
      "Epoch [263/300], Step [123/172], Loss: 11.9063\n",
      "Epoch [263/300], Step [124/172], Loss: 7.6773\n",
      "Epoch [263/300], Step [125/172], Loss: 12.4672\n",
      "Epoch [263/300], Step [126/172], Loss: 12.5797\n",
      "Epoch [263/300], Step [127/172], Loss: 11.2959\n",
      "Epoch [263/300], Step [128/172], Loss: 10.3327\n",
      "Epoch [263/300], Step [129/172], Loss: 8.9926\n",
      "Epoch [263/300], Step [130/172], Loss: 13.5183\n",
      "Epoch [263/300], Step [131/172], Loss: 7.8898\n",
      "Epoch [263/300], Step [132/172], Loss: 9.9202\n",
      "Epoch [263/300], Step [133/172], Loss: 9.8204\n",
      "Epoch [263/300], Step [134/172], Loss: 11.0833\n",
      "Epoch [263/300], Step [135/172], Loss: 9.4194\n",
      "Epoch [263/300], Step [136/172], Loss: 8.6211\n",
      "Epoch [263/300], Step [137/172], Loss: 9.0010\n",
      "Epoch [263/300], Step [138/172], Loss: 8.2042\n",
      "Epoch [263/300], Step [139/172], Loss: 11.7122\n",
      "Epoch [263/300], Step [140/172], Loss: 11.1778\n",
      "Epoch [263/300], Step [141/172], Loss: 9.0251\n",
      "Epoch [263/300], Step [142/172], Loss: 15.2596\n",
      "Epoch [263/300], Step [143/172], Loss: 12.2326\n",
      "Epoch [263/300], Step [144/172], Loss: 9.5416\n",
      "Epoch [263/300], Step [145/172], Loss: 11.0727\n",
      "Epoch [263/300], Step [146/172], Loss: 10.7933\n",
      "Epoch [263/300], Step [147/172], Loss: 5.8158\n",
      "Epoch [263/300], Step [148/172], Loss: 6.3330\n",
      "Epoch [263/300], Step [149/172], Loss: 6.4167\n",
      "Epoch [263/300], Step [150/172], Loss: 5.5147\n",
      "Epoch [263/300], Step [151/172], Loss: 5.3860\n",
      "Epoch [263/300], Step [152/172], Loss: 8.0869\n",
      "Epoch [263/300], Step [153/172], Loss: 6.3642\n",
      "Epoch [263/300], Step [154/172], Loss: 6.6365\n",
      "Epoch [263/300], Step [155/172], Loss: 6.6541\n",
      "Epoch [263/300], Step [156/172], Loss: 14.7875\n",
      "Epoch [263/300], Step [157/172], Loss: 9.7527\n",
      "Epoch [263/300], Step [158/172], Loss: 7.4711\n",
      "Epoch [263/300], Step [159/172], Loss: 10.1103\n",
      "Epoch [263/300], Step [160/172], Loss: 10.7128\n",
      "Epoch [263/300], Step [161/172], Loss: 6.5091\n",
      "Epoch [263/300], Step [162/172], Loss: 4.6779\n",
      "Epoch [263/300], Step [163/172], Loss: 7.1190\n",
      "Epoch [263/300], Step [164/172], Loss: 8.5472\n",
      "Epoch [263/300], Step [165/172], Loss: 6.8631\n",
      "Epoch [263/300], Step [166/172], Loss: 6.2537\n",
      "Epoch [263/300], Step [167/172], Loss: 11.3591\n",
      "Epoch [263/300], Step [168/172], Loss: 6.6681\n",
      "Epoch [263/300], Step [169/172], Loss: 6.5977\n",
      "Epoch [263/300], Step [170/172], Loss: 5.3979\n",
      "Epoch [263/300], Step [171/172], Loss: 9.1312\n",
      "Epoch [263/300], Step [172/172], Loss: 5.5862\n",
      "Epoch [264/300], Step [1/172], Loss: 40.4223\n",
      "Epoch [264/300], Step [2/172], Loss: 43.0602\n",
      "Epoch [264/300], Step [3/172], Loss: 40.5293\n",
      "Epoch [264/300], Step [4/172], Loss: 18.8404\n",
      "Epoch [264/300], Step [5/172], Loss: 35.6198\n",
      "Epoch [264/300], Step [6/172], Loss: 16.2907\n",
      "Epoch [264/300], Step [7/172], Loss: 24.1858\n",
      "Epoch [264/300], Step [8/172], Loss: 4.2611\n",
      "Epoch [264/300], Step [9/172], Loss: 24.9590\n",
      "Epoch [264/300], Step [10/172], Loss: 35.0640\n",
      "Epoch [264/300], Step [11/172], Loss: 50.4838\n",
      "Epoch [264/300], Step [12/172], Loss: 49.4223\n",
      "Epoch [264/300], Step [13/172], Loss: 29.7216\n",
      "Epoch [264/300], Step [14/172], Loss: 50.8420\n",
      "Epoch [264/300], Step [15/172], Loss: 47.7988\n",
      "Epoch [264/300], Step [16/172], Loss: 7.9943\n",
      "Epoch [264/300], Step [17/172], Loss: 34.5169\n",
      "Epoch [264/300], Step [18/172], Loss: 49.2039\n",
      "Epoch [264/300], Step [19/172], Loss: 67.2137\n",
      "Epoch [264/300], Step [20/172], Loss: 26.0339\n",
      "Epoch [264/300], Step [21/172], Loss: 69.7546\n",
      "Epoch [264/300], Step [22/172], Loss: 48.4661\n",
      "Epoch [264/300], Step [23/172], Loss: 1.5717\n",
      "Epoch [264/300], Step [24/172], Loss: 43.7554\n",
      "Epoch [264/300], Step [25/172], Loss: 31.4548\n",
      "Epoch [264/300], Step [26/172], Loss: 41.9830\n",
      "Epoch [264/300], Step [27/172], Loss: 53.8555\n",
      "Epoch [264/300], Step [28/172], Loss: 15.3381\n",
      "Epoch [264/300], Step [29/172], Loss: 13.7219\n",
      "Epoch [264/300], Step [30/172], Loss: 45.1168\n",
      "Epoch [264/300], Step [31/172], Loss: 27.0786\n",
      "Epoch [264/300], Step [32/172], Loss: 39.4133\n",
      "Epoch [264/300], Step [33/172], Loss: 59.8051\n",
      "Epoch [264/300], Step [34/172], Loss: 1.8223\n",
      "Epoch [264/300], Step [35/172], Loss: 14.5258\n",
      "Epoch [264/300], Step [36/172], Loss: 14.9213\n",
      "Epoch [264/300], Step [37/172], Loss: 14.2485\n",
      "Epoch [264/300], Step [38/172], Loss: 29.6733\n",
      "Epoch [264/300], Step [39/172], Loss: 33.0964\n",
      "Epoch [264/300], Step [40/172], Loss: 20.4038\n",
      "Epoch [264/300], Step [41/172], Loss: 29.7986\n",
      "Epoch [264/300], Step [42/172], Loss: 36.1591\n",
      "Epoch [264/300], Step [43/172], Loss: 26.1567\n",
      "Epoch [264/300], Step [44/172], Loss: 21.5990\n",
      "Epoch [264/300], Step [45/172], Loss: 29.4164\n",
      "Epoch [264/300], Step [46/172], Loss: 16.0082\n",
      "Epoch [264/300], Step [47/172], Loss: 48.4629\n",
      "Epoch [264/300], Step [48/172], Loss: 64.3937\n",
      "Epoch [264/300], Step [49/172], Loss: 24.2976\n",
      "Epoch [264/300], Step [50/172], Loss: 51.8923\n",
      "Epoch [264/300], Step [51/172], Loss: 8.5099\n",
      "Epoch [264/300], Step [52/172], Loss: 22.9894\n",
      "Epoch [264/300], Step [53/172], Loss: 24.3840\n",
      "Epoch [264/300], Step [54/172], Loss: 16.3551\n",
      "Epoch [264/300], Step [55/172], Loss: 16.4408\n",
      "Epoch [264/300], Step [56/172], Loss: 16.9760\n",
      "Epoch [264/300], Step [57/172], Loss: 18.6609\n",
      "Epoch [264/300], Step [58/172], Loss: 15.1216\n",
      "Epoch [264/300], Step [59/172], Loss: 29.2414\n",
      "Epoch [264/300], Step [60/172], Loss: 23.7571\n",
      "Epoch [264/300], Step [61/172], Loss: 6.9392\n",
      "Epoch [264/300], Step [62/172], Loss: 16.1242\n",
      "Epoch [264/300], Step [63/172], Loss: 10.6370\n",
      "Epoch [264/300], Step [64/172], Loss: 13.0647\n",
      "Epoch [264/300], Step [65/172], Loss: 20.7058\n",
      "Epoch [264/300], Step [66/172], Loss: 8.5665\n",
      "Epoch [264/300], Step [67/172], Loss: 27.0988\n",
      "Epoch [264/300], Step [68/172], Loss: 5.4521\n",
      "Epoch [264/300], Step [69/172], Loss: 29.7760\n",
      "Epoch [264/300], Step [70/172], Loss: 29.6792\n",
      "Epoch [264/300], Step [71/172], Loss: 32.3998\n",
      "Epoch [264/300], Step [72/172], Loss: 30.1283\n",
      "Epoch [264/300], Step [73/172], Loss: 37.9411\n",
      "Epoch [264/300], Step [74/172], Loss: 20.8664\n",
      "Epoch [264/300], Step [75/172], Loss: 21.1695\n",
      "Epoch [264/300], Step [76/172], Loss: 24.0205\n",
      "Epoch [264/300], Step [77/172], Loss: 42.7526\n",
      "Epoch [264/300], Step [78/172], Loss: 30.2833\n",
      "Epoch [264/300], Step [79/172], Loss: 27.7891\n",
      "Epoch [264/300], Step [80/172], Loss: 45.5758\n",
      "Epoch [264/300], Step [81/172], Loss: 25.1666\n",
      "Epoch [264/300], Step [82/172], Loss: 35.6623\n",
      "Epoch [264/300], Step [83/172], Loss: 38.2314\n",
      "Epoch [264/300], Step [84/172], Loss: 29.1810\n",
      "Epoch [264/300], Step [85/172], Loss: 32.7854\n",
      "Epoch [264/300], Step [86/172], Loss: 29.1326\n",
      "Epoch [264/300], Step [87/172], Loss: 21.4766\n",
      "Epoch [264/300], Step [88/172], Loss: 19.8415\n",
      "Epoch [264/300], Step [89/172], Loss: 25.4436\n",
      "Epoch [264/300], Step [90/172], Loss: 18.2889\n",
      "Epoch [264/300], Step [91/172], Loss: 24.9136\n",
      "Epoch [264/300], Step [92/172], Loss: 18.2069\n",
      "Epoch [264/300], Step [93/172], Loss: 17.8420\n",
      "Epoch [264/300], Step [94/172], Loss: 24.1103\n",
      "Epoch [264/300], Step [95/172], Loss: 18.6103\n",
      "Epoch [264/300], Step [96/172], Loss: 19.5920\n",
      "Epoch [264/300], Step [97/172], Loss: 29.0780\n",
      "Epoch [264/300], Step [98/172], Loss: 17.4908\n",
      "Epoch [264/300], Step [99/172], Loss: 18.2459\n",
      "Epoch [264/300], Step [100/172], Loss: 16.7563\n",
      "Epoch [264/300], Step [101/172], Loss: 18.5791\n",
      "Epoch [264/300], Step [102/172], Loss: 18.3448\n",
      "Epoch [264/300], Step [103/172], Loss: 11.4545\n",
      "Epoch [264/300], Step [104/172], Loss: 19.0486\n",
      "Epoch [264/300], Step [105/172], Loss: 22.7753\n",
      "Epoch [264/300], Step [106/172], Loss: 15.3173\n",
      "Epoch [264/300], Step [107/172], Loss: 16.1510\n",
      "Epoch [264/300], Step [108/172], Loss: 15.6829\n",
      "Epoch [264/300], Step [109/172], Loss: 14.8069\n",
      "Epoch [264/300], Step [110/172], Loss: 17.2083\n",
      "Epoch [264/300], Step [111/172], Loss: 17.6330\n",
      "Epoch [264/300], Step [112/172], Loss: 16.3314\n",
      "Epoch [264/300], Step [113/172], Loss: 14.2656\n",
      "Epoch [264/300], Step [114/172], Loss: 15.3627\n",
      "Epoch [264/300], Step [115/172], Loss: 20.2694\n",
      "Epoch [264/300], Step [116/172], Loss: 14.4783\n",
      "Epoch [264/300], Step [117/172], Loss: 12.7975\n",
      "Epoch [264/300], Step [118/172], Loss: 13.6946\n",
      "Epoch [264/300], Step [119/172], Loss: 18.5606\n",
      "Epoch [264/300], Step [120/172], Loss: 10.0301\n",
      "Epoch [264/300], Step [121/172], Loss: 8.9215\n",
      "Epoch [264/300], Step [122/172], Loss: 12.1788\n",
      "Epoch [264/300], Step [123/172], Loss: 12.0538\n",
      "Epoch [264/300], Step [124/172], Loss: 7.5658\n",
      "Epoch [264/300], Step [125/172], Loss: 12.3898\n",
      "Epoch [264/300], Step [126/172], Loss: 12.5994\n",
      "Epoch [264/300], Step [127/172], Loss: 11.1611\n",
      "Epoch [264/300], Step [128/172], Loss: 10.2596\n",
      "Epoch [264/300], Step [129/172], Loss: 8.9550\n",
      "Epoch [264/300], Step [130/172], Loss: 13.4556\n",
      "Epoch [264/300], Step [131/172], Loss: 7.8049\n",
      "Epoch [264/300], Step [132/172], Loss: 9.8539\n",
      "Epoch [264/300], Step [133/172], Loss: 9.6390\n",
      "Epoch [264/300], Step [134/172], Loss: 10.8724\n",
      "Epoch [264/300], Step [135/172], Loss: 9.4267\n",
      "Epoch [264/300], Step [136/172], Loss: 8.6450\n",
      "Epoch [264/300], Step [137/172], Loss: 8.9160\n",
      "Epoch [264/300], Step [138/172], Loss: 8.1745\n",
      "Epoch [264/300], Step [139/172], Loss: 11.6735\n",
      "Epoch [264/300], Step [140/172], Loss: 11.2287\n",
      "Epoch [264/300], Step [141/172], Loss: 9.0879\n",
      "Epoch [264/300], Step [142/172], Loss: 15.2486\n",
      "Epoch [264/300], Step [143/172], Loss: 12.2716\n",
      "Epoch [264/300], Step [144/172], Loss: 9.4476\n",
      "Epoch [264/300], Step [145/172], Loss: 11.0530\n",
      "Epoch [264/300], Step [146/172], Loss: 10.7568\n",
      "Epoch [264/300], Step [147/172], Loss: 5.7949\n",
      "Epoch [264/300], Step [148/172], Loss: 6.3377\n",
      "Epoch [264/300], Step [149/172], Loss: 6.3969\n",
      "Epoch [264/300], Step [150/172], Loss: 5.5477\n",
      "Epoch [264/300], Step [151/172], Loss: 5.4129\n",
      "Epoch [264/300], Step [152/172], Loss: 8.0588\n",
      "Epoch [264/300], Step [153/172], Loss: 6.3117\n",
      "Epoch [264/300], Step [154/172], Loss: 6.6914\n",
      "Epoch [264/300], Step [155/172], Loss: 6.5899\n",
      "Epoch [264/300], Step [156/172], Loss: 14.6622\n",
      "Epoch [264/300], Step [157/172], Loss: 9.6630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [264/300], Step [158/172], Loss: 7.4664\n",
      "Epoch [264/300], Step [159/172], Loss: 10.0944\n",
      "Epoch [264/300], Step [160/172], Loss: 10.5283\n",
      "Epoch [264/300], Step [161/172], Loss: 6.5128\n",
      "Epoch [264/300], Step [162/172], Loss: 4.6741\n",
      "Epoch [264/300], Step [163/172], Loss: 7.2111\n",
      "Epoch [264/300], Step [164/172], Loss: 8.5935\n",
      "Epoch [264/300], Step [165/172], Loss: 6.8637\n",
      "Epoch [264/300], Step [166/172], Loss: 6.2508\n",
      "Epoch [264/300], Step [167/172], Loss: 11.3142\n",
      "Epoch [264/300], Step [168/172], Loss: 6.5579\n",
      "Epoch [264/300], Step [169/172], Loss: 6.7371\n",
      "Epoch [264/300], Step [170/172], Loss: 5.4838\n",
      "Epoch [264/300], Step [171/172], Loss: 9.1848\n",
      "Epoch [264/300], Step [172/172], Loss: 5.4962\n",
      "Epoch [265/300], Step [1/172], Loss: 40.1828\n",
      "Epoch [265/300], Step [2/172], Loss: 43.1521\n",
      "Epoch [265/300], Step [3/172], Loss: 39.3402\n",
      "Epoch [265/300], Step [4/172], Loss: 18.8440\n",
      "Epoch [265/300], Step [5/172], Loss: 35.9984\n",
      "Epoch [265/300], Step [6/172], Loss: 16.6036\n",
      "Epoch [265/300], Step [7/172], Loss: 24.6759\n",
      "Epoch [265/300], Step [8/172], Loss: 3.4883\n",
      "Epoch [265/300], Step [9/172], Loss: 24.5062\n",
      "Epoch [265/300], Step [10/172], Loss: 34.7856\n",
      "Epoch [265/300], Step [11/172], Loss: 50.3332\n",
      "Epoch [265/300], Step [12/172], Loss: 48.9476\n",
      "Epoch [265/300], Step [13/172], Loss: 29.7325\n",
      "Epoch [265/300], Step [14/172], Loss: 50.3692\n",
      "Epoch [265/300], Step [15/172], Loss: 47.8498\n",
      "Epoch [265/300], Step [16/172], Loss: 8.5573\n",
      "Epoch [265/300], Step [17/172], Loss: 34.5013\n",
      "Epoch [265/300], Step [18/172], Loss: 49.2185\n",
      "Epoch [265/300], Step [19/172], Loss: 67.7112\n",
      "Epoch [265/300], Step [20/172], Loss: 25.3446\n",
      "Epoch [265/300], Step [21/172], Loss: 70.1415\n",
      "Epoch [265/300], Step [22/172], Loss: 48.7300\n",
      "Epoch [265/300], Step [23/172], Loss: 1.6814\n",
      "Epoch [265/300], Step [24/172], Loss: 43.6795\n",
      "Epoch [265/300], Step [25/172], Loss: 31.2188\n",
      "Epoch [265/300], Step [26/172], Loss: 42.2191\n",
      "Epoch [265/300], Step [27/172], Loss: 53.7018\n",
      "Epoch [265/300], Step [28/172], Loss: 15.1723\n",
      "Epoch [265/300], Step [29/172], Loss: 13.4888\n",
      "Epoch [265/300], Step [30/172], Loss: 45.0170\n",
      "Epoch [265/300], Step [31/172], Loss: 26.9533\n",
      "Epoch [265/300], Step [32/172], Loss: 39.6702\n",
      "Epoch [265/300], Step [33/172], Loss: 60.3578\n",
      "Epoch [265/300], Step [34/172], Loss: 1.9172\n",
      "Epoch [265/300], Step [35/172], Loss: 14.7390\n",
      "Epoch [265/300], Step [36/172], Loss: 14.8003\n",
      "Epoch [265/300], Step [37/172], Loss: 14.0523\n",
      "Epoch [265/300], Step [38/172], Loss: 29.5114\n",
      "Epoch [265/300], Step [39/172], Loss: 32.9801\n",
      "Epoch [265/300], Step [40/172], Loss: 20.2413\n",
      "Epoch [265/300], Step [41/172], Loss: 29.5413\n",
      "Epoch [265/300], Step [42/172], Loss: 35.9950\n",
      "Epoch [265/300], Step [43/172], Loss: 25.8680\n",
      "Epoch [265/300], Step [44/172], Loss: 21.7100\n",
      "Epoch [265/300], Step [45/172], Loss: 29.0977\n",
      "Epoch [265/300], Step [46/172], Loss: 15.7665\n",
      "Epoch [265/300], Step [47/172], Loss: 47.9629\n",
      "Epoch [265/300], Step [48/172], Loss: 65.0214\n",
      "Epoch [265/300], Step [49/172], Loss: 23.9493\n",
      "Epoch [265/300], Step [50/172], Loss: 51.9568\n",
      "Epoch [265/300], Step [51/172], Loss: 8.2780\n",
      "Epoch [265/300], Step [52/172], Loss: 22.7471\n",
      "Epoch [265/300], Step [53/172], Loss: 24.2020\n",
      "Epoch [265/300], Step [54/172], Loss: 16.2252\n",
      "Epoch [265/300], Step [55/172], Loss: 16.2644\n",
      "Epoch [265/300], Step [56/172], Loss: 17.1368\n",
      "Epoch [265/300], Step [57/172], Loss: 18.8518\n",
      "Epoch [265/300], Step [58/172], Loss: 15.0158\n",
      "Epoch [265/300], Step [59/172], Loss: 28.9254\n",
      "Epoch [265/300], Step [60/172], Loss: 24.3924\n",
      "Epoch [265/300], Step [61/172], Loss: 6.7587\n",
      "Epoch [265/300], Step [62/172], Loss: 16.0747\n",
      "Epoch [265/300], Step [63/172], Loss: 10.3820\n",
      "Epoch [265/300], Step [64/172], Loss: 12.9240\n",
      "Epoch [265/300], Step [65/172], Loss: 20.4933\n",
      "Epoch [265/300], Step [66/172], Loss: 8.6050\n",
      "Epoch [265/300], Step [67/172], Loss: 27.1265\n",
      "Epoch [265/300], Step [68/172], Loss: 5.5660\n",
      "Epoch [265/300], Step [69/172], Loss: 30.1168\n",
      "Epoch [265/300], Step [70/172], Loss: 29.7776\n",
      "Epoch [265/300], Step [71/172], Loss: 32.6402\n",
      "Epoch [265/300], Step [72/172], Loss: 30.2854\n",
      "Epoch [265/300], Step [73/172], Loss: 38.4245\n",
      "Epoch [265/300], Step [74/172], Loss: 20.9164\n",
      "Epoch [265/300], Step [75/172], Loss: 21.0935\n",
      "Epoch [265/300], Step [76/172], Loss: 24.0748\n",
      "Epoch [265/300], Step [77/172], Loss: 42.7166\n",
      "Epoch [265/300], Step [78/172], Loss: 30.2507\n",
      "Epoch [265/300], Step [79/172], Loss: 27.7293\n",
      "Epoch [265/300], Step [80/172], Loss: 45.1011\n",
      "Epoch [265/300], Step [81/172], Loss: 25.2992\n",
      "Epoch [265/300], Step [82/172], Loss: 35.0857\n",
      "Epoch [265/300], Step [83/172], Loss: 38.0572\n",
      "Epoch [265/300], Step [84/172], Loss: 28.9286\n",
      "Epoch [265/300], Step [85/172], Loss: 32.7814\n",
      "Epoch [265/300], Step [86/172], Loss: 29.2053\n",
      "Epoch [265/300], Step [87/172], Loss: 21.3348\n",
      "Epoch [265/300], Step [88/172], Loss: 19.8091\n",
      "Epoch [265/300], Step [89/172], Loss: 25.4292\n",
      "Epoch [265/300], Step [90/172], Loss: 18.2441\n",
      "Epoch [265/300], Step [91/172], Loss: 24.7731\n",
      "Epoch [265/300], Step [92/172], Loss: 18.0775\n",
      "Epoch [265/300], Step [93/172], Loss: 17.6721\n",
      "Epoch [265/300], Step [94/172], Loss: 24.1195\n",
      "Epoch [265/300], Step [95/172], Loss: 18.5795\n",
      "Epoch [265/300], Step [96/172], Loss: 19.5231\n",
      "Epoch [265/300], Step [97/172], Loss: 29.0120\n",
      "Epoch [265/300], Step [98/172], Loss: 17.3930\n",
      "Epoch [265/300], Step [99/172], Loss: 18.2306\n",
      "Epoch [265/300], Step [100/172], Loss: 16.7056\n",
      "Epoch [265/300], Step [101/172], Loss: 18.4819\n",
      "Epoch [265/300], Step [102/172], Loss: 17.9713\n",
      "Epoch [265/300], Step [103/172], Loss: 11.4255\n",
      "Epoch [265/300], Step [104/172], Loss: 18.9862\n",
      "Epoch [265/300], Step [105/172], Loss: 22.4343\n",
      "Epoch [265/300], Step [106/172], Loss: 15.2592\n",
      "Epoch [265/300], Step [107/172], Loss: 16.0479\n",
      "Epoch [265/300], Step [108/172], Loss: 15.5186\n",
      "Epoch [265/300], Step [109/172], Loss: 14.7722\n",
      "Epoch [265/300], Step [110/172], Loss: 17.0510\n",
      "Epoch [265/300], Step [111/172], Loss: 17.5487\n",
      "Epoch [265/300], Step [112/172], Loss: 16.2943\n",
      "Epoch [265/300], Step [113/172], Loss: 14.1256\n",
      "Epoch [265/300], Step [114/172], Loss: 15.2757\n",
      "Epoch [265/300], Step [115/172], Loss: 20.0955\n",
      "Epoch [265/300], Step [116/172], Loss: 14.2887\n",
      "Epoch [265/300], Step [117/172], Loss: 12.6718\n",
      "Epoch [265/300], Step [118/172], Loss: 13.6916\n",
      "Epoch [265/300], Step [119/172], Loss: 18.3952\n",
      "Epoch [265/300], Step [120/172], Loss: 9.9938\n",
      "Epoch [265/300], Step [121/172], Loss: 8.9117\n",
      "Epoch [265/300], Step [122/172], Loss: 12.1130\n",
      "Epoch [265/300], Step [123/172], Loss: 11.7713\n",
      "Epoch [265/300], Step [124/172], Loss: 7.5689\n",
      "Epoch [265/300], Step [125/172], Loss: 12.3596\n",
      "Epoch [265/300], Step [126/172], Loss: 12.4552\n",
      "Epoch [265/300], Step [127/172], Loss: 11.1552\n",
      "Epoch [265/300], Step [128/172], Loss: 10.2100\n",
      "Epoch [265/300], Step [129/172], Loss: 8.9519\n",
      "Epoch [265/300], Step [130/172], Loss: 13.4438\n",
      "Epoch [265/300], Step [131/172], Loss: 7.7199\n",
      "Epoch [265/300], Step [132/172], Loss: 9.8253\n",
      "Epoch [265/300], Step [133/172], Loss: 9.5205\n",
      "Epoch [265/300], Step [134/172], Loss: 10.8066\n",
      "Epoch [265/300], Step [135/172], Loss: 9.3813\n",
      "Epoch [265/300], Step [136/172], Loss: 8.5650\n",
      "Epoch [265/300], Step [137/172], Loss: 8.8836\n",
      "Epoch [265/300], Step [138/172], Loss: 8.0595\n",
      "Epoch [265/300], Step [139/172], Loss: 11.7016\n",
      "Epoch [265/300], Step [140/172], Loss: 11.1908\n",
      "Epoch [265/300], Step [141/172], Loss: 8.9861\n",
      "Epoch [265/300], Step [142/172], Loss: 14.9729\n",
      "Epoch [265/300], Step [143/172], Loss: 12.2549\n",
      "Epoch [265/300], Step [144/172], Loss: 9.3743\n",
      "Epoch [265/300], Step [145/172], Loss: 10.9948\n",
      "Epoch [265/300], Step [146/172], Loss: 10.6703\n",
      "Epoch [265/300], Step [147/172], Loss: 5.7468\n",
      "Epoch [265/300], Step [148/172], Loss: 6.3224\n",
      "Epoch [265/300], Step [149/172], Loss: 6.3070\n",
      "Epoch [265/300], Step [150/172], Loss: 5.5370\n",
      "Epoch [265/300], Step [151/172], Loss: 5.3496\n",
      "Epoch [265/300], Step [152/172], Loss: 7.9712\n",
      "Epoch [265/300], Step [153/172], Loss: 6.3024\n",
      "Epoch [265/300], Step [154/172], Loss: 6.6714\n",
      "Epoch [265/300], Step [155/172], Loss: 6.5703\n",
      "Epoch [265/300], Step [156/172], Loss: 14.5952\n",
      "Epoch [265/300], Step [157/172], Loss: 9.5780\n",
      "Epoch [265/300], Step [158/172], Loss: 7.3360\n",
      "Epoch [265/300], Step [159/172], Loss: 10.1489\n",
      "Epoch [265/300], Step [160/172], Loss: 10.2977\n",
      "Epoch [265/300], Step [161/172], Loss: 6.4042\n",
      "Epoch [265/300], Step [162/172], Loss: 4.6441\n",
      "Epoch [265/300], Step [163/172], Loss: 7.4421\n",
      "Epoch [265/300], Step [164/172], Loss: 8.2914\n",
      "Epoch [265/300], Step [165/172], Loss: 6.8582\n",
      "Epoch [265/300], Step [166/172], Loss: 6.2561\n",
      "Epoch [265/300], Step [167/172], Loss: 11.3797\n",
      "Epoch [265/300], Step [168/172], Loss: 6.5745\n",
      "Epoch [265/300], Step [169/172], Loss: 6.6994\n",
      "Epoch [265/300], Step [170/172], Loss: 5.3861\n",
      "Epoch [265/300], Step [171/172], Loss: 9.2269\n",
      "Epoch [265/300], Step [172/172], Loss: 5.5001\n",
      "Epoch [266/300], Step [1/172], Loss: 39.9311\n",
      "Epoch [266/300], Step [2/172], Loss: 43.4186\n",
      "Epoch [266/300], Step [3/172], Loss: 39.4206\n",
      "Epoch [266/300], Step [4/172], Loss: 18.5359\n",
      "Epoch [266/300], Step [5/172], Loss: 34.9447\n",
      "Epoch [266/300], Step [6/172], Loss: 17.4177\n",
      "Epoch [266/300], Step [7/172], Loss: 27.6979\n",
      "Epoch [266/300], Step [8/172], Loss: 4.7291\n",
      "Epoch [266/300], Step [9/172], Loss: 25.0172\n",
      "Epoch [266/300], Step [10/172], Loss: 34.3548\n",
      "Epoch [266/300], Step [11/172], Loss: 50.2348\n",
      "Epoch [266/300], Step [12/172], Loss: 49.2231\n",
      "Epoch [266/300], Step [13/172], Loss: 29.9890\n",
      "Epoch [266/300], Step [14/172], Loss: 50.4786\n",
      "Epoch [266/300], Step [15/172], Loss: 48.1800\n",
      "Epoch [266/300], Step [16/172], Loss: 6.6687\n",
      "Epoch [266/300], Step [17/172], Loss: 35.8252\n",
      "Epoch [266/300], Step [18/172], Loss: 49.9239\n",
      "Epoch [266/300], Step [19/172], Loss: 69.3793\n",
      "Epoch [266/300], Step [20/172], Loss: 25.1144\n",
      "Epoch [266/300], Step [21/172], Loss: 71.2238\n",
      "Epoch [266/300], Step [22/172], Loss: 48.7012\n",
      "Epoch [266/300], Step [23/172], Loss: 1.5318\n",
      "Epoch [266/300], Step [24/172], Loss: 44.2103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [266/300], Step [25/172], Loss: 30.2351\n",
      "Epoch [266/300], Step [26/172], Loss: 41.4700\n",
      "Epoch [266/300], Step [27/172], Loss: 54.0508\n",
      "Epoch [266/300], Step [28/172], Loss: 15.2802\n",
      "Epoch [266/300], Step [29/172], Loss: 13.8699\n",
      "Epoch [266/300], Step [30/172], Loss: 44.6128\n",
      "Epoch [266/300], Step [31/172], Loss: 26.4789\n",
      "Epoch [266/300], Step [32/172], Loss: 39.6597\n",
      "Epoch [266/300], Step [33/172], Loss: 60.5675\n",
      "Epoch [266/300], Step [34/172], Loss: 2.1372\n",
      "Epoch [266/300], Step [35/172], Loss: 14.5270\n",
      "Epoch [266/300], Step [36/172], Loss: 15.8196\n",
      "Epoch [266/300], Step [37/172], Loss: 13.9820\n",
      "Epoch [266/300], Step [38/172], Loss: 29.3794\n",
      "Epoch [266/300], Step [39/172], Loss: 32.2648\n",
      "Epoch [266/300], Step [40/172], Loss: 19.7749\n",
      "Epoch [266/300], Step [41/172], Loss: 28.8315\n",
      "Epoch [266/300], Step [42/172], Loss: 35.3999\n",
      "Epoch [266/300], Step [43/172], Loss: 25.2196\n",
      "Epoch [266/300], Step [44/172], Loss: 21.2623\n",
      "Epoch [266/300], Step [45/172], Loss: 28.6390\n",
      "Epoch [266/300], Step [46/172], Loss: 15.8145\n",
      "Epoch [266/300], Step [47/172], Loss: 47.6599\n",
      "Epoch [266/300], Step [48/172], Loss: 64.7804\n",
      "Epoch [266/300], Step [49/172], Loss: 23.8995\n",
      "Epoch [266/300], Step [50/172], Loss: 51.6253\n",
      "Epoch [266/300], Step [51/172], Loss: 8.1983\n",
      "Epoch [266/300], Step [52/172], Loss: 22.8991\n",
      "Epoch [266/300], Step [53/172], Loss: 23.9524\n",
      "Epoch [266/300], Step [54/172], Loss: 16.4041\n",
      "Epoch [266/300], Step [55/172], Loss: 16.4162\n",
      "Epoch [266/300], Step [56/172], Loss: 17.4856\n",
      "Epoch [266/300], Step [57/172], Loss: 18.1848\n",
      "Epoch [266/300], Step [58/172], Loss: 15.1788\n",
      "Epoch [266/300], Step [59/172], Loss: 29.1064\n",
      "Epoch [266/300], Step [60/172], Loss: 23.9211\n",
      "Epoch [266/300], Step [61/172], Loss: 6.9087\n",
      "Epoch [266/300], Step [62/172], Loss: 15.5024\n",
      "Epoch [266/300], Step [63/172], Loss: 10.8583\n",
      "Epoch [266/300], Step [64/172], Loss: 12.9004\n",
      "Epoch [266/300], Step [65/172], Loss: 20.5392\n",
      "Epoch [266/300], Step [66/172], Loss: 8.3027\n",
      "Epoch [266/300], Step [67/172], Loss: 26.9638\n",
      "Epoch [266/300], Step [68/172], Loss: 5.7015\n",
      "Epoch [266/300], Step [69/172], Loss: 29.5669\n",
      "Epoch [266/300], Step [70/172], Loss: 29.9400\n",
      "Epoch [266/300], Step [71/172], Loss: 32.7619\n",
      "Epoch [266/300], Step [72/172], Loss: 30.4334\n",
      "Epoch [266/300], Step [73/172], Loss: 38.6456\n",
      "Epoch [266/300], Step [74/172], Loss: 21.0091\n",
      "Epoch [266/300], Step [75/172], Loss: 21.2580\n",
      "Epoch [266/300], Step [76/172], Loss: 24.3432\n",
      "Epoch [266/300], Step [77/172], Loss: 42.7895\n",
      "Epoch [266/300], Step [78/172], Loss: 30.4222\n",
      "Epoch [266/300], Step [79/172], Loss: 27.9380\n",
      "Epoch [266/300], Step [80/172], Loss: 45.4873\n",
      "Epoch [266/300], Step [81/172], Loss: 25.4313\n",
      "Epoch [266/300], Step [82/172], Loss: 35.6101\n",
      "Epoch [266/300], Step [83/172], Loss: 38.0286\n",
      "Epoch [266/300], Step [84/172], Loss: 28.6507\n",
      "Epoch [266/300], Step [85/172], Loss: 32.6139\n",
      "Epoch [266/300], Step [86/172], Loss: 29.1962\n",
      "Epoch [266/300], Step [87/172], Loss: 21.1109\n",
      "Epoch [266/300], Step [88/172], Loss: 19.8602\n",
      "Epoch [266/300], Step [89/172], Loss: 25.2733\n",
      "Epoch [266/300], Step [90/172], Loss: 18.4079\n",
      "Epoch [266/300], Step [91/172], Loss: 24.5527\n",
      "Epoch [266/300], Step [92/172], Loss: 17.9580\n",
      "Epoch [266/300], Step [93/172], Loss: 17.7851\n",
      "Epoch [266/300], Step [94/172], Loss: 24.2119\n",
      "Epoch [266/300], Step [95/172], Loss: 18.2324\n",
      "Epoch [266/300], Step [96/172], Loss: 19.4954\n",
      "Epoch [266/300], Step [97/172], Loss: 28.8193\n",
      "Epoch [266/300], Step [98/172], Loss: 17.3031\n",
      "Epoch [266/300], Step [99/172], Loss: 18.1716\n",
      "Epoch [266/300], Step [100/172], Loss: 16.5083\n",
      "Epoch [266/300], Step [101/172], Loss: 18.3620\n",
      "Epoch [266/300], Step [102/172], Loss: 17.9530\n",
      "Epoch [266/300], Step [103/172], Loss: 11.3441\n",
      "Epoch [266/300], Step [104/172], Loss: 18.9196\n",
      "Epoch [266/300], Step [105/172], Loss: 22.2439\n",
      "Epoch [266/300], Step [106/172], Loss: 15.0704\n",
      "Epoch [266/300], Step [107/172], Loss: 15.9059\n",
      "Epoch [266/300], Step [108/172], Loss: 15.2660\n",
      "Epoch [266/300], Step [109/172], Loss: 14.8479\n",
      "Epoch [266/300], Step [110/172], Loss: 17.0011\n",
      "Epoch [266/300], Step [111/172], Loss: 17.4017\n",
      "Epoch [266/300], Step [112/172], Loss: 16.2185\n",
      "Epoch [266/300], Step [113/172], Loss: 13.9052\n",
      "Epoch [266/300], Step [114/172], Loss: 15.2476\n",
      "Epoch [266/300], Step [115/172], Loss: 19.9352\n",
      "Epoch [266/300], Step [116/172], Loss: 14.2571\n",
      "Epoch [266/300], Step [117/172], Loss: 12.5952\n",
      "Epoch [266/300], Step [118/172], Loss: 13.5270\n",
      "Epoch [266/300], Step [119/172], Loss: 18.2637\n",
      "Epoch [266/300], Step [120/172], Loss: 9.9678\n",
      "Epoch [266/300], Step [121/172], Loss: 8.9140\n",
      "Epoch [266/300], Step [122/172], Loss: 12.3434\n",
      "Epoch [266/300], Step [123/172], Loss: 11.6358\n",
      "Epoch [266/300], Step [124/172], Loss: 7.6526\n",
      "Epoch [266/300], Step [125/172], Loss: 12.3895\n",
      "Epoch [266/300], Step [126/172], Loss: 12.3954\n",
      "Epoch [266/300], Step [127/172], Loss: 11.2475\n",
      "Epoch [266/300], Step [128/172], Loss: 10.2671\n",
      "Epoch [266/300], Step [129/172], Loss: 8.9602\n",
      "Epoch [266/300], Step [130/172], Loss: 13.2259\n",
      "Epoch [266/300], Step [131/172], Loss: 7.8679\n",
      "Epoch [266/300], Step [132/172], Loss: 9.8583\n",
      "Epoch [266/300], Step [133/172], Loss: 9.5572\n",
      "Epoch [266/300], Step [134/172], Loss: 10.6801\n",
      "Epoch [266/300], Step [135/172], Loss: 9.3356\n",
      "Epoch [266/300], Step [136/172], Loss: 8.5081\n",
      "Epoch [266/300], Step [137/172], Loss: 8.8024\n",
      "Epoch [266/300], Step [138/172], Loss: 7.9638\n",
      "Epoch [266/300], Step [139/172], Loss: 11.6465\n",
      "Epoch [266/300], Step [140/172], Loss: 11.2469\n",
      "Epoch [266/300], Step [141/172], Loss: 9.0206\n",
      "Epoch [266/300], Step [142/172], Loss: 14.8868\n",
      "Epoch [266/300], Step [143/172], Loss: 12.1229\n",
      "Epoch [266/300], Step [144/172], Loss: 9.3975\n",
      "Epoch [266/300], Step [145/172], Loss: 10.9671\n",
      "Epoch [266/300], Step [146/172], Loss: 10.7116\n",
      "Epoch [266/300], Step [147/172], Loss: 5.7645\n",
      "Epoch [266/300], Step [148/172], Loss: 6.3723\n",
      "Epoch [266/300], Step [149/172], Loss: 6.3154\n",
      "Epoch [266/300], Step [150/172], Loss: 5.5373\n",
      "Epoch [266/300], Step [151/172], Loss: 5.3287\n",
      "Epoch [266/300], Step [152/172], Loss: 7.8526\n",
      "Epoch [266/300], Step [153/172], Loss: 6.3424\n",
      "Epoch [266/300], Step [154/172], Loss: 6.6594\n",
      "Epoch [266/300], Step [155/172], Loss: 6.6756\n",
      "Epoch [266/300], Step [156/172], Loss: 14.6752\n",
      "Epoch [266/300], Step [157/172], Loss: 9.3819\n",
      "Epoch [266/300], Step [158/172], Loss: 7.3305\n",
      "Epoch [266/300], Step [159/172], Loss: 10.0565\n",
      "Epoch [266/300], Step [160/172], Loss: 10.1778\n",
      "Epoch [266/300], Step [161/172], Loss: 6.5154\n",
      "Epoch [266/300], Step [162/172], Loss: 4.6234\n",
      "Epoch [266/300], Step [163/172], Loss: 7.4060\n",
      "Epoch [266/300], Step [164/172], Loss: 8.4934\n",
      "Epoch [266/300], Step [165/172], Loss: 6.9066\n",
      "Epoch [266/300], Step [166/172], Loss: 6.2181\n",
      "Epoch [266/300], Step [167/172], Loss: 11.3437\n",
      "Epoch [266/300], Step [168/172], Loss: 6.5412\n",
      "Epoch [266/300], Step [169/172], Loss: 6.6811\n",
      "Epoch [266/300], Step [170/172], Loss: 5.3212\n",
      "Epoch [266/300], Step [171/172], Loss: 9.2668\n",
      "Epoch [266/300], Step [172/172], Loss: 5.5289\n",
      "Epoch [267/300], Step [1/172], Loss: 39.6349\n",
      "Epoch [267/300], Step [2/172], Loss: 43.7105\n",
      "Epoch [267/300], Step [3/172], Loss: 39.3055\n",
      "Epoch [267/300], Step [4/172], Loss: 18.4877\n",
      "Epoch [267/300], Step [5/172], Loss: 35.1642\n",
      "Epoch [267/300], Step [6/172], Loss: 17.9528\n",
      "Epoch [267/300], Step [7/172], Loss: 27.3091\n",
      "Epoch [267/300], Step [8/172], Loss: 4.3220\n",
      "Epoch [267/300], Step [9/172], Loss: 24.6668\n",
      "Epoch [267/300], Step [10/172], Loss: 34.8407\n",
      "Epoch [267/300], Step [11/172], Loss: 49.8615\n",
      "Epoch [267/300], Step [12/172], Loss: 48.3302\n",
      "Epoch [267/300], Step [13/172], Loss: 29.8594\n",
      "Epoch [267/300], Step [14/172], Loss: 50.6587\n",
      "Epoch [267/300], Step [15/172], Loss: 48.0734\n",
      "Epoch [267/300], Step [16/172], Loss: 7.7072\n",
      "Epoch [267/300], Step [17/172], Loss: 35.5759\n",
      "Epoch [267/300], Step [18/172], Loss: 49.7967\n",
      "Epoch [267/300], Step [19/172], Loss: 69.0875\n",
      "Epoch [267/300], Step [20/172], Loss: 25.0138\n",
      "Epoch [267/300], Step [21/172], Loss: 71.3258\n",
      "Epoch [267/300], Step [22/172], Loss: 48.8653\n",
      "Epoch [267/300], Step [23/172], Loss: 1.3590\n",
      "Epoch [267/300], Step [24/172], Loss: 44.6306\n",
      "Epoch [267/300], Step [25/172], Loss: 30.3573\n",
      "Epoch [267/300], Step [26/172], Loss: 41.4392\n",
      "Epoch [267/300], Step [27/172], Loss: 54.3207\n",
      "Epoch [267/300], Step [28/172], Loss: 15.2960\n",
      "Epoch [267/300], Step [29/172], Loss: 13.7526\n",
      "Epoch [267/300], Step [30/172], Loss: 44.6805\n",
      "Epoch [267/300], Step [31/172], Loss: 26.9886\n",
      "Epoch [267/300], Step [32/172], Loss: 39.7946\n",
      "Epoch [267/300], Step [33/172], Loss: 60.0991\n",
      "Epoch [267/300], Step [34/172], Loss: 1.7921\n",
      "Epoch [267/300], Step [35/172], Loss: 14.3128\n",
      "Epoch [267/300], Step [36/172], Loss: 14.6360\n",
      "Epoch [267/300], Step [37/172], Loss: 14.0825\n",
      "Epoch [267/300], Step [38/172], Loss: 29.8157\n",
      "Epoch [267/300], Step [39/172], Loss: 32.3614\n",
      "Epoch [267/300], Step [40/172], Loss: 19.8164\n",
      "Epoch [267/300], Step [41/172], Loss: 28.9798\n",
      "Epoch [267/300], Step [42/172], Loss: 35.5400\n",
      "Epoch [267/300], Step [43/172], Loss: 25.1502\n",
      "Epoch [267/300], Step [44/172], Loss: 21.1514\n",
      "Epoch [267/300], Step [45/172], Loss: 28.5294\n",
      "Epoch [267/300], Step [46/172], Loss: 15.7544\n",
      "Epoch [267/300], Step [47/172], Loss: 47.4818\n",
      "Epoch [267/300], Step [48/172], Loss: 64.2139\n",
      "Epoch [267/300], Step [49/172], Loss: 23.5528\n",
      "Epoch [267/300], Step [50/172], Loss: 51.8343\n",
      "Epoch [267/300], Step [51/172], Loss: 8.2045\n",
      "Epoch [267/300], Step [52/172], Loss: 22.6395\n",
      "Epoch [267/300], Step [53/172], Loss: 23.8712\n",
      "Epoch [267/300], Step [54/172], Loss: 16.5055\n",
      "Epoch [267/300], Step [55/172], Loss: 16.3009\n",
      "Epoch [267/300], Step [56/172], Loss: 17.5890\n",
      "Epoch [267/300], Step [57/172], Loss: 18.3216\n",
      "Epoch [267/300], Step [58/172], Loss: 15.1931\n",
      "Epoch [267/300], Step [59/172], Loss: 29.2825\n",
      "Epoch [267/300], Step [60/172], Loss: 24.0563\n",
      "Epoch [267/300], Step [61/172], Loss: 6.8291\n",
      "Epoch [267/300], Step [62/172], Loss: 16.1912\n",
      "Epoch [267/300], Step [63/172], Loss: 10.6228\n",
      "Epoch [267/300], Step [64/172], Loss: 12.8656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [267/300], Step [65/172], Loss: 20.5516\n",
      "Epoch [267/300], Step [66/172], Loss: 8.5583\n",
      "Epoch [267/300], Step [67/172], Loss: 27.4307\n",
      "Epoch [267/300], Step [68/172], Loss: 5.8843\n",
      "Epoch [267/300], Step [69/172], Loss: 29.7505\n",
      "Epoch [267/300], Step [70/172], Loss: 29.4073\n",
      "Epoch [267/300], Step [71/172], Loss: 32.3333\n",
      "Epoch [267/300], Step [72/172], Loss: 30.2990\n",
      "Epoch [267/300], Step [73/172], Loss: 38.5232\n",
      "Epoch [267/300], Step [74/172], Loss: 20.7287\n",
      "Epoch [267/300], Step [75/172], Loss: 21.0129\n",
      "Epoch [267/300], Step [76/172], Loss: 24.0227\n",
      "Epoch [267/300], Step [77/172], Loss: 42.4968\n",
      "Epoch [267/300], Step [78/172], Loss: 30.2591\n",
      "Epoch [267/300], Step [79/172], Loss: 27.5816\n",
      "Epoch [267/300], Step [80/172], Loss: 45.2392\n",
      "Epoch [267/300], Step [81/172], Loss: 25.2925\n",
      "Epoch [267/300], Step [82/172], Loss: 35.5502\n",
      "Epoch [267/300], Step [83/172], Loss: 37.8948\n",
      "Epoch [267/300], Step [84/172], Loss: 28.4018\n",
      "Epoch [267/300], Step [85/172], Loss: 32.4795\n",
      "Epoch [267/300], Step [86/172], Loss: 29.1990\n",
      "Epoch [267/300], Step [87/172], Loss: 21.0462\n",
      "Epoch [267/300], Step [88/172], Loss: 19.6865\n",
      "Epoch [267/300], Step [89/172], Loss: 25.2242\n",
      "Epoch [267/300], Step [90/172], Loss: 18.1463\n",
      "Epoch [267/300], Step [91/172], Loss: 24.3116\n",
      "Epoch [267/300], Step [92/172], Loss: 17.8169\n",
      "Epoch [267/300], Step [93/172], Loss: 17.5793\n",
      "Epoch [267/300], Step [94/172], Loss: 23.8200\n",
      "Epoch [267/300], Step [95/172], Loss: 18.0933\n",
      "Epoch [267/300], Step [96/172], Loss: 19.3220\n",
      "Epoch [267/300], Step [97/172], Loss: 28.6055\n",
      "Epoch [267/300], Step [98/172], Loss: 17.0381\n",
      "Epoch [267/300], Step [99/172], Loss: 17.8774\n",
      "Epoch [267/300], Step [100/172], Loss: 16.1756\n",
      "Epoch [267/300], Step [101/172], Loss: 18.0947\n",
      "Epoch [267/300], Step [102/172], Loss: 17.8964\n",
      "Epoch [267/300], Step [103/172], Loss: 11.1255\n",
      "Epoch [267/300], Step [104/172], Loss: 18.6135\n",
      "Epoch [267/300], Step [105/172], Loss: 21.9367\n",
      "Epoch [267/300], Step [106/172], Loss: 14.8516\n",
      "Epoch [267/300], Step [107/172], Loss: 15.6501\n",
      "Epoch [267/300], Step [108/172], Loss: 15.0771\n",
      "Epoch [267/300], Step [109/172], Loss: 14.6953\n",
      "Epoch [267/300], Step [110/172], Loss: 16.7887\n",
      "Epoch [267/300], Step [111/172], Loss: 17.2272\n",
      "Epoch [267/300], Step [112/172], Loss: 15.9957\n",
      "Epoch [267/300], Step [113/172], Loss: 13.7962\n",
      "Epoch [267/300], Step [114/172], Loss: 15.0647\n",
      "Epoch [267/300], Step [115/172], Loss: 19.8228\n",
      "Epoch [267/300], Step [116/172], Loss: 14.1694\n",
      "Epoch [267/300], Step [117/172], Loss: 12.4731\n",
      "Epoch [267/300], Step [118/172], Loss: 13.4322\n",
      "Epoch [267/300], Step [119/172], Loss: 18.1522\n",
      "Epoch [267/300], Step [120/172], Loss: 9.9389\n",
      "Epoch [267/300], Step [121/172], Loss: 8.7709\n",
      "Epoch [267/300], Step [122/172], Loss: 12.2862\n",
      "Epoch [267/300], Step [123/172], Loss: 11.6465\n",
      "Epoch [267/300], Step [124/172], Loss: 7.4956\n",
      "Epoch [267/300], Step [125/172], Loss: 12.2706\n",
      "Epoch [267/300], Step [126/172], Loss: 12.3125\n",
      "Epoch [267/300], Step [127/172], Loss: 11.1302\n",
      "Epoch [267/300], Step [128/172], Loss: 10.0685\n",
      "Epoch [267/300], Step [129/172], Loss: 8.8595\n",
      "Epoch [267/300], Step [130/172], Loss: 13.1754\n",
      "Epoch [267/300], Step [131/172], Loss: 7.7390\n",
      "Epoch [267/300], Step [132/172], Loss: 9.6510\n",
      "Epoch [267/300], Step [133/172], Loss: 9.6914\n",
      "Epoch [267/300], Step [134/172], Loss: 10.6814\n",
      "Epoch [267/300], Step [135/172], Loss: 9.2562\n",
      "Epoch [267/300], Step [136/172], Loss: 8.4418\n",
      "Epoch [267/300], Step [137/172], Loss: 8.8168\n",
      "Epoch [267/300], Step [138/172], Loss: 7.8238\n",
      "Epoch [267/300], Step [139/172], Loss: 11.5902\n",
      "Epoch [267/300], Step [140/172], Loss: 11.0752\n",
      "Epoch [267/300], Step [141/172], Loss: 8.9132\n",
      "Epoch [267/300], Step [142/172], Loss: 15.0743\n",
      "Epoch [267/300], Step [143/172], Loss: 12.0216\n",
      "Epoch [267/300], Step [144/172], Loss: 9.3408\n",
      "Epoch [267/300], Step [145/172], Loss: 10.8726\n",
      "Epoch [267/300], Step [146/172], Loss: 10.5312\n",
      "Epoch [267/300], Step [147/172], Loss: 5.6780\n",
      "Epoch [267/300], Step [148/172], Loss: 6.2097\n",
      "Epoch [267/300], Step [149/172], Loss: 6.2000\n",
      "Epoch [267/300], Step [150/172], Loss: 5.4019\n",
      "Epoch [267/300], Step [151/172], Loss: 5.3619\n",
      "Epoch [267/300], Step [152/172], Loss: 7.7749\n",
      "Epoch [267/300], Step [153/172], Loss: 6.1873\n",
      "Epoch [267/300], Step [154/172], Loss: 6.6568\n",
      "Epoch [267/300], Step [155/172], Loss: 6.4631\n",
      "Epoch [267/300], Step [156/172], Loss: 14.7150\n",
      "Epoch [267/300], Step [157/172], Loss: 9.5162\n",
      "Epoch [267/300], Step [158/172], Loss: 7.3549\n",
      "Epoch [267/300], Step [159/172], Loss: 9.9615\n",
      "Epoch [267/300], Step [160/172], Loss: 10.4220\n",
      "Epoch [267/300], Step [161/172], Loss: 6.2277\n",
      "Epoch [267/300], Step [162/172], Loss: 4.5250\n",
      "Epoch [267/300], Step [163/172], Loss: 7.4493\n",
      "Epoch [267/300], Step [164/172], Loss: 8.5286\n",
      "Epoch [267/300], Step [165/172], Loss: 6.8415\n",
      "Epoch [267/300], Step [166/172], Loss: 6.2002\n",
      "Epoch [267/300], Step [167/172], Loss: 11.0689\n",
      "Epoch [267/300], Step [168/172], Loss: 6.3917\n",
      "Epoch [267/300], Step [169/172], Loss: 6.6951\n",
      "Epoch [267/300], Step [170/172], Loss: 5.3735\n",
      "Epoch [267/300], Step [171/172], Loss: 8.9494\n",
      "Epoch [267/300], Step [172/172], Loss: 5.3762\n",
      "Epoch [268/300], Step [1/172], Loss: 39.8120\n",
      "Epoch [268/300], Step [2/172], Loss: 42.8122\n",
      "Epoch [268/300], Step [3/172], Loss: 40.4166\n",
      "Epoch [268/300], Step [4/172], Loss: 18.4680\n",
      "Epoch [268/300], Step [5/172], Loss: 35.8250\n",
      "Epoch [268/300], Step [6/172], Loss: 16.9416\n",
      "Epoch [268/300], Step [7/172], Loss: 23.8096\n",
      "Epoch [268/300], Step [8/172], Loss: 3.6095\n",
      "Epoch [268/300], Step [9/172], Loss: 24.5616\n",
      "Epoch [268/300], Step [10/172], Loss: 35.0915\n",
      "Epoch [268/300], Step [11/172], Loss: 50.2692\n",
      "Epoch [268/300], Step [12/172], Loss: 48.2474\n",
      "Epoch [268/300], Step [13/172], Loss: 29.9333\n",
      "Epoch [268/300], Step [14/172], Loss: 50.5211\n",
      "Epoch [268/300], Step [15/172], Loss: 48.3215\n",
      "Epoch [268/300], Step [16/172], Loss: 9.2041\n",
      "Epoch [268/300], Step [17/172], Loss: 34.6426\n",
      "Epoch [268/300], Step [18/172], Loss: 49.3642\n",
      "Epoch [268/300], Step [19/172], Loss: 68.5687\n",
      "Epoch [268/300], Step [20/172], Loss: 25.5438\n",
      "Epoch [268/300], Step [21/172], Loss: 71.5236\n",
      "Epoch [268/300], Step [22/172], Loss: 48.8883\n",
      "Epoch [268/300], Step [23/172], Loss: 2.0996\n",
      "Epoch [268/300], Step [24/172], Loss: 44.3890\n",
      "Epoch [268/300], Step [25/172], Loss: 31.6090\n",
      "Epoch [268/300], Step [26/172], Loss: 41.8825\n",
      "Epoch [268/300], Step [27/172], Loss: 53.7617\n",
      "Epoch [268/300], Step [28/172], Loss: 15.3720\n",
      "Epoch [268/300], Step [29/172], Loss: 13.8128\n",
      "Epoch [268/300], Step [30/172], Loss: 44.9309\n",
      "Epoch [268/300], Step [31/172], Loss: 27.0434\n",
      "Epoch [268/300], Step [32/172], Loss: 40.0634\n",
      "Epoch [268/300], Step [33/172], Loss: 60.4189\n",
      "Epoch [268/300], Step [34/172], Loss: 1.5795\n",
      "Epoch [268/300], Step [35/172], Loss: 14.0872\n",
      "Epoch [268/300], Step [36/172], Loss: 14.0278\n",
      "Epoch [268/300], Step [37/172], Loss: 13.9725\n",
      "Epoch [268/300], Step [38/172], Loss: 29.6686\n",
      "Epoch [268/300], Step [39/172], Loss: 32.4372\n",
      "Epoch [268/300], Step [40/172], Loss: 19.6983\n",
      "Epoch [268/300], Step [41/172], Loss: 28.9672\n",
      "Epoch [268/300], Step [42/172], Loss: 35.2915\n",
      "Epoch [268/300], Step [43/172], Loss: 25.2895\n",
      "Epoch [268/300], Step [44/172], Loss: 21.0058\n",
      "Epoch [268/300], Step [45/172], Loss: 27.9876\n",
      "Epoch [268/300], Step [46/172], Loss: 15.5544\n",
      "Epoch [268/300], Step [47/172], Loss: 47.3880\n",
      "Epoch [268/300], Step [48/172], Loss: 64.6332\n",
      "Epoch [268/300], Step [49/172], Loss: 23.2573\n",
      "Epoch [268/300], Step [50/172], Loss: 51.5147\n",
      "Epoch [268/300], Step [51/172], Loss: 7.9996\n",
      "Epoch [268/300], Step [52/172], Loss: 22.2210\n",
      "Epoch [268/300], Step [53/172], Loss: 23.9232\n",
      "Epoch [268/300], Step [54/172], Loss: 15.3455\n",
      "Epoch [268/300], Step [55/172], Loss: 15.8876\n",
      "Epoch [268/300], Step [56/172], Loss: 16.9671\n",
      "Epoch [268/300], Step [57/172], Loss: 18.0621\n",
      "Epoch [268/300], Step [58/172], Loss: 14.4069\n",
      "Epoch [268/300], Step [59/172], Loss: 28.7777\n",
      "Epoch [268/300], Step [60/172], Loss: 24.1930\n",
      "Epoch [268/300], Step [61/172], Loss: 6.5575\n",
      "Epoch [268/300], Step [62/172], Loss: 16.1699\n",
      "Epoch [268/300], Step [63/172], Loss: 10.3292\n",
      "Epoch [268/300], Step [64/172], Loss: 13.0103\n",
      "Epoch [268/300], Step [65/172], Loss: 20.3702\n",
      "Epoch [268/300], Step [66/172], Loss: 8.6578\n",
      "Epoch [268/300], Step [67/172], Loss: 27.3640\n",
      "Epoch [268/300], Step [68/172], Loss: 5.8935\n",
      "Epoch [268/300], Step [69/172], Loss: 29.6606\n",
      "Epoch [268/300], Step [70/172], Loss: 29.3196\n",
      "Epoch [268/300], Step [71/172], Loss: 32.2341\n",
      "Epoch [268/300], Step [72/172], Loss: 30.1889\n",
      "Epoch [268/300], Step [73/172], Loss: 38.2780\n",
      "Epoch [268/300], Step [74/172], Loss: 20.7834\n",
      "Epoch [268/300], Step [75/172], Loss: 21.2079\n",
      "Epoch [268/300], Step [76/172], Loss: 23.9166\n",
      "Epoch [268/300], Step [77/172], Loss: 42.4369\n",
      "Epoch [268/300], Step [78/172], Loss: 29.8354\n",
      "Epoch [268/300], Step [79/172], Loss: 27.3334\n",
      "Epoch [268/300], Step [80/172], Loss: 45.0707\n",
      "Epoch [268/300], Step [81/172], Loss: 25.0966\n",
      "Epoch [268/300], Step [82/172], Loss: 34.8471\n",
      "Epoch [268/300], Step [83/172], Loss: 37.8119\n",
      "Epoch [268/300], Step [84/172], Loss: 28.6019\n",
      "Epoch [268/300], Step [85/172], Loss: 32.4958\n",
      "Epoch [268/300], Step [86/172], Loss: 28.7378\n",
      "Epoch [268/300], Step [87/172], Loss: 21.0255\n",
      "Epoch [268/300], Step [88/172], Loss: 19.5096\n",
      "Epoch [268/300], Step [89/172], Loss: 24.9220\n",
      "Epoch [268/300], Step [90/172], Loss: 18.0577\n",
      "Epoch [268/300], Step [91/172], Loss: 24.3229\n",
      "Epoch [268/300], Step [92/172], Loss: 17.7893\n",
      "Epoch [268/300], Step [93/172], Loss: 17.4103\n",
      "Epoch [268/300], Step [94/172], Loss: 23.6983\n",
      "Epoch [268/300], Step [95/172], Loss: 18.1698\n",
      "Epoch [268/300], Step [96/172], Loss: 19.3449\n",
      "Epoch [268/300], Step [97/172], Loss: 28.6297\n",
      "Epoch [268/300], Step [98/172], Loss: 16.8528\n",
      "Epoch [268/300], Step [99/172], Loss: 17.5445\n",
      "Epoch [268/300], Step [100/172], Loss: 15.9711\n",
      "Epoch [268/300], Step [101/172], Loss: 18.0032\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [268/300], Step [102/172], Loss: 17.8203\n",
      "Epoch [268/300], Step [103/172], Loss: 10.8121\n",
      "Epoch [268/300], Step [104/172], Loss: 18.3921\n",
      "Epoch [268/300], Step [105/172], Loss: 21.4987\n",
      "Epoch [268/300], Step [106/172], Loss: 14.6598\n",
      "Epoch [268/300], Step [107/172], Loss: 15.5177\n",
      "Epoch [268/300], Step [108/172], Loss: 14.9349\n",
      "Epoch [268/300], Step [109/172], Loss: 14.5317\n",
      "Epoch [268/300], Step [110/172], Loss: 16.5030\n",
      "Epoch [268/300], Step [111/172], Loss: 16.8975\n",
      "Epoch [268/300], Step [112/172], Loss: 15.9824\n",
      "Epoch [268/300], Step [113/172], Loss: 13.6435\n",
      "Epoch [268/300], Step [114/172], Loss: 14.8889\n",
      "Epoch [268/300], Step [115/172], Loss: 19.7983\n",
      "Epoch [268/300], Step [116/172], Loss: 13.8967\n",
      "Epoch [268/300], Step [117/172], Loss: 12.2883\n",
      "Epoch [268/300], Step [118/172], Loss: 13.5400\n",
      "Epoch [268/300], Step [119/172], Loss: 17.9675\n",
      "Epoch [268/300], Step [120/172], Loss: 9.7726\n",
      "Epoch [268/300], Step [121/172], Loss: 8.6476\n",
      "Epoch [268/300], Step [122/172], Loss: 12.2299\n",
      "Epoch [268/300], Step [123/172], Loss: 11.1914\n",
      "Epoch [268/300], Step [124/172], Loss: 7.4543\n",
      "Epoch [268/300], Step [125/172], Loss: 12.1376\n",
      "Epoch [268/300], Step [126/172], Loss: 12.0517\n",
      "Epoch [268/300], Step [127/172], Loss: 11.0996\n",
      "Epoch [268/300], Step [128/172], Loss: 10.0010\n",
      "Epoch [268/300], Step [129/172], Loss: 8.7056\n",
      "Epoch [268/300], Step [130/172], Loss: 13.1472\n",
      "Epoch [268/300], Step [131/172], Loss: 7.5904\n",
      "Epoch [268/300], Step [132/172], Loss: 9.5136\n",
      "Epoch [268/300], Step [133/172], Loss: 9.6299\n",
      "Epoch [268/300], Step [134/172], Loss: 10.8231\n",
      "Epoch [268/300], Step [135/172], Loss: 9.1059\n",
      "Epoch [268/300], Step [136/172], Loss: 8.1701\n",
      "Epoch [268/300], Step [137/172], Loss: 8.5646\n",
      "Epoch [268/300], Step [138/172], Loss: 7.4876\n",
      "Epoch [268/300], Step [139/172], Loss: 11.5206\n",
      "Epoch [268/300], Step [140/172], Loss: 10.9404\n",
      "Epoch [268/300], Step [141/172], Loss: 8.6183\n",
      "Epoch [268/300], Step [142/172], Loss: 14.9378\n",
      "Epoch [268/300], Step [143/172], Loss: 11.9406\n",
      "Epoch [268/300], Step [144/172], Loss: 9.2072\n",
      "Epoch [268/300], Step [145/172], Loss: 10.7420\n",
      "Epoch [268/300], Step [146/172], Loss: 10.2922\n",
      "Epoch [268/300], Step [147/172], Loss: 5.6327\n",
      "Epoch [268/300], Step [148/172], Loss: 6.1292\n",
      "Epoch [268/300], Step [149/172], Loss: 6.0026\n",
      "Epoch [268/300], Step [150/172], Loss: 5.3034\n",
      "Epoch [268/300], Step [151/172], Loss: 5.2010\n",
      "Epoch [268/300], Step [152/172], Loss: 7.6046\n",
      "Epoch [268/300], Step [153/172], Loss: 6.1986\n",
      "Epoch [268/300], Step [154/172], Loss: 6.4601\n",
      "Epoch [268/300], Step [155/172], Loss: 6.3836\n",
      "Epoch [268/300], Step [156/172], Loss: 14.6225\n",
      "Epoch [268/300], Step [157/172], Loss: 9.6062\n",
      "Epoch [268/300], Step [158/172], Loss: 7.2964\n",
      "Epoch [268/300], Step [159/172], Loss: 9.7503\n",
      "Epoch [268/300], Step [160/172], Loss: 10.4769\n",
      "Epoch [268/300], Step [161/172], Loss: 5.7795\n",
      "Epoch [268/300], Step [162/172], Loss: 4.4317\n",
      "Epoch [268/300], Step [163/172], Loss: 7.1161\n",
      "Epoch [268/300], Step [164/172], Loss: 8.3409\n",
      "Epoch [268/300], Step [165/172], Loss: 6.7143\n",
      "Epoch [268/300], Step [166/172], Loss: 6.0791\n",
      "Epoch [268/300], Step [167/172], Loss: 11.1430\n",
      "Epoch [268/300], Step [168/172], Loss: 6.3852\n",
      "Epoch [268/300], Step [169/172], Loss: 6.4596\n",
      "Epoch [268/300], Step [170/172], Loss: 5.0772\n",
      "Epoch [268/300], Step [171/172], Loss: 8.8990\n",
      "Epoch [268/300], Step [172/172], Loss: 5.3125\n",
      "Epoch [269/300], Step [1/172], Loss: 39.6561\n",
      "Epoch [269/300], Step [2/172], Loss: 42.1499\n",
      "Epoch [269/300], Step [3/172], Loss: 40.9763\n",
      "Epoch [269/300], Step [4/172], Loss: 18.3292\n",
      "Epoch [269/300], Step [5/172], Loss: 35.4586\n",
      "Epoch [269/300], Step [6/172], Loss: 16.4817\n",
      "Epoch [269/300], Step [7/172], Loss: 23.4323\n",
      "Epoch [269/300], Step [8/172], Loss: 3.9434\n",
      "Epoch [269/300], Step [9/172], Loss: 24.5948\n",
      "Epoch [269/300], Step [10/172], Loss: 34.5939\n",
      "Epoch [269/300], Step [11/172], Loss: 50.0459\n",
      "Epoch [269/300], Step [12/172], Loss: 48.1281\n",
      "Epoch [269/300], Step [13/172], Loss: 29.6826\n",
      "Epoch [269/300], Step [14/172], Loss: 49.7000\n",
      "Epoch [269/300], Step [15/172], Loss: 47.9154\n",
      "Epoch [269/300], Step [16/172], Loss: 8.3055\n",
      "Epoch [269/300], Step [17/172], Loss: 34.2712\n",
      "Epoch [269/300], Step [18/172], Loss: 49.0065\n",
      "Epoch [269/300], Step [19/172], Loss: 67.7396\n",
      "Epoch [269/300], Step [20/172], Loss: 25.7183\n",
      "Epoch [269/300], Step [21/172], Loss: 70.5293\n",
      "Epoch [269/300], Step [22/172], Loss: 48.1626\n",
      "Epoch [269/300], Step [23/172], Loss: 1.6616\n",
      "Epoch [269/300], Step [24/172], Loss: 43.8128\n",
      "Epoch [269/300], Step [25/172], Loss: 31.2847\n",
      "Epoch [269/300], Step [26/172], Loss: 41.0766\n",
      "Epoch [269/300], Step [27/172], Loss: 53.4280\n",
      "Epoch [269/300], Step [28/172], Loss: 15.3701\n",
      "Epoch [269/300], Step [29/172], Loss: 13.9229\n",
      "Epoch [269/300], Step [30/172], Loss: 44.7334\n",
      "Epoch [269/300], Step [31/172], Loss: 26.6366\n",
      "Epoch [269/300], Step [32/172], Loss: 39.9241\n",
      "Epoch [269/300], Step [33/172], Loss: 60.1800\n",
      "Epoch [269/300], Step [34/172], Loss: 1.7017\n",
      "Epoch [269/300], Step [35/172], Loss: 14.0928\n",
      "Epoch [269/300], Step [36/172], Loss: 14.3574\n",
      "Epoch [269/300], Step [37/172], Loss: 13.9442\n",
      "Epoch [269/300], Step [38/172], Loss: 29.6314\n",
      "Epoch [269/300], Step [39/172], Loss: 32.2006\n",
      "Epoch [269/300], Step [40/172], Loss: 19.5858\n",
      "Epoch [269/300], Step [41/172], Loss: 28.6566\n",
      "Epoch [269/300], Step [42/172], Loss: 35.0678\n",
      "Epoch [269/300], Step [43/172], Loss: 24.9316\n",
      "Epoch [269/300], Step [44/172], Loss: 20.9166\n",
      "Epoch [269/300], Step [45/172], Loss: 27.8036\n",
      "Epoch [269/300], Step [46/172], Loss: 15.4485\n",
      "Epoch [269/300], Step [47/172], Loss: 47.1287\n",
      "Epoch [269/300], Step [48/172], Loss: 63.5102\n",
      "Epoch [269/300], Step [49/172], Loss: 22.9750\n",
      "Epoch [269/300], Step [50/172], Loss: 50.8886\n",
      "Epoch [269/300], Step [51/172], Loss: 7.9901\n",
      "Epoch [269/300], Step [52/172], Loss: 21.9079\n",
      "Epoch [269/300], Step [53/172], Loss: 23.6613\n",
      "Epoch [269/300], Step [54/172], Loss: 15.2673\n",
      "Epoch [269/300], Step [55/172], Loss: 15.5874\n",
      "Epoch [269/300], Step [56/172], Loss: 16.9108\n",
      "Epoch [269/300], Step [57/172], Loss: 18.0133\n",
      "Epoch [269/300], Step [58/172], Loss: 14.3056\n",
      "Epoch [269/300], Step [59/172], Loss: 28.4140\n",
      "Epoch [269/300], Step [60/172], Loss: 23.5360\n",
      "Epoch [269/300], Step [61/172], Loss: 6.3844\n",
      "Epoch [269/300], Step [62/172], Loss: 15.7586\n",
      "Epoch [269/300], Step [63/172], Loss: 10.3162\n",
      "Epoch [269/300], Step [64/172], Loss: 12.9026\n",
      "Epoch [269/300], Step [65/172], Loss: 20.2954\n",
      "Epoch [269/300], Step [66/172], Loss: 8.4288\n",
      "Epoch [269/300], Step [67/172], Loss: 26.8748\n",
      "Epoch [269/300], Step [68/172], Loss: 5.8724\n",
      "Epoch [269/300], Step [69/172], Loss: 29.4710\n",
      "Epoch [269/300], Step [70/172], Loss: 29.4545\n",
      "Epoch [269/300], Step [71/172], Loss: 32.1820\n",
      "Epoch [269/300], Step [72/172], Loss: 30.2593\n",
      "Epoch [269/300], Step [73/172], Loss: 38.2973\n",
      "Epoch [269/300], Step [74/172], Loss: 20.7610\n",
      "Epoch [269/300], Step [75/172], Loss: 20.9743\n",
      "Epoch [269/300], Step [76/172], Loss: 23.6364\n",
      "Epoch [269/300], Step [77/172], Loss: 42.5067\n",
      "Epoch [269/300], Step [78/172], Loss: 29.9210\n",
      "Epoch [269/300], Step [79/172], Loss: 27.4192\n",
      "Epoch [269/300], Step [80/172], Loss: 45.2501\n",
      "Epoch [269/300], Step [81/172], Loss: 25.1978\n",
      "Epoch [269/300], Step [82/172], Loss: 35.0434\n",
      "Epoch [269/300], Step [83/172], Loss: 38.0454\n",
      "Epoch [269/300], Step [84/172], Loss: 28.6331\n",
      "Epoch [269/300], Step [85/172], Loss: 32.6342\n",
      "Epoch [269/300], Step [86/172], Loss: 28.8187\n",
      "Epoch [269/300], Step [87/172], Loss: 21.0653\n",
      "Epoch [269/300], Step [88/172], Loss: 19.6637\n",
      "Epoch [269/300], Step [89/172], Loss: 25.0408\n",
      "Epoch [269/300], Step [90/172], Loss: 17.9730\n",
      "Epoch [269/300], Step [91/172], Loss: 24.3935\n",
      "Epoch [269/300], Step [92/172], Loss: 17.8320\n",
      "Epoch [269/300], Step [93/172], Loss: 17.3721\n",
      "Epoch [269/300], Step [94/172], Loss: 23.7453\n",
      "Epoch [269/300], Step [95/172], Loss: 18.1000\n",
      "Epoch [269/300], Step [96/172], Loss: 19.5304\n",
      "Epoch [269/300], Step [97/172], Loss: 28.6871\n",
      "Epoch [269/300], Step [98/172], Loss: 16.9831\n",
      "Epoch [269/300], Step [99/172], Loss: 17.7430\n",
      "Epoch [269/300], Step [100/172], Loss: 16.1135\n",
      "Epoch [269/300], Step [101/172], Loss: 18.0867\n",
      "Epoch [269/300], Step [102/172], Loss: 17.8176\n",
      "Epoch [269/300], Step [103/172], Loss: 10.8716\n",
      "Epoch [269/300], Step [104/172], Loss: 18.5356\n",
      "Epoch [269/300], Step [105/172], Loss: 21.6240\n",
      "Epoch [269/300], Step [106/172], Loss: 14.7506\n",
      "Epoch [269/300], Step [107/172], Loss: 15.5109\n",
      "Epoch [269/300], Step [108/172], Loss: 14.9089\n",
      "Epoch [269/300], Step [109/172], Loss: 14.5123\n",
      "Epoch [269/300], Step [110/172], Loss: 16.5328\n",
      "Epoch [269/300], Step [111/172], Loss: 17.0267\n",
      "Epoch [269/300], Step [112/172], Loss: 15.7823\n",
      "Epoch [269/300], Step [113/172], Loss: 13.6359\n",
      "Epoch [269/300], Step [114/172], Loss: 14.9769\n",
      "Epoch [269/300], Step [115/172], Loss: 19.8749\n",
      "Epoch [269/300], Step [116/172], Loss: 13.8753\n",
      "Epoch [269/300], Step [117/172], Loss: 12.3174\n",
      "Epoch [269/300], Step [118/172], Loss: 13.5041\n",
      "Epoch [269/300], Step [119/172], Loss: 17.9877\n",
      "Epoch [269/300], Step [120/172], Loss: 9.7967\n",
      "Epoch [269/300], Step [121/172], Loss: 8.6705\n",
      "Epoch [269/300], Step [122/172], Loss: 11.9675\n",
      "Epoch [269/300], Step [123/172], Loss: 11.3147\n",
      "Epoch [269/300], Step [124/172], Loss: 7.4727\n",
      "Epoch [269/300], Step [125/172], Loss: 12.1670\n",
      "Epoch [269/300], Step [126/172], Loss: 12.0696\n",
      "Epoch [269/300], Step [127/172], Loss: 10.9590\n",
      "Epoch [269/300], Step [128/172], Loss: 9.9760\n",
      "Epoch [269/300], Step [129/172], Loss: 8.7712\n",
      "Epoch [269/300], Step [130/172], Loss: 13.0803\n",
      "Epoch [269/300], Step [131/172], Loss: 7.5546\n",
      "Epoch [269/300], Step [132/172], Loss: 9.5217\n",
      "Epoch [269/300], Step [133/172], Loss: 9.4881\n",
      "Epoch [269/300], Step [134/172], Loss: 10.6486\n",
      "Epoch [269/300], Step [135/172], Loss: 9.1806\n",
      "Epoch [269/300], Step [136/172], Loss: 8.1864\n",
      "Epoch [269/300], Step [137/172], Loss: 8.5308\n",
      "Epoch [269/300], Step [138/172], Loss: 7.5568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [269/300], Step [139/172], Loss: 11.5372\n",
      "Epoch [269/300], Step [140/172], Loss: 11.0288\n",
      "Epoch [269/300], Step [141/172], Loss: 8.7230\n",
      "Epoch [269/300], Step [142/172], Loss: 14.8369\n",
      "Epoch [269/300], Step [143/172], Loss: 11.9498\n",
      "Epoch [269/300], Step [144/172], Loss: 9.2669\n",
      "Epoch [269/300], Step [145/172], Loss: 10.7314\n",
      "Epoch [269/300], Step [146/172], Loss: 10.3213\n",
      "Epoch [269/300], Step [147/172], Loss: 5.6342\n",
      "Epoch [269/300], Step [148/172], Loss: 6.1208\n",
      "Epoch [269/300], Step [149/172], Loss: 6.0001\n",
      "Epoch [269/300], Step [150/172], Loss: 5.3250\n",
      "Epoch [269/300], Step [151/172], Loss: 5.2096\n",
      "Epoch [269/300], Step [152/172], Loss: 7.5760\n",
      "Epoch [269/300], Step [153/172], Loss: 6.2344\n",
      "Epoch [269/300], Step [154/172], Loss: 6.4499\n",
      "Epoch [269/300], Step [155/172], Loss: 6.4753\n",
      "Epoch [269/300], Step [156/172], Loss: 14.5542\n",
      "Epoch [269/300], Step [157/172], Loss: 9.4772\n",
      "Epoch [269/300], Step [158/172], Loss: 7.2273\n",
      "Epoch [269/300], Step [159/172], Loss: 9.8196\n",
      "Epoch [269/300], Step [160/172], Loss: 10.2950\n",
      "Epoch [269/300], Step [161/172], Loss: 5.7244\n",
      "Epoch [269/300], Step [162/172], Loss: 4.4284\n",
      "Epoch [269/300], Step [163/172], Loss: 7.1955\n",
      "Epoch [269/300], Step [164/172], Loss: 8.3388\n",
      "Epoch [269/300], Step [165/172], Loss: 6.7336\n",
      "Epoch [269/300], Step [166/172], Loss: 6.0777\n",
      "Epoch [269/300], Step [167/172], Loss: 11.1532\n",
      "Epoch [269/300], Step [168/172], Loss: 6.3064\n",
      "Epoch [269/300], Step [169/172], Loss: 6.5657\n",
      "Epoch [269/300], Step [170/172], Loss: 5.0853\n",
      "Epoch [269/300], Step [171/172], Loss: 8.9777\n",
      "Epoch [269/300], Step [172/172], Loss: 5.3605\n",
      "Epoch [270/300], Step [1/172], Loss: 39.4715\n",
      "Epoch [270/300], Step [2/172], Loss: 42.2361\n",
      "Epoch [270/300], Step [3/172], Loss: 39.7581\n",
      "Epoch [270/300], Step [4/172], Loss: 18.3180\n",
      "Epoch [270/300], Step [5/172], Loss: 35.0288\n",
      "Epoch [270/300], Step [6/172], Loss: 16.8369\n",
      "Epoch [270/300], Step [7/172], Loss: 24.4512\n",
      "Epoch [270/300], Step [8/172], Loss: 3.7496\n",
      "Epoch [270/300], Step [9/172], Loss: 24.5343\n",
      "Epoch [270/300], Step [10/172], Loss: 34.8061\n",
      "Epoch [270/300], Step [11/172], Loss: 50.1104\n",
      "Epoch [270/300], Step [12/172], Loss: 48.1254\n",
      "Epoch [270/300], Step [13/172], Loss: 30.2679\n",
      "Epoch [270/300], Step [14/172], Loss: 49.7738\n",
      "Epoch [270/300], Step [15/172], Loss: 47.9887\n",
      "Epoch [270/300], Step [16/172], Loss: 8.5650\n",
      "Epoch [270/300], Step [17/172], Loss: 34.0092\n",
      "Epoch [270/300], Step [18/172], Loss: 49.0815\n",
      "Epoch [270/300], Step [19/172], Loss: 67.6074\n",
      "Epoch [270/300], Step [20/172], Loss: 25.8903\n",
      "Epoch [270/300], Step [21/172], Loss: 70.0904\n",
      "Epoch [270/300], Step [22/172], Loss: 48.2610\n",
      "Epoch [270/300], Step [23/172], Loss: 1.7159\n",
      "Epoch [270/300], Step [24/172], Loss: 43.9069\n",
      "Epoch [270/300], Step [25/172], Loss: 31.1178\n",
      "Epoch [270/300], Step [26/172], Loss: 41.1353\n",
      "Epoch [270/300], Step [27/172], Loss: 53.6803\n",
      "Epoch [270/300], Step [28/172], Loss: 15.1243\n",
      "Epoch [270/300], Step [29/172], Loss: 14.0040\n",
      "Epoch [270/300], Step [30/172], Loss: 44.1154\n",
      "Epoch [270/300], Step [31/172], Loss: 26.5732\n",
      "Epoch [270/300], Step [32/172], Loss: 40.0027\n",
      "Epoch [270/300], Step [33/172], Loss: 59.7535\n",
      "Epoch [270/300], Step [34/172], Loss: 1.6291\n",
      "Epoch [270/300], Step [35/172], Loss: 14.1771\n",
      "Epoch [270/300], Step [36/172], Loss: 14.4008\n",
      "Epoch [270/300], Step [37/172], Loss: 13.9243\n",
      "Epoch [270/300], Step [38/172], Loss: 29.4864\n",
      "Epoch [270/300], Step [39/172], Loss: 32.1614\n",
      "Epoch [270/300], Step [40/172], Loss: 19.6002\n",
      "Epoch [270/300], Step [41/172], Loss: 28.7807\n",
      "Epoch [270/300], Step [42/172], Loss: 35.1582\n",
      "Epoch [270/300], Step [43/172], Loss: 24.9132\n",
      "Epoch [270/300], Step [44/172], Loss: 20.8840\n",
      "Epoch [270/300], Step [45/172], Loss: 27.5707\n",
      "Epoch [270/300], Step [46/172], Loss: 15.3488\n",
      "Epoch [270/300], Step [47/172], Loss: 46.7936\n",
      "Epoch [270/300], Step [48/172], Loss: 63.8493\n",
      "Epoch [270/300], Step [49/172], Loss: 23.0340\n",
      "Epoch [270/300], Step [50/172], Loss: 50.6004\n",
      "Epoch [270/300], Step [51/172], Loss: 7.8458\n",
      "Epoch [270/300], Step [52/172], Loss: 21.6772\n",
      "Epoch [270/300], Step [53/172], Loss: 23.3962\n",
      "Epoch [270/300], Step [54/172], Loss: 15.1442\n",
      "Epoch [270/300], Step [55/172], Loss: 15.3833\n",
      "Epoch [270/300], Step [56/172], Loss: 16.8578\n",
      "Epoch [270/300], Step [57/172], Loss: 18.5349\n",
      "Epoch [270/300], Step [58/172], Loss: 14.0880\n",
      "Epoch [270/300], Step [59/172], Loss: 28.7592\n",
      "Epoch [270/300], Step [60/172], Loss: 23.7064\n",
      "Epoch [270/300], Step [61/172], Loss: 6.2247\n",
      "Epoch [270/300], Step [62/172], Loss: 15.7519\n",
      "Epoch [270/300], Step [63/172], Loss: 9.9383\n",
      "Epoch [270/300], Step [64/172], Loss: 12.4421\n",
      "Epoch [270/300], Step [65/172], Loss: 20.0247\n",
      "Epoch [270/300], Step [66/172], Loss: 8.1933\n",
      "Epoch [270/300], Step [67/172], Loss: 26.8817\n",
      "Epoch [270/300], Step [68/172], Loss: 5.4305\n",
      "Epoch [270/300], Step [69/172], Loss: 29.3301\n",
      "Epoch [270/300], Step [70/172], Loss: 29.4058\n",
      "Epoch [270/300], Step [71/172], Loss: 32.0330\n",
      "Epoch [270/300], Step [72/172], Loss: 30.1283\n",
      "Epoch [270/300], Step [73/172], Loss: 37.9437\n",
      "Epoch [270/300], Step [74/172], Loss: 20.7476\n",
      "Epoch [270/300], Step [75/172], Loss: 20.9081\n",
      "Epoch [270/300], Step [76/172], Loss: 23.4540\n",
      "Epoch [270/300], Step [77/172], Loss: 42.2048\n",
      "Epoch [270/300], Step [78/172], Loss: 29.5338\n",
      "Epoch [270/300], Step [79/172], Loss: 26.9855\n",
      "Epoch [270/300], Step [80/172], Loss: 44.5929\n",
      "Epoch [270/300], Step [81/172], Loss: 24.8472\n",
      "Epoch [270/300], Step [82/172], Loss: 34.4654\n",
      "Epoch [270/300], Step [83/172], Loss: 37.8553\n",
      "Epoch [270/300], Step [84/172], Loss: 28.4360\n",
      "Epoch [270/300], Step [85/172], Loss: 32.1012\n",
      "Epoch [270/300], Step [86/172], Loss: 28.2358\n",
      "Epoch [270/300], Step [87/172], Loss: 20.9265\n",
      "Epoch [270/300], Step [88/172], Loss: 19.4490\n",
      "Epoch [270/300], Step [89/172], Loss: 24.5928\n",
      "Epoch [270/300], Step [90/172], Loss: 17.7160\n",
      "Epoch [270/300], Step [91/172], Loss: 24.1736\n",
      "Epoch [270/300], Step [92/172], Loss: 17.6802\n",
      "Epoch [270/300], Step [93/172], Loss: 17.3160\n",
      "Epoch [270/300], Step [94/172], Loss: 23.5738\n",
      "Epoch [270/300], Step [95/172], Loss: 17.9461\n",
      "Epoch [270/300], Step [96/172], Loss: 19.3910\n",
      "Epoch [270/300], Step [97/172], Loss: 28.4956\n",
      "Epoch [270/300], Step [98/172], Loss: 16.6826\n",
      "Epoch [270/300], Step [99/172], Loss: 17.4190\n",
      "Epoch [270/300], Step [100/172], Loss: 15.6397\n",
      "Epoch [270/300], Step [101/172], Loss: 17.9223\n",
      "Epoch [270/300], Step [102/172], Loss: 17.5406\n",
      "Epoch [270/300], Step [103/172], Loss: 10.6210\n",
      "Epoch [270/300], Step [104/172], Loss: 18.4380\n",
      "Epoch [270/300], Step [105/172], Loss: 21.2641\n",
      "Epoch [270/300], Step [106/172], Loss: 14.5758\n",
      "Epoch [270/300], Step [107/172], Loss: 15.3655\n",
      "Epoch [270/300], Step [108/172], Loss: 14.7326\n",
      "Epoch [270/300], Step [109/172], Loss: 14.3542\n",
      "Epoch [270/300], Step [110/172], Loss: 16.2821\n",
      "Epoch [270/300], Step [111/172], Loss: 16.8025\n",
      "Epoch [270/300], Step [112/172], Loss: 15.7200\n",
      "Epoch [270/300], Step [113/172], Loss: 13.5036\n",
      "Epoch [270/300], Step [114/172], Loss: 14.8688\n",
      "Epoch [270/300], Step [115/172], Loss: 19.7404\n",
      "Epoch [270/300], Step [116/172], Loss: 13.7371\n",
      "Epoch [270/300], Step [117/172], Loss: 12.1431\n",
      "Epoch [270/300], Step [118/172], Loss: 13.4692\n",
      "Epoch [270/300], Step [119/172], Loss: 17.8625\n",
      "Epoch [270/300], Step [120/172], Loss: 9.6379\n",
      "Epoch [270/300], Step [121/172], Loss: 8.6311\n",
      "Epoch [270/300], Step [122/172], Loss: 11.9111\n",
      "Epoch [270/300], Step [123/172], Loss: 11.0280\n",
      "Epoch [270/300], Step [124/172], Loss: 7.4603\n",
      "Epoch [270/300], Step [125/172], Loss: 12.0587\n",
      "Epoch [270/300], Step [126/172], Loss: 11.9479\n",
      "Epoch [270/300], Step [127/172], Loss: 10.9441\n",
      "Epoch [270/300], Step [128/172], Loss: 9.9057\n",
      "Epoch [270/300], Step [129/172], Loss: 8.7668\n",
      "Epoch [270/300], Step [130/172], Loss: 13.0304\n",
      "Epoch [270/300], Step [131/172], Loss: 7.4821\n",
      "Epoch [270/300], Step [132/172], Loss: 9.4138\n",
      "Epoch [270/300], Step [133/172], Loss: 9.4304\n",
      "Epoch [270/300], Step [134/172], Loss: 10.6981\n",
      "Epoch [270/300], Step [135/172], Loss: 9.0805\n",
      "Epoch [270/300], Step [136/172], Loss: 8.0489\n",
      "Epoch [270/300], Step [137/172], Loss: 8.3909\n",
      "Epoch [270/300], Step [138/172], Loss: 7.4550\n",
      "Epoch [270/300], Step [139/172], Loss: 11.4517\n",
      "Epoch [270/300], Step [140/172], Loss: 10.9060\n",
      "Epoch [270/300], Step [141/172], Loss: 8.5611\n",
      "Epoch [270/300], Step [142/172], Loss: 14.6792\n",
      "Epoch [270/300], Step [143/172], Loss: 11.8559\n",
      "Epoch [270/300], Step [144/172], Loss: 9.2060\n",
      "Epoch [270/300], Step [145/172], Loss: 10.5621\n",
      "Epoch [270/300], Step [146/172], Loss: 10.1906\n",
      "Epoch [270/300], Step [147/172], Loss: 5.6286\n",
      "Epoch [270/300], Step [148/172], Loss: 6.0627\n",
      "Epoch [270/300], Step [149/172], Loss: 5.9270\n",
      "Epoch [270/300], Step [150/172], Loss: 5.2276\n",
      "Epoch [270/300], Step [151/172], Loss: 5.1281\n",
      "Epoch [270/300], Step [152/172], Loss: 7.4680\n",
      "Epoch [270/300], Step [153/172], Loss: 6.2178\n",
      "Epoch [270/300], Step [154/172], Loss: 6.3496\n",
      "Epoch [270/300], Step [155/172], Loss: 6.4423\n",
      "Epoch [270/300], Step [156/172], Loss: 14.4644\n",
      "Epoch [270/300], Step [157/172], Loss: 9.4811\n",
      "Epoch [270/300], Step [158/172], Loss: 7.2003\n",
      "Epoch [270/300], Step [159/172], Loss: 9.6956\n",
      "Epoch [270/300], Step [160/172], Loss: 10.3136\n",
      "Epoch [270/300], Step [161/172], Loss: 5.6185\n",
      "Epoch [270/300], Step [162/172], Loss: 4.3415\n",
      "Epoch [270/300], Step [163/172], Loss: 7.0723\n",
      "Epoch [270/300], Step [164/172], Loss: 8.2155\n",
      "Epoch [270/300], Step [165/172], Loss: 6.6934\n",
      "Epoch [270/300], Step [166/172], Loss: 5.9781\n",
      "Epoch [270/300], Step [167/172], Loss: 11.0567\n",
      "Epoch [270/300], Step [168/172], Loss: 6.2228\n",
      "Epoch [270/300], Step [169/172], Loss: 6.3643\n",
      "Epoch [270/300], Step [170/172], Loss: 5.0016\n",
      "Epoch [270/300], Step [171/172], Loss: 8.8488\n",
      "Epoch [270/300], Step [172/172], Loss: 5.3835\n",
      "Epoch [271/300], Step [1/172], Loss: 39.1725\n",
      "Epoch [271/300], Step [2/172], Loss: 41.8065\n",
      "Epoch [271/300], Step [3/172], Loss: 40.9802\n",
      "Epoch [271/300], Step [4/172], Loss: 18.1122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [271/300], Step [5/172], Loss: 35.3045\n",
      "Epoch [271/300], Step [6/172], Loss: 16.8229\n",
      "Epoch [271/300], Step [7/172], Loss: 24.8343\n",
      "Epoch [271/300], Step [8/172], Loss: 4.0294\n",
      "Epoch [271/300], Step [9/172], Loss: 24.6014\n",
      "Epoch [271/300], Step [10/172], Loss: 34.5766\n",
      "Epoch [271/300], Step [11/172], Loss: 49.8592\n",
      "Epoch [271/300], Step [12/172], Loss: 48.2410\n",
      "Epoch [271/300], Step [13/172], Loss: 29.7018\n",
      "Epoch [271/300], Step [14/172], Loss: 50.1481\n",
      "Epoch [271/300], Step [15/172], Loss: 48.1281\n",
      "Epoch [271/300], Step [16/172], Loss: 8.2102\n",
      "Epoch [271/300], Step [17/172], Loss: 34.4689\n",
      "Epoch [271/300], Step [18/172], Loss: 49.3144\n",
      "Epoch [271/300], Step [19/172], Loss: 67.5518\n",
      "Epoch [271/300], Step [20/172], Loss: 25.4626\n",
      "Epoch [271/300], Step [21/172], Loss: 70.6113\n",
      "Epoch [271/300], Step [22/172], Loss: 48.0853\n",
      "Epoch [271/300], Step [23/172], Loss: 1.6625\n",
      "Epoch [271/300], Step [24/172], Loss: 43.1835\n",
      "Epoch [271/300], Step [25/172], Loss: 30.6203\n",
      "Epoch [271/300], Step [26/172], Loss: 40.8944\n",
      "Epoch [271/300], Step [27/172], Loss: 54.1022\n",
      "Epoch [271/300], Step [28/172], Loss: 15.0469\n",
      "Epoch [271/300], Step [29/172], Loss: 13.8250\n",
      "Epoch [271/300], Step [30/172], Loss: 44.0432\n",
      "Epoch [271/300], Step [31/172], Loss: 25.9934\n",
      "Epoch [271/300], Step [32/172], Loss: 39.3693\n",
      "Epoch [271/300], Step [33/172], Loss: 58.9507\n",
      "Epoch [271/300], Step [34/172], Loss: 1.7585\n",
      "Epoch [271/300], Step [35/172], Loss: 14.6092\n",
      "Epoch [271/300], Step [36/172], Loss: 14.4804\n",
      "Epoch [271/300], Step [37/172], Loss: 13.8691\n",
      "Epoch [271/300], Step [38/172], Loss: 29.4934\n",
      "Epoch [271/300], Step [39/172], Loss: 31.9984\n",
      "Epoch [271/300], Step [40/172], Loss: 19.3810\n",
      "Epoch [271/300], Step [41/172], Loss: 28.5592\n",
      "Epoch [271/300], Step [42/172], Loss: 34.9358\n",
      "Epoch [271/300], Step [43/172], Loss: 24.9160\n",
      "Epoch [271/300], Step [44/172], Loss: 20.6492\n",
      "Epoch [271/300], Step [45/172], Loss: 27.5453\n",
      "Epoch [271/300], Step [46/172], Loss: 15.3700\n",
      "Epoch [271/300], Step [47/172], Loss: 46.5402\n",
      "Epoch [271/300], Step [48/172], Loss: 63.6338\n",
      "Epoch [271/300], Step [49/172], Loss: 22.5725\n",
      "Epoch [271/300], Step [50/172], Loss: 50.6370\n",
      "Epoch [271/300], Step [51/172], Loss: 7.8011\n",
      "Epoch [271/300], Step [52/172], Loss: 21.2193\n",
      "Epoch [271/300], Step [53/172], Loss: 23.3166\n",
      "Epoch [271/300], Step [54/172], Loss: 15.0926\n",
      "Epoch [271/300], Step [55/172], Loss: 15.1707\n",
      "Epoch [271/300], Step [56/172], Loss: 17.1478\n",
      "Epoch [271/300], Step [57/172], Loss: 18.3042\n",
      "Epoch [271/300], Step [58/172], Loss: 14.2172\n",
      "Epoch [271/300], Step [59/172], Loss: 28.1104\n",
      "Epoch [271/300], Step [60/172], Loss: 23.7402\n",
      "Epoch [271/300], Step [61/172], Loss: 6.1909\n",
      "Epoch [271/300], Step [62/172], Loss: 15.6499\n",
      "Epoch [271/300], Step [63/172], Loss: 9.9136\n",
      "Epoch [271/300], Step [64/172], Loss: 12.3463\n",
      "Epoch [271/300], Step [65/172], Loss: 19.9928\n",
      "Epoch [271/300], Step [66/172], Loss: 8.1319\n",
      "Epoch [271/300], Step [67/172], Loss: 26.8971\n",
      "Epoch [271/300], Step [68/172], Loss: 5.5126\n",
      "Epoch [271/300], Step [69/172], Loss: 29.5708\n",
      "Epoch [271/300], Step [70/172], Loss: 29.1282\n",
      "Epoch [271/300], Step [71/172], Loss: 32.0095\n",
      "Epoch [271/300], Step [72/172], Loss: 30.2317\n",
      "Epoch [271/300], Step [73/172], Loss: 38.1426\n",
      "Epoch [271/300], Step [74/172], Loss: 20.7617\n",
      "Epoch [271/300], Step [75/172], Loss: 20.6525\n",
      "Epoch [271/300], Step [76/172], Loss: 23.0297\n",
      "Epoch [271/300], Step [77/172], Loss: 41.9247\n",
      "Epoch [271/300], Step [78/172], Loss: 29.6049\n",
      "Epoch [271/300], Step [79/172], Loss: 27.1863\n",
      "Epoch [271/300], Step [80/172], Loss: 44.7962\n",
      "Epoch [271/300], Step [81/172], Loss: 24.9213\n",
      "Epoch [271/300], Step [82/172], Loss: 34.6448\n",
      "Epoch [271/300], Step [83/172], Loss: 37.8457\n",
      "Epoch [271/300], Step [84/172], Loss: 28.3098\n",
      "Epoch [271/300], Step [85/172], Loss: 32.0875\n",
      "Epoch [271/300], Step [86/172], Loss: 28.3343\n",
      "Epoch [271/300], Step [87/172], Loss: 20.8637\n",
      "Epoch [271/300], Step [88/172], Loss: 19.4660\n",
      "Epoch [271/300], Step [89/172], Loss: 24.6597\n",
      "Epoch [271/300], Step [90/172], Loss: 17.7163\n",
      "Epoch [271/300], Step [91/172], Loss: 24.1193\n",
      "Epoch [271/300], Step [92/172], Loss: 17.8284\n",
      "Epoch [271/300], Step [93/172], Loss: 17.3753\n",
      "Epoch [271/300], Step [94/172], Loss: 23.4846\n",
      "Epoch [271/300], Step [95/172], Loss: 17.8787\n",
      "Epoch [271/300], Step [96/172], Loss: 19.3935\n",
      "Epoch [271/300], Step [97/172], Loss: 28.3665\n",
      "Epoch [271/300], Step [98/172], Loss: 16.6982\n",
      "Epoch [271/300], Step [99/172], Loss: 17.4431\n",
      "Epoch [271/300], Step [100/172], Loss: 15.6000\n",
      "Epoch [271/300], Step [101/172], Loss: 17.8498\n",
      "Epoch [271/300], Step [102/172], Loss: 17.5242\n",
      "Epoch [271/300], Step [103/172], Loss: 10.5886\n",
      "Epoch [271/300], Step [104/172], Loss: 18.2954\n",
      "Epoch [271/300], Step [105/172], Loss: 21.1649\n",
      "Epoch [271/300], Step [106/172], Loss: 14.6071\n",
      "Epoch [271/300], Step [107/172], Loss: 15.2912\n",
      "Epoch [271/300], Step [108/172], Loss: 14.5981\n",
      "Epoch [271/300], Step [109/172], Loss: 14.2984\n",
      "Epoch [271/300], Step [110/172], Loss: 16.1512\n",
      "Epoch [271/300], Step [111/172], Loss: 16.8466\n",
      "Epoch [271/300], Step [112/172], Loss: 15.5173\n",
      "Epoch [271/300], Step [113/172], Loss: 13.3972\n",
      "Epoch [271/300], Step [114/172], Loss: 14.8428\n",
      "Epoch [271/300], Step [115/172], Loss: 19.7476\n",
      "Epoch [271/300], Step [116/172], Loss: 13.6921\n",
      "Epoch [271/300], Step [117/172], Loss: 12.2301\n",
      "Epoch [271/300], Step [118/172], Loss: 13.3412\n",
      "Epoch [271/300], Step [119/172], Loss: 17.8011\n",
      "Epoch [271/300], Step [120/172], Loss: 9.6772\n",
      "Epoch [271/300], Step [121/172], Loss: 8.5431\n",
      "Epoch [271/300], Step [122/172], Loss: 11.6693\n",
      "Epoch [271/300], Step [123/172], Loss: 11.1716\n",
      "Epoch [271/300], Step [124/172], Loss: 7.3960\n",
      "Epoch [271/300], Step [125/172], Loss: 12.0217\n",
      "Epoch [271/300], Step [126/172], Loss: 11.9874\n",
      "Epoch [271/300], Step [127/172], Loss: 10.8601\n",
      "Epoch [271/300], Step [128/172], Loss: 9.8687\n",
      "Epoch [271/300], Step [129/172], Loss: 8.7350\n",
      "Epoch [271/300], Step [130/172], Loss: 12.9918\n",
      "Epoch [271/300], Step [131/172], Loss: 7.3942\n",
      "Epoch [271/300], Step [132/172], Loss: 9.3612\n",
      "Epoch [271/300], Step [133/172], Loss: 9.2872\n",
      "Epoch [271/300], Step [134/172], Loss: 10.5321\n",
      "Epoch [271/300], Step [135/172], Loss: 9.1271\n",
      "Epoch [271/300], Step [136/172], Loss: 8.1363\n",
      "Epoch [271/300], Step [137/172], Loss: 8.3361\n",
      "Epoch [271/300], Step [138/172], Loss: 7.3877\n",
      "Epoch [271/300], Step [139/172], Loss: 11.4960\n",
      "Epoch [271/300], Step [140/172], Loss: 10.9575\n",
      "Epoch [271/300], Step [141/172], Loss: 8.6840\n",
      "Epoch [271/300], Step [142/172], Loss: 14.6270\n",
      "Epoch [271/300], Step [143/172], Loss: 11.8515\n",
      "Epoch [271/300], Step [144/172], Loss: 9.1778\n",
      "Epoch [271/300], Step [145/172], Loss: 10.5179\n",
      "Epoch [271/300], Step [146/172], Loss: 10.2565\n",
      "Epoch [271/300], Step [147/172], Loss: 5.5926\n",
      "Epoch [271/300], Step [148/172], Loss: 6.0474\n",
      "Epoch [271/300], Step [149/172], Loss: 5.9171\n",
      "Epoch [271/300], Step [150/172], Loss: 5.2282\n",
      "Epoch [271/300], Step [151/172], Loss: 5.1655\n",
      "Epoch [271/300], Step [152/172], Loss: 7.4847\n",
      "Epoch [271/300], Step [153/172], Loss: 6.1532\n",
      "Epoch [271/300], Step [154/172], Loss: 6.3970\n",
      "Epoch [271/300], Step [155/172], Loss: 6.3942\n",
      "Epoch [271/300], Step [156/172], Loss: 14.2753\n",
      "Epoch [271/300], Step [157/172], Loss: 9.3169\n",
      "Epoch [271/300], Step [158/172], Loss: 7.1120\n",
      "Epoch [271/300], Step [159/172], Loss: 9.7787\n",
      "Epoch [271/300], Step [160/172], Loss: 10.0566\n",
      "Epoch [271/300], Step [161/172], Loss: 5.6528\n",
      "Epoch [271/300], Step [162/172], Loss: 4.3355\n",
      "Epoch [271/300], Step [163/172], Loss: 7.1374\n",
      "Epoch [271/300], Step [164/172], Loss: 8.2425\n",
      "Epoch [271/300], Step [165/172], Loss: 6.7291\n",
      "Epoch [271/300], Step [166/172], Loss: 5.9558\n",
      "Epoch [271/300], Step [167/172], Loss: 11.0366\n",
      "Epoch [271/300], Step [168/172], Loss: 6.1619\n",
      "Epoch [271/300], Step [169/172], Loss: 6.4458\n",
      "Epoch [271/300], Step [170/172], Loss: 5.0485\n",
      "Epoch [271/300], Step [171/172], Loss: 8.9311\n",
      "Epoch [271/300], Step [172/172], Loss: 5.2766\n",
      "Epoch [272/300], Step [1/172], Loss: 38.7191\n",
      "Epoch [272/300], Step [2/172], Loss: 42.5977\n",
      "Epoch [272/300], Step [3/172], Loss: 39.3558\n",
      "Epoch [272/300], Step [4/172], Loss: 17.9920\n",
      "Epoch [272/300], Step [5/172], Loss: 34.8059\n",
      "Epoch [272/300], Step [6/172], Loss: 17.6274\n",
      "Epoch [272/300], Step [7/172], Loss: 27.4391\n",
      "Epoch [272/300], Step [8/172], Loss: 3.9127\n",
      "Epoch [272/300], Step [9/172], Loss: 24.4153\n",
      "Epoch [272/300], Step [10/172], Loss: 34.6371\n",
      "Epoch [272/300], Step [11/172], Loss: 50.0131\n",
      "Epoch [272/300], Step [12/172], Loss: 48.1928\n",
      "Epoch [272/300], Step [13/172], Loss: 30.0252\n",
      "Epoch [272/300], Step [14/172], Loss: 50.2256\n",
      "Epoch [272/300], Step [15/172], Loss: 48.2377\n",
      "Epoch [272/300], Step [16/172], Loss: 7.1877\n",
      "Epoch [272/300], Step [17/172], Loss: 34.8125\n",
      "Epoch [272/300], Step [18/172], Loss: 49.5093\n",
      "Epoch [272/300], Step [19/172], Loss: 68.0074\n",
      "Epoch [272/300], Step [20/172], Loss: 25.3077\n",
      "Epoch [272/300], Step [21/172], Loss: 70.2669\n",
      "Epoch [272/300], Step [22/172], Loss: 47.8638\n",
      "Epoch [272/300], Step [23/172], Loss: 1.4565\n",
      "Epoch [272/300], Step [24/172], Loss: 43.2452\n",
      "Epoch [272/300], Step [25/172], Loss: 29.6270\n",
      "Epoch [272/300], Step [26/172], Loss: 40.6232\n",
      "Epoch [272/300], Step [27/172], Loss: 53.7665\n",
      "Epoch [272/300], Step [28/172], Loss: 14.8310\n",
      "Epoch [272/300], Step [29/172], Loss: 13.6690\n",
      "Epoch [272/300], Step [30/172], Loss: 43.7952\n",
      "Epoch [272/300], Step [31/172], Loss: 25.7287\n",
      "Epoch [272/300], Step [32/172], Loss: 39.5655\n",
      "Epoch [272/300], Step [33/172], Loss: 59.1473\n",
      "Epoch [272/300], Step [34/172], Loss: 1.7307\n",
      "Epoch [272/300], Step [35/172], Loss: 14.3975\n",
      "Epoch [272/300], Step [36/172], Loss: 14.7167\n",
      "Epoch [272/300], Step [37/172], Loss: 13.7364\n",
      "Epoch [272/300], Step [38/172], Loss: 29.2100\n",
      "Epoch [272/300], Step [39/172], Loss: 31.8537\n",
      "Epoch [272/300], Step [40/172], Loss: 19.3054\n",
      "Epoch [272/300], Step [41/172], Loss: 28.1410\n",
      "Epoch [272/300], Step [42/172], Loss: 34.7453\n",
      "Epoch [272/300], Step [43/172], Loss: 24.6676\n",
      "Epoch [272/300], Step [44/172], Loss: 20.8958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [272/300], Step [45/172], Loss: 27.3601\n",
      "Epoch [272/300], Step [46/172], Loss: 15.2016\n",
      "Epoch [272/300], Step [47/172], Loss: 46.3838\n",
      "Epoch [272/300], Step [48/172], Loss: 63.9547\n",
      "Epoch [272/300], Step [49/172], Loss: 22.6531\n",
      "Epoch [272/300], Step [50/172], Loss: 50.5974\n",
      "Epoch [272/300], Step [51/172], Loss: 7.7159\n",
      "Epoch [272/300], Step [52/172], Loss: 21.2076\n",
      "Epoch [272/300], Step [53/172], Loss: 23.0973\n",
      "Epoch [272/300], Step [54/172], Loss: 14.8794\n",
      "Epoch [272/300], Step [55/172], Loss: 15.1431\n",
      "Epoch [272/300], Step [56/172], Loss: 17.2331\n",
      "Epoch [272/300], Step [57/172], Loss: 18.6069\n",
      "Epoch [272/300], Step [58/172], Loss: 14.0258\n",
      "Epoch [272/300], Step [59/172], Loss: 28.6501\n",
      "Epoch [272/300], Step [60/172], Loss: 24.8322\n",
      "Epoch [272/300], Step [61/172], Loss: 6.1333\n",
      "Epoch [272/300], Step [62/172], Loss: 15.9771\n",
      "Epoch [272/300], Step [63/172], Loss: 9.7688\n",
      "Epoch [272/300], Step [64/172], Loss: 12.0506\n",
      "Epoch [272/300], Step [65/172], Loss: 19.7129\n",
      "Epoch [272/300], Step [66/172], Loss: 7.9247\n",
      "Epoch [272/300], Step [67/172], Loss: 26.7934\n",
      "Epoch [272/300], Step [68/172], Loss: 5.4579\n",
      "Epoch [272/300], Step [69/172], Loss: 29.7275\n",
      "Epoch [272/300], Step [70/172], Loss: 29.5848\n",
      "Epoch [272/300], Step [71/172], Loss: 32.1920\n",
      "Epoch [272/300], Step [72/172], Loss: 30.5209\n",
      "Epoch [272/300], Step [73/172], Loss: 38.7728\n",
      "Epoch [272/300], Step [74/172], Loss: 20.9633\n",
      "Epoch [272/300], Step [75/172], Loss: 20.7990\n",
      "Epoch [272/300], Step [76/172], Loss: 23.3202\n",
      "Epoch [272/300], Step [77/172], Loss: 42.1386\n",
      "Epoch [272/300], Step [78/172], Loss: 29.7556\n",
      "Epoch [272/300], Step [79/172], Loss: 27.1256\n",
      "Epoch [272/300], Step [80/172], Loss: 44.7669\n",
      "Epoch [272/300], Step [81/172], Loss: 24.8041\n",
      "Epoch [272/300], Step [82/172], Loss: 34.4658\n",
      "Epoch [272/300], Step [83/172], Loss: 37.6076\n",
      "Epoch [272/300], Step [84/172], Loss: 27.9299\n",
      "Epoch [272/300], Step [85/172], Loss: 31.8073\n",
      "Epoch [272/300], Step [86/172], Loss: 28.0292\n",
      "Epoch [272/300], Step [87/172], Loss: 20.6922\n",
      "Epoch [272/300], Step [88/172], Loss: 19.3866\n",
      "Epoch [272/300], Step [89/172], Loss: 24.2078\n",
      "Epoch [272/300], Step [90/172], Loss: 17.7264\n",
      "Epoch [272/300], Step [91/172], Loss: 23.7742\n",
      "Epoch [272/300], Step [92/172], Loss: 17.5787\n",
      "Epoch [272/300], Step [93/172], Loss: 17.3261\n",
      "Epoch [272/300], Step [94/172], Loss: 23.4013\n",
      "Epoch [272/300], Step [95/172], Loss: 17.7272\n",
      "Epoch [272/300], Step [96/172], Loss: 19.1982\n",
      "Epoch [272/300], Step [97/172], Loss: 28.1458\n",
      "Epoch [272/300], Step [98/172], Loss: 16.4138\n",
      "Epoch [272/300], Step [99/172], Loss: 17.1067\n",
      "Epoch [272/300], Step [100/172], Loss: 15.0705\n",
      "Epoch [272/300], Step [101/172], Loss: 17.6109\n",
      "Epoch [272/300], Step [102/172], Loss: 17.5186\n",
      "Epoch [272/300], Step [103/172], Loss: 10.3447\n",
      "Epoch [272/300], Step [104/172], Loss: 18.1431\n",
      "Epoch [272/300], Step [105/172], Loss: 20.8328\n",
      "Epoch [272/300], Step [106/172], Loss: 14.3342\n",
      "Epoch [272/300], Step [107/172], Loss: 15.1195\n",
      "Epoch [272/300], Step [108/172], Loss: 14.5066\n",
      "Epoch [272/300], Step [109/172], Loss: 14.3002\n",
      "Epoch [272/300], Step [110/172], Loss: 15.8331\n",
      "Epoch [272/300], Step [111/172], Loss: 16.6069\n",
      "Epoch [272/300], Step [112/172], Loss: 15.5689\n",
      "Epoch [272/300], Step [113/172], Loss: 13.2287\n",
      "Epoch [272/300], Step [114/172], Loss: 14.6855\n",
      "Epoch [272/300], Step [115/172], Loss: 19.6596\n",
      "Epoch [272/300], Step [116/172], Loss: 13.5641\n",
      "Epoch [272/300], Step [117/172], Loss: 12.1962\n",
      "Epoch [272/300], Step [118/172], Loss: 13.2533\n",
      "Epoch [272/300], Step [119/172], Loss: 17.6920\n",
      "Epoch [272/300], Step [120/172], Loss: 9.7136\n",
      "Epoch [272/300], Step [121/172], Loss: 8.6013\n",
      "Epoch [272/300], Step [122/172], Loss: 11.8667\n",
      "Epoch [272/300], Step [123/172], Loss: 11.1966\n",
      "Epoch [272/300], Step [124/172], Loss: 7.3699\n",
      "Epoch [272/300], Step [125/172], Loss: 11.9689\n",
      "Epoch [272/300], Step [126/172], Loss: 11.8848\n",
      "Epoch [272/300], Step [127/172], Loss: 10.8642\n",
      "Epoch [272/300], Step [128/172], Loss: 9.6133\n",
      "Epoch [272/300], Step [129/172], Loss: 8.7431\n",
      "Epoch [272/300], Step [130/172], Loss: 12.8475\n",
      "Epoch [272/300], Step [131/172], Loss: 7.4254\n",
      "Epoch [272/300], Step [132/172], Loss: 9.2109\n",
      "Epoch [272/300], Step [133/172], Loss: 9.4884\n",
      "Epoch [272/300], Step [134/172], Loss: 10.6198\n",
      "Epoch [272/300], Step [135/172], Loss: 8.9928\n",
      "Epoch [272/300], Step [136/172], Loss: 8.1224\n",
      "Epoch [272/300], Step [137/172], Loss: 8.3213\n",
      "Epoch [272/300], Step [138/172], Loss: 7.3933\n",
      "Epoch [272/300], Step [139/172], Loss: 11.4334\n",
      "Epoch [272/300], Step [140/172], Loss: 10.7984\n",
      "Epoch [272/300], Step [141/172], Loss: 8.5639\n",
      "Epoch [272/300], Step [142/172], Loss: 14.8121\n",
      "Epoch [272/300], Step [143/172], Loss: 11.6914\n",
      "Epoch [272/300], Step [144/172], Loss: 9.1297\n",
      "Epoch [272/300], Step [145/172], Loss: 10.4939\n",
      "Epoch [272/300], Step [146/172], Loss: 10.1218\n",
      "Epoch [272/300], Step [147/172], Loss: 5.5391\n",
      "Epoch [272/300], Step [148/172], Loss: 5.9336\n",
      "Epoch [272/300], Step [149/172], Loss: 5.8610\n",
      "Epoch [272/300], Step [150/172], Loss: 5.1324\n",
      "Epoch [272/300], Step [151/172], Loss: 5.1501\n",
      "Epoch [272/300], Step [152/172], Loss: 7.3430\n",
      "Epoch [272/300], Step [153/172], Loss: 6.1216\n",
      "Epoch [272/300], Step [154/172], Loss: 6.3409\n",
      "Epoch [272/300], Step [155/172], Loss: 6.3879\n",
      "Epoch [272/300], Step [156/172], Loss: 14.4209\n",
      "Epoch [272/300], Step [157/172], Loss: 9.4602\n",
      "Epoch [272/300], Step [158/172], Loss: 7.2151\n",
      "Epoch [272/300], Step [159/172], Loss: 9.6371\n",
      "Epoch [272/300], Step [160/172], Loss: 10.3884\n",
      "Epoch [272/300], Step [161/172], Loss: 5.5464\n",
      "Epoch [272/300], Step [162/172], Loss: 4.2490\n",
      "Epoch [272/300], Step [163/172], Loss: 7.0986\n",
      "Epoch [272/300], Step [164/172], Loss: 8.2315\n",
      "Epoch [272/300], Step [165/172], Loss: 6.6778\n",
      "Epoch [272/300], Step [166/172], Loss: 5.8565\n",
      "Epoch [272/300], Step [167/172], Loss: 11.0728\n",
      "Epoch [272/300], Step [168/172], Loss: 6.0726\n",
      "Epoch [272/300], Step [169/172], Loss: 6.3160\n",
      "Epoch [272/300], Step [170/172], Loss: 5.0030\n",
      "Epoch [272/300], Step [171/172], Loss: 8.8023\n",
      "Epoch [272/300], Step [172/172], Loss: 5.3338\n",
      "Epoch [273/300], Step [1/172], Loss: 38.7577\n",
      "Epoch [273/300], Step [2/172], Loss: 42.2930\n",
      "Epoch [273/300], Step [3/172], Loss: 39.5924\n",
      "Epoch [273/300], Step [4/172], Loss: 17.8612\n",
      "Epoch [273/300], Step [5/172], Loss: 35.3962\n",
      "Epoch [273/300], Step [6/172], Loss: 16.5653\n",
      "Epoch [273/300], Step [7/172], Loss: 23.9020\n",
      "Epoch [273/300], Step [8/172], Loss: 3.5491\n",
      "Epoch [273/300], Step [9/172], Loss: 24.1703\n",
      "Epoch [273/300], Step [10/172], Loss: 34.8736\n",
      "Epoch [273/300], Step [11/172], Loss: 49.9301\n",
      "Epoch [273/300], Step [12/172], Loss: 47.3520\n",
      "Epoch [273/300], Step [13/172], Loss: 29.3295\n",
      "Epoch [273/300], Step [14/172], Loss: 49.6551\n",
      "Epoch [273/300], Step [15/172], Loss: 48.1305\n",
      "Epoch [273/300], Step [16/172], Loss: 8.8076\n",
      "Epoch [273/300], Step [17/172], Loss: 34.0782\n",
      "Epoch [273/300], Step [18/172], Loss: 49.3166\n",
      "Epoch [273/300], Step [19/172], Loss: 67.4368\n",
      "Epoch [273/300], Step [20/172], Loss: 25.5019\n",
      "Epoch [273/300], Step [21/172], Loss: 70.4118\n",
      "Epoch [273/300], Step [22/172], Loss: 47.7639\n",
      "Epoch [273/300], Step [23/172], Loss: 2.1239\n",
      "Epoch [273/300], Step [24/172], Loss: 43.3288\n",
      "Epoch [273/300], Step [25/172], Loss: 30.5666\n",
      "Epoch [273/300], Step [26/172], Loss: 40.9436\n",
      "Epoch [273/300], Step [27/172], Loss: 54.0377\n",
      "Epoch [273/300], Step [28/172], Loss: 14.6925\n",
      "Epoch [273/300], Step [29/172], Loss: 13.4667\n",
      "Epoch [273/300], Step [30/172], Loss: 44.1793\n",
      "Epoch [273/300], Step [31/172], Loss: 25.6186\n",
      "Epoch [273/300], Step [32/172], Loss: 39.2906\n",
      "Epoch [273/300], Step [33/172], Loss: 58.8039\n",
      "Epoch [273/300], Step [34/172], Loss: 1.5373\n",
      "Epoch [273/300], Step [35/172], Loss: 14.2798\n",
      "Epoch [273/300], Step [36/172], Loss: 13.8785\n",
      "Epoch [273/300], Step [37/172], Loss: 13.6429\n",
      "Epoch [273/300], Step [38/172], Loss: 29.1890\n",
      "Epoch [273/300], Step [39/172], Loss: 31.7514\n",
      "Epoch [273/300], Step [40/172], Loss: 19.0394\n",
      "Epoch [273/300], Step [41/172], Loss: 28.2419\n",
      "Epoch [273/300], Step [42/172], Loss: 34.4168\n",
      "Epoch [273/300], Step [43/172], Loss: 24.4767\n",
      "Epoch [273/300], Step [44/172], Loss: 20.8447\n",
      "Epoch [273/300], Step [45/172], Loss: 27.2289\n",
      "Epoch [273/300], Step [46/172], Loss: 15.4677\n",
      "Epoch [273/300], Step [47/172], Loss: 46.6687\n",
      "Epoch [273/300], Step [48/172], Loss: 63.8835\n",
      "Epoch [273/300], Step [49/172], Loss: 22.2656\n",
      "Epoch [273/300], Step [50/172], Loss: 50.1626\n",
      "Epoch [273/300], Step [51/172], Loss: 7.6467\n",
      "Epoch [273/300], Step [52/172], Loss: 20.9312\n",
      "Epoch [273/300], Step [53/172], Loss: 23.1974\n",
      "Epoch [273/300], Step [54/172], Loss: 14.3543\n",
      "Epoch [273/300], Step [55/172], Loss: 14.9291\n",
      "Epoch [273/300], Step [56/172], Loss: 16.9087\n",
      "Epoch [273/300], Step [57/172], Loss: 18.0244\n",
      "Epoch [273/300], Step [58/172], Loss: 13.8195\n",
      "Epoch [273/300], Step [59/172], Loss: 28.3550\n",
      "Epoch [273/300], Step [60/172], Loss: 24.4082\n",
      "Epoch [273/300], Step [61/172], Loss: 6.0056\n",
      "Epoch [273/300], Step [62/172], Loss: 16.0448\n",
      "Epoch [273/300], Step [63/172], Loss: 9.6963\n",
      "Epoch [273/300], Step [64/172], Loss: 12.1217\n",
      "Epoch [273/300], Step [65/172], Loss: 19.6471\n",
      "Epoch [273/300], Step [66/172], Loss: 7.9541\n",
      "Epoch [273/300], Step [67/172], Loss: 26.8369\n",
      "Epoch [273/300], Step [68/172], Loss: 5.4508\n",
      "Epoch [273/300], Step [69/172], Loss: 29.8230\n",
      "Epoch [273/300], Step [70/172], Loss: 29.3023\n",
      "Epoch [273/300], Step [71/172], Loss: 32.1028\n",
      "Epoch [273/300], Step [72/172], Loss: 30.5250\n",
      "Epoch [273/300], Step [73/172], Loss: 38.9234\n",
      "Epoch [273/300], Step [74/172], Loss: 20.9482\n",
      "Epoch [273/300], Step [75/172], Loss: 20.7123\n",
      "Epoch [273/300], Step [76/172], Loss: 23.0817\n",
      "Epoch [273/300], Step [77/172], Loss: 41.9507\n",
      "Epoch [273/300], Step [78/172], Loss: 29.7000\n",
      "Epoch [273/300], Step [79/172], Loss: 27.1100\n",
      "Epoch [273/300], Step [80/172], Loss: 44.8548\n",
      "Epoch [273/300], Step [81/172], Loss: 24.9225\n",
      "Epoch [273/300], Step [82/172], Loss: 34.2833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [273/300], Step [83/172], Loss: 37.6912\n",
      "Epoch [273/300], Step [84/172], Loss: 28.0602\n",
      "Epoch [273/300], Step [85/172], Loss: 32.1311\n",
      "Epoch [273/300], Step [86/172], Loss: 28.1518\n",
      "Epoch [273/300], Step [87/172], Loss: 20.7534\n",
      "Epoch [273/300], Step [88/172], Loss: 19.4076\n",
      "Epoch [273/300], Step [89/172], Loss: 24.2482\n",
      "Epoch [273/300], Step [90/172], Loss: 17.5610\n",
      "Epoch [273/300], Step [91/172], Loss: 23.8221\n",
      "Epoch [273/300], Step [92/172], Loss: 17.6077\n",
      "Epoch [273/300], Step [93/172], Loss: 16.9875\n",
      "Epoch [273/300], Step [94/172], Loss: 23.2280\n",
      "Epoch [273/300], Step [95/172], Loss: 17.5682\n",
      "Epoch [273/300], Step [96/172], Loss: 19.1995\n",
      "Epoch [273/300], Step [97/172], Loss: 28.0363\n",
      "Epoch [273/300], Step [98/172], Loss: 16.3824\n",
      "Epoch [273/300], Step [99/172], Loss: 17.1033\n",
      "Epoch [273/300], Step [100/172], Loss: 15.1077\n",
      "Epoch [273/300], Step [101/172], Loss: 17.5475\n",
      "Epoch [273/300], Step [102/172], Loss: 17.3329\n",
      "Epoch [273/300], Step [103/172], Loss: 10.3136\n",
      "Epoch [273/300], Step [104/172], Loss: 17.9802\n",
      "Epoch [273/300], Step [105/172], Loss: 20.5223\n",
      "Epoch [273/300], Step [106/172], Loss: 14.2309\n",
      "Epoch [273/300], Step [107/172], Loss: 15.0732\n",
      "Epoch [273/300], Step [108/172], Loss: 14.3152\n",
      "Epoch [273/300], Step [109/172], Loss: 14.1465\n",
      "Epoch [273/300], Step [110/172], Loss: 15.7397\n",
      "Epoch [273/300], Step [111/172], Loss: 16.6021\n",
      "Epoch [273/300], Step [112/172], Loss: 15.4042\n",
      "Epoch [273/300], Step [113/172], Loss: 13.1304\n",
      "Epoch [273/300], Step [114/172], Loss: 14.6558\n",
      "Epoch [273/300], Step [115/172], Loss: 19.5320\n",
      "Epoch [273/300], Step [116/172], Loss: 13.4838\n",
      "Epoch [273/300], Step [117/172], Loss: 12.1495\n",
      "Epoch [273/300], Step [118/172], Loss: 13.3337\n",
      "Epoch [273/300], Step [119/172], Loss: 17.5781\n",
      "Epoch [273/300], Step [120/172], Loss: 9.7284\n",
      "Epoch [273/300], Step [121/172], Loss: 8.4948\n",
      "Epoch [273/300], Step [122/172], Loss: 11.5835\n",
      "Epoch [273/300], Step [123/172], Loss: 11.0675\n",
      "Epoch [273/300], Step [124/172], Loss: 7.3238\n",
      "Epoch [273/300], Step [125/172], Loss: 11.9297\n",
      "Epoch [273/300], Step [126/172], Loss: 11.7913\n",
      "Epoch [273/300], Step [127/172], Loss: 10.8313\n",
      "Epoch [273/300], Step [128/172], Loss: 9.5684\n",
      "Epoch [273/300], Step [129/172], Loss: 8.6976\n",
      "Epoch [273/300], Step [130/172], Loss: 12.7766\n",
      "Epoch [273/300], Step [131/172], Loss: 7.3082\n",
      "Epoch [273/300], Step [132/172], Loss: 9.1437\n",
      "Epoch [273/300], Step [133/172], Loss: 9.3663\n",
      "Epoch [273/300], Step [134/172], Loss: 10.4516\n",
      "Epoch [273/300], Step [135/172], Loss: 8.9672\n",
      "Epoch [273/300], Step [136/172], Loss: 8.1205\n",
      "Epoch [273/300], Step [137/172], Loss: 8.2678\n",
      "Epoch [273/300], Step [138/172], Loss: 7.2195\n",
      "Epoch [273/300], Step [139/172], Loss: 11.4144\n",
      "Epoch [273/300], Step [140/172], Loss: 10.7805\n",
      "Epoch [273/300], Step [141/172], Loss: 8.4525\n",
      "Epoch [273/300], Step [142/172], Loss: 14.7622\n",
      "Epoch [273/300], Step [143/172], Loss: 11.7054\n",
      "Epoch [273/300], Step [144/172], Loss: 9.0336\n",
      "Epoch [273/300], Step [145/172], Loss: 10.5265\n",
      "Epoch [273/300], Step [146/172], Loss: 10.0059\n",
      "Epoch [273/300], Step [147/172], Loss: 5.4762\n",
      "Epoch [273/300], Step [148/172], Loss: 5.8927\n",
      "Epoch [273/300], Step [149/172], Loss: 5.7537\n",
      "Epoch [273/300], Step [150/172], Loss: 5.1075\n",
      "Epoch [273/300], Step [151/172], Loss: 5.1406\n",
      "Epoch [273/300], Step [152/172], Loss: 7.2416\n",
      "Epoch [273/300], Step [153/172], Loss: 6.0989\n",
      "Epoch [273/300], Step [154/172], Loss: 6.2977\n",
      "Epoch [273/300], Step [155/172], Loss: 6.3008\n",
      "Epoch [273/300], Step [156/172], Loss: 14.3127\n",
      "Epoch [273/300], Step [157/172], Loss: 9.4067\n",
      "Epoch [273/300], Step [158/172], Loss: 7.1229\n",
      "Epoch [273/300], Step [159/172], Loss: 9.6836\n",
      "Epoch [273/300], Step [160/172], Loss: 10.2265\n",
      "Epoch [273/300], Step [161/172], Loss: 5.3946\n",
      "Epoch [273/300], Step [162/172], Loss: 4.2106\n",
      "Epoch [273/300], Step [163/172], Loss: 7.0979\n",
      "Epoch [273/300], Step [164/172], Loss: 8.0768\n",
      "Epoch [273/300], Step [165/172], Loss: 6.6748\n",
      "Epoch [273/300], Step [166/172], Loss: 5.8457\n",
      "Epoch [273/300], Step [167/172], Loss: 11.0690\n",
      "Epoch [273/300], Step [168/172], Loss: 6.0121\n",
      "Epoch [273/300], Step [169/172], Loss: 6.3493\n",
      "Epoch [273/300], Step [170/172], Loss: 5.0091\n",
      "Epoch [273/300], Step [171/172], Loss: 8.7678\n",
      "Epoch [273/300], Step [172/172], Loss: 5.2970\n",
      "Epoch [274/300], Step [1/172], Loss: 38.6080\n",
      "Epoch [274/300], Step [2/172], Loss: 41.6327\n",
      "Epoch [274/300], Step [3/172], Loss: 39.6130\n",
      "Epoch [274/300], Step [4/172], Loss: 17.8606\n",
      "Epoch [274/300], Step [5/172], Loss: 34.7878\n",
      "Epoch [274/300], Step [6/172], Loss: 16.8304\n",
      "Epoch [274/300], Step [7/172], Loss: 24.6437\n",
      "Epoch [274/300], Step [8/172], Loss: 4.3665\n",
      "Epoch [274/300], Step [9/172], Loss: 24.3171\n",
      "Epoch [274/300], Step [10/172], Loss: 34.8498\n",
      "Epoch [274/300], Step [11/172], Loss: 50.0098\n",
      "Epoch [274/300], Step [12/172], Loss: 47.7636\n",
      "Epoch [274/300], Step [13/172], Loss: 29.6338\n",
      "Epoch [274/300], Step [14/172], Loss: 49.9622\n",
      "Epoch [274/300], Step [15/172], Loss: 48.3454\n",
      "Epoch [274/300], Step [16/172], Loss: 7.7607\n",
      "Epoch [274/300], Step [17/172], Loss: 34.2360\n",
      "Epoch [274/300], Step [18/172], Loss: 49.4189\n",
      "Epoch [274/300], Step [19/172], Loss: 67.5172\n",
      "Epoch [274/300], Step [20/172], Loss: 25.5264\n",
      "Epoch [274/300], Step [21/172], Loss: 69.4946\n",
      "Epoch [274/300], Step [22/172], Loss: 47.2435\n",
      "Epoch [274/300], Step [23/172], Loss: 1.7089\n",
      "Epoch [274/300], Step [24/172], Loss: 43.1705\n",
      "Epoch [274/300], Step [25/172], Loss: 30.3038\n",
      "Epoch [274/300], Step [26/172], Loss: 40.6188\n",
      "Epoch [274/300], Step [27/172], Loss: 53.5951\n",
      "Epoch [274/300], Step [28/172], Loss: 14.7822\n",
      "Epoch [274/300], Step [29/172], Loss: 13.7158\n",
      "Epoch [274/300], Step [30/172], Loss: 43.6340\n",
      "Epoch [274/300], Step [31/172], Loss: 25.5499\n",
      "Epoch [274/300], Step [32/172], Loss: 39.6092\n",
      "Epoch [274/300], Step [33/172], Loss: 59.0596\n",
      "Epoch [274/300], Step [34/172], Loss: 1.5506\n",
      "Epoch [274/300], Step [35/172], Loss: 14.3833\n",
      "Epoch [274/300], Step [36/172], Loss: 14.1504\n",
      "Epoch [274/300], Step [37/172], Loss: 13.5540\n",
      "Epoch [274/300], Step [38/172], Loss: 29.0821\n",
      "Epoch [274/300], Step [39/172], Loss: 31.8203\n",
      "Epoch [274/300], Step [40/172], Loss: 19.0711\n",
      "Epoch [274/300], Step [41/172], Loss: 28.2716\n",
      "Epoch [274/300], Step [42/172], Loss: 34.4097\n",
      "Epoch [274/300], Step [43/172], Loss: 24.6172\n",
      "Epoch [274/300], Step [44/172], Loss: 20.7675\n",
      "Epoch [274/300], Step [45/172], Loss: 27.1044\n",
      "Epoch [274/300], Step [46/172], Loss: 15.4168\n",
      "Epoch [274/300], Step [47/172], Loss: 46.3257\n",
      "Epoch [274/300], Step [48/172], Loss: 63.7931\n",
      "Epoch [274/300], Step [49/172], Loss: 22.1289\n",
      "Epoch [274/300], Step [50/172], Loss: 50.1426\n",
      "Epoch [274/300], Step [51/172], Loss: 7.5078\n",
      "Epoch [274/300], Step [52/172], Loss: 20.6500\n",
      "Epoch [274/300], Step [53/172], Loss: 22.8148\n",
      "Epoch [274/300], Step [54/172], Loss: 13.7640\n",
      "Epoch [274/300], Step [55/172], Loss: 14.5588\n",
      "Epoch [274/300], Step [56/172], Loss: 16.5032\n",
      "Epoch [274/300], Step [57/172], Loss: 17.8977\n",
      "Epoch [274/300], Step [58/172], Loss: 13.4019\n",
      "Epoch [274/300], Step [59/172], Loss: 27.9844\n",
      "Epoch [274/300], Step [60/172], Loss: 24.7497\n",
      "Epoch [274/300], Step [61/172], Loss: 5.8579\n",
      "Epoch [274/300], Step [62/172], Loss: 15.9754\n",
      "Epoch [274/300], Step [63/172], Loss: 9.4011\n",
      "Epoch [274/300], Step [64/172], Loss: 11.9307\n",
      "Epoch [274/300], Step [65/172], Loss: 19.4196\n",
      "Epoch [274/300], Step [66/172], Loss: 7.8776\n",
      "Epoch [274/300], Step [67/172], Loss: 26.9215\n",
      "Epoch [274/300], Step [68/172], Loss: 5.2003\n",
      "Epoch [274/300], Step [69/172], Loss: 29.9172\n",
      "Epoch [274/300], Step [70/172], Loss: 29.5937\n",
      "Epoch [274/300], Step [71/172], Loss: 32.1715\n",
      "Epoch [274/300], Step [72/172], Loss: 30.6249\n",
      "Epoch [274/300], Step [73/172], Loss: 38.8550\n",
      "Epoch [274/300], Step [74/172], Loss: 21.0253\n",
      "Epoch [274/300], Step [75/172], Loss: 20.5399\n",
      "Epoch [274/300], Step [76/172], Loss: 23.1089\n",
      "Epoch [274/300], Step [77/172], Loss: 41.8251\n",
      "Epoch [274/300], Step [78/172], Loss: 29.5991\n",
      "Epoch [274/300], Step [79/172], Loss: 26.8956\n",
      "Epoch [274/300], Step [80/172], Loss: 44.7980\n",
      "Epoch [274/300], Step [81/172], Loss: 24.7941\n",
      "Epoch [274/300], Step [82/172], Loss: 34.3941\n",
      "Epoch [274/300], Step [83/172], Loss: 37.6466\n",
      "Epoch [274/300], Step [84/172], Loss: 27.9604\n",
      "Epoch [274/300], Step [85/172], Loss: 31.9728\n",
      "Epoch [274/300], Step [86/172], Loss: 27.8275\n",
      "Epoch [274/300], Step [87/172], Loss: 20.6058\n",
      "Epoch [274/300], Step [88/172], Loss: 19.2010\n",
      "Epoch [274/300], Step [89/172], Loss: 23.9657\n",
      "Epoch [274/300], Step [90/172], Loss: 17.5075\n",
      "Epoch [274/300], Step [91/172], Loss: 23.6520\n",
      "Epoch [274/300], Step [92/172], Loss: 17.4930\n",
      "Epoch [274/300], Step [93/172], Loss: 17.0916\n",
      "Epoch [274/300], Step [94/172], Loss: 23.0635\n",
      "Epoch [274/300], Step [95/172], Loss: 17.5118\n",
      "Epoch [274/300], Step [96/172], Loss: 19.1207\n",
      "Epoch [274/300], Step [97/172], Loss: 27.8357\n",
      "Epoch [274/300], Step [98/172], Loss: 16.1436\n",
      "Epoch [274/300], Step [99/172], Loss: 16.9783\n",
      "Epoch [274/300], Step [100/172], Loss: 14.7606\n",
      "Epoch [274/300], Step [101/172], Loss: 17.3524\n",
      "Epoch [274/300], Step [102/172], Loss: 17.2759\n",
      "Epoch [274/300], Step [103/172], Loss: 10.2163\n",
      "Epoch [274/300], Step [104/172], Loss: 17.8810\n",
      "Epoch [274/300], Step [105/172], Loss: 20.5689\n",
      "Epoch [274/300], Step [106/172], Loss: 14.2629\n",
      "Epoch [274/300], Step [107/172], Loss: 14.9896\n",
      "Epoch [274/300], Step [108/172], Loss: 14.1781\n",
      "Epoch [274/300], Step [109/172], Loss: 14.0628\n",
      "Epoch [274/300], Step [110/172], Loss: 15.6280\n",
      "Epoch [274/300], Step [111/172], Loss: 16.6018\n",
      "Epoch [274/300], Step [112/172], Loss: 15.3044\n",
      "Epoch [274/300], Step [113/172], Loss: 13.0283\n",
      "Epoch [274/300], Step [114/172], Loss: 14.6993\n",
      "Epoch [274/300], Step [115/172], Loss: 19.4900\n",
      "Epoch [274/300], Step [116/172], Loss: 13.4207\n",
      "Epoch [274/300], Step [117/172], Loss: 12.0607\n",
      "Epoch [274/300], Step [118/172], Loss: 13.2594\n",
      "Epoch [274/300], Step [119/172], Loss: 17.4715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [274/300], Step [120/172], Loss: 9.6382\n",
      "Epoch [274/300], Step [121/172], Loss: 8.4933\n",
      "Epoch [274/300], Step [122/172], Loss: 11.5756\n",
      "Epoch [274/300], Step [123/172], Loss: 11.1510\n",
      "Epoch [274/300], Step [124/172], Loss: 7.3161\n",
      "Epoch [274/300], Step [125/172], Loss: 11.9366\n",
      "Epoch [274/300], Step [126/172], Loss: 11.7426\n",
      "Epoch [274/300], Step [127/172], Loss: 10.8143\n",
      "Epoch [274/300], Step [128/172], Loss: 9.6245\n",
      "Epoch [274/300], Step [129/172], Loss: 8.7080\n",
      "Epoch [274/300], Step [130/172], Loss: 12.7280\n",
      "Epoch [274/300], Step [131/172], Loss: 7.3006\n",
      "Epoch [274/300], Step [132/172], Loss: 9.1546\n",
      "Epoch [274/300], Step [133/172], Loss: 9.2968\n",
      "Epoch [274/300], Step [134/172], Loss: 10.4124\n",
      "Epoch [274/300], Step [135/172], Loss: 8.9882\n",
      "Epoch [274/300], Step [136/172], Loss: 8.1256\n",
      "Epoch [274/300], Step [137/172], Loss: 8.2384\n",
      "Epoch [274/300], Step [138/172], Loss: 7.2822\n",
      "Epoch [274/300], Step [139/172], Loss: 11.4237\n",
      "Epoch [274/300], Step [140/172], Loss: 10.8302\n",
      "Epoch [274/300], Step [141/172], Loss: 8.4390\n",
      "Epoch [274/300], Step [142/172], Loss: 14.7320\n",
      "Epoch [274/300], Step [143/172], Loss: 11.6875\n",
      "Epoch [274/300], Step [144/172], Loss: 9.0474\n",
      "Epoch [274/300], Step [145/172], Loss: 10.5146\n",
      "Epoch [274/300], Step [146/172], Loss: 9.9765\n",
      "Epoch [274/300], Step [147/172], Loss: 5.4923\n",
      "Epoch [274/300], Step [148/172], Loss: 5.9133\n",
      "Epoch [274/300], Step [149/172], Loss: 5.7717\n",
      "Epoch [274/300], Step [150/172], Loss: 5.1243\n",
      "Epoch [274/300], Step [151/172], Loss: 5.1338\n",
      "Epoch [274/300], Step [152/172], Loss: 7.2429\n",
      "Epoch [274/300], Step [153/172], Loss: 6.1313\n",
      "Epoch [274/300], Step [154/172], Loss: 6.3137\n",
      "Epoch [274/300], Step [155/172], Loss: 6.3019\n",
      "Epoch [274/300], Step [156/172], Loss: 14.3129\n",
      "Epoch [274/300], Step [157/172], Loss: 9.3788\n",
      "Epoch [274/300], Step [158/172], Loss: 7.1276\n",
      "Epoch [274/300], Step [159/172], Loss: 9.8296\n",
      "Epoch [274/300], Step [160/172], Loss: 10.1379\n",
      "Epoch [274/300], Step [161/172], Loss: 5.4361\n",
      "Epoch [274/300], Step [162/172], Loss: 4.1671\n",
      "Epoch [274/300], Step [163/172], Loss: 7.1833\n",
      "Epoch [274/300], Step [164/172], Loss: 8.1548\n",
      "Epoch [274/300], Step [165/172], Loss: 6.6676\n",
      "Epoch [274/300], Step [166/172], Loss: 5.8398\n",
      "Epoch [274/300], Step [167/172], Loss: 11.1057\n",
      "Epoch [274/300], Step [168/172], Loss: 6.0105\n",
      "Epoch [274/300], Step [169/172], Loss: 6.3083\n",
      "Epoch [274/300], Step [170/172], Loss: 4.9429\n",
      "Epoch [274/300], Step [171/172], Loss: 8.8440\n",
      "Epoch [274/300], Step [172/172], Loss: 5.3203\n",
      "Epoch [275/300], Step [1/172], Loss: 38.4797\n",
      "Epoch [275/300], Step [2/172], Loss: 41.7067\n",
      "Epoch [275/300], Step [3/172], Loss: 38.6949\n",
      "Epoch [275/300], Step [4/172], Loss: 17.7719\n",
      "Epoch [275/300], Step [5/172], Loss: 34.1398\n",
      "Epoch [275/300], Step [6/172], Loss: 16.9208\n",
      "Epoch [275/300], Step [7/172], Loss: 24.6558\n",
      "Epoch [275/300], Step [8/172], Loss: 3.4323\n",
      "Epoch [275/300], Step [9/172], Loss: 23.9108\n",
      "Epoch [275/300], Step [10/172], Loss: 34.5570\n",
      "Epoch [275/300], Step [11/172], Loss: 49.6660\n",
      "Epoch [275/300], Step [12/172], Loss: 47.0953\n",
      "Epoch [275/300], Step [13/172], Loss: 29.3340\n",
      "Epoch [275/300], Step [14/172], Loss: 49.3878\n",
      "Epoch [275/300], Step [15/172], Loss: 48.1929\n",
      "Epoch [275/300], Step [16/172], Loss: 8.7666\n",
      "Epoch [275/300], Step [17/172], Loss: 33.9731\n",
      "Epoch [275/300], Step [18/172], Loss: 49.1063\n",
      "Epoch [275/300], Step [19/172], Loss: 67.1861\n",
      "Epoch [275/300], Step [20/172], Loss: 25.7366\n",
      "Epoch [275/300], Step [21/172], Loss: 69.6353\n",
      "Epoch [275/300], Step [22/172], Loss: 47.7624\n",
      "Epoch [275/300], Step [23/172], Loss: 2.0008\n",
      "Epoch [275/300], Step [24/172], Loss: 43.4320\n",
      "Epoch [275/300], Step [25/172], Loss: 30.5245\n",
      "Epoch [275/300], Step [26/172], Loss: 40.9939\n",
      "Epoch [275/300], Step [27/172], Loss: 53.8635\n",
      "Epoch [275/300], Step [28/172], Loss: 14.8200\n",
      "Epoch [275/300], Step [29/172], Loss: 13.3032\n",
      "Epoch [275/300], Step [30/172], Loss: 43.5276\n",
      "Epoch [275/300], Step [31/172], Loss: 25.2112\n",
      "Epoch [275/300], Step [32/172], Loss: 39.5164\n",
      "Epoch [275/300], Step [33/172], Loss: 58.9835\n",
      "Epoch [275/300], Step [34/172], Loss: 1.5776\n",
      "Epoch [275/300], Step [35/172], Loss: 14.3808\n",
      "Epoch [275/300], Step [36/172], Loss: 14.0218\n",
      "Epoch [275/300], Step [37/172], Loss: 13.4516\n",
      "Epoch [275/300], Step [38/172], Loss: 29.0163\n",
      "Epoch [275/300], Step [39/172], Loss: 31.4974\n",
      "Epoch [275/300], Step [40/172], Loss: 18.8798\n",
      "Epoch [275/300], Step [41/172], Loss: 28.0200\n",
      "Epoch [275/300], Step [42/172], Loss: 34.1758\n",
      "Epoch [275/300], Step [43/172], Loss: 24.2345\n",
      "Epoch [275/300], Step [44/172], Loss: 20.7726\n",
      "Epoch [275/300], Step [45/172], Loss: 27.0081\n",
      "Epoch [275/300], Step [46/172], Loss: 15.0367\n",
      "Epoch [275/300], Step [47/172], Loss: 46.0001\n",
      "Epoch [275/300], Step [48/172], Loss: 63.2838\n",
      "Epoch [275/300], Step [49/172], Loss: 22.1859\n",
      "Epoch [275/300], Step [50/172], Loss: 49.9414\n",
      "Epoch [275/300], Step [51/172], Loss: 7.4224\n",
      "Epoch [275/300], Step [52/172], Loss: 20.4738\n",
      "Epoch [275/300], Step [53/172], Loss: 22.9295\n",
      "Epoch [275/300], Step [54/172], Loss: 13.7996\n",
      "Epoch [275/300], Step [55/172], Loss: 14.5846\n",
      "Epoch [275/300], Step [56/172], Loss: 16.5301\n",
      "Epoch [275/300], Step [57/172], Loss: 18.1868\n",
      "Epoch [275/300], Step [58/172], Loss: 13.4169\n",
      "Epoch [275/300], Step [59/172], Loss: 27.9262\n",
      "Epoch [275/300], Step [60/172], Loss: 24.5626\n",
      "Epoch [275/300], Step [61/172], Loss: 5.8259\n",
      "Epoch [275/300], Step [62/172], Loss: 15.8099\n",
      "Epoch [275/300], Step [63/172], Loss: 9.2239\n",
      "Epoch [275/300], Step [64/172], Loss: 11.7417\n",
      "Epoch [275/300], Step [65/172], Loss: 19.2731\n",
      "Epoch [275/300], Step [66/172], Loss: 7.6439\n",
      "Epoch [275/300], Step [67/172], Loss: 26.6320\n",
      "Epoch [275/300], Step [68/172], Loss: 5.1274\n",
      "Epoch [275/300], Step [69/172], Loss: 29.7414\n",
      "Epoch [275/300], Step [70/172], Loss: 29.6239\n",
      "Epoch [275/300], Step [71/172], Loss: 32.3426\n",
      "Epoch [275/300], Step [72/172], Loss: 30.6837\n",
      "Epoch [275/300], Step [73/172], Loss: 39.0022\n",
      "Epoch [275/300], Step [74/172], Loss: 20.9868\n",
      "Epoch [275/300], Step [75/172], Loss: 20.6616\n",
      "Epoch [275/300], Step [76/172], Loss: 23.1573\n",
      "Epoch [275/300], Step [77/172], Loss: 42.0982\n",
      "Epoch [275/300], Step [78/172], Loss: 29.7001\n",
      "Epoch [275/300], Step [79/172], Loss: 27.1115\n",
      "Epoch [275/300], Step [80/172], Loss: 44.9402\n",
      "Epoch [275/300], Step [81/172], Loss: 24.9184\n",
      "Epoch [275/300], Step [82/172], Loss: 34.0696\n",
      "Epoch [275/300], Step [83/172], Loss: 37.5652\n",
      "Epoch [275/300], Step [84/172], Loss: 28.0399\n",
      "Epoch [275/300], Step [85/172], Loss: 32.1593\n",
      "Epoch [275/300], Step [86/172], Loss: 27.8056\n",
      "Epoch [275/300], Step [87/172], Loss: 20.6917\n",
      "Epoch [275/300], Step [88/172], Loss: 19.3342\n",
      "Epoch [275/300], Step [89/172], Loss: 23.8711\n",
      "Epoch [275/300], Step [90/172], Loss: 17.4847\n",
      "Epoch [275/300], Step [91/172], Loss: 23.6670\n",
      "Epoch [275/300], Step [92/172], Loss: 17.4862\n",
      "Epoch [275/300], Step [93/172], Loss: 16.9764\n",
      "Epoch [275/300], Step [94/172], Loss: 22.9775\n",
      "Epoch [275/300], Step [95/172], Loss: 17.3670\n",
      "Epoch [275/300], Step [96/172], Loss: 19.2113\n",
      "Epoch [275/300], Step [97/172], Loss: 27.9242\n",
      "Epoch [275/300], Step [98/172], Loss: 16.1676\n",
      "Epoch [275/300], Step [99/172], Loss: 16.9301\n",
      "Epoch [275/300], Step [100/172], Loss: 14.7871\n",
      "Epoch [275/300], Step [101/172], Loss: 17.2410\n",
      "Epoch [275/300], Step [102/172], Loss: 17.1067\n",
      "Epoch [275/300], Step [103/172], Loss: 10.0666\n",
      "Epoch [275/300], Step [104/172], Loss: 17.9075\n",
      "Epoch [275/300], Step [105/172], Loss: 20.1695\n",
      "Epoch [275/300], Step [106/172], Loss: 14.1746\n",
      "Epoch [275/300], Step [107/172], Loss: 14.9388\n",
      "Epoch [275/300], Step [108/172], Loss: 14.0135\n",
      "Epoch [275/300], Step [109/172], Loss: 13.9411\n",
      "Epoch [275/300], Step [110/172], Loss: 15.3724\n",
      "Epoch [275/300], Step [111/172], Loss: 16.4107\n",
      "Epoch [275/300], Step [112/172], Loss: 15.2000\n",
      "Epoch [275/300], Step [113/172], Loss: 12.9032\n",
      "Epoch [275/300], Step [114/172], Loss: 14.5775\n",
      "Epoch [275/300], Step [115/172], Loss: 19.4382\n",
      "Epoch [275/300], Step [116/172], Loss: 13.1456\n",
      "Epoch [275/300], Step [117/172], Loss: 11.8611\n",
      "Epoch [275/300], Step [118/172], Loss: 13.0419\n",
      "Epoch [275/300], Step [119/172], Loss: 17.4466\n",
      "Epoch [275/300], Step [120/172], Loss: 9.4057\n",
      "Epoch [275/300], Step [121/172], Loss: 8.5340\n",
      "Epoch [275/300], Step [122/172], Loss: 11.4114\n",
      "Epoch [275/300], Step [123/172], Loss: 10.7338\n",
      "Epoch [275/300], Step [124/172], Loss: 7.3709\n",
      "Epoch [275/300], Step [125/172], Loss: 11.8695\n",
      "Epoch [275/300], Step [126/172], Loss: 11.6666\n",
      "Epoch [275/300], Step [127/172], Loss: 10.8058\n",
      "Epoch [275/300], Step [128/172], Loss: 9.6591\n",
      "Epoch [275/300], Step [129/172], Loss: 8.7424\n",
      "Epoch [275/300], Step [130/172], Loss: 12.7087\n",
      "Epoch [275/300], Step [131/172], Loss: 7.3053\n",
      "Epoch [275/300], Step [132/172], Loss: 9.1326\n",
      "Epoch [275/300], Step [133/172], Loss: 9.1274\n",
      "Epoch [275/300], Step [134/172], Loss: 10.4189\n",
      "Epoch [275/300], Step [135/172], Loss: 8.9631\n",
      "Epoch [275/300], Step [136/172], Loss: 7.9680\n",
      "Epoch [275/300], Step [137/172], Loss: 8.1434\n",
      "Epoch [275/300], Step [138/172], Loss: 7.2272\n",
      "Epoch [275/300], Step [139/172], Loss: 11.4069\n",
      "Epoch [275/300], Step [140/172], Loss: 10.7811\n",
      "Epoch [275/300], Step [141/172], Loss: 8.3537\n",
      "Epoch [275/300], Step [142/172], Loss: 14.4009\n",
      "Epoch [275/300], Step [143/172], Loss: 11.6347\n",
      "Epoch [275/300], Step [144/172], Loss: 9.0371\n",
      "Epoch [275/300], Step [145/172], Loss: 10.3351\n",
      "Epoch [275/300], Step [146/172], Loss: 9.9054\n",
      "Epoch [275/300], Step [147/172], Loss: 5.5077\n",
      "Epoch [275/300], Step [148/172], Loss: 5.8815\n",
      "Epoch [275/300], Step [149/172], Loss: 5.7081\n",
      "Epoch [275/300], Step [150/172], Loss: 5.0923\n",
      "Epoch [275/300], Step [151/172], Loss: 5.0223\n",
      "Epoch [275/300], Step [152/172], Loss: 7.1929\n",
      "Epoch [275/300], Step [153/172], Loss: 6.1446\n",
      "Epoch [275/300], Step [154/172], Loss: 6.2348\n",
      "Epoch [275/300], Step [155/172], Loss: 6.4033\n",
      "Epoch [275/300], Step [156/172], Loss: 14.2370\n",
      "Epoch [275/300], Step [157/172], Loss: 9.3517\n",
      "Epoch [275/300], Step [158/172], Loss: 7.0625\n",
      "Epoch [275/300], Step [159/172], Loss: 9.5778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [275/300], Step [160/172], Loss: 10.0628\n",
      "Epoch [275/300], Step [161/172], Loss: 5.3710\n",
      "Epoch [275/300], Step [162/172], Loss: 4.1169\n",
      "Epoch [275/300], Step [163/172], Loss: 7.0122\n",
      "Epoch [275/300], Step [164/172], Loss: 7.9176\n",
      "Epoch [275/300], Step [165/172], Loss: 6.6375\n",
      "Epoch [275/300], Step [166/172], Loss: 5.7899\n",
      "Epoch [275/300], Step [167/172], Loss: 11.0768\n",
      "Epoch [275/300], Step [168/172], Loss: 6.0108\n",
      "Epoch [275/300], Step [169/172], Loss: 6.1297\n",
      "Epoch [275/300], Step [170/172], Loss: 4.7817\n",
      "Epoch [275/300], Step [171/172], Loss: 8.8194\n",
      "Epoch [275/300], Step [172/172], Loss: 5.3600\n",
      "Epoch [276/300], Step [1/172], Loss: 38.2953\n",
      "Epoch [276/300], Step [2/172], Loss: 41.2158\n",
      "Epoch [276/300], Step [3/172], Loss: 39.3337\n",
      "Epoch [276/300], Step [4/172], Loss: 17.6675\n",
      "Epoch [276/300], Step [5/172], Loss: 34.0159\n",
      "Epoch [276/300], Step [6/172], Loss: 17.1810\n",
      "Epoch [276/300], Step [7/172], Loss: 26.1009\n",
      "Epoch [276/300], Step [8/172], Loss: 4.5660\n",
      "Epoch [276/300], Step [9/172], Loss: 24.4105\n",
      "Epoch [276/300], Step [10/172], Loss: 34.7093\n",
      "Epoch [276/300], Step [11/172], Loss: 49.7376\n",
      "Epoch [276/300], Step [12/172], Loss: 47.5875\n",
      "Epoch [276/300], Step [13/172], Loss: 29.4733\n",
      "Epoch [276/300], Step [14/172], Loss: 49.8514\n",
      "Epoch [276/300], Step [15/172], Loss: 48.5507\n",
      "Epoch [276/300], Step [16/172], Loss: 7.4770\n",
      "Epoch [276/300], Step [17/172], Loss: 34.6534\n",
      "Epoch [276/300], Step [18/172], Loss: 49.6300\n",
      "Epoch [276/300], Step [19/172], Loss: 68.0303\n",
      "Epoch [276/300], Step [20/172], Loss: 25.2496\n",
      "Epoch [276/300], Step [21/172], Loss: 69.8888\n",
      "Epoch [276/300], Step [22/172], Loss: 47.2027\n",
      "Epoch [276/300], Step [23/172], Loss: 1.4635\n",
      "Epoch [276/300], Step [24/172], Loss: 43.0080\n",
      "Epoch [276/300], Step [25/172], Loss: 29.8125\n",
      "Epoch [276/300], Step [26/172], Loss: 40.4419\n",
      "Epoch [276/300], Step [27/172], Loss: 54.4260\n",
      "Epoch [276/300], Step [28/172], Loss: 14.4868\n",
      "Epoch [276/300], Step [29/172], Loss: 13.6173\n",
      "Epoch [276/300], Step [30/172], Loss: 43.3216\n",
      "Epoch [276/300], Step [31/172], Loss: 25.2018\n",
      "Epoch [276/300], Step [32/172], Loss: 39.3088\n",
      "Epoch [276/300], Step [33/172], Loss: 58.5222\n",
      "Epoch [276/300], Step [34/172], Loss: 1.6384\n",
      "Epoch [276/300], Step [35/172], Loss: 14.5075\n",
      "Epoch [276/300], Step [36/172], Loss: 13.9843\n",
      "Epoch [276/300], Step [37/172], Loss: 13.4462\n",
      "Epoch [276/300], Step [38/172], Loss: 29.0648\n",
      "Epoch [276/300], Step [39/172], Loss: 31.5792\n",
      "Epoch [276/300], Step [40/172], Loss: 18.9184\n",
      "Epoch [276/300], Step [41/172], Loss: 28.0151\n",
      "Epoch [276/300], Step [42/172], Loss: 33.9307\n",
      "Epoch [276/300], Step [43/172], Loss: 24.3532\n",
      "Epoch [276/300], Step [44/172], Loss: 20.4463\n",
      "Epoch [276/300], Step [45/172], Loss: 26.5770\n",
      "Epoch [276/300], Step [46/172], Loss: 15.3866\n",
      "Epoch [276/300], Step [47/172], Loss: 45.9747\n",
      "Epoch [276/300], Step [48/172], Loss: 64.8342\n",
      "Epoch [276/300], Step [49/172], Loss: 21.8676\n",
      "Epoch [276/300], Step [50/172], Loss: 50.3696\n",
      "Epoch [276/300], Step [51/172], Loss: 7.4276\n",
      "Epoch [276/300], Step [52/172], Loss: 20.2852\n",
      "Epoch [276/300], Step [53/172], Loss: 22.7292\n",
      "Epoch [276/300], Step [54/172], Loss: 14.0536\n",
      "Epoch [276/300], Step [55/172], Loss: 14.5386\n",
      "Epoch [276/300], Step [56/172], Loss: 16.6041\n",
      "Epoch [276/300], Step [57/172], Loss: 18.2182\n",
      "Epoch [276/300], Step [58/172], Loss: 13.6270\n",
      "Epoch [276/300], Step [59/172], Loss: 27.6892\n",
      "Epoch [276/300], Step [60/172], Loss: 24.5538\n",
      "Epoch [276/300], Step [61/172], Loss: 5.8561\n",
      "Epoch [276/300], Step [62/172], Loss: 15.8718\n",
      "Epoch [276/300], Step [63/172], Loss: 9.3091\n",
      "Epoch [276/300], Step [64/172], Loss: 11.6792\n",
      "Epoch [276/300], Step [65/172], Loss: 19.2238\n",
      "Epoch [276/300], Step [66/172], Loss: 7.5314\n",
      "Epoch [276/300], Step [67/172], Loss: 26.8100\n",
      "Epoch [276/300], Step [68/172], Loss: 4.9635\n",
      "Epoch [276/300], Step [69/172], Loss: 30.0128\n",
      "Epoch [276/300], Step [70/172], Loss: 29.6327\n",
      "Epoch [276/300], Step [71/172], Loss: 32.3159\n",
      "Epoch [276/300], Step [72/172], Loss: 30.8517\n",
      "Epoch [276/300], Step [73/172], Loss: 39.4376\n",
      "Epoch [276/300], Step [74/172], Loss: 20.9790\n",
      "Epoch [276/300], Step [75/172], Loss: 20.4145\n",
      "Epoch [276/300], Step [76/172], Loss: 22.9303\n",
      "Epoch [276/300], Step [77/172], Loss: 41.8999\n",
      "Epoch [276/300], Step [78/172], Loss: 29.9218\n",
      "Epoch [276/300], Step [79/172], Loss: 27.1548\n",
      "Epoch [276/300], Step [80/172], Loss: 45.1316\n",
      "Epoch [276/300], Step [81/172], Loss: 24.9860\n",
      "Epoch [276/300], Step [82/172], Loss: 34.4482\n",
      "Epoch [276/300], Step [83/172], Loss: 37.5618\n",
      "Epoch [276/300], Step [84/172], Loss: 27.8164\n",
      "Epoch [276/300], Step [85/172], Loss: 32.2776\n",
      "Epoch [276/300], Step [86/172], Loss: 28.0097\n",
      "Epoch [276/300], Step [87/172], Loss: 20.4631\n",
      "Epoch [276/300], Step [88/172], Loss: 19.3286\n",
      "Epoch [276/300], Step [89/172], Loss: 23.9178\n",
      "Epoch [276/300], Step [90/172], Loss: 17.4224\n",
      "Epoch [276/300], Step [91/172], Loss: 23.5708\n",
      "Epoch [276/300], Step [92/172], Loss: 17.4751\n",
      "Epoch [276/300], Step [93/172], Loss: 16.9796\n",
      "Epoch [276/300], Step [94/172], Loss: 22.9463\n",
      "Epoch [276/300], Step [95/172], Loss: 17.1573\n",
      "Epoch [276/300], Step [96/172], Loss: 19.1253\n",
      "Epoch [276/300], Step [97/172], Loss: 27.6468\n",
      "Epoch [276/300], Step [98/172], Loss: 16.0746\n",
      "Epoch [276/300], Step [99/172], Loss: 16.8422\n",
      "Epoch [276/300], Step [100/172], Loss: 14.6543\n",
      "Epoch [276/300], Step [101/172], Loss: 17.2430\n",
      "Epoch [276/300], Step [102/172], Loss: 17.1718\n",
      "Epoch [276/300], Step [103/172], Loss: 10.0616\n",
      "Epoch [276/300], Step [104/172], Loss: 17.6782\n",
      "Epoch [276/300], Step [105/172], Loss: 20.3431\n",
      "Epoch [276/300], Step [106/172], Loss: 14.0491\n",
      "Epoch [276/300], Step [107/172], Loss: 14.8528\n",
      "Epoch [276/300], Step [108/172], Loss: 13.9149\n",
      "Epoch [276/300], Step [109/172], Loss: 13.9384\n",
      "Epoch [276/300], Step [110/172], Loss: 15.4012\n",
      "Epoch [276/300], Step [111/172], Loss: 16.3324\n",
      "Epoch [276/300], Step [112/172], Loss: 15.0399\n",
      "Epoch [276/300], Step [113/172], Loss: 12.8743\n",
      "Epoch [276/300], Step [114/172], Loss: 14.5647\n",
      "Epoch [276/300], Step [115/172], Loss: 19.2756\n",
      "Epoch [276/300], Step [116/172], Loss: 13.1237\n",
      "Epoch [276/300], Step [117/172], Loss: 11.8211\n",
      "Epoch [276/300], Step [118/172], Loss: 13.1182\n",
      "Epoch [276/300], Step [119/172], Loss: 17.3807\n",
      "Epoch [276/300], Step [120/172], Loss: 9.5374\n",
      "Epoch [276/300], Step [121/172], Loss: 8.4154\n",
      "Epoch [276/300], Step [122/172], Loss: 11.4555\n",
      "Epoch [276/300], Step [123/172], Loss: 10.8502\n",
      "Epoch [276/300], Step [124/172], Loss: 7.3105\n",
      "Epoch [276/300], Step [125/172], Loss: 11.7975\n",
      "Epoch [276/300], Step [126/172], Loss: 11.6025\n",
      "Epoch [276/300], Step [127/172], Loss: 10.7703\n",
      "Epoch [276/300], Step [128/172], Loss: 9.6193\n",
      "Epoch [276/300], Step [129/172], Loss: 8.6723\n",
      "Epoch [276/300], Step [130/172], Loss: 12.6972\n",
      "Epoch [276/300], Step [131/172], Loss: 7.2665\n",
      "Epoch [276/300], Step [132/172], Loss: 9.0636\n",
      "Epoch [276/300], Step [133/172], Loss: 9.2164\n",
      "Epoch [276/300], Step [134/172], Loss: 10.3660\n",
      "Epoch [276/300], Step [135/172], Loss: 8.9980\n",
      "Epoch [276/300], Step [136/172], Loss: 8.0076\n",
      "Epoch [276/300], Step [137/172], Loss: 8.1536\n",
      "Epoch [276/300], Step [138/172], Loss: 7.1132\n",
      "Epoch [276/300], Step [139/172], Loss: 11.4261\n",
      "Epoch [276/300], Step [140/172], Loss: 10.7864\n",
      "Epoch [276/300], Step [141/172], Loss: 8.3083\n",
      "Epoch [276/300], Step [142/172], Loss: 14.5682\n",
      "Epoch [276/300], Step [143/172], Loss: 11.5593\n",
      "Epoch [276/300], Step [144/172], Loss: 8.8837\n",
      "Epoch [276/300], Step [145/172], Loss: 10.3654\n",
      "Epoch [276/300], Step [146/172], Loss: 9.8980\n",
      "Epoch [276/300], Step [147/172], Loss: 5.4365\n",
      "Epoch [276/300], Step [148/172], Loss: 5.8685\n",
      "Epoch [276/300], Step [149/172], Loss: 5.6719\n",
      "Epoch [276/300], Step [150/172], Loss: 5.0763\n",
      "Epoch [276/300], Step [151/172], Loss: 5.0818\n",
      "Epoch [276/300], Step [152/172], Loss: 7.1957\n",
      "Epoch [276/300], Step [153/172], Loss: 6.0639\n",
      "Epoch [276/300], Step [154/172], Loss: 6.2537\n",
      "Epoch [276/300], Step [155/172], Loss: 6.2583\n",
      "Epoch [276/300], Step [156/172], Loss: 14.2458\n",
      "Epoch [276/300], Step [157/172], Loss: 9.3346\n",
      "Epoch [276/300], Step [158/172], Loss: 7.0667\n",
      "Epoch [276/300], Step [159/172], Loss: 9.7634\n",
      "Epoch [276/300], Step [160/172], Loss: 10.0773\n",
      "Epoch [276/300], Step [161/172], Loss: 5.3394\n",
      "Epoch [276/300], Step [162/172], Loss: 4.0844\n",
      "Epoch [276/300], Step [163/172], Loss: 7.1342\n",
      "Epoch [276/300], Step [164/172], Loss: 8.1494\n",
      "Epoch [276/300], Step [165/172], Loss: 6.6568\n",
      "Epoch [276/300], Step [166/172], Loss: 5.8355\n",
      "Epoch [276/300], Step [167/172], Loss: 11.1158\n",
      "Epoch [276/300], Step [168/172], Loss: 5.9730\n",
      "Epoch [276/300], Step [169/172], Loss: 6.2691\n",
      "Epoch [276/300], Step [170/172], Loss: 4.8444\n",
      "Epoch [276/300], Step [171/172], Loss: 8.8343\n",
      "Epoch [276/300], Step [172/172], Loss: 5.3307\n",
      "Epoch [277/300], Step [1/172], Loss: 38.2570\n",
      "Epoch [277/300], Step [2/172], Loss: 41.2625\n",
      "Epoch [277/300], Step [3/172], Loss: 38.7845\n",
      "Epoch [277/300], Step [4/172], Loss: 17.7534\n",
      "Epoch [277/300], Step [5/172], Loss: 33.7471\n",
      "Epoch [277/300], Step [6/172], Loss: 17.2664\n",
      "Epoch [277/300], Step [7/172], Loss: 24.6801\n",
      "Epoch [277/300], Step [8/172], Loss: 3.5018\n",
      "Epoch [277/300], Step [9/172], Loss: 24.0579\n",
      "Epoch [277/300], Step [10/172], Loss: 34.9280\n",
      "Epoch [277/300], Step [11/172], Loss: 49.6846\n",
      "Epoch [277/300], Step [12/172], Loss: 47.1733\n",
      "Epoch [277/300], Step [13/172], Loss: 29.5856\n",
      "Epoch [277/300], Step [14/172], Loss: 49.5818\n",
      "Epoch [277/300], Step [15/172], Loss: 48.7719\n",
      "Epoch [277/300], Step [16/172], Loss: 8.8593\n",
      "Epoch [277/300], Step [17/172], Loss: 34.0634\n",
      "Epoch [277/300], Step [18/172], Loss: 49.4239\n",
      "Epoch [277/300], Step [19/172], Loss: 68.0076\n",
      "Epoch [277/300], Step [20/172], Loss: 25.4821\n",
      "Epoch [277/300], Step [21/172], Loss: 69.7968\n",
      "Epoch [277/300], Step [22/172], Loss: 47.5362\n",
      "Epoch [277/300], Step [23/172], Loss: 1.7134\n",
      "Epoch [277/300], Step [24/172], Loss: 43.2398\n",
      "Epoch [277/300], Step [25/172], Loss: 30.4102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [277/300], Step [26/172], Loss: 40.9463\n",
      "Epoch [277/300], Step [27/172], Loss: 53.9551\n",
      "Epoch [277/300], Step [28/172], Loss: 14.4342\n",
      "Epoch [277/300], Step [29/172], Loss: 13.2529\n",
      "Epoch [277/300], Step [30/172], Loss: 43.0351\n",
      "Epoch [277/300], Step [31/172], Loss: 25.0141\n",
      "Epoch [277/300], Step [32/172], Loss: 39.5666\n",
      "Epoch [277/300], Step [33/172], Loss: 58.8590\n",
      "Epoch [277/300], Step [34/172], Loss: 1.5888\n",
      "Epoch [277/300], Step [35/172], Loss: 14.3679\n",
      "Epoch [277/300], Step [36/172], Loss: 13.7582\n",
      "Epoch [277/300], Step [37/172], Loss: 13.3639\n",
      "Epoch [277/300], Step [38/172], Loss: 29.1497\n",
      "Epoch [277/300], Step [39/172], Loss: 31.4890\n",
      "Epoch [277/300], Step [40/172], Loss: 18.7939\n",
      "Epoch [277/300], Step [41/172], Loss: 28.1257\n",
      "Epoch [277/300], Step [42/172], Loss: 33.9376\n",
      "Epoch [277/300], Step [43/172], Loss: 24.3073\n",
      "Epoch [277/300], Step [44/172], Loss: 20.5183\n",
      "Epoch [277/300], Step [45/172], Loss: 26.4202\n",
      "Epoch [277/300], Step [46/172], Loss: 15.0844\n",
      "Epoch [277/300], Step [47/172], Loss: 45.6384\n",
      "Epoch [277/300], Step [48/172], Loss: 63.9039\n",
      "Epoch [277/300], Step [49/172], Loss: 21.4948\n",
      "Epoch [277/300], Step [50/172], Loss: 49.9087\n",
      "Epoch [277/300], Step [51/172], Loss: 7.2199\n",
      "Epoch [277/300], Step [52/172], Loss: 19.9722\n",
      "Epoch [277/300], Step [53/172], Loss: 22.3479\n",
      "Epoch [277/300], Step [54/172], Loss: 13.4995\n",
      "Epoch [277/300], Step [55/172], Loss: 14.1593\n",
      "Epoch [277/300], Step [56/172], Loss: 16.2430\n",
      "Epoch [277/300], Step [57/172], Loss: 17.5306\n",
      "Epoch [277/300], Step [58/172], Loss: 13.3981\n",
      "Epoch [277/300], Step [59/172], Loss: 28.1236\n",
      "Epoch [277/300], Step [60/172], Loss: 24.2400\n",
      "Epoch [277/300], Step [61/172], Loss: 5.6756\n",
      "Epoch [277/300], Step [62/172], Loss: 15.9843\n",
      "Epoch [277/300], Step [63/172], Loss: 9.0635\n",
      "Epoch [277/300], Step [64/172], Loss: 11.5029\n",
      "Epoch [277/300], Step [65/172], Loss: 19.0178\n",
      "Epoch [277/300], Step [66/172], Loss: 7.4546\n",
      "Epoch [277/300], Step [67/172], Loss: 26.7410\n",
      "Epoch [277/300], Step [68/172], Loss: 5.1356\n",
      "Epoch [277/300], Step [69/172], Loss: 29.9919\n",
      "Epoch [277/300], Step [70/172], Loss: 29.4981\n",
      "Epoch [277/300], Step [71/172], Loss: 32.3497\n",
      "Epoch [277/300], Step [72/172], Loss: 30.8205\n",
      "Epoch [277/300], Step [73/172], Loss: 39.1805\n",
      "Epoch [277/300], Step [74/172], Loss: 21.0710\n",
      "Epoch [277/300], Step [75/172], Loss: 20.5287\n",
      "Epoch [277/300], Step [76/172], Loss: 22.8642\n",
      "Epoch [277/300], Step [77/172], Loss: 42.0652\n",
      "Epoch [277/300], Step [78/172], Loss: 29.8089\n",
      "Epoch [277/300], Step [79/172], Loss: 26.8770\n",
      "Epoch [277/300], Step [80/172], Loss: 44.7980\n",
      "Epoch [277/300], Step [81/172], Loss: 24.8557\n",
      "Epoch [277/300], Step [82/172], Loss: 33.8539\n",
      "Epoch [277/300], Step [83/172], Loss: 37.5810\n",
      "Epoch [277/300], Step [84/172], Loss: 27.6901\n",
      "Epoch [277/300], Step [85/172], Loss: 31.9268\n",
      "Epoch [277/300], Step [86/172], Loss: 27.5441\n",
      "Epoch [277/300], Step [87/172], Loss: 20.4882\n",
      "Epoch [277/300], Step [88/172], Loss: 19.2282\n",
      "Epoch [277/300], Step [89/172], Loss: 23.6344\n",
      "Epoch [277/300], Step [90/172], Loss: 17.2311\n",
      "Epoch [277/300], Step [91/172], Loss: 23.4726\n",
      "Epoch [277/300], Step [92/172], Loss: 17.3697\n",
      "Epoch [277/300], Step [93/172], Loss: 17.0800\n",
      "Epoch [277/300], Step [94/172], Loss: 22.7981\n",
      "Epoch [277/300], Step [95/172], Loss: 17.0340\n",
      "Epoch [277/300], Step [96/172], Loss: 19.0211\n",
      "Epoch [277/300], Step [97/172], Loss: 27.6057\n",
      "Epoch [277/300], Step [98/172], Loss: 15.9623\n",
      "Epoch [277/300], Step [99/172], Loss: 16.7331\n",
      "Epoch [277/300], Step [100/172], Loss: 14.4368\n",
      "Epoch [277/300], Step [101/172], Loss: 17.1364\n",
      "Epoch [277/300], Step [102/172], Loss: 16.9563\n",
      "Epoch [277/300], Step [103/172], Loss: 9.9946\n",
      "Epoch [277/300], Step [104/172], Loss: 17.6289\n",
      "Epoch [277/300], Step [105/172], Loss: 20.0014\n",
      "Epoch [277/300], Step [106/172], Loss: 14.0021\n",
      "Epoch [277/300], Step [107/172], Loss: 14.8655\n",
      "Epoch [277/300], Step [108/172], Loss: 13.8912\n",
      "Epoch [277/300], Step [109/172], Loss: 13.8759\n",
      "Epoch [277/300], Step [110/172], Loss: 15.2622\n",
      "Epoch [277/300], Step [111/172], Loss: 16.3822\n",
      "Epoch [277/300], Step [112/172], Loss: 14.9799\n",
      "Epoch [277/300], Step [113/172], Loss: 12.9139\n",
      "Epoch [277/300], Step [114/172], Loss: 14.4923\n",
      "Epoch [277/300], Step [115/172], Loss: 19.2109\n",
      "Epoch [277/300], Step [116/172], Loss: 13.0788\n",
      "Epoch [277/300], Step [117/172], Loss: 11.7423\n",
      "Epoch [277/300], Step [118/172], Loss: 13.2278\n",
      "Epoch [277/300], Step [119/172], Loss: 17.3915\n",
      "Epoch [277/300], Step [120/172], Loss: 9.4126\n",
      "Epoch [277/300], Step [121/172], Loss: 8.3326\n",
      "Epoch [277/300], Step [122/172], Loss: 11.4026\n",
      "Epoch [277/300], Step [123/172], Loss: 10.6469\n",
      "Epoch [277/300], Step [124/172], Loss: 7.2480\n",
      "Epoch [277/300], Step [125/172], Loss: 11.7021\n",
      "Epoch [277/300], Step [126/172], Loss: 11.5365\n",
      "Epoch [277/300], Step [127/172], Loss: 10.8430\n",
      "Epoch [277/300], Step [128/172], Loss: 9.7188\n",
      "Epoch [277/300], Step [129/172], Loss: 8.7136\n",
      "Epoch [277/300], Step [130/172], Loss: 12.7743\n",
      "Epoch [277/300], Step [131/172], Loss: 7.2599\n",
      "Epoch [277/300], Step [132/172], Loss: 9.0866\n",
      "Epoch [277/300], Step [133/172], Loss: 9.2279\n",
      "Epoch [277/300], Step [134/172], Loss: 10.4191\n",
      "Epoch [277/300], Step [135/172], Loss: 9.0109\n",
      "Epoch [277/300], Step [136/172], Loss: 7.8959\n",
      "Epoch [277/300], Step [137/172], Loss: 8.0297\n",
      "Epoch [277/300], Step [138/172], Loss: 7.0077\n",
      "Epoch [277/300], Step [139/172], Loss: 11.3811\n",
      "Epoch [277/300], Step [140/172], Loss: 10.8021\n",
      "Epoch [277/300], Step [141/172], Loss: 8.2746\n",
      "Epoch [277/300], Step [142/172], Loss: 14.5761\n",
      "Epoch [277/300], Step [143/172], Loss: 11.6196\n",
      "Epoch [277/300], Step [144/172], Loss: 8.8677\n",
      "Epoch [277/300], Step [145/172], Loss: 10.3948\n",
      "Epoch [277/300], Step [146/172], Loss: 9.8698\n",
      "Epoch [277/300], Step [147/172], Loss: 5.4302\n",
      "Epoch [277/300], Step [148/172], Loss: 5.8760\n",
      "Epoch [277/300], Step [149/172], Loss: 5.6470\n",
      "Epoch [277/300], Step [150/172], Loss: 5.0466\n",
      "Epoch [277/300], Step [151/172], Loss: 5.0546\n",
      "Epoch [277/300], Step [152/172], Loss: 7.2484\n",
      "Epoch [277/300], Step [153/172], Loss: 6.1025\n",
      "Epoch [277/300], Step [154/172], Loss: 6.2499\n",
      "Epoch [277/300], Step [155/172], Loss: 6.2358\n",
      "Epoch [277/300], Step [156/172], Loss: 14.2425\n",
      "Epoch [277/300], Step [157/172], Loss: 9.3811\n",
      "Epoch [277/300], Step [158/172], Loss: 7.0755\n",
      "Epoch [277/300], Step [159/172], Loss: 9.6822\n",
      "Epoch [277/300], Step [160/172], Loss: 10.1477\n",
      "Epoch [277/300], Step [161/172], Loss: 5.2168\n",
      "Epoch [277/300], Step [162/172], Loss: 4.1047\n",
      "Epoch [277/300], Step [163/172], Loss: 7.0139\n",
      "Epoch [277/300], Step [164/172], Loss: 7.9816\n",
      "Epoch [277/300], Step [165/172], Loss: 6.6732\n",
      "Epoch [277/300], Step [166/172], Loss: 5.7905\n",
      "Epoch [277/300], Step [167/172], Loss: 11.1124\n",
      "Epoch [277/300], Step [168/172], Loss: 5.8858\n",
      "Epoch [277/300], Step [169/172], Loss: 6.3439\n",
      "Epoch [277/300], Step [170/172], Loss: 4.8501\n",
      "Epoch [277/300], Step [171/172], Loss: 8.8263\n",
      "Epoch [277/300], Step [172/172], Loss: 5.3509\n",
      "Epoch [278/300], Step [1/172], Loss: 38.0592\n",
      "Epoch [278/300], Step [2/172], Loss: 40.7934\n",
      "Epoch [278/300], Step [3/172], Loss: 38.6773\n",
      "Epoch [278/300], Step [4/172], Loss: 17.5844\n",
      "Epoch [278/300], Step [5/172], Loss: 33.8969\n",
      "Epoch [278/300], Step [6/172], Loss: 17.0056\n",
      "Epoch [278/300], Step [7/172], Loss: 24.7411\n",
      "Epoch [278/300], Step [8/172], Loss: 4.1760\n",
      "Epoch [278/300], Step [9/172], Loss: 24.2484\n",
      "Epoch [278/300], Step [10/172], Loss: 34.6018\n",
      "Epoch [278/300], Step [11/172], Loss: 49.7301\n",
      "Epoch [278/300], Step [12/172], Loss: 47.1353\n",
      "Epoch [278/300], Step [13/172], Loss: 29.7965\n",
      "Epoch [278/300], Step [14/172], Loss: 49.7182\n",
      "Epoch [278/300], Step [15/172], Loss: 48.5661\n",
      "Epoch [278/300], Step [16/172], Loss: 7.8204\n",
      "Epoch [278/300], Step [17/172], Loss: 34.1847\n",
      "Epoch [278/300], Step [18/172], Loss: 49.3575\n",
      "Epoch [278/300], Step [19/172], Loss: 67.7788\n",
      "Epoch [278/300], Step [20/172], Loss: 25.4077\n",
      "Epoch [278/300], Step [21/172], Loss: 69.7709\n",
      "Epoch [278/300], Step [22/172], Loss: 46.7345\n",
      "Epoch [278/300], Step [23/172], Loss: 1.8502\n",
      "Epoch [278/300], Step [24/172], Loss: 43.1183\n",
      "Epoch [278/300], Step [25/172], Loss: 30.2547\n",
      "Epoch [278/300], Step [26/172], Loss: 40.6141\n",
      "Epoch [278/300], Step [27/172], Loss: 53.8128\n",
      "Epoch [278/300], Step [28/172], Loss: 14.6910\n",
      "Epoch [278/300], Step [29/172], Loss: 13.5395\n",
      "Epoch [278/300], Step [30/172], Loss: 42.9009\n",
      "Epoch [278/300], Step [31/172], Loss: 25.2473\n",
      "Epoch [278/300], Step [32/172], Loss: 39.8088\n",
      "Epoch [278/300], Step [33/172], Loss: 59.0542\n",
      "Epoch [278/300], Step [34/172], Loss: 1.6333\n",
      "Epoch [278/300], Step [35/172], Loss: 14.1829\n",
      "Epoch [278/300], Step [36/172], Loss: 14.0130\n",
      "Epoch [278/300], Step [37/172], Loss: 13.3858\n",
      "Epoch [278/300], Step [38/172], Loss: 28.9411\n",
      "Epoch [278/300], Step [39/172], Loss: 31.4246\n",
      "Epoch [278/300], Step [40/172], Loss: 18.8043\n",
      "Epoch [278/300], Step [41/172], Loss: 27.9585\n",
      "Epoch [278/300], Step [42/172], Loss: 33.8099\n",
      "Epoch [278/300], Step [43/172], Loss: 24.1449\n",
      "Epoch [278/300], Step [44/172], Loss: 20.3129\n",
      "Epoch [278/300], Step [45/172], Loss: 26.0399\n",
      "Epoch [278/300], Step [46/172], Loss: 14.7185\n",
      "Epoch [278/300], Step [47/172], Loss: 45.3658\n",
      "Epoch [278/300], Step [48/172], Loss: 63.4008\n",
      "Epoch [278/300], Step [49/172], Loss: 21.2349\n",
      "Epoch [278/300], Step [50/172], Loss: 50.3835\n",
      "Epoch [278/300], Step [51/172], Loss: 7.0766\n",
      "Epoch [278/300], Step [52/172], Loss: 19.7332\n",
      "Epoch [278/300], Step [53/172], Loss: 22.3024\n",
      "Epoch [278/300], Step [54/172], Loss: 13.3390\n",
      "Epoch [278/300], Step [55/172], Loss: 14.0095\n",
      "Epoch [278/300], Step [56/172], Loss: 16.3268\n",
      "Epoch [278/300], Step [57/172], Loss: 17.9715\n",
      "Epoch [278/300], Step [58/172], Loss: 13.0932\n",
      "Epoch [278/300], Step [59/172], Loss: 27.3857\n",
      "Epoch [278/300], Step [60/172], Loss: 24.2021\n",
      "Epoch [278/300], Step [61/172], Loss: 5.5772\n",
      "Epoch [278/300], Step [62/172], Loss: 15.7397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [278/300], Step [63/172], Loss: 8.8855\n",
      "Epoch [278/300], Step [64/172], Loss: 11.3093\n",
      "Epoch [278/300], Step [65/172], Loss: 18.8040\n",
      "Epoch [278/300], Step [66/172], Loss: 7.0766\n",
      "Epoch [278/300], Step [67/172], Loss: 26.2783\n",
      "Epoch [278/300], Step [68/172], Loss: 4.6296\n",
      "Epoch [278/300], Step [69/172], Loss: 29.4763\n",
      "Epoch [278/300], Step [70/172], Loss: 29.7560\n",
      "Epoch [278/300], Step [71/172], Loss: 32.3936\n",
      "Epoch [278/300], Step [72/172], Loss: 30.8833\n",
      "Epoch [278/300], Step [73/172], Loss: 39.2833\n",
      "Epoch [278/300], Step [74/172], Loss: 20.9523\n",
      "Epoch [278/300], Step [75/172], Loss: 20.4793\n",
      "Epoch [278/300], Step [76/172], Loss: 22.8413\n",
      "Epoch [278/300], Step [77/172], Loss: 42.0746\n",
      "Epoch [278/300], Step [78/172], Loss: 29.7165\n",
      "Epoch [278/300], Step [79/172], Loss: 27.0108\n",
      "Epoch [278/300], Step [80/172], Loss: 44.9947\n",
      "Epoch [278/300], Step [81/172], Loss: 24.7843\n",
      "Epoch [278/300], Step [82/172], Loss: 34.1920\n",
      "Epoch [278/300], Step [83/172], Loss: 37.4103\n",
      "Epoch [278/300], Step [84/172], Loss: 27.6067\n",
      "Epoch [278/300], Step [85/172], Loss: 31.9200\n",
      "Epoch [278/300], Step [86/172], Loss: 27.5099\n",
      "Epoch [278/300], Step [87/172], Loss: 20.3836\n",
      "Epoch [278/300], Step [88/172], Loss: 19.0951\n",
      "Epoch [278/300], Step [89/172], Loss: 23.6070\n",
      "Epoch [278/300], Step [90/172], Loss: 17.1680\n",
      "Epoch [278/300], Step [91/172], Loss: 23.3295\n",
      "Epoch [278/300], Step [92/172], Loss: 17.2782\n",
      "Epoch [278/300], Step [93/172], Loss: 16.9962\n",
      "Epoch [278/300], Step [94/172], Loss: 22.6657\n",
      "Epoch [278/300], Step [95/172], Loss: 16.7741\n",
      "Epoch [278/300], Step [96/172], Loss: 19.0282\n",
      "Epoch [278/300], Step [97/172], Loss: 27.4699\n",
      "Epoch [278/300], Step [98/172], Loss: 15.8491\n",
      "Epoch [278/300], Step [99/172], Loss: 16.6326\n",
      "Epoch [278/300], Step [100/172], Loss: 14.2905\n",
      "Epoch [278/300], Step [101/172], Loss: 16.9960\n",
      "Epoch [278/300], Step [102/172], Loss: 16.9959\n",
      "Epoch [278/300], Step [103/172], Loss: 9.8461\n",
      "Epoch [278/300], Step [104/172], Loss: 17.6777\n",
      "Epoch [278/300], Step [105/172], Loss: 20.0258\n",
      "Epoch [278/300], Step [106/172], Loss: 13.8808\n",
      "Epoch [278/300], Step [107/172], Loss: 14.7231\n",
      "Epoch [278/300], Step [108/172], Loss: 13.7383\n",
      "Epoch [278/300], Step [109/172], Loss: 13.7780\n",
      "Epoch [278/300], Step [110/172], Loss: 15.0825\n",
      "Epoch [278/300], Step [111/172], Loss: 16.2112\n",
      "Epoch [278/300], Step [112/172], Loss: 14.9239\n",
      "Epoch [278/300], Step [113/172], Loss: 12.7097\n",
      "Epoch [278/300], Step [114/172], Loss: 14.5493\n",
      "Epoch [278/300], Step [115/172], Loss: 19.0979\n",
      "Epoch [278/300], Step [116/172], Loss: 13.0028\n",
      "Epoch [278/300], Step [117/172], Loss: 11.6944\n",
      "Epoch [278/300], Step [118/172], Loss: 12.9987\n",
      "Epoch [278/300], Step [119/172], Loss: 17.2321\n",
      "Epoch [278/300], Step [120/172], Loss: 9.3399\n",
      "Epoch [278/300], Step [121/172], Loss: 8.3757\n",
      "Epoch [278/300], Step [122/172], Loss: 11.2831\n",
      "Epoch [278/300], Step [123/172], Loss: 10.7476\n",
      "Epoch [278/300], Step [124/172], Loss: 7.3186\n",
      "Epoch [278/300], Step [125/172], Loss: 11.6686\n",
      "Epoch [278/300], Step [126/172], Loss: 11.5687\n",
      "Epoch [278/300], Step [127/172], Loss: 10.6883\n",
      "Epoch [278/300], Step [128/172], Loss: 9.5533\n",
      "Epoch [278/300], Step [129/172], Loss: 8.6584\n",
      "Epoch [278/300], Step [130/172], Loss: 12.6720\n",
      "Epoch [278/300], Step [131/172], Loss: 7.2710\n",
      "Epoch [278/300], Step [132/172], Loss: 9.0307\n",
      "Epoch [278/300], Step [133/172], Loss: 9.0757\n",
      "Epoch [278/300], Step [134/172], Loss: 10.2968\n",
      "Epoch [278/300], Step [135/172], Loss: 8.9641\n",
      "Epoch [278/300], Step [136/172], Loss: 7.9340\n",
      "Epoch [278/300], Step [137/172], Loss: 7.9909\n",
      "Epoch [278/300], Step [138/172], Loss: 7.0530\n",
      "Epoch [278/300], Step [139/172], Loss: 11.3478\n",
      "Epoch [278/300], Step [140/172], Loss: 10.7971\n",
      "Epoch [278/300], Step [141/172], Loss: 8.2610\n",
      "Epoch [278/300], Step [142/172], Loss: 14.4030\n",
      "Epoch [278/300], Step [143/172], Loss: 11.5045\n",
      "Epoch [278/300], Step [144/172], Loss: 8.8848\n",
      "Epoch [278/300], Step [145/172], Loss: 10.2944\n",
      "Epoch [278/300], Step [146/172], Loss: 9.8567\n",
      "Epoch [278/300], Step [147/172], Loss: 5.4937\n",
      "Epoch [278/300], Step [148/172], Loss: 5.8719\n",
      "Epoch [278/300], Step [149/172], Loss: 5.6680\n",
      "Epoch [278/300], Step [150/172], Loss: 5.0430\n",
      "Epoch [278/300], Step [151/172], Loss: 5.0324\n",
      "Epoch [278/300], Step [152/172], Loss: 7.2097\n",
      "Epoch [278/300], Step [153/172], Loss: 6.0980\n",
      "Epoch [278/300], Step [154/172], Loss: 6.1997\n",
      "Epoch [278/300], Step [155/172], Loss: 6.3473\n",
      "Epoch [278/300], Step [156/172], Loss: 14.1128\n",
      "Epoch [278/300], Step [157/172], Loss: 9.2676\n",
      "Epoch [278/300], Step [158/172], Loss: 7.0039\n",
      "Epoch [278/300], Step [159/172], Loss: 9.7835\n",
      "Epoch [278/300], Step [160/172], Loss: 9.9789\n",
      "Epoch [278/300], Step [161/172], Loss: 5.2662\n",
      "Epoch [278/300], Step [162/172], Loss: 4.0478\n",
      "Epoch [278/300], Step [163/172], Loss: 7.0080\n",
      "Epoch [278/300], Step [164/172], Loss: 8.0295\n",
      "Epoch [278/300], Step [165/172], Loss: 6.6929\n",
      "Epoch [278/300], Step [166/172], Loss: 5.7831\n",
      "Epoch [278/300], Step [167/172], Loss: 11.1370\n",
      "Epoch [278/300], Step [168/172], Loss: 5.8872\n",
      "Epoch [278/300], Step [169/172], Loss: 6.1922\n",
      "Epoch [278/300], Step [170/172], Loss: 4.7943\n",
      "Epoch [278/300], Step [171/172], Loss: 8.8736\n",
      "Epoch [278/300], Step [172/172], Loss: 5.4215\n",
      "Epoch [279/300], Step [1/172], Loss: 38.0243\n",
      "Epoch [279/300], Step [2/172], Loss: 41.5104\n",
      "Epoch [279/300], Step [3/172], Loss: 38.9060\n",
      "Epoch [279/300], Step [4/172], Loss: 17.6402\n",
      "Epoch [279/300], Step [5/172], Loss: 33.5439\n",
      "Epoch [279/300], Step [6/172], Loss: 17.2096\n",
      "Epoch [279/300], Step [7/172], Loss: 25.0337\n",
      "Epoch [279/300], Step [8/172], Loss: 3.5997\n",
      "Epoch [279/300], Step [9/172], Loss: 23.9358\n",
      "Epoch [279/300], Step [10/172], Loss: 34.6318\n",
      "Epoch [279/300], Step [11/172], Loss: 49.7278\n",
      "Epoch [279/300], Step [12/172], Loss: 47.0811\n",
      "Epoch [279/300], Step [13/172], Loss: 29.7531\n",
      "Epoch [279/300], Step [14/172], Loss: 49.5551\n",
      "Epoch [279/300], Step [15/172], Loss: 48.7539\n",
      "Epoch [279/300], Step [16/172], Loss: 7.9186\n",
      "Epoch [279/300], Step [17/172], Loss: 34.1039\n",
      "Epoch [279/300], Step [18/172], Loss: 49.6988\n",
      "Epoch [279/300], Step [19/172], Loss: 67.8346\n",
      "Epoch [279/300], Step [20/172], Loss: 25.5743\n",
      "Epoch [279/300], Step [21/172], Loss: 68.9450\n",
      "Epoch [279/300], Step [22/172], Loss: 46.8764\n",
      "Epoch [279/300], Step [23/172], Loss: 1.6775\n",
      "Epoch [279/300], Step [24/172], Loss: 43.1901\n",
      "Epoch [279/300], Step [25/172], Loss: 29.9028\n",
      "Epoch [279/300], Step [26/172], Loss: 40.7089\n",
      "Epoch [279/300], Step [27/172], Loss: 54.3059\n",
      "Epoch [279/300], Step [28/172], Loss: 14.4909\n",
      "Epoch [279/300], Step [29/172], Loss: 13.2927\n",
      "Epoch [279/300], Step [30/172], Loss: 42.6384\n",
      "Epoch [279/300], Step [31/172], Loss: 24.8714\n",
      "Epoch [279/300], Step [32/172], Loss: 39.4718\n",
      "Epoch [279/300], Step [33/172], Loss: 58.4896\n",
      "Epoch [279/300], Step [34/172], Loss: 1.6135\n",
      "Epoch [279/300], Step [35/172], Loss: 14.2924\n",
      "Epoch [279/300], Step [36/172], Loss: 13.7970\n",
      "Epoch [279/300], Step [37/172], Loss: 13.3113\n",
      "Epoch [279/300], Step [38/172], Loss: 28.8208\n",
      "Epoch [279/300], Step [39/172], Loss: 31.4053\n",
      "Epoch [279/300], Step [40/172], Loss: 18.7401\n",
      "Epoch [279/300], Step [41/172], Loss: 27.8502\n",
      "Epoch [279/300], Step [42/172], Loss: 33.6505\n",
      "Epoch [279/300], Step [43/172], Loss: 24.1890\n",
      "Epoch [279/300], Step [44/172], Loss: 20.2565\n",
      "Epoch [279/300], Step [45/172], Loss: 26.0802\n",
      "Epoch [279/300], Step [46/172], Loss: 14.7694\n",
      "Epoch [279/300], Step [47/172], Loss: 45.3763\n",
      "Epoch [279/300], Step [48/172], Loss: 63.8776\n",
      "Epoch [279/300], Step [49/172], Loss: 21.4018\n",
      "Epoch [279/300], Step [50/172], Loss: 50.2276\n",
      "Epoch [279/300], Step [51/172], Loss: 7.1446\n",
      "Epoch [279/300], Step [52/172], Loss: 19.7418\n",
      "Epoch [279/300], Step [53/172], Loss: 22.2134\n",
      "Epoch [279/300], Step [54/172], Loss: 13.1234\n",
      "Epoch [279/300], Step [55/172], Loss: 13.9118\n",
      "Epoch [279/300], Step [56/172], Loss: 16.4574\n",
      "Epoch [279/300], Step [57/172], Loss: 17.8291\n",
      "Epoch [279/300], Step [58/172], Loss: 13.0172\n",
      "Epoch [279/300], Step [59/172], Loss: 27.5788\n",
      "Epoch [279/300], Step [60/172], Loss: 24.5038\n",
      "Epoch [279/300], Step [61/172], Loss: 5.4964\n",
      "Epoch [279/300], Step [62/172], Loss: 15.7526\n",
      "Epoch [279/300], Step [63/172], Loss: 8.7142\n",
      "Epoch [279/300], Step [64/172], Loss: 11.1851\n",
      "Epoch [279/300], Step [65/172], Loss: 18.6827\n",
      "Epoch [279/300], Step [66/172], Loss: 7.1761\n",
      "Epoch [279/300], Step [67/172], Loss: 26.4536\n",
      "Epoch [279/300], Step [68/172], Loss: 4.6860\n",
      "Epoch [279/300], Step [69/172], Loss: 29.8657\n",
      "Epoch [279/300], Step [70/172], Loss: 29.6527\n",
      "Epoch [279/300], Step [71/172], Loss: 32.4549\n",
      "Epoch [279/300], Step [72/172], Loss: 30.9023\n",
      "Epoch [279/300], Step [73/172], Loss: 39.4501\n",
      "Epoch [279/300], Step [74/172], Loss: 21.0319\n",
      "Epoch [279/300], Step [75/172], Loss: 20.2573\n",
      "Epoch [279/300], Step [76/172], Loss: 22.6586\n",
      "Epoch [279/300], Step [77/172], Loss: 41.8401\n",
      "Epoch [279/300], Step [78/172], Loss: 29.6897\n",
      "Epoch [279/300], Step [79/172], Loss: 26.8309\n",
      "Epoch [279/300], Step [80/172], Loss: 44.5909\n",
      "Epoch [279/300], Step [81/172], Loss: 24.7428\n",
      "Epoch [279/300], Step [82/172], Loss: 33.7892\n",
      "Epoch [279/300], Step [83/172], Loss: 37.1330\n",
      "Epoch [279/300], Step [84/172], Loss: 27.3394\n",
      "Epoch [279/300], Step [85/172], Loss: 31.7210\n",
      "Epoch [279/300], Step [86/172], Loss: 27.1950\n",
      "Epoch [279/300], Step [87/172], Loss: 20.2559\n",
      "Epoch [279/300], Step [88/172], Loss: 18.9373\n",
      "Epoch [279/300], Step [89/172], Loss: 23.4196\n",
      "Epoch [279/300], Step [90/172], Loss: 17.0157\n",
      "Epoch [279/300], Step [91/172], Loss: 23.0930\n",
      "Epoch [279/300], Step [92/172], Loss: 17.2004\n",
      "Epoch [279/300], Step [93/172], Loss: 16.8397\n",
      "Epoch [279/300], Step [94/172], Loss: 22.4358\n",
      "Epoch [279/300], Step [95/172], Loss: 16.7165\n",
      "Epoch [279/300], Step [96/172], Loss: 18.8724\n",
      "Epoch [279/300], Step [97/172], Loss: 27.3231\n",
      "Epoch [279/300], Step [98/172], Loss: 15.6533\n",
      "Epoch [279/300], Step [99/172], Loss: 16.4740\n",
      "Epoch [279/300], Step [100/172], Loss: 14.0910\n",
      "Epoch [279/300], Step [101/172], Loss: 16.8052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [279/300], Step [102/172], Loss: 16.7747\n",
      "Epoch [279/300], Step [103/172], Loss: 9.7474\n",
      "Epoch [279/300], Step [104/172], Loss: 17.3999\n",
      "Epoch [279/300], Step [105/172], Loss: 19.6720\n",
      "Epoch [279/300], Step [106/172], Loss: 13.7950\n",
      "Epoch [279/300], Step [107/172], Loss: 14.6500\n",
      "Epoch [279/300], Step [108/172], Loss: 13.5832\n",
      "Epoch [279/300], Step [109/172], Loss: 13.7682\n",
      "Epoch [279/300], Step [110/172], Loss: 15.0028\n",
      "Epoch [279/300], Step [111/172], Loss: 16.1844\n",
      "Epoch [279/300], Step [112/172], Loss: 14.7504\n",
      "Epoch [279/300], Step [113/172], Loss: 12.6723\n",
      "Epoch [279/300], Step [114/172], Loss: 14.4604\n",
      "Epoch [279/300], Step [115/172], Loss: 19.0986\n",
      "Epoch [279/300], Step [116/172], Loss: 12.8838\n",
      "Epoch [279/300], Step [117/172], Loss: 11.5893\n",
      "Epoch [279/300], Step [118/172], Loss: 13.0954\n",
      "Epoch [279/300], Step [119/172], Loss: 17.1387\n",
      "Epoch [279/300], Step [120/172], Loss: 9.2837\n",
      "Epoch [279/300], Step [121/172], Loss: 8.2790\n",
      "Epoch [279/300], Step [122/172], Loss: 11.1876\n",
      "Epoch [279/300], Step [123/172], Loss: 10.5927\n",
      "Epoch [279/300], Step [124/172], Loss: 7.2307\n",
      "Epoch [279/300], Step [125/172], Loss: 11.6224\n",
      "Epoch [279/300], Step [126/172], Loss: 11.4667\n",
      "Epoch [279/300], Step [127/172], Loss: 10.6814\n",
      "Epoch [279/300], Step [128/172], Loss: 9.5601\n",
      "Epoch [279/300], Step [129/172], Loss: 8.6437\n",
      "Epoch [279/300], Step [130/172], Loss: 12.6387\n",
      "Epoch [279/300], Step [131/172], Loss: 7.2032\n",
      "Epoch [279/300], Step [132/172], Loss: 8.9970\n",
      "Epoch [279/300], Step [133/172], Loss: 9.0522\n",
      "Epoch [279/300], Step [134/172], Loss: 10.2307\n",
      "Epoch [279/300], Step [135/172], Loss: 8.9719\n",
      "Epoch [279/300], Step [136/172], Loss: 7.7812\n",
      "Epoch [279/300], Step [137/172], Loss: 7.9850\n",
      "Epoch [279/300], Step [138/172], Loss: 6.9561\n",
      "Epoch [279/300], Step [139/172], Loss: 11.3056\n",
      "Epoch [279/300], Step [140/172], Loss: 10.7912\n",
      "Epoch [279/300], Step [141/172], Loss: 8.1672\n",
      "Epoch [279/300], Step [142/172], Loss: 14.2938\n",
      "Epoch [279/300], Step [143/172], Loss: 11.4358\n",
      "Epoch [279/300], Step [144/172], Loss: 8.8036\n",
      "Epoch [279/300], Step [145/172], Loss: 10.2786\n",
      "Epoch [279/300], Step [146/172], Loss: 9.8123\n",
      "Epoch [279/300], Step [147/172], Loss: 5.4534\n",
      "Epoch [279/300], Step [148/172], Loss: 5.8665\n",
      "Epoch [279/300], Step [149/172], Loss: 5.6012\n",
      "Epoch [279/300], Step [150/172], Loss: 5.0268\n",
      "Epoch [279/300], Step [151/172], Loss: 5.0273\n",
      "Epoch [279/300], Step [152/172], Loss: 7.1650\n",
      "Epoch [279/300], Step [153/172], Loss: 6.0489\n",
      "Epoch [279/300], Step [154/172], Loss: 6.1352\n",
      "Epoch [279/300], Step [155/172], Loss: 6.2494\n",
      "Epoch [279/300], Step [156/172], Loss: 14.0946\n",
      "Epoch [279/300], Step [157/172], Loss: 9.1440\n",
      "Epoch [279/300], Step [158/172], Loss: 6.9482\n",
      "Epoch [279/300], Step [159/172], Loss: 9.7501\n",
      "Epoch [279/300], Step [160/172], Loss: 9.8862\n",
      "Epoch [279/300], Step [161/172], Loss: 5.2063\n",
      "Epoch [279/300], Step [162/172], Loss: 4.0284\n",
      "Epoch [279/300], Step [163/172], Loss: 7.0458\n",
      "Epoch [279/300], Step [164/172], Loss: 7.9349\n",
      "Epoch [279/300], Step [165/172], Loss: 6.6763\n",
      "Epoch [279/300], Step [166/172], Loss: 5.8150\n",
      "Epoch [279/300], Step [167/172], Loss: 11.1360\n",
      "Epoch [279/300], Step [168/172], Loss: 5.9063\n",
      "Epoch [279/300], Step [169/172], Loss: 6.2263\n",
      "Epoch [279/300], Step [170/172], Loss: 4.7727\n",
      "Epoch [279/300], Step [171/172], Loss: 8.8895\n",
      "Epoch [279/300], Step [172/172], Loss: 5.3273\n",
      "Epoch [280/300], Step [1/172], Loss: 38.0064\n",
      "Epoch [280/300], Step [2/172], Loss: 41.2574\n",
      "Epoch [280/300], Step [3/172], Loss: 38.8637\n",
      "Epoch [280/300], Step [4/172], Loss: 17.5414\n",
      "Epoch [280/300], Step [5/172], Loss: 33.3314\n",
      "Epoch [280/300], Step [6/172], Loss: 17.4436\n",
      "Epoch [280/300], Step [7/172], Loss: 25.4708\n",
      "Epoch [280/300], Step [8/172], Loss: 4.1720\n",
      "Epoch [280/300], Step [9/172], Loss: 24.0861\n",
      "Epoch [280/300], Step [10/172], Loss: 34.6189\n",
      "Epoch [280/300], Step [11/172], Loss: 49.4575\n",
      "Epoch [280/300], Step [12/172], Loss: 46.9365\n",
      "Epoch [280/300], Step [13/172], Loss: 29.8257\n",
      "Epoch [280/300], Step [14/172], Loss: 49.9470\n",
      "Epoch [280/300], Step [15/172], Loss: 48.8306\n",
      "Epoch [280/300], Step [16/172], Loss: 7.5325\n",
      "Epoch [280/300], Step [17/172], Loss: 34.4137\n",
      "Epoch [280/300], Step [18/172], Loss: 49.6837\n",
      "Epoch [280/300], Step [19/172], Loss: 68.1437\n",
      "Epoch [280/300], Step [20/172], Loss: 25.2711\n",
      "Epoch [280/300], Step [21/172], Loss: 69.6065\n",
      "Epoch [280/300], Step [22/172], Loss: 46.6299\n",
      "Epoch [280/300], Step [23/172], Loss: 1.6981\n",
      "Epoch [280/300], Step [24/172], Loss: 43.1450\n",
      "Epoch [280/300], Step [25/172], Loss: 29.8630\n",
      "Epoch [280/300], Step [26/172], Loss: 40.5722\n",
      "Epoch [280/300], Step [27/172], Loss: 54.1326\n",
      "Epoch [280/300], Step [28/172], Loss: 14.6051\n",
      "Epoch [280/300], Step [29/172], Loss: 13.2353\n",
      "Epoch [280/300], Step [30/172], Loss: 42.5235\n",
      "Epoch [280/300], Step [31/172], Loss: 24.9115\n",
      "Epoch [280/300], Step [32/172], Loss: 39.7631\n",
      "Epoch [280/300], Step [33/172], Loss: 58.8855\n",
      "Epoch [280/300], Step [34/172], Loss: 1.5806\n",
      "Epoch [280/300], Step [35/172], Loss: 14.3203\n",
      "Epoch [280/300], Step [36/172], Loss: 14.3401\n",
      "Epoch [280/300], Step [37/172], Loss: 13.4658\n",
      "Epoch [280/300], Step [38/172], Loss: 29.0677\n",
      "Epoch [280/300], Step [39/172], Loss: 31.5632\n",
      "Epoch [280/300], Step [40/172], Loss: 18.8825\n",
      "Epoch [280/300], Step [41/172], Loss: 27.9346\n",
      "Epoch [280/300], Step [42/172], Loss: 34.0380\n",
      "Epoch [280/300], Step [43/172], Loss: 24.2876\n",
      "Epoch [280/300], Step [44/172], Loss: 20.5250\n",
      "Epoch [280/300], Step [45/172], Loss: 26.2549\n",
      "Epoch [280/300], Step [46/172], Loss: 14.7730\n",
      "Epoch [280/300], Step [47/172], Loss: 45.4316\n",
      "Epoch [280/300], Step [48/172], Loss: 63.3835\n",
      "Epoch [280/300], Step [49/172], Loss: 21.0511\n",
      "Epoch [280/300], Step [50/172], Loss: 50.2766\n",
      "Epoch [280/300], Step [51/172], Loss: 6.9963\n",
      "Epoch [280/300], Step [52/172], Loss: 19.6245\n",
      "Epoch [280/300], Step [53/172], Loss: 22.0312\n",
      "Epoch [280/300], Step [54/172], Loss: 12.9113\n",
      "Epoch [280/300], Step [55/172], Loss: 13.6415\n",
      "Epoch [280/300], Step [56/172], Loss: 16.2083\n",
      "Epoch [280/300], Step [57/172], Loss: 17.7369\n",
      "Epoch [280/300], Step [58/172], Loss: 12.8974\n",
      "Epoch [280/300], Step [59/172], Loss: 27.8368\n",
      "Epoch [280/300], Step [60/172], Loss: 24.3879\n",
      "Epoch [280/300], Step [61/172], Loss: 5.5026\n",
      "Epoch [280/300], Step [62/172], Loss: 15.7267\n",
      "Epoch [280/300], Step [63/172], Loss: 8.7137\n",
      "Epoch [280/300], Step [64/172], Loss: 10.9961\n",
      "Epoch [280/300], Step [65/172], Loss: 18.5111\n",
      "Epoch [280/300], Step [66/172], Loss: 6.9107\n",
      "Epoch [280/300], Step [67/172], Loss: 26.1148\n",
      "Epoch [280/300], Step [68/172], Loss: 4.5427\n",
      "Epoch [280/300], Step [69/172], Loss: 29.5535\n",
      "Epoch [280/300], Step [70/172], Loss: 29.8172\n",
      "Epoch [280/300], Step [71/172], Loss: 32.5789\n",
      "Epoch [280/300], Step [72/172], Loss: 30.9813\n",
      "Epoch [280/300], Step [73/172], Loss: 39.8115\n",
      "Epoch [280/300], Step [74/172], Loss: 21.1154\n",
      "Epoch [280/300], Step [75/172], Loss: 20.5128\n",
      "Epoch [280/300], Step [76/172], Loss: 22.7497\n",
      "Epoch [280/300], Step [77/172], Loss: 42.0808\n",
      "Epoch [280/300], Step [78/172], Loss: 29.7409\n",
      "Epoch [280/300], Step [79/172], Loss: 26.9809\n",
      "Epoch [280/300], Step [80/172], Loss: 44.7583\n",
      "Epoch [280/300], Step [81/172], Loss: 24.8479\n",
      "Epoch [280/300], Step [82/172], Loss: 33.9857\n",
      "Epoch [280/300], Step [83/172], Loss: 37.1778\n",
      "Epoch [280/300], Step [84/172], Loss: 27.3024\n",
      "Epoch [280/300], Step [85/172], Loss: 31.7191\n",
      "Epoch [280/300], Step [86/172], Loss: 27.1907\n",
      "Epoch [280/300], Step [87/172], Loss: 20.3441\n",
      "Epoch [280/300], Step [88/172], Loss: 18.9579\n",
      "Epoch [280/300], Step [89/172], Loss: 23.4728\n",
      "Epoch [280/300], Step [90/172], Loss: 16.9579\n",
      "Epoch [280/300], Step [91/172], Loss: 23.0183\n",
      "Epoch [280/300], Step [92/172], Loss: 17.1318\n",
      "Epoch [280/300], Step [93/172], Loss: 16.9631\n",
      "Epoch [280/300], Step [94/172], Loss: 22.5079\n",
      "Epoch [280/300], Step [95/172], Loss: 16.6244\n",
      "Epoch [280/300], Step [96/172], Loss: 18.9129\n",
      "Epoch [280/300], Step [97/172], Loss: 27.1752\n",
      "Epoch [280/300], Step [98/172], Loss: 15.6776\n",
      "Epoch [280/300], Step [99/172], Loss: 16.4601\n",
      "Epoch [280/300], Step [100/172], Loss: 14.0111\n",
      "Epoch [280/300], Step [101/172], Loss: 16.8641\n",
      "Epoch [280/300], Step [102/172], Loss: 16.8638\n",
      "Epoch [280/300], Step [103/172], Loss: 9.7801\n",
      "Epoch [280/300], Step [104/172], Loss: 17.4885\n",
      "Epoch [280/300], Step [105/172], Loss: 19.8552\n",
      "Epoch [280/300], Step [106/172], Loss: 13.7524\n",
      "Epoch [280/300], Step [107/172], Loss: 14.5801\n",
      "Epoch [280/300], Step [108/172], Loss: 13.5681\n",
      "Epoch [280/300], Step [109/172], Loss: 13.7054\n",
      "Epoch [280/300], Step [110/172], Loss: 14.9904\n",
      "Epoch [280/300], Step [111/172], Loss: 16.1780\n",
      "Epoch [280/300], Step [112/172], Loss: 14.7866\n",
      "Epoch [280/300], Step [113/172], Loss: 12.5599\n",
      "Epoch [280/300], Step [114/172], Loss: 14.4138\n",
      "Epoch [280/300], Step [115/172], Loss: 18.9879\n",
      "Epoch [280/300], Step [116/172], Loss: 12.9096\n",
      "Epoch [280/300], Step [117/172], Loss: 11.5922\n",
      "Epoch [280/300], Step [118/172], Loss: 12.9152\n",
      "Epoch [280/300], Step [119/172], Loss: 17.0940\n",
      "Epoch [280/300], Step [120/172], Loss: 9.3783\n",
      "Epoch [280/300], Step [121/172], Loss: 8.3357\n",
      "Epoch [280/300], Step [122/172], Loss: 11.2527\n",
      "Epoch [280/300], Step [123/172], Loss: 10.6726\n",
      "Epoch [280/300], Step [124/172], Loss: 7.2752\n",
      "Epoch [280/300], Step [125/172], Loss: 11.5897\n",
      "Epoch [280/300], Step [126/172], Loss: 11.5567\n",
      "Epoch [280/300], Step [127/172], Loss: 10.6329\n",
      "Epoch [280/300], Step [128/172], Loss: 9.4853\n",
      "Epoch [280/300], Step [129/172], Loss: 8.6528\n",
      "Epoch [280/300], Step [130/172], Loss: 12.6223\n",
      "Epoch [280/300], Step [131/172], Loss: 7.2686\n",
      "Epoch [280/300], Step [132/172], Loss: 8.9653\n",
      "Epoch [280/300], Step [133/172], Loss: 9.0835\n",
      "Epoch [280/300], Step [134/172], Loss: 10.1862\n",
      "Epoch [280/300], Step [135/172], Loss: 8.9497\n",
      "Epoch [280/300], Step [136/172], Loss: 7.8243\n",
      "Epoch [280/300], Step [137/172], Loss: 8.0011\n",
      "Epoch [280/300], Step [138/172], Loss: 6.9979\n",
      "Epoch [280/300], Step [139/172], Loss: 11.3165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [280/300], Step [140/172], Loss: 10.7858\n",
      "Epoch [280/300], Step [141/172], Loss: 8.2105\n",
      "Epoch [280/300], Step [142/172], Loss: 14.3591\n",
      "Epoch [280/300], Step [143/172], Loss: 11.4084\n",
      "Epoch [280/300], Step [144/172], Loss: 8.8129\n",
      "Epoch [280/300], Step [145/172], Loss: 10.2414\n",
      "Epoch [280/300], Step [146/172], Loss: 9.8685\n",
      "Epoch [280/300], Step [147/172], Loss: 5.4760\n",
      "Epoch [280/300], Step [148/172], Loss: 5.8751\n",
      "Epoch [280/300], Step [149/172], Loss: 5.6484\n",
      "Epoch [280/300], Step [150/172], Loss: 5.0311\n",
      "Epoch [280/300], Step [151/172], Loss: 5.0786\n",
      "Epoch [280/300], Step [152/172], Loss: 7.1767\n",
      "Epoch [280/300], Step [153/172], Loss: 6.0628\n",
      "Epoch [280/300], Step [154/172], Loss: 6.2488\n",
      "Epoch [280/300], Step [155/172], Loss: 6.3609\n",
      "Epoch [280/300], Step [156/172], Loss: 14.0195\n",
      "Epoch [280/300], Step [157/172], Loss: 9.1191\n",
      "Epoch [280/300], Step [158/172], Loss: 6.9451\n",
      "Epoch [280/300], Step [159/172], Loss: 9.7767\n",
      "Epoch [280/300], Step [160/172], Loss: 9.8446\n",
      "Epoch [280/300], Step [161/172], Loss: 5.2817\n",
      "Epoch [280/300], Step [162/172], Loss: 4.0369\n",
      "Epoch [280/300], Step [163/172], Loss: 7.0424\n",
      "Epoch [280/300], Step [164/172], Loss: 8.0182\n",
      "Epoch [280/300], Step [165/172], Loss: 6.7167\n",
      "Epoch [280/300], Step [166/172], Loss: 5.8425\n",
      "Epoch [280/300], Step [167/172], Loss: 11.1424\n",
      "Epoch [280/300], Step [168/172], Loss: 5.9018\n",
      "Epoch [280/300], Step [169/172], Loss: 6.2659\n",
      "Epoch [280/300], Step [170/172], Loss: 4.8292\n",
      "Epoch [280/300], Step [171/172], Loss: 8.8911\n",
      "Epoch [280/300], Step [172/172], Loss: 5.3854\n",
      "Epoch [281/300], Step [1/172], Loss: 37.7900\n",
      "Epoch [281/300], Step [2/172], Loss: 41.5851\n",
      "Epoch [281/300], Step [3/172], Loss: 39.0484\n",
      "Epoch [281/300], Step [4/172], Loss: 17.6053\n",
      "Epoch [281/300], Step [5/172], Loss: 33.5180\n",
      "Epoch [281/300], Step [6/172], Loss: 17.5051\n",
      "Epoch [281/300], Step [7/172], Loss: 25.4718\n",
      "Epoch [281/300], Step [8/172], Loss: 3.5873\n",
      "Epoch [281/300], Step [9/172], Loss: 23.8682\n",
      "Epoch [281/300], Step [10/172], Loss: 34.4758\n",
      "Epoch [281/300], Step [11/172], Loss: 49.2314\n",
      "Epoch [281/300], Step [12/172], Loss: 46.7666\n",
      "Epoch [281/300], Step [13/172], Loss: 29.8649\n",
      "Epoch [281/300], Step [14/172], Loss: 50.1833\n",
      "Epoch [281/300], Step [15/172], Loss: 49.0466\n",
      "Epoch [281/300], Step [16/172], Loss: 8.3211\n",
      "Epoch [281/300], Step [17/172], Loss: 34.5394\n",
      "Epoch [281/300], Step [18/172], Loss: 49.8038\n",
      "Epoch [281/300], Step [19/172], Loss: 68.3749\n",
      "Epoch [281/300], Step [20/172], Loss: 25.0786\n",
      "Epoch [281/300], Step [21/172], Loss: 70.7938\n",
      "Epoch [281/300], Step [22/172], Loss: 46.4434\n",
      "Epoch [281/300], Step [23/172], Loss: 1.7885\n",
      "Epoch [281/300], Step [24/172], Loss: 43.1769\n",
      "Epoch [281/300], Step [25/172], Loss: 29.9016\n",
      "Epoch [281/300], Step [26/172], Loss: 40.5407\n",
      "Epoch [281/300], Step [27/172], Loss: 54.0582\n",
      "Epoch [281/300], Step [28/172], Loss: 14.4313\n",
      "Epoch [281/300], Step [29/172], Loss: 13.4044\n",
      "Epoch [281/300], Step [30/172], Loss: 42.5493\n",
      "Epoch [281/300], Step [31/172], Loss: 24.7727\n",
      "Epoch [281/300], Step [32/172], Loss: 39.4197\n",
      "Epoch [281/300], Step [33/172], Loss: 58.4081\n",
      "Epoch [281/300], Step [34/172], Loss: 1.7694\n",
      "Epoch [281/300], Step [35/172], Loss: 14.3561\n",
      "Epoch [281/300], Step [36/172], Loss: 13.7704\n",
      "Epoch [281/300], Step [37/172], Loss: 13.1563\n",
      "Epoch [281/300], Step [38/172], Loss: 28.8355\n",
      "Epoch [281/300], Step [39/172], Loss: 31.1171\n",
      "Epoch [281/300], Step [40/172], Loss: 18.5696\n",
      "Epoch [281/300], Step [41/172], Loss: 27.5887\n",
      "Epoch [281/300], Step [42/172], Loss: 33.4155\n",
      "Epoch [281/300], Step [43/172], Loss: 23.9485\n",
      "Epoch [281/300], Step [44/172], Loss: 20.0958\n",
      "Epoch [281/300], Step [45/172], Loss: 25.8103\n",
      "Epoch [281/300], Step [46/172], Loss: 14.8181\n",
      "Epoch [281/300], Step [47/172], Loss: 45.2991\n",
      "Epoch [281/300], Step [48/172], Loss: 63.5395\n",
      "Epoch [281/300], Step [49/172], Loss: 20.9839\n",
      "Epoch [281/300], Step [50/172], Loss: 50.5325\n",
      "Epoch [281/300], Step [51/172], Loss: 6.9628\n",
      "Epoch [281/300], Step [52/172], Loss: 19.5838\n",
      "Epoch [281/300], Step [53/172], Loss: 22.1394\n",
      "Epoch [281/300], Step [54/172], Loss: 13.3111\n",
      "Epoch [281/300], Step [55/172], Loss: 13.7360\n",
      "Epoch [281/300], Step [56/172], Loss: 16.7006\n",
      "Epoch [281/300], Step [57/172], Loss: 18.1954\n",
      "Epoch [281/300], Step [58/172], Loss: 13.1528\n",
      "Epoch [281/300], Step [59/172], Loss: 27.6021\n",
      "Epoch [281/300], Step [60/172], Loss: 24.8534\n",
      "Epoch [281/300], Step [61/172], Loss: 5.4426\n",
      "Epoch [281/300], Step [62/172], Loss: 15.8311\n",
      "Epoch [281/300], Step [63/172], Loss: 8.5538\n",
      "Epoch [281/300], Step [64/172], Loss: 10.8077\n",
      "Epoch [281/300], Step [65/172], Loss: 18.3932\n",
      "Epoch [281/300], Step [66/172], Loss: 7.0305\n",
      "Epoch [281/300], Step [67/172], Loss: 26.4694\n",
      "Epoch [281/300], Step [68/172], Loss: 4.4787\n",
      "Epoch [281/300], Step [69/172], Loss: 30.2071\n",
      "Epoch [281/300], Step [70/172], Loss: 29.7386\n",
      "Epoch [281/300], Step [71/172], Loss: 32.4513\n",
      "Epoch [281/300], Step [72/172], Loss: 31.0630\n",
      "Epoch [281/300], Step [73/172], Loss: 39.5662\n",
      "Epoch [281/300], Step [74/172], Loss: 20.9384\n",
      "Epoch [281/300], Step [75/172], Loss: 19.9388\n",
      "Epoch [281/300], Step [76/172], Loss: 22.3753\n",
      "Epoch [281/300], Step [77/172], Loss: 41.7075\n",
      "Epoch [281/300], Step [78/172], Loss: 29.4234\n",
      "Epoch [281/300], Step [79/172], Loss: 26.6984\n",
      "Epoch [281/300], Step [80/172], Loss: 44.2179\n",
      "Epoch [281/300], Step [81/172], Loss: 24.5934\n",
      "Epoch [281/300], Step [82/172], Loss: 33.6339\n",
      "Epoch [281/300], Step [83/172], Loss: 36.9544\n",
      "Epoch [281/300], Step [84/172], Loss: 26.9439\n",
      "Epoch [281/300], Step [85/172], Loss: 31.6318\n",
      "Epoch [281/300], Step [86/172], Loss: 27.2756\n",
      "Epoch [281/300], Step [87/172], Loss: 20.0881\n",
      "Epoch [281/300], Step [88/172], Loss: 18.9208\n",
      "Epoch [281/300], Step [89/172], Loss: 23.3389\n",
      "Epoch [281/300], Step [90/172], Loss: 16.7638\n",
      "Epoch [281/300], Step [91/172], Loss: 22.8563\n",
      "Epoch [281/300], Step [92/172], Loss: 17.0548\n",
      "Epoch [281/300], Step [93/172], Loss: 16.5725\n",
      "Epoch [281/300], Step [94/172], Loss: 22.3050\n",
      "Epoch [281/300], Step [95/172], Loss: 16.6613\n",
      "Epoch [281/300], Step [96/172], Loss: 18.7032\n",
      "Epoch [281/300], Step [97/172], Loss: 27.0112\n",
      "Epoch [281/300], Step [98/172], Loss: 15.5171\n",
      "Epoch [281/300], Step [99/172], Loss: 16.3575\n",
      "Epoch [281/300], Step [100/172], Loss: 13.8971\n",
      "Epoch [281/300], Step [101/172], Loss: 16.6248\n",
      "Epoch [281/300], Step [102/172], Loss: 16.5213\n",
      "Epoch [281/300], Step [103/172], Loss: 9.6611\n",
      "Epoch [281/300], Step [104/172], Loss: 17.2463\n",
      "Epoch [281/300], Step [105/172], Loss: 19.5894\n",
      "Epoch [281/300], Step [106/172], Loss: 13.6600\n",
      "Epoch [281/300], Step [107/172], Loss: 14.4455\n",
      "Epoch [281/300], Step [108/172], Loss: 13.4058\n",
      "Epoch [281/300], Step [109/172], Loss: 13.6368\n",
      "Epoch [281/300], Step [110/172], Loss: 14.7863\n",
      "Epoch [281/300], Step [111/172], Loss: 16.0995\n",
      "Epoch [281/300], Step [112/172], Loss: 14.5866\n",
      "Epoch [281/300], Step [113/172], Loss: 12.4581\n",
      "Epoch [281/300], Step [114/172], Loss: 14.3751\n",
      "Epoch [281/300], Step [115/172], Loss: 18.9491\n",
      "Epoch [281/300], Step [116/172], Loss: 12.7554\n",
      "Epoch [281/300], Step [117/172], Loss: 11.5091\n",
      "Epoch [281/300], Step [118/172], Loss: 12.8665\n",
      "Epoch [281/300], Step [119/172], Loss: 17.0738\n",
      "Epoch [281/300], Step [120/172], Loss: 9.2447\n",
      "Epoch [281/300], Step [121/172], Loss: 8.2391\n",
      "Epoch [281/300], Step [122/172], Loss: 11.0972\n",
      "Epoch [281/300], Step [123/172], Loss: 10.5180\n",
      "Epoch [281/300], Step [124/172], Loss: 7.1881\n",
      "Epoch [281/300], Step [125/172], Loss: 11.4794\n",
      "Epoch [281/300], Step [126/172], Loss: 11.4689\n",
      "Epoch [281/300], Step [127/172], Loss: 10.6393\n",
      "Epoch [281/300], Step [128/172], Loss: 9.5533\n",
      "Epoch [281/300], Step [129/172], Loss: 8.6300\n",
      "Epoch [281/300], Step [130/172], Loss: 12.5774\n",
      "Epoch [281/300], Step [131/172], Loss: 7.2070\n",
      "Epoch [281/300], Step [132/172], Loss: 8.9550\n",
      "Epoch [281/300], Step [133/172], Loss: 8.9949\n",
      "Epoch [281/300], Step [134/172], Loss: 10.3055\n",
      "Epoch [281/300], Step [135/172], Loss: 8.9538\n",
      "Epoch [281/300], Step [136/172], Loss: 7.6901\n",
      "Epoch [281/300], Step [137/172], Loss: 7.9072\n",
      "Epoch [281/300], Step [138/172], Loss: 6.9121\n",
      "Epoch [281/300], Step [139/172], Loss: 11.3138\n",
      "Epoch [281/300], Step [140/172], Loss: 10.7648\n",
      "Epoch [281/300], Step [141/172], Loss: 8.1328\n",
      "Epoch [281/300], Step [142/172], Loss: 14.3578\n",
      "Epoch [281/300], Step [143/172], Loss: 11.4615\n",
      "Epoch [281/300], Step [144/172], Loss: 8.8309\n",
      "Epoch [281/300], Step [145/172], Loss: 10.2543\n",
      "Epoch [281/300], Step [146/172], Loss: 9.8507\n",
      "Epoch [281/300], Step [147/172], Loss: 5.4030\n",
      "Epoch [281/300], Step [148/172], Loss: 5.8247\n",
      "Epoch [281/300], Step [149/172], Loss: 5.4764\n",
      "Epoch [281/300], Step [150/172], Loss: 5.0409\n",
      "Epoch [281/300], Step [151/172], Loss: 5.0435\n",
      "Epoch [281/300], Step [152/172], Loss: 7.1068\n",
      "Epoch [281/300], Step [153/172], Loss: 6.0835\n",
      "Epoch [281/300], Step [154/172], Loss: 6.1865\n",
      "Epoch [281/300], Step [155/172], Loss: 6.2527\n",
      "Epoch [281/300], Step [156/172], Loss: 13.9613\n",
      "Epoch [281/300], Step [157/172], Loss: 9.1385\n",
      "Epoch [281/300], Step [158/172], Loss: 6.8569\n",
      "Epoch [281/300], Step [159/172], Loss: 9.6437\n",
      "Epoch [281/300], Step [160/172], Loss: 9.7597\n",
      "Epoch [281/300], Step [161/172], Loss: 5.0755\n",
      "Epoch [281/300], Step [162/172], Loss: 3.9922\n",
      "Epoch [281/300], Step [163/172], Loss: 7.0601\n",
      "Epoch [281/300], Step [164/172], Loss: 7.9220\n",
      "Epoch [281/300], Step [165/172], Loss: 6.6905\n",
      "Epoch [281/300], Step [166/172], Loss: 5.8207\n",
      "Epoch [281/300], Step [167/172], Loss: 11.0574\n",
      "Epoch [281/300], Step [168/172], Loss: 5.8883\n",
      "Epoch [281/300], Step [169/172], Loss: 6.1851\n",
      "Epoch [281/300], Step [170/172], Loss: 4.7299\n",
      "Epoch [281/300], Step [171/172], Loss: 8.8904\n",
      "Epoch [281/300], Step [172/172], Loss: 5.3143\n",
      "Epoch [282/300], Step [1/172], Loss: 37.5793\n",
      "Epoch [282/300], Step [2/172], Loss: 41.8541\n",
      "Epoch [282/300], Step [3/172], Loss: 37.9049\n",
      "Epoch [282/300], Step [4/172], Loss: 17.4608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [282/300], Step [5/172], Loss: 33.4847\n",
      "Epoch [282/300], Step [6/172], Loss: 18.2384\n",
      "Epoch [282/300], Step [7/172], Loss: 27.7146\n",
      "Epoch [282/300], Step [8/172], Loss: 4.2179\n",
      "Epoch [282/300], Step [9/172], Loss: 23.8668\n",
      "Epoch [282/300], Step [10/172], Loss: 34.3787\n",
      "Epoch [282/300], Step [11/172], Loss: 49.3312\n",
      "Epoch [282/300], Step [12/172], Loss: 46.7364\n",
      "Epoch [282/300], Step [13/172], Loss: 29.9246\n",
      "Epoch [282/300], Step [14/172], Loss: 49.8198\n",
      "Epoch [282/300], Step [15/172], Loss: 48.9222\n",
      "Epoch [282/300], Step [16/172], Loss: 6.6728\n",
      "Epoch [282/300], Step [17/172], Loss: 34.4986\n",
      "Epoch [282/300], Step [18/172], Loss: 49.7274\n",
      "Epoch [282/300], Step [19/172], Loss: 68.1334\n",
      "Epoch [282/300], Step [20/172], Loss: 25.5508\n",
      "Epoch [282/300], Step [21/172], Loss: 69.2542\n",
      "Epoch [282/300], Step [22/172], Loss: 46.2083\n",
      "Epoch [282/300], Step [23/172], Loss: 1.4246\n",
      "Epoch [282/300], Step [24/172], Loss: 43.5780\n",
      "Epoch [282/300], Step [25/172], Loss: 29.1269\n",
      "Epoch [282/300], Step [26/172], Loss: 40.5107\n",
      "Epoch [282/300], Step [27/172], Loss: 54.3276\n",
      "Epoch [282/300], Step [28/172], Loss: 14.4254\n",
      "Epoch [282/300], Step [29/172], Loss: 13.6255\n",
      "Epoch [282/300], Step [30/172], Loss: 42.1069\n",
      "Epoch [282/300], Step [31/172], Loss: 24.4525\n",
      "Epoch [282/300], Step [32/172], Loss: 39.6760\n",
      "Epoch [282/300], Step [33/172], Loss: 58.8196\n",
      "Epoch [282/300], Step [34/172], Loss: 1.6087\n",
      "Epoch [282/300], Step [35/172], Loss: 14.3941\n",
      "Epoch [282/300], Step [36/172], Loss: 14.4242\n",
      "Epoch [282/300], Step [37/172], Loss: 13.1943\n",
      "Epoch [282/300], Step [38/172], Loss: 28.8075\n",
      "Epoch [282/300], Step [39/172], Loss: 31.2228\n",
      "Epoch [282/300], Step [40/172], Loss: 18.6733\n",
      "Epoch [282/300], Step [41/172], Loss: 27.5578\n",
      "Epoch [282/300], Step [42/172], Loss: 33.8165\n",
      "Epoch [282/300], Step [43/172], Loss: 24.2014\n",
      "Epoch [282/300], Step [44/172], Loss: 20.8063\n",
      "Epoch [282/300], Step [45/172], Loss: 26.2452\n",
      "Epoch [282/300], Step [46/172], Loss: 14.8553\n",
      "Epoch [282/300], Step [47/172], Loss: 45.4395\n",
      "Epoch [282/300], Step [48/172], Loss: 63.2573\n",
      "Epoch [282/300], Step [49/172], Loss: 21.4644\n",
      "Epoch [282/300], Step [50/172], Loss: 50.2472\n",
      "Epoch [282/300], Step [51/172], Loss: 6.9658\n",
      "Epoch [282/300], Step [52/172], Loss: 19.6715\n",
      "Epoch [282/300], Step [53/172], Loss: 22.0526\n",
      "Epoch [282/300], Step [54/172], Loss: 12.9699\n",
      "Epoch [282/300], Step [55/172], Loss: 13.6853\n",
      "Epoch [282/300], Step [56/172], Loss: 16.5328\n",
      "Epoch [282/300], Step [57/172], Loss: 18.2412\n",
      "Epoch [282/300], Step [58/172], Loss: 12.8465\n",
      "Epoch [282/300], Step [59/172], Loss: 27.7433\n",
      "Epoch [282/300], Step [60/172], Loss: 24.7563\n",
      "Epoch [282/300], Step [61/172], Loss: 5.3468\n",
      "Epoch [282/300], Step [62/172], Loss: 15.7379\n",
      "Epoch [282/300], Step [63/172], Loss: 8.4984\n",
      "Epoch [282/300], Step [64/172], Loss: 10.7124\n",
      "Epoch [282/300], Step [65/172], Loss: 18.0971\n",
      "Epoch [282/300], Step [66/172], Loss: 6.9414\n",
      "Epoch [282/300], Step [67/172], Loss: 26.1042\n",
      "Epoch [282/300], Step [68/172], Loss: 4.5676\n",
      "Epoch [282/300], Step [69/172], Loss: 29.7852\n",
      "Epoch [282/300], Step [70/172], Loss: 29.9814\n",
      "Epoch [282/300], Step [71/172], Loss: 32.6485\n",
      "Epoch [282/300], Step [72/172], Loss: 31.1904\n",
      "Epoch [282/300], Step [73/172], Loss: 40.1902\n",
      "Epoch [282/300], Step [74/172], Loss: 21.0526\n",
      "Epoch [282/300], Step [75/172], Loss: 20.3072\n",
      "Epoch [282/300], Step [76/172], Loss: 22.8450\n",
      "Epoch [282/300], Step [77/172], Loss: 42.0412\n",
      "Epoch [282/300], Step [78/172], Loss: 29.5982\n",
      "Epoch [282/300], Step [79/172], Loss: 26.9225\n",
      "Epoch [282/300], Step [80/172], Loss: 44.6500\n",
      "Epoch [282/300], Step [81/172], Loss: 24.8831\n",
      "Epoch [282/300], Step [82/172], Loss: 33.8976\n",
      "Epoch [282/300], Step [83/172], Loss: 37.0985\n",
      "Epoch [282/300], Step [84/172], Loss: 26.8609\n",
      "Epoch [282/300], Step [85/172], Loss: 31.5933\n",
      "Epoch [282/300], Step [86/172], Loss: 27.2916\n",
      "Epoch [282/300], Step [87/172], Loss: 20.2551\n",
      "Epoch [282/300], Step [88/172], Loss: 18.9553\n",
      "Epoch [282/300], Step [89/172], Loss: 23.3476\n",
      "Epoch [282/300], Step [90/172], Loss: 17.0606\n",
      "Epoch [282/300], Step [91/172], Loss: 22.7685\n",
      "Epoch [282/300], Step [92/172], Loss: 17.0572\n",
      "Epoch [282/300], Step [93/172], Loss: 16.9479\n",
      "Epoch [282/300], Step [94/172], Loss: 22.5377\n",
      "Epoch [282/300], Step [95/172], Loss: 16.5608\n",
      "Epoch [282/300], Step [96/172], Loss: 18.7889\n",
      "Epoch [282/300], Step [97/172], Loss: 27.0266\n",
      "Epoch [282/300], Step [98/172], Loss: 15.5929\n",
      "Epoch [282/300], Step [99/172], Loss: 16.3164\n",
      "Epoch [282/300], Step [100/172], Loss: 13.7968\n",
      "Epoch [282/300], Step [101/172], Loss: 16.6593\n",
      "Epoch [282/300], Step [102/172], Loss: 16.7796\n",
      "Epoch [282/300], Step [103/172], Loss: 9.7261\n",
      "Epoch [282/300], Step [104/172], Loss: 17.3082\n",
      "Epoch [282/300], Step [105/172], Loss: 19.6166\n",
      "Epoch [282/300], Step [106/172], Loss: 13.5596\n",
      "Epoch [282/300], Step [107/172], Loss: 14.4866\n",
      "Epoch [282/300], Step [108/172], Loss: 13.5072\n",
      "Epoch [282/300], Step [109/172], Loss: 13.8322\n",
      "Epoch [282/300], Step [110/172], Loss: 14.9374\n",
      "Epoch [282/300], Step [111/172], Loss: 16.1331\n",
      "Epoch [282/300], Step [112/172], Loss: 14.6711\n",
      "Epoch [282/300], Step [113/172], Loss: 12.5146\n",
      "Epoch [282/300], Step [114/172], Loss: 14.3985\n",
      "Epoch [282/300], Step [115/172], Loss: 18.7315\n",
      "Epoch [282/300], Step [116/172], Loss: 12.8828\n",
      "Epoch [282/300], Step [117/172], Loss: 11.4178\n",
      "Epoch [282/300], Step [118/172], Loss: 12.8878\n",
      "Epoch [282/300], Step [119/172], Loss: 16.9926\n",
      "Epoch [282/300], Step [120/172], Loss: 9.3332\n",
      "Epoch [282/300], Step [121/172], Loss: 8.3257\n",
      "Epoch [282/300], Step [122/172], Loss: 11.4861\n",
      "Epoch [282/300], Step [123/172], Loss: 10.4205\n",
      "Epoch [282/300], Step [124/172], Loss: 7.2328\n",
      "Epoch [282/300], Step [125/172], Loss: 11.4873\n",
      "Epoch [282/300], Step [126/172], Loss: 11.4344\n",
      "Epoch [282/300], Step [127/172], Loss: 10.7458\n",
      "Epoch [282/300], Step [128/172], Loss: 9.5312\n",
      "Epoch [282/300], Step [129/172], Loss: 8.6704\n",
      "Epoch [282/300], Step [130/172], Loss: 12.5647\n",
      "Epoch [282/300], Step [131/172], Loss: 7.3188\n",
      "Epoch [282/300], Step [132/172], Loss: 8.9755\n",
      "Epoch [282/300], Step [133/172], Loss: 9.1889\n",
      "Epoch [282/300], Step [134/172], Loss: 10.2100\n",
      "Epoch [282/300], Step [135/172], Loss: 8.9260\n",
      "Epoch [282/300], Step [136/172], Loss: 7.7024\n",
      "Epoch [282/300], Step [137/172], Loss: 7.9801\n",
      "Epoch [282/300], Step [138/172], Loss: 6.8732\n",
      "Epoch [282/300], Step [139/172], Loss: 11.2567\n",
      "Epoch [282/300], Step [140/172], Loss: 10.7556\n",
      "Epoch [282/300], Step [141/172], Loss: 8.0046\n",
      "Epoch [282/300], Step [142/172], Loss: 14.3929\n",
      "Epoch [282/300], Step [143/172], Loss: 11.4253\n",
      "Epoch [282/300], Step [144/172], Loss: 8.7225\n",
      "Epoch [282/300], Step [145/172], Loss: 10.3199\n",
      "Epoch [282/300], Step [146/172], Loss: 9.7378\n",
      "Epoch [282/300], Step [147/172], Loss: 5.4464\n",
      "Epoch [282/300], Step [148/172], Loss: 5.8647\n",
      "Epoch [282/300], Step [149/172], Loss: 5.5437\n",
      "Epoch [282/300], Step [150/172], Loss: 5.0209\n",
      "Epoch [282/300], Step [151/172], Loss: 5.1220\n",
      "Epoch [282/300], Step [152/172], Loss: 7.0705\n",
      "Epoch [282/300], Step [153/172], Loss: 6.0895\n",
      "Epoch [282/300], Step [154/172], Loss: 6.3106\n",
      "Epoch [282/300], Step [155/172], Loss: 6.2935\n",
      "Epoch [282/300], Step [156/172], Loss: 14.0275\n",
      "Epoch [282/300], Step [157/172], Loss: 9.2401\n",
      "Epoch [282/300], Step [158/172], Loss: 6.9468\n",
      "Epoch [282/300], Step [159/172], Loss: 9.8757\n",
      "Epoch [282/300], Step [160/172], Loss: 9.8884\n",
      "Epoch [282/300], Step [161/172], Loss: 5.1082\n",
      "Epoch [282/300], Step [162/172], Loss: 3.9740\n",
      "Epoch [282/300], Step [163/172], Loss: 6.9641\n",
      "Epoch [282/300], Step [164/172], Loss: 8.0833\n",
      "Epoch [282/300], Step [165/172], Loss: 6.7548\n",
      "Epoch [282/300], Step [166/172], Loss: 5.8531\n",
      "Epoch [282/300], Step [167/172], Loss: 11.0973\n",
      "Epoch [282/300], Step [168/172], Loss: 5.8003\n",
      "Epoch [282/300], Step [169/172], Loss: 6.4287\n",
      "Epoch [282/300], Step [170/172], Loss: 4.8349\n",
      "Epoch [282/300], Step [171/172], Loss: 8.7311\n",
      "Epoch [282/300], Step [172/172], Loss: 5.4802\n",
      "Epoch [283/300], Step [1/172], Loss: 37.6965\n",
      "Epoch [283/300], Step [2/172], Loss: 41.0412\n",
      "Epoch [283/300], Step [3/172], Loss: 39.6124\n",
      "Epoch [283/300], Step [4/172], Loss: 17.6848\n",
      "Epoch [283/300], Step [5/172], Loss: 33.3442\n",
      "Epoch [283/300], Step [6/172], Loss: 17.2971\n",
      "Epoch [283/300], Step [7/172], Loss: 24.1322\n",
      "Epoch [283/300], Step [8/172], Loss: 3.5696\n",
      "Epoch [283/300], Step [9/172], Loss: 23.7935\n",
      "Epoch [283/300], Step [10/172], Loss: 34.4714\n",
      "Epoch [283/300], Step [11/172], Loss: 49.0345\n",
      "Epoch [283/300], Step [12/172], Loss: 46.4392\n",
      "Epoch [283/300], Step [13/172], Loss: 29.6376\n",
      "Epoch [283/300], Step [14/172], Loss: 50.0687\n",
      "Epoch [283/300], Step [15/172], Loss: 49.2138\n",
      "Epoch [283/300], Step [16/172], Loss: 8.9842\n",
      "Epoch [283/300], Step [17/172], Loss: 34.2989\n",
      "Epoch [283/300], Step [18/172], Loss: 49.7795\n",
      "Epoch [283/300], Step [19/172], Loss: 68.5187\n",
      "Epoch [283/300], Step [20/172], Loss: 25.5247\n",
      "Epoch [283/300], Step [21/172], Loss: 70.5303\n",
      "Epoch [283/300], Step [22/172], Loss: 46.7230\n",
      "Epoch [283/300], Step [23/172], Loss: 2.0871\n",
      "Epoch [283/300], Step [24/172], Loss: 44.1047\n",
      "Epoch [283/300], Step [25/172], Loss: 30.8020\n",
      "Epoch [283/300], Step [26/172], Loss: 41.1604\n",
      "Epoch [283/300], Step [27/172], Loss: 54.6886\n",
      "Epoch [283/300], Step [28/172], Loss: 14.7901\n",
      "Epoch [283/300], Step [29/172], Loss: 13.7594\n",
      "Epoch [283/300], Step [30/172], Loss: 42.6335\n",
      "Epoch [283/300], Step [31/172], Loss: 25.1873\n",
      "Epoch [283/300], Step [32/172], Loss: 39.7729\n",
      "Epoch [283/300], Step [33/172], Loss: 58.5375\n",
      "Epoch [283/300], Step [34/172], Loss: 1.5337\n",
      "Epoch [283/300], Step [35/172], Loss: 14.2277\n",
      "Epoch [283/300], Step [36/172], Loss: 13.4654\n",
      "Epoch [283/300], Step [37/172], Loss: 13.1587\n",
      "Epoch [283/300], Step [38/172], Loss: 28.9345\n",
      "Epoch [283/300], Step [39/172], Loss: 31.1409\n",
      "Epoch [283/300], Step [40/172], Loss: 18.4086\n",
      "Epoch [283/300], Step [41/172], Loss: 27.5575\n",
      "Epoch [283/300], Step [42/172], Loss: 33.2936\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [283/300], Step [43/172], Loss: 23.8505\n",
      "Epoch [283/300], Step [44/172], Loss: 20.0500\n",
      "Epoch [283/300], Step [45/172], Loss: 25.6567\n",
      "Epoch [283/300], Step [46/172], Loss: 14.8546\n",
      "Epoch [283/300], Step [47/172], Loss: 45.4575\n",
      "Epoch [283/300], Step [48/172], Loss: 62.9829\n",
      "Epoch [283/300], Step [49/172], Loss: 20.9332\n",
      "Epoch [283/300], Step [50/172], Loss: 50.0364\n",
      "Epoch [283/300], Step [51/172], Loss: 7.0415\n",
      "Epoch [283/300], Step [52/172], Loss: 19.6234\n",
      "Epoch [283/300], Step [53/172], Loss: 22.2700\n",
      "Epoch [283/300], Step [54/172], Loss: 13.0350\n",
      "Epoch [283/300], Step [55/172], Loss: 13.7605\n",
      "Epoch [283/300], Step [56/172], Loss: 16.4129\n",
      "Epoch [283/300], Step [57/172], Loss: 17.8567\n",
      "Epoch [283/300], Step [58/172], Loss: 12.8606\n",
      "Epoch [283/300], Step [59/172], Loss: 27.6572\n",
      "Epoch [283/300], Step [60/172], Loss: 24.2431\n",
      "Epoch [283/300], Step [61/172], Loss: 5.3098\n",
      "Epoch [283/300], Step [62/172], Loss: 15.8136\n",
      "Epoch [283/300], Step [63/172], Loss: 8.8043\n",
      "Epoch [283/300], Step [64/172], Loss: 10.9752\n",
      "Epoch [283/300], Step [65/172], Loss: 18.1459\n",
      "Epoch [283/300], Step [66/172], Loss: 6.9157\n",
      "Epoch [283/300], Step [67/172], Loss: 26.5539\n",
      "Epoch [283/300], Step [68/172], Loss: 4.5083\n",
      "Epoch [283/300], Step [69/172], Loss: 30.2153\n",
      "Epoch [283/300], Step [70/172], Loss: 29.4293\n",
      "Epoch [283/300], Step [71/172], Loss: 32.5256\n",
      "Epoch [283/300], Step [72/172], Loss: 30.9793\n",
      "Epoch [283/300], Step [73/172], Loss: 40.0252\n",
      "Epoch [283/300], Step [74/172], Loss: 21.0374\n",
      "Epoch [283/300], Step [75/172], Loss: 20.1702\n",
      "Epoch [283/300], Step [76/172], Loss: 22.5701\n",
      "Epoch [283/300], Step [77/172], Loss: 41.6923\n",
      "Epoch [283/300], Step [78/172], Loss: 29.5490\n",
      "Epoch [283/300], Step [79/172], Loss: 26.8790\n",
      "Epoch [283/300], Step [80/172], Loss: 44.4860\n",
      "Epoch [283/300], Step [81/172], Loss: 24.9533\n",
      "Epoch [283/300], Step [82/172], Loss: 33.4961\n",
      "Epoch [283/300], Step [83/172], Loss: 37.0312\n",
      "Epoch [283/300], Step [84/172], Loss: 26.9637\n",
      "Epoch [283/300], Step [85/172], Loss: 31.9554\n",
      "Epoch [283/300], Step [86/172], Loss: 27.5382\n",
      "Epoch [283/300], Step [87/172], Loss: 20.0853\n",
      "Epoch [283/300], Step [88/172], Loss: 18.9255\n",
      "Epoch [283/300], Step [89/172], Loss: 23.4230\n",
      "Epoch [283/300], Step [90/172], Loss: 16.9786\n",
      "Epoch [283/300], Step [91/172], Loss: 22.6239\n",
      "Epoch [283/300], Step [92/172], Loss: 16.9541\n",
      "Epoch [283/300], Step [93/172], Loss: 16.6602\n",
      "Epoch [283/300], Step [94/172], Loss: 22.4285\n",
      "Epoch [283/300], Step [95/172], Loss: 16.4644\n",
      "Epoch [283/300], Step [96/172], Loss: 18.5632\n",
      "Epoch [283/300], Step [97/172], Loss: 26.8117\n",
      "Epoch [283/300], Step [98/172], Loss: 15.5550\n",
      "Epoch [283/300], Step [99/172], Loss: 16.2805\n",
      "Epoch [283/300], Step [100/172], Loss: 13.7888\n",
      "Epoch [283/300], Step [101/172], Loss: 16.5777\n",
      "Epoch [283/300], Step [102/172], Loss: 16.4783\n",
      "Epoch [283/300], Step [103/172], Loss: 9.6068\n",
      "Epoch [283/300], Step [104/172], Loss: 17.0206\n",
      "Epoch [283/300], Step [105/172], Loss: 19.3615\n",
      "Epoch [283/300], Step [106/172], Loss: 13.4314\n",
      "Epoch [283/300], Step [107/172], Loss: 14.4440\n",
      "Epoch [283/300], Step [108/172], Loss: 13.3546\n",
      "Epoch [283/300], Step [109/172], Loss: 13.7320\n",
      "Epoch [283/300], Step [110/172], Loss: 14.8037\n",
      "Epoch [283/300], Step [111/172], Loss: 16.0968\n",
      "Epoch [283/300], Step [112/172], Loss: 14.4908\n",
      "Epoch [283/300], Step [113/172], Loss: 12.3003\n",
      "Epoch [283/300], Step [114/172], Loss: 14.3456\n",
      "Epoch [283/300], Step [115/172], Loss: 18.8397\n",
      "Epoch [283/300], Step [116/172], Loss: 12.7986\n",
      "Epoch [283/300], Step [117/172], Loss: 11.4870\n",
      "Epoch [283/300], Step [118/172], Loss: 12.8992\n",
      "Epoch [283/300], Step [119/172], Loss: 16.8575\n",
      "Epoch [283/300], Step [120/172], Loss: 9.3445\n",
      "Epoch [283/300], Step [121/172], Loss: 8.1498\n",
      "Epoch [283/300], Step [122/172], Loss: 11.3765\n",
      "Epoch [283/300], Step [123/172], Loss: 10.3330\n",
      "Epoch [283/300], Step [124/172], Loss: 7.1000\n",
      "Epoch [283/300], Step [125/172], Loss: 11.2982\n",
      "Epoch [283/300], Step [126/172], Loss: 11.3036\n",
      "Epoch [283/300], Step [127/172], Loss: 10.5674\n",
      "Epoch [283/300], Step [128/172], Loss: 9.3884\n",
      "Epoch [283/300], Step [129/172], Loss: 8.5062\n",
      "Epoch [283/300], Step [130/172], Loss: 12.4249\n",
      "Epoch [283/300], Step [131/172], Loss: 7.1526\n",
      "Epoch [283/300], Step [132/172], Loss: 8.8259\n",
      "Epoch [283/300], Step [133/172], Loss: 9.0450\n",
      "Epoch [283/300], Step [134/172], Loss: 10.1029\n",
      "Epoch [283/300], Step [135/172], Loss: 8.9162\n",
      "Epoch [283/300], Step [136/172], Loss: 7.5316\n",
      "Epoch [283/300], Step [137/172], Loss: 7.9169\n",
      "Epoch [283/300], Step [138/172], Loss: 6.7007\n",
      "Epoch [283/300], Step [139/172], Loss: 11.1958\n",
      "Epoch [283/300], Step [140/172], Loss: 10.7399\n",
      "Epoch [283/300], Step [141/172], Loss: 7.9431\n",
      "Epoch [283/300], Step [142/172], Loss: 14.4378\n",
      "Epoch [283/300], Step [143/172], Loss: 11.3603\n",
      "Epoch [283/300], Step [144/172], Loss: 8.6668\n",
      "Epoch [283/300], Step [145/172], Loss: 10.4451\n",
      "Epoch [283/300], Step [146/172], Loss: 9.7639\n",
      "Epoch [283/300], Step [147/172], Loss: 5.3701\n",
      "Epoch [283/300], Step [148/172], Loss: 5.8612\n",
      "Epoch [283/300], Step [149/172], Loss: 5.4376\n",
      "Epoch [283/300], Step [150/172], Loss: 4.9705\n",
      "Epoch [283/300], Step [151/172], Loss: 5.0608\n",
      "Epoch [283/300], Step [152/172], Loss: 7.0276\n",
      "Epoch [283/300], Step [153/172], Loss: 6.0404\n",
      "Epoch [283/300], Step [154/172], Loss: 6.2408\n",
      "Epoch [283/300], Step [155/172], Loss: 6.2243\n",
      "Epoch [283/300], Step [156/172], Loss: 13.9305\n",
      "Epoch [283/300], Step [157/172], Loss: 9.0341\n",
      "Epoch [283/300], Step [158/172], Loss: 6.8292\n",
      "Epoch [283/300], Step [159/172], Loss: 9.7996\n",
      "Epoch [283/300], Step [160/172], Loss: 9.7206\n",
      "Epoch [283/300], Step [161/172], Loss: 5.0508\n",
      "Epoch [283/300], Step [162/172], Loss: 3.9821\n",
      "Epoch [283/300], Step [163/172], Loss: 6.8229\n",
      "Epoch [283/300], Step [164/172], Loss: 7.9811\n",
      "Epoch [283/300], Step [165/172], Loss: 6.7340\n",
      "Epoch [283/300], Step [166/172], Loss: 5.9233\n",
      "Epoch [283/300], Step [167/172], Loss: 10.9660\n",
      "Epoch [283/300], Step [168/172], Loss: 5.8050\n",
      "Epoch [283/300], Step [169/172], Loss: 6.4800\n",
      "Epoch [283/300], Step [170/172], Loss: 4.7799\n",
      "Epoch [283/300], Step [171/172], Loss: 8.6625\n",
      "Epoch [283/300], Step [172/172], Loss: 5.4069\n",
      "Epoch [284/300], Step [1/172], Loss: 37.9666\n",
      "Epoch [284/300], Step [2/172], Loss: 42.1596\n",
      "Epoch [284/300], Step [3/172], Loss: 40.0372\n",
      "Epoch [284/300], Step [4/172], Loss: 17.8040\n",
      "Epoch [284/300], Step [5/172], Loss: 33.4360\n",
      "Epoch [284/300], Step [6/172], Loss: 17.6943\n",
      "Epoch [284/300], Step [7/172], Loss: 24.7549\n",
      "Epoch [284/300], Step [8/172], Loss: 4.4055\n",
      "Epoch [284/300], Step [9/172], Loss: 23.9353\n",
      "Epoch [284/300], Step [10/172], Loss: 34.3100\n",
      "Epoch [284/300], Step [11/172], Loss: 49.0333\n",
      "Epoch [284/300], Step [12/172], Loss: 46.5066\n",
      "Epoch [284/300], Step [13/172], Loss: 29.9008\n",
      "Epoch [284/300], Step [14/172], Loss: 50.6182\n",
      "Epoch [284/300], Step [15/172], Loss: 48.9150\n",
      "Epoch [284/300], Step [16/172], Loss: 7.7977\n",
      "Epoch [284/300], Step [17/172], Loss: 34.6517\n",
      "Epoch [284/300], Step [18/172], Loss: 49.8127\n",
      "Epoch [284/300], Step [19/172], Loss: 68.5603\n",
      "Epoch [284/300], Step [20/172], Loss: 25.4112\n",
      "Epoch [284/300], Step [21/172], Loss: 70.9384\n",
      "Epoch [284/300], Step [22/172], Loss: 46.7800\n",
      "Epoch [284/300], Step [23/172], Loss: 1.7703\n",
      "Epoch [284/300], Step [24/172], Loss: 44.0494\n",
      "Epoch [284/300], Step [25/172], Loss: 30.7015\n",
      "Epoch [284/300], Step [26/172], Loss: 41.2262\n",
      "Epoch [284/300], Step [27/172], Loss: 54.6560\n",
      "Epoch [284/300], Step [28/172], Loss: 14.8764\n",
      "Epoch [284/300], Step [29/172], Loss: 13.9480\n",
      "Epoch [284/300], Step [30/172], Loss: 43.0238\n",
      "Epoch [284/300], Step [31/172], Loss: 25.6959\n",
      "Epoch [284/300], Step [32/172], Loss: 40.5353\n",
      "Epoch [284/300], Step [33/172], Loss: 59.5148\n",
      "Epoch [284/300], Step [34/172], Loss: 1.7003\n",
      "Epoch [284/300], Step [35/172], Loss: 14.1834\n",
      "Epoch [284/300], Step [36/172], Loss: 13.7723\n",
      "Epoch [284/300], Step [37/172], Loss: 13.2762\n",
      "Epoch [284/300], Step [38/172], Loss: 29.2779\n",
      "Epoch [284/300], Step [39/172], Loss: 31.0812\n",
      "Epoch [284/300], Step [40/172], Loss: 18.5365\n",
      "Epoch [284/300], Step [41/172], Loss: 27.3901\n",
      "Epoch [284/300], Step [42/172], Loss: 33.0853\n",
      "Epoch [284/300], Step [43/172], Loss: 23.8393\n",
      "Epoch [284/300], Step [44/172], Loss: 20.0479\n",
      "Epoch [284/300], Step [45/172], Loss: 25.4748\n",
      "Epoch [284/300], Step [46/172], Loss: 14.5810\n",
      "Epoch [284/300], Step [47/172], Loss: 44.8805\n",
      "Epoch [284/300], Step [48/172], Loss: 63.1428\n",
      "Epoch [284/300], Step [49/172], Loss: 20.5578\n",
      "Epoch [284/300], Step [50/172], Loss: 49.4576\n",
      "Epoch [284/300], Step [51/172], Loss: 6.8711\n",
      "Epoch [284/300], Step [52/172], Loss: 19.2185\n",
      "Epoch [284/300], Step [53/172], Loss: 22.0807\n",
      "Epoch [284/300], Step [54/172], Loss: 12.7657\n",
      "Epoch [284/300], Step [55/172], Loss: 13.5453\n",
      "Epoch [284/300], Step [56/172], Loss: 16.1046\n",
      "Epoch [284/300], Step [57/172], Loss: 17.4149\n",
      "Epoch [284/300], Step [58/172], Loss: 12.6737\n",
      "Epoch [284/300], Step [59/172], Loss: 27.2381\n",
      "Epoch [284/300], Step [60/172], Loss: 23.7885\n",
      "Epoch [284/300], Step [61/172], Loss: 5.2036\n",
      "Epoch [284/300], Step [62/172], Loss: 15.6154\n",
      "Epoch [284/300], Step [63/172], Loss: 8.5867\n",
      "Epoch [284/300], Step [64/172], Loss: 10.9399\n",
      "Epoch [284/300], Step [65/172], Loss: 17.8831\n",
      "Epoch [284/300], Step [66/172], Loss: 6.6536\n",
      "Epoch [284/300], Step [67/172], Loss: 26.0403\n",
      "Epoch [284/300], Step [68/172], Loss: 4.3933\n",
      "Epoch [284/300], Step [69/172], Loss: 29.9648\n",
      "Epoch [284/300], Step [70/172], Loss: 29.5460\n",
      "Epoch [284/300], Step [71/172], Loss: 32.5538\n",
      "Epoch [284/300], Step [72/172], Loss: 30.9385\n",
      "Epoch [284/300], Step [73/172], Loss: 39.8093\n",
      "Epoch [284/300], Step [74/172], Loss: 20.8848\n",
      "Epoch [284/300], Step [75/172], Loss: 19.9329\n",
      "Epoch [284/300], Step [76/172], Loss: 22.4729\n",
      "Epoch [284/300], Step [77/172], Loss: 41.5822\n",
      "Epoch [284/300], Step [78/172], Loss: 29.3285\n",
      "Epoch [284/300], Step [79/172], Loss: 26.5905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [284/300], Step [80/172], Loss: 44.4340\n",
      "Epoch [284/300], Step [81/172], Loss: 24.8292\n",
      "Epoch [284/300], Step [82/172], Loss: 33.7197\n",
      "Epoch [284/300], Step [83/172], Loss: 36.9457\n",
      "Epoch [284/300], Step [84/172], Loss: 26.8658\n",
      "Epoch [284/300], Step [85/172], Loss: 31.8599\n",
      "Epoch [284/300], Step [86/172], Loss: 27.2460\n",
      "Epoch [284/300], Step [87/172], Loss: 20.0185\n",
      "Epoch [284/300], Step [88/172], Loss: 18.8146\n",
      "Epoch [284/300], Step [89/172], Loss: 23.3826\n",
      "Epoch [284/300], Step [90/172], Loss: 16.7802\n",
      "Epoch [284/300], Step [91/172], Loss: 22.5860\n",
      "Epoch [284/300], Step [92/172], Loss: 16.8957\n",
      "Epoch [284/300], Step [93/172], Loss: 16.7146\n",
      "Epoch [284/300], Step [94/172], Loss: 22.3671\n",
      "Epoch [284/300], Step [95/172], Loss: 16.4922\n",
      "Epoch [284/300], Step [96/172], Loss: 18.6563\n",
      "Epoch [284/300], Step [97/172], Loss: 26.7282\n",
      "Epoch [284/300], Step [98/172], Loss: 15.5240\n",
      "Epoch [284/300], Step [99/172], Loss: 16.3111\n",
      "Epoch [284/300], Step [100/172], Loss: 13.7841\n",
      "Epoch [284/300], Step [101/172], Loss: 16.5648\n",
      "Epoch [284/300], Step [102/172], Loss: 16.5675\n",
      "Epoch [284/300], Step [103/172], Loss: 9.6458\n",
      "Epoch [284/300], Step [104/172], Loss: 17.1769\n",
      "Epoch [284/300], Step [105/172], Loss: 19.6930\n",
      "Epoch [284/300], Step [106/172], Loss: 13.4600\n",
      "Epoch [284/300], Step [107/172], Loss: 14.4806\n",
      "Epoch [284/300], Step [108/172], Loss: 13.3228\n",
      "Epoch [284/300], Step [109/172], Loss: 13.7248\n",
      "Epoch [284/300], Step [110/172], Loss: 14.7400\n",
      "Epoch [284/300], Step [111/172], Loss: 16.1570\n",
      "Epoch [284/300], Step [112/172], Loss: 14.4190\n",
      "Epoch [284/300], Step [113/172], Loss: 12.2917\n",
      "Epoch [284/300], Step [114/172], Loss: 14.3863\n",
      "Epoch [284/300], Step [115/172], Loss: 18.7664\n",
      "Epoch [284/300], Step [116/172], Loss: 12.7593\n",
      "Epoch [284/300], Step [117/172], Loss: 11.4252\n",
      "Epoch [284/300], Step [118/172], Loss: 12.8569\n",
      "Epoch [284/300], Step [119/172], Loss: 16.8664\n",
      "Epoch [284/300], Step [120/172], Loss: 9.3045\n",
      "Epoch [284/300], Step [121/172], Loss: 8.2291\n",
      "Epoch [284/300], Step [122/172], Loss: 11.3553\n",
      "Epoch [284/300], Step [123/172], Loss: 10.2794\n",
      "Epoch [284/300], Step [124/172], Loss: 7.1512\n",
      "Epoch [284/300], Step [125/172], Loss: 11.2223\n",
      "Epoch [284/300], Step [126/172], Loss: 11.2771\n",
      "Epoch [284/300], Step [127/172], Loss: 10.6024\n",
      "Epoch [284/300], Step [128/172], Loss: 9.5307\n",
      "Epoch [284/300], Step [129/172], Loss: 8.5326\n",
      "Epoch [284/300], Step [130/172], Loss: 12.4369\n",
      "Epoch [284/300], Step [131/172], Loss: 7.1940\n",
      "Epoch [284/300], Step [132/172], Loss: 8.9078\n",
      "Epoch [284/300], Step [133/172], Loss: 9.0644\n",
      "Epoch [284/300], Step [134/172], Loss: 10.1670\n",
      "Epoch [284/300], Step [135/172], Loss: 8.9624\n",
      "Epoch [284/300], Step [136/172], Loss: 7.5215\n",
      "Epoch [284/300], Step [137/172], Loss: 7.8558\n",
      "Epoch [284/300], Step [138/172], Loss: 6.7132\n",
      "Epoch [284/300], Step [139/172], Loss: 11.1637\n",
      "Epoch [284/300], Step [140/172], Loss: 10.7954\n",
      "Epoch [284/300], Step [141/172], Loss: 7.9378\n",
      "Epoch [284/300], Step [142/172], Loss: 14.5235\n",
      "Epoch [284/300], Step [143/172], Loss: 11.3112\n",
      "Epoch [284/300], Step [144/172], Loss: 8.6873\n",
      "Epoch [284/300], Step [145/172], Loss: 10.5078\n",
      "Epoch [284/300], Step [146/172], Loss: 9.7818\n",
      "Epoch [284/300], Step [147/172], Loss: 5.4013\n",
      "Epoch [284/300], Step [148/172], Loss: 5.9054\n",
      "Epoch [284/300], Step [149/172], Loss: 5.4278\n",
      "Epoch [284/300], Step [150/172], Loss: 5.0149\n",
      "Epoch [284/300], Step [151/172], Loss: 5.0442\n",
      "Epoch [284/300], Step [152/172], Loss: 7.0267\n",
      "Epoch [284/300], Step [153/172], Loss: 6.0750\n",
      "Epoch [284/300], Step [154/172], Loss: 6.2643\n",
      "Epoch [284/300], Step [155/172], Loss: 6.3442\n",
      "Epoch [284/300], Step [156/172], Loss: 13.9810\n",
      "Epoch [284/300], Step [157/172], Loss: 9.0505\n",
      "Epoch [284/300], Step [158/172], Loss: 6.8477\n",
      "Epoch [284/300], Step [159/172], Loss: 9.7808\n",
      "Epoch [284/300], Step [160/172], Loss: 9.7303\n",
      "Epoch [284/300], Step [161/172], Loss: 5.0305\n",
      "Epoch [284/300], Step [162/172], Loss: 3.9974\n",
      "Epoch [284/300], Step [163/172], Loss: 6.8290\n",
      "Epoch [284/300], Step [164/172], Loss: 8.1060\n",
      "Epoch [284/300], Step [165/172], Loss: 6.7561\n",
      "Epoch [284/300], Step [166/172], Loss: 5.9265\n",
      "Epoch [284/300], Step [167/172], Loss: 11.0215\n",
      "Epoch [284/300], Step [168/172], Loss: 5.8337\n",
      "Epoch [284/300], Step [169/172], Loss: 6.4775\n",
      "Epoch [284/300], Step [170/172], Loss: 4.7741\n",
      "Epoch [284/300], Step [171/172], Loss: 8.7998\n",
      "Epoch [284/300], Step [172/172], Loss: 5.4142\n",
      "Epoch [285/300], Step [1/172], Loss: 38.0061\n",
      "Epoch [285/300], Step [2/172], Loss: 41.9605\n",
      "Epoch [285/300], Step [3/172], Loss: 39.3254\n",
      "Epoch [285/300], Step [4/172], Loss: 17.7795\n",
      "Epoch [285/300], Step [5/172], Loss: 33.5239\n",
      "Epoch [285/300], Step [6/172], Loss: 17.7224\n",
      "Epoch [285/300], Step [7/172], Loss: 24.6809\n",
      "Epoch [285/300], Step [8/172], Loss: 3.5552\n",
      "Epoch [285/300], Step [9/172], Loss: 23.6530\n",
      "Epoch [285/300], Step [10/172], Loss: 34.3634\n",
      "Epoch [285/300], Step [11/172], Loss: 48.9270\n",
      "Epoch [285/300], Step [12/172], Loss: 46.3655\n",
      "Epoch [285/300], Step [13/172], Loss: 29.8342\n",
      "Epoch [285/300], Step [14/172], Loss: 50.5545\n",
      "Epoch [285/300], Step [15/172], Loss: 49.0953\n",
      "Epoch [285/300], Step [16/172], Loss: 8.9309\n",
      "Epoch [285/300], Step [17/172], Loss: 34.2277\n",
      "Epoch [285/300], Step [18/172], Loss: 49.5578\n",
      "Epoch [285/300], Step [19/172], Loss: 67.9062\n",
      "Epoch [285/300], Step [20/172], Loss: 24.8968\n",
      "Epoch [285/300], Step [21/172], Loss: 71.0234\n",
      "Epoch [285/300], Step [22/172], Loss: 46.3762\n",
      "Epoch [285/300], Step [23/172], Loss: 1.6410\n",
      "Epoch [285/300], Step [24/172], Loss: 44.0397\n",
      "Epoch [285/300], Step [25/172], Loss: 30.5927\n",
      "Epoch [285/300], Step [26/172], Loss: 41.1799\n",
      "Epoch [285/300], Step [27/172], Loss: 54.4957\n",
      "Epoch [285/300], Step [28/172], Loss: 14.9748\n",
      "Epoch [285/300], Step [29/172], Loss: 13.9678\n",
      "Epoch [285/300], Step [30/172], Loss: 43.1860\n",
      "Epoch [285/300], Step [31/172], Loss: 26.2043\n",
      "Epoch [285/300], Step [32/172], Loss: 40.3900\n",
      "Epoch [285/300], Step [33/172], Loss: 58.9483\n",
      "Epoch [285/300], Step [34/172], Loss: 1.5813\n",
      "Epoch [285/300], Step [35/172], Loss: 14.2734\n",
      "Epoch [285/300], Step [36/172], Loss: 13.8887\n",
      "Epoch [285/300], Step [37/172], Loss: 13.3805\n",
      "Epoch [285/300], Step [38/172], Loss: 29.3845\n",
      "Epoch [285/300], Step [39/172], Loss: 31.3658\n",
      "Epoch [285/300], Step [40/172], Loss: 18.7257\n",
      "Epoch [285/300], Step [41/172], Loss: 27.8669\n",
      "Epoch [285/300], Step [42/172], Loss: 33.4549\n",
      "Epoch [285/300], Step [43/172], Loss: 24.0892\n",
      "Epoch [285/300], Step [44/172], Loss: 20.1395\n",
      "Epoch [285/300], Step [45/172], Loss: 25.4982\n",
      "Epoch [285/300], Step [46/172], Loss: 14.5707\n",
      "Epoch [285/300], Step [47/172], Loss: 45.0883\n",
      "Epoch [285/300], Step [48/172], Loss: 62.3096\n",
      "Epoch [285/300], Step [49/172], Loss: 20.3286\n",
      "Epoch [285/300], Step [50/172], Loss: 49.6926\n",
      "Epoch [285/300], Step [51/172], Loss: 6.7657\n",
      "Epoch [285/300], Step [52/172], Loss: 18.9623\n",
      "Epoch [285/300], Step [53/172], Loss: 21.8261\n",
      "Epoch [285/300], Step [54/172], Loss: 12.7227\n",
      "Epoch [285/300], Step [55/172], Loss: 13.2128\n",
      "Epoch [285/300], Step [56/172], Loss: 16.7230\n",
      "Epoch [285/300], Step [57/172], Loss: 17.9558\n",
      "Epoch [285/300], Step [58/172], Loss: 12.6730\n",
      "Epoch [285/300], Step [59/172], Loss: 27.1619\n",
      "Epoch [285/300], Step [60/172], Loss: 24.2188\n",
      "Epoch [285/300], Step [61/172], Loss: 5.0999\n",
      "Epoch [285/300], Step [62/172], Loss: 15.6364\n",
      "Epoch [285/300], Step [63/172], Loss: 8.4737\n",
      "Epoch [285/300], Step [64/172], Loss: 10.8896\n",
      "Epoch [285/300], Step [65/172], Loss: 17.8829\n",
      "Epoch [285/300], Step [66/172], Loss: 6.7790\n",
      "Epoch [285/300], Step [67/172], Loss: 26.0026\n",
      "Epoch [285/300], Step [68/172], Loss: 4.4422\n",
      "Epoch [285/300], Step [69/172], Loss: 29.8531\n",
      "Epoch [285/300], Step [70/172], Loss: 29.1054\n",
      "Epoch [285/300], Step [71/172], Loss: 32.2557\n",
      "Epoch [285/300], Step [72/172], Loss: 30.7801\n",
      "Epoch [285/300], Step [73/172], Loss: 39.6238\n",
      "Epoch [285/300], Step [74/172], Loss: 20.8220\n",
      "Epoch [285/300], Step [75/172], Loss: 19.5032\n",
      "Epoch [285/300], Step [76/172], Loss: 22.0839\n",
      "Epoch [285/300], Step [77/172], Loss: 41.4744\n",
      "Epoch [285/300], Step [78/172], Loss: 29.0667\n",
      "Epoch [285/300], Step [79/172], Loss: 26.4810\n",
      "Epoch [285/300], Step [80/172], Loss: 43.9541\n",
      "Epoch [285/300], Step [81/172], Loss: 24.6456\n",
      "Epoch [285/300], Step [82/172], Loss: 33.2710\n",
      "Epoch [285/300], Step [83/172], Loss: 36.8598\n",
      "Epoch [285/300], Step [84/172], Loss: 26.5943\n",
      "Epoch [285/300], Step [85/172], Loss: 31.5760\n",
      "Epoch [285/300], Step [86/172], Loss: 27.1643\n",
      "Epoch [285/300], Step [87/172], Loss: 19.8452\n",
      "Epoch [285/300], Step [88/172], Loss: 18.6945\n",
      "Epoch [285/300], Step [89/172], Loss: 23.1665\n",
      "Epoch [285/300], Step [90/172], Loss: 16.5332\n",
      "Epoch [285/300], Step [91/172], Loss: 22.4665\n",
      "Epoch [285/300], Step [92/172], Loss: 16.8006\n",
      "Epoch [285/300], Step [93/172], Loss: 16.3833\n",
      "Epoch [285/300], Step [94/172], Loss: 22.1290\n",
      "Epoch [285/300], Step [95/172], Loss: 16.3808\n",
      "Epoch [285/300], Step [96/172], Loss: 18.4252\n",
      "Epoch [285/300], Step [97/172], Loss: 26.4209\n",
      "Epoch [285/300], Step [98/172], Loss: 15.3337\n",
      "Epoch [285/300], Step [99/172], Loss: 16.1434\n",
      "Epoch [285/300], Step [100/172], Loss: 13.6069\n",
      "Epoch [285/300], Step [101/172], Loss: 16.4056\n",
      "Epoch [285/300], Step [102/172], Loss: 16.3997\n",
      "Epoch [285/300], Step [103/172], Loss: 9.5042\n",
      "Epoch [285/300], Step [104/172], Loss: 16.9255\n",
      "Epoch [285/300], Step [105/172], Loss: 19.4191\n",
      "Epoch [285/300], Step [106/172], Loss: 13.2883\n",
      "Epoch [285/300], Step [107/172], Loss: 14.3512\n",
      "Epoch [285/300], Step [108/172], Loss: 13.2870\n",
      "Epoch [285/300], Step [109/172], Loss: 13.7034\n",
      "Epoch [285/300], Step [110/172], Loss: 14.6269\n",
      "Epoch [285/300], Step [111/172], Loss: 16.0008\n",
      "Epoch [285/300], Step [112/172], Loss: 14.2553\n",
      "Epoch [285/300], Step [113/172], Loss: 12.3371\n",
      "Epoch [285/300], Step [114/172], Loss: 14.2982\n",
      "Epoch [285/300], Step [115/172], Loss: 18.8410\n",
      "Epoch [285/300], Step [116/172], Loss: 12.7555\n",
      "Epoch [285/300], Step [117/172], Loss: 11.4205\n",
      "Epoch [285/300], Step [118/172], Loss: 12.8259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [285/300], Step [119/172], Loss: 16.9340\n",
      "Epoch [285/300], Step [120/172], Loss: 9.4226\n",
      "Epoch [285/300], Step [121/172], Loss: 8.1485\n",
      "Epoch [285/300], Step [122/172], Loss: 11.2818\n",
      "Epoch [285/300], Step [123/172], Loss: 10.3358\n",
      "Epoch [285/300], Step [124/172], Loss: 7.0405\n",
      "Epoch [285/300], Step [125/172], Loss: 11.1526\n",
      "Epoch [285/300], Step [126/172], Loss: 11.2590\n",
      "Epoch [285/300], Step [127/172], Loss: 10.5367\n",
      "Epoch [285/300], Step [128/172], Loss: 9.4577\n",
      "Epoch [285/300], Step [129/172], Loss: 8.4294\n",
      "Epoch [285/300], Step [130/172], Loss: 12.3127\n",
      "Epoch [285/300], Step [131/172], Loss: 7.1312\n",
      "Epoch [285/300], Step [132/172], Loss: 8.8865\n",
      "Epoch [285/300], Step [133/172], Loss: 9.0461\n",
      "Epoch [285/300], Step [134/172], Loss: 10.2727\n",
      "Epoch [285/300], Step [135/172], Loss: 8.9922\n",
      "Epoch [285/300], Step [136/172], Loss: 7.4630\n",
      "Epoch [285/300], Step [137/172], Loss: 7.8566\n",
      "Epoch [285/300], Step [138/172], Loss: 6.6472\n",
      "Epoch [285/300], Step [139/172], Loss: 11.1755\n",
      "Epoch [285/300], Step [140/172], Loss: 10.7879\n",
      "Epoch [285/300], Step [141/172], Loss: 7.9906\n",
      "Epoch [285/300], Step [142/172], Loss: 14.5566\n",
      "Epoch [285/300], Step [143/172], Loss: 11.3297\n",
      "Epoch [285/300], Step [144/172], Loss: 8.6841\n",
      "Epoch [285/300], Step [145/172], Loss: 10.6107\n",
      "Epoch [285/300], Step [146/172], Loss: 9.8202\n",
      "Epoch [285/300], Step [147/172], Loss: 5.3535\n",
      "Epoch [285/300], Step [148/172], Loss: 5.8880\n",
      "Epoch [285/300], Step [149/172], Loss: 5.3621\n",
      "Epoch [285/300], Step [150/172], Loss: 5.0146\n",
      "Epoch [285/300], Step [151/172], Loss: 5.0801\n",
      "Epoch [285/300], Step [152/172], Loss: 7.0775\n",
      "Epoch [285/300], Step [153/172], Loss: 6.0507\n",
      "Epoch [285/300], Step [154/172], Loss: 6.2227\n",
      "Epoch [285/300], Step [155/172], Loss: 6.2420\n",
      "Epoch [285/300], Step [156/172], Loss: 13.9174\n",
      "Epoch [285/300], Step [157/172], Loss: 8.9230\n",
      "Epoch [285/300], Step [158/172], Loss: 6.8244\n",
      "Epoch [285/300], Step [159/172], Loss: 9.7074\n",
      "Epoch [285/300], Step [160/172], Loss: 9.6971\n",
      "Epoch [285/300], Step [161/172], Loss: 4.8922\n",
      "Epoch [285/300], Step [162/172], Loss: 4.0024\n",
      "Epoch [285/300], Step [163/172], Loss: 6.7690\n",
      "Epoch [285/300], Step [164/172], Loss: 8.0874\n",
      "Epoch [285/300], Step [165/172], Loss: 6.7534\n",
      "Epoch [285/300], Step [166/172], Loss: 5.9782\n",
      "Epoch [285/300], Step [167/172], Loss: 11.0015\n",
      "Epoch [285/300], Step [168/172], Loss: 5.8377\n",
      "Epoch [285/300], Step [169/172], Loss: 6.5315\n",
      "Epoch [285/300], Step [170/172], Loss: 4.7782\n",
      "Epoch [285/300], Step [171/172], Loss: 8.8811\n",
      "Epoch [285/300], Step [172/172], Loss: 5.3471\n",
      "Epoch [286/300], Step [1/172], Loss: 38.0532\n",
      "Epoch [286/300], Step [2/172], Loss: 42.1451\n",
      "Epoch [286/300], Step [3/172], Loss: 38.7102\n",
      "Epoch [286/300], Step [4/172], Loss: 17.6550\n",
      "Epoch [286/300], Step [5/172], Loss: 33.6161\n",
      "Epoch [286/300], Step [6/172], Loss: 17.7481\n",
      "Epoch [286/300], Step [7/172], Loss: 24.4154\n",
      "Epoch [286/300], Step [8/172], Loss: 3.8325\n",
      "Epoch [286/300], Step [9/172], Loss: 23.7648\n",
      "Epoch [286/300], Step [10/172], Loss: 34.2744\n",
      "Epoch [286/300], Step [11/172], Loss: 49.0418\n",
      "Epoch [286/300], Step [12/172], Loss: 46.5549\n",
      "Epoch [286/300], Step [13/172], Loss: 29.9469\n",
      "Epoch [286/300], Step [14/172], Loss: 50.7216\n",
      "Epoch [286/300], Step [15/172], Loss: 48.8064\n",
      "Epoch [286/300], Step [16/172], Loss: 7.7403\n",
      "Epoch [286/300], Step [17/172], Loss: 34.5143\n",
      "Epoch [286/300], Step [18/172], Loss: 49.5859\n",
      "Epoch [286/300], Step [19/172], Loss: 67.5602\n",
      "Epoch [286/300], Step [20/172], Loss: 24.6310\n",
      "Epoch [286/300], Step [21/172], Loss: 70.8587\n",
      "Epoch [286/300], Step [22/172], Loss: 46.0699\n",
      "Epoch [286/300], Step [23/172], Loss: 1.6296\n",
      "Epoch [286/300], Step [24/172], Loss: 43.8498\n",
      "Epoch [286/300], Step [25/172], Loss: 30.4399\n",
      "Epoch [286/300], Step [26/172], Loss: 41.0889\n",
      "Epoch [286/300], Step [27/172], Loss: 53.8320\n",
      "Epoch [286/300], Step [28/172], Loss: 15.0224\n",
      "Epoch [286/300], Step [29/172], Loss: 13.6873\n",
      "Epoch [286/300], Step [30/172], Loss: 43.2484\n",
      "Epoch [286/300], Step [31/172], Loss: 26.5491\n",
      "Epoch [286/300], Step [32/172], Loss: 40.7908\n",
      "Epoch [286/300], Step [33/172], Loss: 59.3798\n",
      "Epoch [286/300], Step [34/172], Loss: 1.5745\n",
      "Epoch [286/300], Step [35/172], Loss: 14.2683\n",
      "Epoch [286/300], Step [36/172], Loss: 14.2497\n",
      "Epoch [286/300], Step [37/172], Loss: 13.5854\n",
      "Epoch [286/300], Step [38/172], Loss: 29.5302\n",
      "Epoch [286/300], Step [39/172], Loss: 31.8921\n",
      "Epoch [286/300], Step [40/172], Loss: 19.0887\n",
      "Epoch [286/300], Step [41/172], Loss: 27.9691\n",
      "Epoch [286/300], Step [42/172], Loss: 33.9622\n",
      "Epoch [286/300], Step [43/172], Loss: 24.4026\n",
      "Epoch [286/300], Step [44/172], Loss: 20.1301\n",
      "Epoch [286/300], Step [45/172], Loss: 25.8322\n",
      "Epoch [286/300], Step [46/172], Loss: 14.5280\n",
      "Epoch [286/300], Step [47/172], Loss: 45.1811\n",
      "Epoch [286/300], Step [48/172], Loss: 62.1267\n",
      "Epoch [286/300], Step [49/172], Loss: 20.0952\n",
      "Epoch [286/300], Step [50/172], Loss: 49.8657\n",
      "Epoch [286/300], Step [51/172], Loss: 6.7010\n",
      "Epoch [286/300], Step [52/172], Loss: 18.7670\n",
      "Epoch [286/300], Step [53/172], Loss: 21.7780\n",
      "Epoch [286/300], Step [54/172], Loss: 11.9323\n",
      "Epoch [286/300], Step [55/172], Loss: 12.9110\n",
      "Epoch [286/300], Step [56/172], Loss: 16.6321\n",
      "Epoch [286/300], Step [57/172], Loss: 17.7938\n",
      "Epoch [286/300], Step [58/172], Loss: 12.0621\n",
      "Epoch [286/300], Step [59/172], Loss: 27.0783\n",
      "Epoch [286/300], Step [60/172], Loss: 24.6307\n",
      "Epoch [286/300], Step [61/172], Loss: 4.9868\n",
      "Epoch [286/300], Step [62/172], Loss: 15.3411\n",
      "Epoch [286/300], Step [63/172], Loss: 8.1202\n",
      "Epoch [286/300], Step [64/172], Loss: 10.4905\n",
      "Epoch [286/300], Step [65/172], Loss: 17.4821\n",
      "Epoch [286/300], Step [66/172], Loss: 6.8496\n",
      "Epoch [286/300], Step [67/172], Loss: 25.8188\n",
      "Epoch [286/300], Step [68/172], Loss: 4.5152\n",
      "Epoch [286/300], Step [69/172], Loss: 29.9143\n",
      "Epoch [286/300], Step [70/172], Loss: 29.0820\n",
      "Epoch [286/300], Step [71/172], Loss: 32.3817\n",
      "Epoch [286/300], Step [72/172], Loss: 31.0620\n",
      "Epoch [286/300], Step [73/172], Loss: 39.6542\n",
      "Epoch [286/300], Step [74/172], Loss: 21.0451\n",
      "Epoch [286/300], Step [75/172], Loss: 19.7405\n",
      "Epoch [286/300], Step [76/172], Loss: 22.0317\n",
      "Epoch [286/300], Step [77/172], Loss: 41.3480\n",
      "Epoch [286/300], Step [78/172], Loss: 28.9104\n",
      "Epoch [286/300], Step [79/172], Loss: 26.4354\n",
      "Epoch [286/300], Step [80/172], Loss: 43.9463\n",
      "Epoch [286/300], Step [81/172], Loss: 24.5649\n",
      "Epoch [286/300], Step [82/172], Loss: 33.0871\n",
      "Epoch [286/300], Step [83/172], Loss: 36.9105\n",
      "Epoch [286/300], Step [84/172], Loss: 26.5758\n",
      "Epoch [286/300], Step [85/172], Loss: 31.5535\n",
      "Epoch [286/300], Step [86/172], Loss: 26.8916\n",
      "Epoch [286/300], Step [87/172], Loss: 19.8291\n",
      "Epoch [286/300], Step [88/172], Loss: 18.5749\n",
      "Epoch [286/300], Step [89/172], Loss: 22.9147\n",
      "Epoch [286/300], Step [90/172], Loss: 16.6701\n",
      "Epoch [286/300], Step [91/172], Loss: 22.3797\n",
      "Epoch [286/300], Step [92/172], Loss: 16.8438\n",
      "Epoch [286/300], Step [93/172], Loss: 16.5305\n",
      "Epoch [286/300], Step [94/172], Loss: 22.1617\n",
      "Epoch [286/300], Step [95/172], Loss: 16.6225\n",
      "Epoch [286/300], Step [96/172], Loss: 18.5073\n",
      "Epoch [286/300], Step [97/172], Loss: 26.4787\n",
      "Epoch [286/300], Step [98/172], Loss: 15.2987\n",
      "Epoch [286/300], Step [99/172], Loss: 16.1096\n",
      "Epoch [286/300], Step [100/172], Loss: 13.5577\n",
      "Epoch [286/300], Step [101/172], Loss: 16.4374\n",
      "Epoch [286/300], Step [102/172], Loss: 16.3163\n",
      "Epoch [286/300], Step [103/172], Loss: 9.5331\n",
      "Epoch [286/300], Step [104/172], Loss: 17.0663\n",
      "Epoch [286/300], Step [105/172], Loss: 19.4963\n",
      "Epoch [286/300], Step [106/172], Loss: 13.4109\n",
      "Epoch [286/300], Step [107/172], Loss: 14.4541\n",
      "Epoch [286/300], Step [108/172], Loss: 13.3233\n",
      "Epoch [286/300], Step [109/172], Loss: 13.8204\n",
      "Epoch [286/300], Step [110/172], Loss: 14.6644\n",
      "Epoch [286/300], Step [111/172], Loss: 16.1242\n",
      "Epoch [286/300], Step [112/172], Loss: 14.2641\n",
      "Epoch [286/300], Step [113/172], Loss: 12.3708\n",
      "Epoch [286/300], Step [114/172], Loss: 14.3719\n",
      "Epoch [286/300], Step [115/172], Loss: 18.9543\n",
      "Epoch [286/300], Step [116/172], Loss: 12.7311\n",
      "Epoch [286/300], Step [117/172], Loss: 11.3779\n",
      "Epoch [286/300], Step [118/172], Loss: 12.9068\n",
      "Epoch [286/300], Step [119/172], Loss: 16.9052\n",
      "Epoch [286/300], Step [120/172], Loss: 9.4900\n",
      "Epoch [286/300], Step [121/172], Loss: 8.3176\n",
      "Epoch [286/300], Step [122/172], Loss: 11.4375\n",
      "Epoch [286/300], Step [123/172], Loss: 10.3966\n",
      "Epoch [286/300], Step [124/172], Loss: 7.1516\n",
      "Epoch [286/300], Step [125/172], Loss: 11.2869\n",
      "Epoch [286/300], Step [126/172], Loss: 11.3252\n",
      "Epoch [286/300], Step [127/172], Loss: 10.5721\n",
      "Epoch [286/300], Step [128/172], Loss: 9.5947\n",
      "Epoch [286/300], Step [129/172], Loss: 8.5508\n",
      "Epoch [286/300], Step [130/172], Loss: 12.4049\n",
      "Epoch [286/300], Step [131/172], Loss: 7.2139\n",
      "Epoch [286/300], Step [132/172], Loss: 8.9570\n",
      "Epoch [286/300], Step [133/172], Loss: 9.1014\n",
      "Epoch [286/300], Step [134/172], Loss: 10.2907\n",
      "Epoch [286/300], Step [135/172], Loss: 9.0615\n",
      "Epoch [286/300], Step [136/172], Loss: 7.4286\n",
      "Epoch [286/300], Step [137/172], Loss: 7.8553\n",
      "Epoch [286/300], Step [138/172], Loss: 6.6968\n",
      "Epoch [286/300], Step [139/172], Loss: 11.2289\n",
      "Epoch [286/300], Step [140/172], Loss: 10.8350\n",
      "Epoch [286/300], Step [141/172], Loss: 8.0958\n",
      "Epoch [286/300], Step [142/172], Loss: 14.6044\n",
      "Epoch [286/300], Step [143/172], Loss: 11.4048\n",
      "Epoch [286/300], Step [144/172], Loss: 8.7518\n",
      "Epoch [286/300], Step [145/172], Loss: 10.7066\n",
      "Epoch [286/300], Step [146/172], Loss: 9.9257\n",
      "Epoch [286/300], Step [147/172], Loss: 5.4273\n",
      "Epoch [286/300], Step [148/172], Loss: 5.9593\n",
      "Epoch [286/300], Step [149/172], Loss: 5.4476\n",
      "Epoch [286/300], Step [150/172], Loss: 4.9701\n",
      "Epoch [286/300], Step [151/172], Loss: 5.0773\n",
      "Epoch [286/300], Step [152/172], Loss: 7.0671\n",
      "Epoch [286/300], Step [153/172], Loss: 6.1695\n",
      "Epoch [286/300], Step [154/172], Loss: 6.2770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [286/300], Step [155/172], Loss: 6.4372\n",
      "Epoch [286/300], Step [156/172], Loss: 13.9378\n",
      "Epoch [286/300], Step [157/172], Loss: 8.9641\n",
      "Epoch [286/300], Step [158/172], Loss: 6.8356\n",
      "Epoch [286/300], Step [159/172], Loss: 9.6426\n",
      "Epoch [286/300], Step [160/172], Loss: 9.6703\n",
      "Epoch [286/300], Step [161/172], Loss: 4.9855\n",
      "Epoch [286/300], Step [162/172], Loss: 4.0233\n",
      "Epoch [286/300], Step [163/172], Loss: 6.6881\n",
      "Epoch [286/300], Step [164/172], Loss: 8.1500\n",
      "Epoch [286/300], Step [165/172], Loss: 6.8214\n",
      "Epoch [286/300], Step [166/172], Loss: 5.9767\n",
      "Epoch [286/300], Step [167/172], Loss: 11.0173\n",
      "Epoch [286/300], Step [168/172], Loss: 5.8630\n",
      "Epoch [286/300], Step [169/172], Loss: 6.4902\n",
      "Epoch [286/300], Step [170/172], Loss: 4.8053\n",
      "Epoch [286/300], Step [171/172], Loss: 8.9529\n",
      "Epoch [286/300], Step [172/172], Loss: 5.4868\n",
      "Epoch [287/300], Step [1/172], Loss: 37.6413\n",
      "Epoch [287/300], Step [2/172], Loss: 41.9781\n",
      "Epoch [287/300], Step [3/172], Loss: 39.8176\n",
      "Epoch [287/300], Step [4/172], Loss: 17.8977\n",
      "Epoch [287/300], Step [5/172], Loss: 34.4065\n",
      "Epoch [287/300], Step [6/172], Loss: 17.8320\n",
      "Epoch [287/300], Step [7/172], Loss: 24.9014\n",
      "Epoch [287/300], Step [8/172], Loss: 4.1159\n",
      "Epoch [287/300], Step [9/172], Loss: 23.7102\n",
      "Epoch [287/300], Step [10/172], Loss: 34.4410\n",
      "Epoch [287/300], Step [11/172], Loss: 49.0194\n",
      "Epoch [287/300], Step [12/172], Loss: 46.5371\n",
      "Epoch [287/300], Step [13/172], Loss: 30.3008\n",
      "Epoch [287/300], Step [14/172], Loss: 51.0854\n",
      "Epoch [287/300], Step [15/172], Loss: 49.4493\n",
      "Epoch [287/300], Step [16/172], Loss: 7.5537\n",
      "Epoch [287/300], Step [17/172], Loss: 34.6788\n",
      "Epoch [287/300], Step [18/172], Loss: 49.9880\n",
      "Epoch [287/300], Step [19/172], Loss: 67.9217\n",
      "Epoch [287/300], Step [20/172], Loss: 24.6216\n",
      "Epoch [287/300], Step [21/172], Loss: 71.4150\n",
      "Epoch [287/300], Step [22/172], Loss: 45.8968\n",
      "Epoch [287/300], Step [23/172], Loss: 1.5256\n",
      "Epoch [287/300], Step [24/172], Loss: 43.9880\n",
      "Epoch [287/300], Step [25/172], Loss: 30.2645\n",
      "Epoch [287/300], Step [26/172], Loss: 40.8420\n",
      "Epoch [287/300], Step [27/172], Loss: 53.8235\n",
      "Epoch [287/300], Step [28/172], Loss: 15.1100\n",
      "Epoch [287/300], Step [29/172], Loss: 13.7195\n",
      "Epoch [287/300], Step [30/172], Loss: 43.2057\n",
      "Epoch [287/300], Step [31/172], Loss: 26.3510\n",
      "Epoch [287/300], Step [32/172], Loss: 40.5714\n",
      "Epoch [287/300], Step [33/172], Loss: 58.9486\n",
      "Epoch [287/300], Step [34/172], Loss: 1.6251\n",
      "Epoch [287/300], Step [35/172], Loss: 14.2511\n",
      "Epoch [287/300], Step [36/172], Loss: 14.2182\n",
      "Epoch [287/300], Step [37/172], Loss: 13.5623\n",
      "Epoch [287/300], Step [38/172], Loss: 29.4619\n",
      "Epoch [287/300], Step [39/172], Loss: 31.8677\n",
      "Epoch [287/300], Step [40/172], Loss: 19.1826\n",
      "Epoch [287/300], Step [41/172], Loss: 28.1637\n",
      "Epoch [287/300], Step [42/172], Loss: 34.0844\n",
      "Epoch [287/300], Step [43/172], Loss: 24.6364\n",
      "Epoch [287/300], Step [44/172], Loss: 20.0134\n",
      "Epoch [287/300], Step [45/172], Loss: 26.1930\n",
      "Epoch [287/300], Step [46/172], Loss: 14.7564\n",
      "Epoch [287/300], Step [47/172], Loss: 45.2868\n",
      "Epoch [287/300], Step [48/172], Loss: 62.1366\n",
      "Epoch [287/300], Step [49/172], Loss: 20.3264\n",
      "Epoch [287/300], Step [50/172], Loss: 49.8886\n",
      "Epoch [287/300], Step [51/172], Loss: 6.9605\n",
      "Epoch [287/300], Step [52/172], Loss: 18.9855\n",
      "Epoch [287/300], Step [53/172], Loss: 21.8539\n",
      "Epoch [287/300], Step [54/172], Loss: 12.2637\n",
      "Epoch [287/300], Step [55/172], Loss: 13.1575\n",
      "Epoch [287/300], Step [56/172], Loss: 16.6126\n",
      "Epoch [287/300], Step [57/172], Loss: 17.4476\n",
      "Epoch [287/300], Step [58/172], Loss: 12.1293\n",
      "Epoch [287/300], Step [59/172], Loss: 26.9429\n",
      "Epoch [287/300], Step [60/172], Loss: 24.2646\n",
      "Epoch [287/300], Step [61/172], Loss: 4.8993\n",
      "Epoch [287/300], Step [62/172], Loss: 15.4539\n",
      "Epoch [287/300], Step [63/172], Loss: 8.1012\n",
      "Epoch [287/300], Step [64/172], Loss: 10.3627\n",
      "Epoch [287/300], Step [65/172], Loss: 17.4139\n",
      "Epoch [287/300], Step [66/172], Loss: 6.8079\n",
      "Epoch [287/300], Step [67/172], Loss: 25.7010\n",
      "Epoch [287/300], Step [68/172], Loss: 4.0478\n",
      "Epoch [287/300], Step [69/172], Loss: 30.1308\n",
      "Epoch [287/300], Step [70/172], Loss: 28.7244\n",
      "Epoch [287/300], Step [71/172], Loss: 32.3124\n",
      "Epoch [287/300], Step [72/172], Loss: 30.7871\n",
      "Epoch [287/300], Step [73/172], Loss: 39.3450\n",
      "Epoch [287/300], Step [74/172], Loss: 20.7424\n",
      "Epoch [287/300], Step [75/172], Loss: 19.3947\n",
      "Epoch [287/300], Step [76/172], Loss: 21.7241\n",
      "Epoch [287/300], Step [77/172], Loss: 40.8816\n",
      "Epoch [287/300], Step [78/172], Loss: 28.6794\n",
      "Epoch [287/300], Step [79/172], Loss: 26.1684\n",
      "Epoch [287/300], Step [80/172], Loss: 43.5800\n",
      "Epoch [287/300], Step [81/172], Loss: 24.3837\n",
      "Epoch [287/300], Step [82/172], Loss: 33.1196\n",
      "Epoch [287/300], Step [83/172], Loss: 36.4483\n",
      "Epoch [287/300], Step [84/172], Loss: 26.3216\n",
      "Epoch [287/300], Step [85/172], Loss: 31.3444\n",
      "Epoch [287/300], Step [86/172], Loss: 26.6378\n",
      "Epoch [287/300], Step [87/172], Loss: 19.6963\n",
      "Epoch [287/300], Step [88/172], Loss: 18.4667\n",
      "Epoch [287/300], Step [89/172], Loss: 22.9671\n",
      "Epoch [287/300], Step [90/172], Loss: 16.3240\n",
      "Epoch [287/300], Step [91/172], Loss: 22.2458\n",
      "Epoch [287/300], Step [92/172], Loss: 16.7214\n",
      "Epoch [287/300], Step [93/172], Loss: 16.3636\n",
      "Epoch [287/300], Step [94/172], Loss: 22.0347\n",
      "Epoch [287/300], Step [95/172], Loss: 16.4251\n",
      "Epoch [287/300], Step [96/172], Loss: 18.4256\n",
      "Epoch [287/300], Step [97/172], Loss: 26.3939\n",
      "Epoch [287/300], Step [98/172], Loss: 15.2786\n",
      "Epoch [287/300], Step [99/172], Loss: 16.1487\n",
      "Epoch [287/300], Step [100/172], Loss: 13.5839\n",
      "Epoch [287/300], Step [101/172], Loss: 16.3295\n",
      "Epoch [287/300], Step [102/172], Loss: 16.2943\n",
      "Epoch [287/300], Step [103/172], Loss: 9.5099\n",
      "Epoch [287/300], Step [104/172], Loss: 16.9673\n",
      "Epoch [287/300], Step [105/172], Loss: 19.6588\n",
      "Epoch [287/300], Step [106/172], Loss: 13.4283\n",
      "Epoch [287/300], Step [107/172], Loss: 14.3917\n",
      "Epoch [287/300], Step [108/172], Loss: 13.2263\n",
      "Epoch [287/300], Step [109/172], Loss: 13.8054\n",
      "Epoch [287/300], Step [110/172], Loss: 14.6840\n",
      "Epoch [287/300], Step [111/172], Loss: 16.0970\n",
      "Epoch [287/300], Step [112/172], Loss: 14.0900\n",
      "Epoch [287/300], Step [113/172], Loss: 12.3088\n",
      "Epoch [287/300], Step [114/172], Loss: 14.4610\n",
      "Epoch [287/300], Step [115/172], Loss: 19.0024\n",
      "Epoch [287/300], Step [116/172], Loss: 12.7261\n",
      "Epoch [287/300], Step [117/172], Loss: 11.4450\n",
      "Epoch [287/300], Step [118/172], Loss: 12.8584\n",
      "Epoch [287/300], Step [119/172], Loss: 16.8905\n",
      "Epoch [287/300], Step [120/172], Loss: 9.4844\n",
      "Epoch [287/300], Step [121/172], Loss: 8.2841\n",
      "Epoch [287/300], Step [122/172], Loss: 11.3889\n",
      "Epoch [287/300], Step [123/172], Loss: 10.3679\n",
      "Epoch [287/300], Step [124/172], Loss: 7.1255\n",
      "Epoch [287/300], Step [125/172], Loss: 11.2407\n",
      "Epoch [287/300], Step [126/172], Loss: 11.3337\n",
      "Epoch [287/300], Step [127/172], Loss: 10.4957\n",
      "Epoch [287/300], Step [128/172], Loss: 9.5864\n",
      "Epoch [287/300], Step [129/172], Loss: 8.4726\n",
      "Epoch [287/300], Step [130/172], Loss: 12.2616\n",
      "Epoch [287/300], Step [131/172], Loss: 7.1822\n",
      "Epoch [287/300], Step [132/172], Loss: 9.0234\n",
      "Epoch [287/300], Step [133/172], Loss: 9.0591\n",
      "Epoch [287/300], Step [134/172], Loss: 10.1963\n",
      "Epoch [287/300], Step [135/172], Loss: 9.0294\n",
      "Epoch [287/300], Step [136/172], Loss: 7.4220\n",
      "Epoch [287/300], Step [137/172], Loss: 7.8115\n",
      "Epoch [287/300], Step [138/172], Loss: 6.6047\n",
      "Epoch [287/300], Step [139/172], Loss: 11.2229\n",
      "Epoch [287/300], Step [140/172], Loss: 10.8770\n",
      "Epoch [287/300], Step [141/172], Loss: 8.1613\n",
      "Epoch [287/300], Step [142/172], Loss: 14.5562\n",
      "Epoch [287/300], Step [143/172], Loss: 11.3997\n",
      "Epoch [287/300], Step [144/172], Loss: 8.7437\n",
      "Epoch [287/300], Step [145/172], Loss: 10.6805\n",
      "Epoch [287/300], Step [146/172], Loss: 10.0002\n",
      "Epoch [287/300], Step [147/172], Loss: 5.3641\n",
      "Epoch [287/300], Step [148/172], Loss: 5.9674\n",
      "Epoch [287/300], Step [149/172], Loss: 5.4060\n",
      "Epoch [287/300], Step [150/172], Loss: 5.0152\n",
      "Epoch [287/300], Step [151/172], Loss: 5.0489\n",
      "Epoch [287/300], Step [152/172], Loss: 7.0485\n",
      "Epoch [287/300], Step [153/172], Loss: 6.0858\n",
      "Epoch [287/300], Step [154/172], Loss: 6.2823\n",
      "Epoch [287/300], Step [155/172], Loss: 6.3636\n",
      "Epoch [287/300], Step [156/172], Loss: 13.8724\n",
      "Epoch [287/300], Step [157/172], Loss: 8.9050\n",
      "Epoch [287/300], Step [158/172], Loss: 6.7894\n",
      "Epoch [287/300], Step [159/172], Loss: 9.6705\n",
      "Epoch [287/300], Step [160/172], Loss: 9.5978\n",
      "Epoch [287/300], Step [161/172], Loss: 4.9320\n",
      "Epoch [287/300], Step [162/172], Loss: 4.0322\n",
      "Epoch [287/300], Step [163/172], Loss: 6.6532\n",
      "Epoch [287/300], Step [164/172], Loss: 8.2469\n",
      "Epoch [287/300], Step [165/172], Loss: 6.7988\n",
      "Epoch [287/300], Step [166/172], Loss: 6.0333\n",
      "Epoch [287/300], Step [167/172], Loss: 10.9057\n",
      "Epoch [287/300], Step [168/172], Loss: 5.9084\n",
      "Epoch [287/300], Step [169/172], Loss: 6.5048\n",
      "Epoch [287/300], Step [170/172], Loss: 4.7973\n",
      "Epoch [287/300], Step [171/172], Loss: 8.9528\n",
      "Epoch [287/300], Step [172/172], Loss: 5.3390\n",
      "Epoch [288/300], Step [1/172], Loss: 37.6376\n",
      "Epoch [288/300], Step [2/172], Loss: 42.1423\n",
      "Epoch [288/300], Step [3/172], Loss: 38.2709\n",
      "Epoch [288/300], Step [4/172], Loss: 17.8221\n",
      "Epoch [288/300], Step [5/172], Loss: 34.2996\n",
      "Epoch [288/300], Step [6/172], Loss: 18.0036\n",
      "Epoch [288/300], Step [7/172], Loss: 24.9626\n",
      "Epoch [288/300], Step [8/172], Loss: 3.7092\n",
      "Epoch [288/300], Step [9/172], Loss: 23.4180\n",
      "Epoch [288/300], Step [10/172], Loss: 34.1536\n",
      "Epoch [288/300], Step [11/172], Loss: 49.0322\n",
      "Epoch [288/300], Step [12/172], Loss: 46.4381\n",
      "Epoch [288/300], Step [13/172], Loss: 30.4908\n",
      "Epoch [288/300], Step [14/172], Loss: 51.2775\n",
      "Epoch [288/300], Step [15/172], Loss: 48.9429\n",
      "Epoch [288/300], Step [16/172], Loss: 8.1175\n",
      "Epoch [288/300], Step [17/172], Loss: 34.4581\n",
      "Epoch [288/300], Step [18/172], Loss: 49.7495\n",
      "Epoch [288/300], Step [19/172], Loss: 67.2873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [288/300], Step [20/172], Loss: 24.6358\n",
      "Epoch [288/300], Step [21/172], Loss: 72.3814\n",
      "Epoch [288/300], Step [22/172], Loss: 46.3126\n",
      "Epoch [288/300], Step [23/172], Loss: 1.6562\n",
      "Epoch [288/300], Step [24/172], Loss: 44.1077\n",
      "Epoch [288/300], Step [25/172], Loss: 30.3498\n",
      "Epoch [288/300], Step [26/172], Loss: 41.1312\n",
      "Epoch [288/300], Step [27/172], Loss: 53.6848\n",
      "Epoch [288/300], Step [28/172], Loss: 15.2179\n",
      "Epoch [288/300], Step [29/172], Loss: 13.7039\n",
      "Epoch [288/300], Step [30/172], Loss: 44.0528\n",
      "Epoch [288/300], Step [31/172], Loss: 27.1756\n",
      "Epoch [288/300], Step [32/172], Loss: 41.0185\n",
      "Epoch [288/300], Step [33/172], Loss: 59.4785\n",
      "Epoch [288/300], Step [34/172], Loss: 1.5377\n",
      "Epoch [288/300], Step [35/172], Loss: 14.2452\n",
      "Epoch [288/300], Step [36/172], Loss: 14.2027\n",
      "Epoch [288/300], Step [37/172], Loss: 13.4903\n",
      "Epoch [288/300], Step [38/172], Loss: 29.2052\n",
      "Epoch [288/300], Step [39/172], Loss: 32.0589\n",
      "Epoch [288/300], Step [40/172], Loss: 19.1174\n",
      "Epoch [288/300], Step [41/172], Loss: 28.0275\n",
      "Epoch [288/300], Step [42/172], Loss: 33.7166\n",
      "Epoch [288/300], Step [43/172], Loss: 24.6935\n",
      "Epoch [288/300], Step [44/172], Loss: 19.9791\n",
      "Epoch [288/300], Step [45/172], Loss: 25.9174\n",
      "Epoch [288/300], Step [46/172], Loss: 14.5707\n",
      "Epoch [288/300], Step [47/172], Loss: 44.9570\n",
      "Epoch [288/300], Step [48/172], Loss: 61.9524\n",
      "Epoch [288/300], Step [49/172], Loss: 20.0265\n",
      "Epoch [288/300], Step [50/172], Loss: 49.3769\n",
      "Epoch [288/300], Step [51/172], Loss: 6.9118\n",
      "Epoch [288/300], Step [52/172], Loss: 18.8142\n",
      "Epoch [288/300], Step [53/172], Loss: 21.6168\n",
      "Epoch [288/300], Step [54/172], Loss: 12.3835\n",
      "Epoch [288/300], Step [55/172], Loss: 13.0771\n",
      "Epoch [288/300], Step [56/172], Loss: 17.0770\n",
      "Epoch [288/300], Step [57/172], Loss: 17.4606\n",
      "Epoch [288/300], Step [58/172], Loss: 12.2333\n",
      "Epoch [288/300], Step [59/172], Loss: 26.6923\n",
      "Epoch [288/300], Step [60/172], Loss: 24.9407\n",
      "Epoch [288/300], Step [61/172], Loss: 4.8172\n",
      "Epoch [288/300], Step [62/172], Loss: 15.6697\n",
      "Epoch [288/300], Step [63/172], Loss: 8.1082\n",
      "Epoch [288/300], Step [64/172], Loss: 10.3675\n",
      "Epoch [288/300], Step [65/172], Loss: 17.3379\n",
      "Epoch [288/300], Step [66/172], Loss: 6.6374\n",
      "Epoch [288/300], Step [67/172], Loss: 25.6121\n",
      "Epoch [288/300], Step [68/172], Loss: 4.4978\n",
      "Epoch [288/300], Step [69/172], Loss: 30.1863\n",
      "Epoch [288/300], Step [70/172], Loss: 28.5066\n",
      "Epoch [288/300], Step [71/172], Loss: 32.2221\n",
      "Epoch [288/300], Step [72/172], Loss: 30.6649\n",
      "Epoch [288/300], Step [73/172], Loss: 39.4074\n",
      "Epoch [288/300], Step [74/172], Loss: 20.6445\n",
      "Epoch [288/300], Step [75/172], Loss: 19.1321\n",
      "Epoch [288/300], Step [76/172], Loss: 21.4652\n",
      "Epoch [288/300], Step [77/172], Loss: 40.4870\n",
      "Epoch [288/300], Step [78/172], Loss: 28.4176\n",
      "Epoch [288/300], Step [79/172], Loss: 25.9461\n",
      "Epoch [288/300], Step [80/172], Loss: 43.2900\n",
      "Epoch [288/300], Step [81/172], Loss: 24.3409\n",
      "Epoch [288/300], Step [82/172], Loss: 32.6310\n",
      "Epoch [288/300], Step [83/172], Loss: 36.3996\n",
      "Epoch [288/300], Step [84/172], Loss: 26.1260\n",
      "Epoch [288/300], Step [85/172], Loss: 31.3921\n",
      "Epoch [288/300], Step [86/172], Loss: 26.6502\n",
      "Epoch [288/300], Step [87/172], Loss: 19.6466\n",
      "Epoch [288/300], Step [88/172], Loss: 18.4118\n",
      "Epoch [288/300], Step [89/172], Loss: 22.8212\n",
      "Epoch [288/300], Step [90/172], Loss: 16.2418\n",
      "Epoch [288/300], Step [91/172], Loss: 22.0579\n",
      "Epoch [288/300], Step [92/172], Loss: 16.6342\n",
      "Epoch [288/300], Step [93/172], Loss: 16.1086\n",
      "Epoch [288/300], Step [94/172], Loss: 22.1224\n",
      "Epoch [288/300], Step [95/172], Loss: 16.4537\n",
      "Epoch [288/300], Step [96/172], Loss: 18.2196\n",
      "Epoch [288/300], Step [97/172], Loss: 26.2446\n",
      "Epoch [288/300], Step [98/172], Loss: 15.2321\n",
      "Epoch [288/300], Step [99/172], Loss: 16.0853\n",
      "Epoch [288/300], Step [100/172], Loss: 13.5160\n",
      "Epoch [288/300], Step [101/172], Loss: 16.2597\n",
      "Epoch [288/300], Step [102/172], Loss: 16.2382\n",
      "Epoch [288/300], Step [103/172], Loss: 9.4917\n",
      "Epoch [288/300], Step [104/172], Loss: 16.8832\n",
      "Epoch [288/300], Step [105/172], Loss: 19.6357\n",
      "Epoch [288/300], Step [106/172], Loss: 13.3748\n",
      "Epoch [288/300], Step [107/172], Loss: 14.3483\n",
      "Epoch [288/300], Step [108/172], Loss: 13.1559\n",
      "Epoch [288/300], Step [109/172], Loss: 13.7779\n",
      "Epoch [288/300], Step [110/172], Loss: 14.5482\n",
      "Epoch [288/300], Step [111/172], Loss: 16.1612\n",
      "Epoch [288/300], Step [112/172], Loss: 14.0426\n",
      "Epoch [288/300], Step [113/172], Loss: 12.1554\n",
      "Epoch [288/300], Step [114/172], Loss: 14.3303\n",
      "Epoch [288/300], Step [115/172], Loss: 19.0032\n",
      "Epoch [288/300], Step [116/172], Loss: 12.6195\n",
      "Epoch [288/300], Step [117/172], Loss: 11.4687\n",
      "Epoch [288/300], Step [118/172], Loss: 12.8328\n",
      "Epoch [288/300], Step [119/172], Loss: 16.8637\n",
      "Epoch [288/300], Step [120/172], Loss: 9.6290\n",
      "Epoch [288/300], Step [121/172], Loss: 8.2718\n",
      "Epoch [288/300], Step [122/172], Loss: 11.3718\n",
      "Epoch [288/300], Step [123/172], Loss: 10.3929\n",
      "Epoch [288/300], Step [124/172], Loss: 7.0811\n",
      "Epoch [288/300], Step [125/172], Loss: 11.1698\n",
      "Epoch [288/300], Step [126/172], Loss: 11.2606\n",
      "Epoch [288/300], Step [127/172], Loss: 10.3517\n",
      "Epoch [288/300], Step [128/172], Loss: 9.4663\n",
      "Epoch [288/300], Step [129/172], Loss: 8.3708\n",
      "Epoch [288/300], Step [130/172], Loss: 12.2079\n",
      "Epoch [288/300], Step [131/172], Loss: 7.1056\n",
      "Epoch [288/300], Step [132/172], Loss: 8.9682\n",
      "Epoch [288/300], Step [133/172], Loss: 9.0890\n",
      "Epoch [288/300], Step [134/172], Loss: 10.2612\n",
      "Epoch [288/300], Step [135/172], Loss: 9.0485\n",
      "Epoch [288/300], Step [136/172], Loss: 7.3639\n",
      "Epoch [288/300], Step [137/172], Loss: 7.7982\n",
      "Epoch [288/300], Step [138/172], Loss: 6.5372\n",
      "Epoch [288/300], Step [139/172], Loss: 11.1581\n",
      "Epoch [288/300], Step [140/172], Loss: 10.7455\n",
      "Epoch [288/300], Step [141/172], Loss: 8.1097\n",
      "Epoch [288/300], Step [142/172], Loss: 14.4535\n",
      "Epoch [288/300], Step [143/172], Loss: 11.2956\n",
      "Epoch [288/300], Step [144/172], Loss: 8.6886\n",
      "Epoch [288/300], Step [145/172], Loss: 10.7145\n",
      "Epoch [288/300], Step [146/172], Loss: 9.9482\n",
      "Epoch [288/300], Step [147/172], Loss: 5.3151\n",
      "Epoch [288/300], Step [148/172], Loss: 5.9507\n",
      "Epoch [288/300], Step [149/172], Loss: 5.3643\n",
      "Epoch [288/300], Step [150/172], Loss: 4.9823\n",
      "Epoch [288/300], Step [151/172], Loss: 5.0036\n",
      "Epoch [288/300], Step [152/172], Loss: 7.0757\n",
      "Epoch [288/300], Step [153/172], Loss: 6.0402\n",
      "Epoch [288/300], Step [154/172], Loss: 6.2812\n",
      "Epoch [288/300], Step [155/172], Loss: 6.2897\n",
      "Epoch [288/300], Step [156/172], Loss: 13.7835\n",
      "Epoch [288/300], Step [157/172], Loss: 8.9139\n",
      "Epoch [288/300], Step [158/172], Loss: 6.6975\n",
      "Epoch [288/300], Step [159/172], Loss: 9.7677\n",
      "Epoch [288/300], Step [160/172], Loss: 9.5474\n",
      "Epoch [288/300], Step [161/172], Loss: 4.8630\n",
      "Epoch [288/300], Step [162/172], Loss: 4.0077\n",
      "Epoch [288/300], Step [163/172], Loss: 6.5753\n",
      "Epoch [288/300], Step [164/172], Loss: 8.2674\n",
      "Epoch [288/300], Step [165/172], Loss: 6.7365\n",
      "Epoch [288/300], Step [166/172], Loss: 6.0400\n",
      "Epoch [288/300], Step [167/172], Loss: 10.9865\n",
      "Epoch [288/300], Step [168/172], Loss: 5.9072\n",
      "Epoch [288/300], Step [169/172], Loss: 6.5185\n",
      "Epoch [288/300], Step [170/172], Loss: 4.7733\n",
      "Epoch [288/300], Step [171/172], Loss: 8.9526\n",
      "Epoch [288/300], Step [172/172], Loss: 5.3769\n",
      "Epoch [289/300], Step [1/172], Loss: 37.7335\n",
      "Epoch [289/300], Step [2/172], Loss: 42.4142\n",
      "Epoch [289/300], Step [3/172], Loss: 37.9457\n",
      "Epoch [289/300], Step [4/172], Loss: 17.9754\n",
      "Epoch [289/300], Step [5/172], Loss: 33.9066\n",
      "Epoch [289/300], Step [6/172], Loss: 18.0200\n",
      "Epoch [289/300], Step [7/172], Loss: 25.1865\n",
      "Epoch [289/300], Step [8/172], Loss: 3.6419\n",
      "Epoch [289/300], Step [9/172], Loss: 23.3270\n",
      "Epoch [289/300], Step [10/172], Loss: 34.0754\n",
      "Epoch [289/300], Step [11/172], Loss: 49.1425\n",
      "Epoch [289/300], Step [12/172], Loss: 46.5530\n",
      "Epoch [289/300], Step [13/172], Loss: 30.7029\n",
      "Epoch [289/300], Step [14/172], Loss: 50.6679\n",
      "Epoch [289/300], Step [15/172], Loss: 48.1787\n",
      "Epoch [289/300], Step [16/172], Loss: 8.0808\n",
      "Epoch [289/300], Step [17/172], Loss: 34.0553\n",
      "Epoch [289/300], Step [18/172], Loss: 49.4601\n",
      "Epoch [289/300], Step [19/172], Loss: 66.7296\n",
      "Epoch [289/300], Step [20/172], Loss: 25.0362\n",
      "Epoch [289/300], Step [21/172], Loss: 69.6440\n",
      "Epoch [289/300], Step [22/172], Loss: 46.2941\n",
      "Epoch [289/300], Step [23/172], Loss: 1.7415\n",
      "Epoch [289/300], Step [24/172], Loss: 44.2258\n",
      "Epoch [289/300], Step [25/172], Loss: 30.5919\n",
      "Epoch [289/300], Step [26/172], Loss: 41.4865\n",
      "Epoch [289/300], Step [27/172], Loss: 53.6803\n",
      "Epoch [289/300], Step [28/172], Loss: 15.2137\n",
      "Epoch [289/300], Step [29/172], Loss: 13.6533\n",
      "Epoch [289/300], Step [30/172], Loss: 43.3336\n",
      "Epoch [289/300], Step [31/172], Loss: 26.9900\n",
      "Epoch [289/300], Step [32/172], Loss: 41.2059\n",
      "Epoch [289/300], Step [33/172], Loss: 59.4514\n",
      "Epoch [289/300], Step [34/172], Loss: 1.3977\n",
      "Epoch [289/300], Step [35/172], Loss: 14.2365\n",
      "Epoch [289/300], Step [36/172], Loss: 14.0279\n",
      "Epoch [289/300], Step [37/172], Loss: 13.7673\n",
      "Epoch [289/300], Step [38/172], Loss: 30.2635\n",
      "Epoch [289/300], Step [39/172], Loss: 32.5439\n",
      "Epoch [289/300], Step [40/172], Loss: 19.5318\n",
      "Epoch [289/300], Step [41/172], Loss: 28.9772\n",
      "Epoch [289/300], Step [42/172], Loss: 34.5354\n",
      "Epoch [289/300], Step [43/172], Loss: 25.2595\n",
      "Epoch [289/300], Step [44/172], Loss: 20.4482\n",
      "Epoch [289/300], Step [45/172], Loss: 26.5834\n",
      "Epoch [289/300], Step [46/172], Loss: 14.6150\n",
      "Epoch [289/300], Step [47/172], Loss: 45.3065\n",
      "Epoch [289/300], Step [48/172], Loss: 62.0979\n",
      "Epoch [289/300], Step [49/172], Loss: 20.0893\n",
      "Epoch [289/300], Step [50/172], Loss: 49.4410\n",
      "Epoch [289/300], Step [51/172], Loss: 6.8613\n",
      "Epoch [289/300], Step [52/172], Loss: 18.5724\n",
      "Epoch [289/300], Step [53/172], Loss: 21.3563\n",
      "Epoch [289/300], Step [54/172], Loss: 11.8714\n",
      "Epoch [289/300], Step [55/172], Loss: 12.7750\n",
      "Epoch [289/300], Step [56/172], Loss: 16.6279\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [289/300], Step [57/172], Loss: 17.8566\n",
      "Epoch [289/300], Step [58/172], Loss: 11.9479\n",
      "Epoch [289/300], Step [59/172], Loss: 27.2969\n",
      "Epoch [289/300], Step [60/172], Loss: 24.2967\n",
      "Epoch [289/300], Step [61/172], Loss: 4.7376\n",
      "Epoch [289/300], Step [62/172], Loss: 15.3638\n",
      "Epoch [289/300], Step [63/172], Loss: 7.8667\n",
      "Epoch [289/300], Step [64/172], Loss: 10.0358\n",
      "Epoch [289/300], Step [65/172], Loss: 17.0166\n",
      "Epoch [289/300], Step [66/172], Loss: 6.4213\n",
      "Epoch [289/300], Step [67/172], Loss: 25.6056\n",
      "Epoch [289/300], Step [68/172], Loss: 3.8749\n",
      "Epoch [289/300], Step [69/172], Loss: 29.7916\n",
      "Epoch [289/300], Step [70/172], Loss: 28.9093\n",
      "Epoch [289/300], Step [71/172], Loss: 32.2836\n",
      "Epoch [289/300], Step [72/172], Loss: 30.8799\n",
      "Epoch [289/300], Step [73/172], Loss: 39.8554\n",
      "Epoch [289/300], Step [74/172], Loss: 20.8590\n",
      "Epoch [289/300], Step [75/172], Loss: 19.3176\n",
      "Epoch [289/300], Step [76/172], Loss: 21.3754\n",
      "Epoch [289/300], Step [77/172], Loss: 40.7415\n",
      "Epoch [289/300], Step [78/172], Loss: 28.4297\n",
      "Epoch [289/300], Step [79/172], Loss: 25.9973\n",
      "Epoch [289/300], Step [80/172], Loss: 43.4385\n",
      "Epoch [289/300], Step [81/172], Loss: 24.3426\n",
      "Epoch [289/300], Step [82/172], Loss: 32.4141\n",
      "Epoch [289/300], Step [83/172], Loss: 36.2716\n",
      "Epoch [289/300], Step [84/172], Loss: 26.1448\n",
      "Epoch [289/300], Step [85/172], Loss: 31.4713\n",
      "Epoch [289/300], Step [86/172], Loss: 26.7340\n",
      "Epoch [289/300], Step [87/172], Loss: 19.6372\n",
      "Epoch [289/300], Step [88/172], Loss: 18.4013\n",
      "Epoch [289/300], Step [89/172], Loss: 22.8791\n",
      "Epoch [289/300], Step [90/172], Loss: 16.2627\n",
      "Epoch [289/300], Step [91/172], Loss: 22.0773\n",
      "Epoch [289/300], Step [92/172], Loss: 16.5043\n",
      "Epoch [289/300], Step [93/172], Loss: 16.0216\n",
      "Epoch [289/300], Step [94/172], Loss: 22.1416\n",
      "Epoch [289/300], Step [95/172], Loss: 16.4800\n",
      "Epoch [289/300], Step [96/172], Loss: 18.2380\n",
      "Epoch [289/300], Step [97/172], Loss: 26.2404\n",
      "Epoch [289/300], Step [98/172], Loss: 15.2187\n",
      "Epoch [289/300], Step [99/172], Loss: 16.1082\n",
      "Epoch [289/300], Step [100/172], Loss: 13.5656\n",
      "Epoch [289/300], Step [101/172], Loss: 16.2386\n",
      "Epoch [289/300], Step [102/172], Loss: 16.1412\n",
      "Epoch [289/300], Step [103/172], Loss: 9.5395\n",
      "Epoch [289/300], Step [104/172], Loss: 16.9086\n",
      "Epoch [289/300], Step [105/172], Loss: 19.4418\n",
      "Epoch [289/300], Step [106/172], Loss: 13.1628\n",
      "Epoch [289/300], Step [107/172], Loss: 14.4203\n",
      "Epoch [289/300], Step [108/172], Loss: 13.2199\n",
      "Epoch [289/300], Step [109/172], Loss: 13.8133\n",
      "Epoch [289/300], Step [110/172], Loss: 14.6297\n",
      "Epoch [289/300], Step [111/172], Loss: 16.2533\n",
      "Epoch [289/300], Step [112/172], Loss: 14.1568\n",
      "Epoch [289/300], Step [113/172], Loss: 12.1967\n",
      "Epoch [289/300], Step [114/172], Loss: 14.2887\n",
      "Epoch [289/300], Step [115/172], Loss: 19.0626\n",
      "Epoch [289/300], Step [116/172], Loss: 12.5184\n",
      "Epoch [289/300], Step [117/172], Loss: 11.3941\n",
      "Epoch [289/300], Step [118/172], Loss: 13.0359\n",
      "Epoch [289/300], Step [119/172], Loss: 16.9139\n",
      "Epoch [289/300], Step [120/172], Loss: 9.5985\n",
      "Epoch [289/300], Step [121/172], Loss: 8.2836\n",
      "Epoch [289/300], Step [122/172], Loss: 11.5185\n",
      "Epoch [289/300], Step [123/172], Loss: 10.3120\n",
      "Epoch [289/300], Step [124/172], Loss: 7.0655\n",
      "Epoch [289/300], Step [125/172], Loss: 11.1197\n",
      "Epoch [289/300], Step [126/172], Loss: 11.2521\n",
      "Epoch [289/300], Step [127/172], Loss: 10.3881\n",
      "Epoch [289/300], Step [128/172], Loss: 9.4100\n",
      "Epoch [289/300], Step [129/172], Loss: 8.3478\n",
      "Epoch [289/300], Step [130/172], Loss: 12.1911\n",
      "Epoch [289/300], Step [131/172], Loss: 7.1141\n",
      "Epoch [289/300], Step [132/172], Loss: 8.9312\n",
      "Epoch [289/300], Step [133/172], Loss: 9.0582\n",
      "Epoch [289/300], Step [134/172], Loss: 10.2765\n",
      "Epoch [289/300], Step [135/172], Loss: 9.0407\n",
      "Epoch [289/300], Step [136/172], Loss: 7.2393\n",
      "Epoch [289/300], Step [137/172], Loss: 7.7706\n",
      "Epoch [289/300], Step [138/172], Loss: 6.5178\n",
      "Epoch [289/300], Step [139/172], Loss: 11.1400\n",
      "Epoch [289/300], Step [140/172], Loss: 10.7701\n",
      "Epoch [289/300], Step [141/172], Loss: 7.9884\n",
      "Epoch [289/300], Step [142/172], Loss: 14.4764\n",
      "Epoch [289/300], Step [143/172], Loss: 11.2855\n",
      "Epoch [289/300], Step [144/172], Loss: 8.6455\n",
      "Epoch [289/300], Step [145/172], Loss: 10.7582\n",
      "Epoch [289/300], Step [146/172], Loss: 9.9605\n",
      "Epoch [289/300], Step [147/172], Loss: 5.3407\n",
      "Epoch [289/300], Step [148/172], Loss: 5.9185\n",
      "Epoch [289/300], Step [149/172], Loss: 5.3924\n",
      "Epoch [289/300], Step [150/172], Loss: 4.9061\n",
      "Epoch [289/300], Step [151/172], Loss: 4.9488\n",
      "Epoch [289/300], Step [152/172], Loss: 7.0968\n",
      "Epoch [289/300], Step [153/172], Loss: 6.0884\n",
      "Epoch [289/300], Step [154/172], Loss: 6.2430\n",
      "Epoch [289/300], Step [155/172], Loss: 6.3826\n",
      "Epoch [289/300], Step [156/172], Loss: 13.8100\n",
      "Epoch [289/300], Step [157/172], Loss: 8.8736\n",
      "Epoch [289/300], Step [158/172], Loss: 6.7021\n",
      "Epoch [289/300], Step [159/172], Loss: 9.8668\n",
      "Epoch [289/300], Step [160/172], Loss: 9.5571\n",
      "Epoch [289/300], Step [161/172], Loss: 4.8915\n",
      "Epoch [289/300], Step [162/172], Loss: 3.9833\n",
      "Epoch [289/300], Step [163/172], Loss: 6.5025\n",
      "Epoch [289/300], Step [164/172], Loss: 8.1707\n",
      "Epoch [289/300], Step [165/172], Loss: 6.6855\n",
      "Epoch [289/300], Step [166/172], Loss: 6.0152\n",
      "Epoch [289/300], Step [167/172], Loss: 10.9876\n",
      "Epoch [289/300], Step [168/172], Loss: 5.7615\n",
      "Epoch [289/300], Step [169/172], Loss: 6.6536\n",
      "Epoch [289/300], Step [170/172], Loss: 4.8055\n",
      "Epoch [289/300], Step [171/172], Loss: 8.9261\n",
      "Epoch [289/300], Step [172/172], Loss: 5.5639\n",
      "Epoch [290/300], Step [1/172], Loss: 37.8549\n",
      "Epoch [290/300], Step [2/172], Loss: 42.4229\n",
      "Epoch [290/300], Step [3/172], Loss: 38.0492\n",
      "Epoch [290/300], Step [4/172], Loss: 17.9085\n",
      "Epoch [290/300], Step [5/172], Loss: 34.1319\n",
      "Epoch [290/300], Step [6/172], Loss: 18.2348\n",
      "Epoch [290/300], Step [7/172], Loss: 25.0503\n",
      "Epoch [290/300], Step [8/172], Loss: 5.2407\n",
      "Epoch [290/300], Step [9/172], Loss: 23.6472\n",
      "Epoch [290/300], Step [10/172], Loss: 34.2274\n",
      "Epoch [290/300], Step [11/172], Loss: 48.9746\n",
      "Epoch [290/300], Step [12/172], Loss: 46.6653\n",
      "Epoch [290/300], Step [13/172], Loss: 30.9261\n",
      "Epoch [290/300], Step [14/172], Loss: 51.2837\n",
      "Epoch [290/300], Step [15/172], Loss: 48.1456\n",
      "Epoch [290/300], Step [16/172], Loss: 6.9253\n",
      "Epoch [290/300], Step [17/172], Loss: 34.3822\n",
      "Epoch [290/300], Step [18/172], Loss: 49.7856\n",
      "Epoch [290/300], Step [19/172], Loss: 66.4145\n",
      "Epoch [290/300], Step [20/172], Loss: 24.9620\n",
      "Epoch [290/300], Step [21/172], Loss: 69.1956\n",
      "Epoch [290/300], Step [22/172], Loss: 46.2395\n",
      "Epoch [290/300], Step [23/172], Loss: 1.5528\n",
      "Epoch [290/300], Step [24/172], Loss: 44.3751\n",
      "Epoch [290/300], Step [25/172], Loss: 30.1028\n",
      "Epoch [290/300], Step [26/172], Loss: 41.3308\n",
      "Epoch [290/300], Step [27/172], Loss: 54.1861\n",
      "Epoch [290/300], Step [28/172], Loss: 15.3370\n",
      "Epoch [290/300], Step [29/172], Loss: 13.9013\n",
      "Epoch [290/300], Step [30/172], Loss: 43.0931\n",
      "Epoch [290/300], Step [31/172], Loss: 27.2649\n",
      "Epoch [290/300], Step [32/172], Loss: 41.5560\n",
      "Epoch [290/300], Step [33/172], Loss: 59.6576\n",
      "Epoch [290/300], Step [34/172], Loss: 1.3557\n",
      "Epoch [290/300], Step [35/172], Loss: 14.2074\n",
      "Epoch [290/300], Step [36/172], Loss: 14.1707\n",
      "Epoch [290/300], Step [37/172], Loss: 14.0473\n",
      "Epoch [290/300], Step [38/172], Loss: 30.6804\n",
      "Epoch [290/300], Step [39/172], Loss: 33.0566\n",
      "Epoch [290/300], Step [40/172], Loss: 19.7449\n",
      "Epoch [290/300], Step [41/172], Loss: 29.1495\n",
      "Epoch [290/300], Step [42/172], Loss: 34.8638\n",
      "Epoch [290/300], Step [43/172], Loss: 25.3194\n",
      "Epoch [290/300], Step [44/172], Loss: 20.2990\n",
      "Epoch [290/300], Step [45/172], Loss: 26.8765\n",
      "Epoch [290/300], Step [46/172], Loss: 14.5615\n",
      "Epoch [290/300], Step [47/172], Loss: 45.4287\n",
      "Epoch [290/300], Step [48/172], Loss: 61.7749\n",
      "Epoch [290/300], Step [49/172], Loss: 20.2953\n",
      "Epoch [290/300], Step [50/172], Loss: 49.3377\n",
      "Epoch [290/300], Step [51/172], Loss: 6.9524\n",
      "Epoch [290/300], Step [52/172], Loss: 18.4809\n",
      "Epoch [290/300], Step [53/172], Loss: 21.3552\n",
      "Epoch [290/300], Step [54/172], Loss: 11.6587\n",
      "Epoch [290/300], Step [55/172], Loss: 12.5544\n",
      "Epoch [290/300], Step [56/172], Loss: 16.9834\n",
      "Epoch [290/300], Step [57/172], Loss: 17.3455\n",
      "Epoch [290/300], Step [58/172], Loss: 11.9489\n",
      "Epoch [290/300], Step [59/172], Loss: 27.4161\n",
      "Epoch [290/300], Step [60/172], Loss: 24.2686\n",
      "Epoch [290/300], Step [61/172], Loss: 4.5441\n",
      "Epoch [290/300], Step [62/172], Loss: 15.3399\n",
      "Epoch [290/300], Step [63/172], Loss: 7.7647\n",
      "Epoch [290/300], Step [64/172], Loss: 9.8400\n",
      "Epoch [290/300], Step [65/172], Loss: 16.7326\n",
      "Epoch [290/300], Step [66/172], Loss: 6.3511\n",
      "Epoch [290/300], Step [67/172], Loss: 25.3926\n",
      "Epoch [290/300], Step [68/172], Loss: 3.4179\n",
      "Epoch [290/300], Step [69/172], Loss: 30.0145\n",
      "Epoch [290/300], Step [70/172], Loss: 28.8568\n",
      "Epoch [290/300], Step [71/172], Loss: 32.3464\n",
      "Epoch [290/300], Step [72/172], Loss: 30.8683\n",
      "Epoch [290/300], Step [73/172], Loss: 39.9729\n",
      "Epoch [290/300], Step [74/172], Loss: 20.7968\n",
      "Epoch [290/300], Step [75/172], Loss: 19.1293\n",
      "Epoch [290/300], Step [76/172], Loss: 21.2233\n",
      "Epoch [290/300], Step [77/172], Loss: 40.5445\n",
      "Epoch [290/300], Step [78/172], Loss: 28.3131\n",
      "Epoch [290/300], Step [79/172], Loss: 25.8958\n",
      "Epoch [290/300], Step [80/172], Loss: 43.5595\n",
      "Epoch [290/300], Step [81/172], Loss: 24.2423\n",
      "Epoch [290/300], Step [82/172], Loss: 32.8994\n",
      "Epoch [290/300], Step [83/172], Loss: 36.0797\n",
      "Epoch [290/300], Step [84/172], Loss: 26.0749\n",
      "Epoch [290/300], Step [85/172], Loss: 31.2077\n",
      "Epoch [290/300], Step [86/172], Loss: 26.3589\n",
      "Epoch [290/300], Step [87/172], Loss: 19.6046\n",
      "Epoch [290/300], Step [88/172], Loss: 18.2144\n",
      "Epoch [290/300], Step [89/172], Loss: 22.7724\n",
      "Epoch [290/300], Step [90/172], Loss: 15.9912\n",
      "Epoch [290/300], Step [91/172], Loss: 21.9444\n",
      "Epoch [290/300], Step [92/172], Loss: 16.4234\n",
      "Epoch [290/300], Step [93/172], Loss: 15.9101\n",
      "Epoch [290/300], Step [94/172], Loss: 21.9682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [290/300], Step [95/172], Loss: 16.3315\n",
      "Epoch [290/300], Step [96/172], Loss: 18.1864\n",
      "Epoch [290/300], Step [97/172], Loss: 26.0896\n",
      "Epoch [290/300], Step [98/172], Loss: 15.2019\n",
      "Epoch [290/300], Step [99/172], Loss: 16.0550\n",
      "Epoch [290/300], Step [100/172], Loss: 13.6063\n",
      "Epoch [290/300], Step [101/172], Loss: 16.2544\n",
      "Epoch [290/300], Step [102/172], Loss: 16.3681\n",
      "Epoch [290/300], Step [103/172], Loss: 9.6898\n",
      "Epoch [290/300], Step [104/172], Loss: 16.8602\n",
      "Epoch [290/300], Step [105/172], Loss: 19.8644\n",
      "Epoch [290/300], Step [106/172], Loss: 13.1407\n",
      "Epoch [290/300], Step [107/172], Loss: 14.4081\n",
      "Epoch [290/300], Step [108/172], Loss: 13.2112\n",
      "Epoch [290/300], Step [109/172], Loss: 13.9231\n",
      "Epoch [290/300], Step [110/172], Loss: 14.9048\n",
      "Epoch [290/300], Step [111/172], Loss: 16.4322\n",
      "Epoch [290/300], Step [112/172], Loss: 13.9855\n",
      "Epoch [290/300], Step [113/172], Loss: 12.3204\n",
      "Epoch [290/300], Step [114/172], Loss: 14.2688\n",
      "Epoch [290/300], Step [115/172], Loss: 18.9746\n",
      "Epoch [290/300], Step [116/172], Loss: 12.6474\n",
      "Epoch [290/300], Step [117/172], Loss: 11.3945\n",
      "Epoch [290/300], Step [118/172], Loss: 13.0108\n",
      "Epoch [290/300], Step [119/172], Loss: 16.9534\n",
      "Epoch [290/300], Step [120/172], Loss: 9.5907\n",
      "Epoch [290/300], Step [121/172], Loss: 8.3013\n",
      "Epoch [290/300], Step [122/172], Loss: 11.6176\n",
      "Epoch [290/300], Step [123/172], Loss: 10.3108\n",
      "Epoch [290/300], Step [124/172], Loss: 7.0555\n",
      "Epoch [290/300], Step [125/172], Loss: 11.1639\n",
      "Epoch [290/300], Step [126/172], Loss: 11.3750\n",
      "Epoch [290/300], Step [127/172], Loss: 10.3130\n",
      "Epoch [290/300], Step [128/172], Loss: 9.3832\n",
      "Epoch [290/300], Step [129/172], Loss: 8.2642\n",
      "Epoch [290/300], Step [130/172], Loss: 12.1025\n",
      "Epoch [290/300], Step [131/172], Loss: 7.1396\n",
      "Epoch [290/300], Step [132/172], Loss: 9.1028\n",
      "Epoch [290/300], Step [133/172], Loss: 8.9983\n",
      "Epoch [290/300], Step [134/172], Loss: 10.1018\n",
      "Epoch [290/300], Step [135/172], Loss: 9.0855\n",
      "Epoch [290/300], Step [136/172], Loss: 7.3763\n",
      "Epoch [290/300], Step [137/172], Loss: 7.8511\n",
      "Epoch [290/300], Step [138/172], Loss: 6.4549\n",
      "Epoch [290/300], Step [139/172], Loss: 11.0687\n",
      "Epoch [290/300], Step [140/172], Loss: 10.9891\n",
      "Epoch [290/300], Step [141/172], Loss: 8.1029\n",
      "Epoch [290/300], Step [142/172], Loss: 14.2863\n",
      "Epoch [290/300], Step [143/172], Loss: 11.2185\n",
      "Epoch [290/300], Step [144/172], Loss: 8.6060\n",
      "Epoch [290/300], Step [145/172], Loss: 10.9013\n",
      "Epoch [290/300], Step [146/172], Loss: 10.0491\n",
      "Epoch [290/300], Step [147/172], Loss: 5.3213\n",
      "Epoch [290/300], Step [148/172], Loss: 6.0855\n",
      "Epoch [290/300], Step [149/172], Loss: 5.5711\n",
      "Epoch [290/300], Step [150/172], Loss: 5.0205\n",
      "Epoch [290/300], Step [151/172], Loss: 5.0283\n",
      "Epoch [290/300], Step [152/172], Loss: 7.2636\n",
      "Epoch [290/300], Step [153/172], Loss: 6.1460\n",
      "Epoch [290/300], Step [154/172], Loss: 6.3054\n",
      "Epoch [290/300], Step [155/172], Loss: 6.4896\n",
      "Epoch [290/300], Step [156/172], Loss: 13.6166\n",
      "Epoch [290/300], Step [157/172], Loss: 8.5875\n",
      "Epoch [290/300], Step [158/172], Loss: 6.6616\n",
      "Epoch [290/300], Step [159/172], Loss: 10.0507\n",
      "Epoch [290/300], Step [160/172], Loss: 9.2516\n",
      "Epoch [290/300], Step [161/172], Loss: 5.0544\n",
      "Epoch [290/300], Step [162/172], Loss: 4.0965\n",
      "Epoch [290/300], Step [163/172], Loss: 6.4800\n",
      "Epoch [290/300], Step [164/172], Loss: 8.6533\n",
      "Epoch [290/300], Step [165/172], Loss: 6.8123\n",
      "Epoch [290/300], Step [166/172], Loss: 6.2356\n",
      "Epoch [290/300], Step [167/172], Loss: 10.9208\n",
      "Epoch [290/300], Step [168/172], Loss: 5.9160\n",
      "Epoch [290/300], Step [169/172], Loss: 6.7921\n",
      "Epoch [290/300], Step [170/172], Loss: 5.0315\n",
      "Epoch [290/300], Step [171/172], Loss: 9.2041\n",
      "Epoch [290/300], Step [172/172], Loss: 5.5042\n",
      "Epoch [291/300], Step [1/172], Loss: 38.0576\n",
      "Epoch [291/300], Step [2/172], Loss: 43.2848\n",
      "Epoch [291/300], Step [3/172], Loss: 37.8293\n",
      "Epoch [291/300], Step [4/172], Loss: 18.0706\n",
      "Epoch [291/300], Step [5/172], Loss: 34.2685\n",
      "Epoch [291/300], Step [6/172], Loss: 20.0043\n",
      "Epoch [291/300], Step [7/172], Loss: 28.3403\n",
      "Epoch [291/300], Step [8/172], Loss: 3.6391\n",
      "Epoch [291/300], Step [9/172], Loss: 22.8726\n",
      "Epoch [291/300], Step [10/172], Loss: 34.4955\n",
      "Epoch [291/300], Step [11/172], Loss: 48.3371\n",
      "Epoch [291/300], Step [12/172], Loss: 46.6263\n",
      "Epoch [291/300], Step [13/172], Loss: 31.1479\n",
      "Epoch [291/300], Step [14/172], Loss: 52.0193\n",
      "Epoch [291/300], Step [15/172], Loss: 47.9364\n",
      "Epoch [291/300], Step [16/172], Loss: 7.5659\n",
      "Epoch [291/300], Step [17/172], Loss: 34.7496\n",
      "Epoch [291/300], Step [18/172], Loss: 49.3926\n",
      "Epoch [291/300], Step [19/172], Loss: 65.7254\n",
      "Epoch [291/300], Step [20/172], Loss: 24.7049\n",
      "Epoch [291/300], Step [21/172], Loss: 70.2975\n",
      "Epoch [291/300], Step [22/172], Loss: 45.7489\n",
      "Epoch [291/300], Step [23/172], Loss: 1.4694\n",
      "Epoch [291/300], Step [24/172], Loss: 44.1405\n",
      "Epoch [291/300], Step [25/172], Loss: 29.2643\n",
      "Epoch [291/300], Step [26/172], Loss: 41.2253\n",
      "Epoch [291/300], Step [27/172], Loss: 54.2327\n",
      "Epoch [291/300], Step [28/172], Loss: 15.1334\n",
      "Epoch [291/300], Step [29/172], Loss: 13.5733\n",
      "Epoch [291/300], Step [30/172], Loss: 42.8577\n",
      "Epoch [291/300], Step [31/172], Loss: 27.3218\n",
      "Epoch [291/300], Step [32/172], Loss: 41.7657\n",
      "Epoch [291/300], Step [33/172], Loss: 59.6746\n",
      "Epoch [291/300], Step [34/172], Loss: 1.4334\n",
      "Epoch [291/300], Step [35/172], Loss: 14.3529\n",
      "Epoch [291/300], Step [36/172], Loss: 14.8065\n",
      "Epoch [291/300], Step [37/172], Loss: 14.1102\n",
      "Epoch [291/300], Step [38/172], Loss: 30.6325\n",
      "Epoch [291/300], Step [39/172], Loss: 33.2998\n",
      "Epoch [291/300], Step [40/172], Loss: 20.0311\n",
      "Epoch [291/300], Step [41/172], Loss: 29.3756\n",
      "Epoch [291/300], Step [42/172], Loss: 36.1387\n",
      "Epoch [291/300], Step [43/172], Loss: 26.0648\n",
      "Epoch [291/300], Step [44/172], Loss: 20.3058\n",
      "Epoch [291/300], Step [45/172], Loss: 27.4449\n",
      "Epoch [291/300], Step [46/172], Loss: 14.4038\n",
      "Epoch [291/300], Step [47/172], Loss: 45.5218\n",
      "Epoch [291/300], Step [48/172], Loss: 62.0535\n",
      "Epoch [291/300], Step [49/172], Loss: 20.6860\n",
      "Epoch [291/300], Step [50/172], Loss: 49.4360\n",
      "Epoch [291/300], Step [51/172], Loss: 6.9699\n",
      "Epoch [291/300], Step [52/172], Loss: 18.5503\n",
      "Epoch [291/300], Step [53/172], Loss: 21.4210\n",
      "Epoch [291/300], Step [54/172], Loss: 12.0445\n",
      "Epoch [291/300], Step [55/172], Loss: 12.4844\n",
      "Epoch [291/300], Step [56/172], Loss: 17.8278\n",
      "Epoch [291/300], Step [57/172], Loss: 17.2357\n",
      "Epoch [291/300], Step [58/172], Loss: 11.7827\n",
      "Epoch [291/300], Step [59/172], Loss: 27.2424\n",
      "Epoch [291/300], Step [60/172], Loss: 24.6905\n",
      "Epoch [291/300], Step [61/172], Loss: 4.5868\n",
      "Epoch [291/300], Step [62/172], Loss: 15.6197\n",
      "Epoch [291/300], Step [63/172], Loss: 7.6620\n",
      "Epoch [291/300], Step [64/172], Loss: 9.6841\n",
      "Epoch [291/300], Step [65/172], Loss: 16.4847\n",
      "Epoch [291/300], Step [66/172], Loss: 6.1147\n",
      "Epoch [291/300], Step [67/172], Loss: 25.3969\n",
      "Epoch [291/300], Step [68/172], Loss: 3.7315\n",
      "Epoch [291/300], Step [69/172], Loss: 30.2150\n",
      "Epoch [291/300], Step [70/172], Loss: 29.0408\n",
      "Epoch [291/300], Step [71/172], Loss: 32.5487\n",
      "Epoch [291/300], Step [72/172], Loss: 30.7821\n",
      "Epoch [291/300], Step [73/172], Loss: 40.3914\n",
      "Epoch [291/300], Step [74/172], Loss: 20.8018\n",
      "Epoch [291/300], Step [75/172], Loss: 19.1528\n",
      "Epoch [291/300], Step [76/172], Loss: 21.4020\n",
      "Epoch [291/300], Step [77/172], Loss: 40.1439\n",
      "Epoch [291/300], Step [78/172], Loss: 28.1605\n",
      "Epoch [291/300], Step [79/172], Loss: 25.7985\n",
      "Epoch [291/300], Step [80/172], Loss: 42.7377\n",
      "Epoch [291/300], Step [81/172], Loss: 24.3164\n",
      "Epoch [291/300], Step [82/172], Loss: 32.1597\n",
      "Epoch [291/300], Step [83/172], Loss: 36.0185\n",
      "Epoch [291/300], Step [84/172], Loss: 25.9213\n",
      "Epoch [291/300], Step [85/172], Loss: 31.4287\n",
      "Epoch [291/300], Step [86/172], Loss: 26.5121\n",
      "Epoch [291/300], Step [87/172], Loss: 19.5588\n",
      "Epoch [291/300], Step [88/172], Loss: 18.4329\n",
      "Epoch [291/300], Step [89/172], Loss: 23.0865\n",
      "Epoch [291/300], Step [90/172], Loss: 16.1085\n",
      "Epoch [291/300], Step [91/172], Loss: 21.8837\n",
      "Epoch [291/300], Step [92/172], Loss: 16.3903\n",
      "Epoch [291/300], Step [93/172], Loss: 15.8761\n",
      "Epoch [291/300], Step [94/172], Loss: 22.0970\n",
      "Epoch [291/300], Step [95/172], Loss: 16.2705\n",
      "Epoch [291/300], Step [96/172], Loss: 18.3374\n",
      "Epoch [291/300], Step [97/172], Loss: 25.9550\n",
      "Epoch [291/300], Step [98/172], Loss: 15.3794\n",
      "Epoch [291/300], Step [99/172], Loss: 16.3461\n",
      "Epoch [291/300], Step [100/172], Loss: 14.0784\n",
      "Epoch [291/300], Step [101/172], Loss: 16.3665\n",
      "Epoch [291/300], Step [102/172], Loss: 16.2760\n",
      "Epoch [291/300], Step [103/172], Loss: 9.9902\n",
      "Epoch [291/300], Step [104/172], Loss: 17.1031\n",
      "Epoch [291/300], Step [105/172], Loss: 20.0152\n",
      "Epoch [291/300], Step [106/172], Loss: 13.2769\n",
      "Epoch [291/300], Step [107/172], Loss: 14.4266\n",
      "Epoch [291/300], Step [108/172], Loss: 13.4221\n",
      "Epoch [291/300], Step [109/172], Loss: 13.9514\n",
      "Epoch [291/300], Step [110/172], Loss: 15.2295\n",
      "Epoch [291/300], Step [111/172], Loss: 16.7129\n",
      "Epoch [291/300], Step [112/172], Loss: 13.9961\n",
      "Epoch [291/300], Step [113/172], Loss: 12.1893\n",
      "Epoch [291/300], Step [114/172], Loss: 14.3463\n",
      "Epoch [291/300], Step [115/172], Loss: 18.8219\n",
      "Epoch [291/300], Step [116/172], Loss: 12.7160\n",
      "Epoch [291/300], Step [117/172], Loss: 11.4357\n",
      "Epoch [291/300], Step [118/172], Loss: 12.8387\n",
      "Epoch [291/300], Step [119/172], Loss: 17.0533\n",
      "Epoch [291/300], Step [120/172], Loss: 9.7491\n",
      "Epoch [291/300], Step [121/172], Loss: 8.5285\n",
      "Epoch [291/300], Step [122/172], Loss: 12.1200\n",
      "Epoch [291/300], Step [123/172], Loss: 10.3934\n",
      "Epoch [291/300], Step [124/172], Loss: 7.1130\n",
      "Epoch [291/300], Step [125/172], Loss: 11.2484\n",
      "Epoch [291/300], Step [126/172], Loss: 11.2691\n",
      "Epoch [291/300], Step [127/172], Loss: 10.1682\n",
      "Epoch [291/300], Step [128/172], Loss: 9.2791\n",
      "Epoch [291/300], Step [129/172], Loss: 8.3204\n",
      "Epoch [291/300], Step [130/172], Loss: 11.9696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [291/300], Step [131/172], Loss: 7.2846\n",
      "Epoch [291/300], Step [132/172], Loss: 9.1519\n",
      "Epoch [291/300], Step [133/172], Loss: 9.3177\n",
      "Epoch [291/300], Step [134/172], Loss: 10.1257\n",
      "Epoch [291/300], Step [135/172], Loss: 9.1434\n",
      "Epoch [291/300], Step [136/172], Loss: 7.4190\n",
      "Epoch [291/300], Step [137/172], Loss: 7.9546\n",
      "Epoch [291/300], Step [138/172], Loss: 6.5724\n",
      "Epoch [291/300], Step [139/172], Loss: 11.1455\n",
      "Epoch [291/300], Step [140/172], Loss: 10.7872\n",
      "Epoch [291/300], Step [141/172], Loss: 8.1110\n",
      "Epoch [291/300], Step [142/172], Loss: 14.4161\n",
      "Epoch [291/300], Step [143/172], Loss: 11.1078\n",
      "Epoch [291/300], Step [144/172], Loss: 8.7097\n",
      "Epoch [291/300], Step [145/172], Loss: 10.8564\n",
      "Epoch [291/300], Step [146/172], Loss: 10.1255\n",
      "Epoch [291/300], Step [147/172], Loss: 5.3203\n",
      "Epoch [291/300], Step [148/172], Loss: 6.0508\n",
      "Epoch [291/300], Step [149/172], Loss: 5.6388\n",
      "Epoch [291/300], Step [150/172], Loss: 5.0261\n",
      "Epoch [291/300], Step [151/172], Loss: 4.8992\n",
      "Epoch [291/300], Step [152/172], Loss: 7.4267\n",
      "Epoch [291/300], Step [153/172], Loss: 6.1086\n",
      "Epoch [291/300], Step [154/172], Loss: 6.3011\n",
      "Epoch [291/300], Step [155/172], Loss: 6.6406\n",
      "Epoch [291/300], Step [156/172], Loss: 13.6137\n",
      "Epoch [291/300], Step [157/172], Loss: 8.6768\n",
      "Epoch [291/300], Step [158/172], Loss: 6.6793\n",
      "Epoch [291/300], Step [159/172], Loss: 10.0334\n",
      "Epoch [291/300], Step [160/172], Loss: 9.4326\n",
      "Epoch [291/300], Step [161/172], Loss: 5.2690\n",
      "Epoch [291/300], Step [162/172], Loss: 4.1552\n",
      "Epoch [291/300], Step [163/172], Loss: 6.3589\n",
      "Epoch [291/300], Step [164/172], Loss: 8.5295\n",
      "Epoch [291/300], Step [165/172], Loss: 6.6607\n",
      "Epoch [291/300], Step [166/172], Loss: 6.3957\n",
      "Epoch [291/300], Step [167/172], Loss: 10.9528\n",
      "Epoch [291/300], Step [168/172], Loss: 5.9214\n",
      "Epoch [291/300], Step [169/172], Loss: 6.8017\n",
      "Epoch [291/300], Step [170/172], Loss: 5.0940\n",
      "Epoch [291/300], Step [171/172], Loss: 9.0282\n",
      "Epoch [291/300], Step [172/172], Loss: 5.6015\n",
      "Epoch [292/300], Step [1/172], Loss: 38.4365\n",
      "Epoch [292/300], Step [2/172], Loss: 43.6840\n",
      "Epoch [292/300], Step [3/172], Loss: 38.2442\n",
      "Epoch [292/300], Step [4/172], Loss: 18.4785\n",
      "Epoch [292/300], Step [5/172], Loss: 34.8200\n",
      "Epoch [292/300], Step [6/172], Loss: 19.3663\n",
      "Epoch [292/300], Step [7/172], Loss: 24.9875\n",
      "Epoch [292/300], Step [8/172], Loss: 5.1606\n",
      "Epoch [292/300], Step [9/172], Loss: 23.6333\n",
      "Epoch [292/300], Step [10/172], Loss: 35.1543\n",
      "Epoch [292/300], Step [11/172], Loss: 48.3128\n",
      "Epoch [292/300], Step [12/172], Loss: 47.3309\n",
      "Epoch [292/300], Step [13/172], Loss: 32.0725\n",
      "Epoch [292/300], Step [14/172], Loss: 52.4500\n",
      "Epoch [292/300], Step [15/172], Loss: 47.5077\n",
      "Epoch [292/300], Step [16/172], Loss: 7.7432\n",
      "Epoch [292/300], Step [17/172], Loss: 34.8427\n",
      "Epoch [292/300], Step [18/172], Loss: 49.1278\n",
      "Epoch [292/300], Step [19/172], Loss: 64.8158\n",
      "Epoch [292/300], Step [20/172], Loss: 25.6300\n",
      "Epoch [292/300], Step [21/172], Loss: 70.6932\n",
      "Epoch [292/300], Step [22/172], Loss: 46.4915\n",
      "Epoch [292/300], Step [23/172], Loss: 1.8034\n",
      "Epoch [292/300], Step [24/172], Loss: 44.5778\n",
      "Epoch [292/300], Step [25/172], Loss: 30.2379\n",
      "Epoch [292/300], Step [26/172], Loss: 41.9039\n",
      "Epoch [292/300], Step [27/172], Loss: 54.4491\n",
      "Epoch [292/300], Step [28/172], Loss: 15.0642\n",
      "Epoch [292/300], Step [29/172], Loss: 13.4490\n",
      "Epoch [292/300], Step [30/172], Loss: 43.2160\n",
      "Epoch [292/300], Step [31/172], Loss: 27.4622\n",
      "Epoch [292/300], Step [32/172], Loss: 41.9654\n",
      "Epoch [292/300], Step [33/172], Loss: 60.1743\n",
      "Epoch [292/300], Step [34/172], Loss: 1.1942\n",
      "Epoch [292/300], Step [35/172], Loss: 14.2871\n",
      "Epoch [292/300], Step [36/172], Loss: 14.2325\n",
      "Epoch [292/300], Step [37/172], Loss: 14.4189\n",
      "Epoch [292/300], Step [38/172], Loss: 31.6671\n",
      "Epoch [292/300], Step [39/172], Loss: 33.8538\n",
      "Epoch [292/300], Step [40/172], Loss: 20.5781\n",
      "Epoch [292/300], Step [41/172], Loss: 30.7595\n",
      "Epoch [292/300], Step [42/172], Loss: 37.2280\n",
      "Epoch [292/300], Step [43/172], Loss: 26.8690\n",
      "Epoch [292/300], Step [44/172], Loss: 20.7892\n",
      "Epoch [292/300], Step [45/172], Loss: 28.5763\n",
      "Epoch [292/300], Step [46/172], Loss: 14.8431\n",
      "Epoch [292/300], Step [47/172], Loss: 46.3667\n",
      "Epoch [292/300], Step [48/172], Loss: 61.6355\n",
      "Epoch [292/300], Step [49/172], Loss: 21.3338\n",
      "Epoch [292/300], Step [50/172], Loss: 48.4208\n",
      "Epoch [292/300], Step [51/172], Loss: 7.3588\n",
      "Epoch [292/300], Step [52/172], Loss: 19.0040\n",
      "Epoch [292/300], Step [53/172], Loss: 21.7616\n",
      "Epoch [292/300], Step [54/172], Loss: 12.2406\n",
      "Epoch [292/300], Step [55/172], Loss: 12.5735\n",
      "Epoch [292/300], Step [56/172], Loss: 16.9101\n",
      "Epoch [292/300], Step [57/172], Loss: 16.8259\n",
      "Epoch [292/300], Step [58/172], Loss: 11.4892\n",
      "Epoch [292/300], Step [59/172], Loss: 26.5451\n",
      "Epoch [292/300], Step [60/172], Loss: 23.2106\n",
      "Epoch [292/300], Step [61/172], Loss: 4.3840\n",
      "Epoch [292/300], Step [62/172], Loss: 15.0937\n",
      "Epoch [292/300], Step [63/172], Loss: 7.4309\n",
      "Epoch [292/300], Step [64/172], Loss: 9.6556\n",
      "Epoch [292/300], Step [65/172], Loss: 15.9029\n",
      "Epoch [292/300], Step [66/172], Loss: 5.9132\n",
      "Epoch [292/300], Step [67/172], Loss: 25.0400\n",
      "Epoch [292/300], Step [68/172], Loss: 2.8840\n",
      "Epoch [292/300], Step [69/172], Loss: 30.3700\n",
      "Epoch [292/300], Step [70/172], Loss: 29.2760\n",
      "Epoch [292/300], Step [71/172], Loss: 32.4649\n",
      "Epoch [292/300], Step [72/172], Loss: 30.6939\n",
      "Epoch [292/300], Step [73/172], Loss: 40.2131\n",
      "Epoch [292/300], Step [74/172], Loss: 20.7553\n",
      "Epoch [292/300], Step [75/172], Loss: 18.9332\n",
      "Epoch [292/300], Step [76/172], Loss: 21.0689\n",
      "Epoch [292/300], Step [77/172], Loss: 39.8369\n",
      "Epoch [292/300], Step [78/172], Loss: 28.0257\n",
      "Epoch [292/300], Step [79/172], Loss: 25.5509\n",
      "Epoch [292/300], Step [80/172], Loss: 42.9419\n",
      "Epoch [292/300], Step [81/172], Loss: 24.0898\n",
      "Epoch [292/300], Step [82/172], Loss: 32.6574\n",
      "Epoch [292/300], Step [83/172], Loss: 35.7769\n",
      "Epoch [292/300], Step [84/172], Loss: 25.8087\n",
      "Epoch [292/300], Step [85/172], Loss: 31.5157\n",
      "Epoch [292/300], Step [86/172], Loss: 26.6193\n",
      "Epoch [292/300], Step [87/172], Loss: 19.4358\n",
      "Epoch [292/300], Step [88/172], Loss: 18.3185\n",
      "Epoch [292/300], Step [89/172], Loss: 23.1356\n",
      "Epoch [292/300], Step [90/172], Loss: 16.1166\n",
      "Epoch [292/300], Step [91/172], Loss: 21.9485\n",
      "Epoch [292/300], Step [92/172], Loss: 16.3479\n",
      "Epoch [292/300], Step [93/172], Loss: 15.8531\n",
      "Epoch [292/300], Step [94/172], Loss: 22.2265\n",
      "Epoch [292/300], Step [95/172], Loss: 16.5125\n",
      "Epoch [292/300], Step [96/172], Loss: 18.1653\n",
      "Epoch [292/300], Step [97/172], Loss: 25.7862\n",
      "Epoch [292/300], Step [98/172], Loss: 15.5809\n",
      "Epoch [292/300], Step [99/172], Loss: 16.4847\n",
      "Epoch [292/300], Step [100/172], Loss: 14.5351\n",
      "Epoch [292/300], Step [101/172], Loss: 16.4088\n",
      "Epoch [292/300], Step [102/172], Loss: 16.4877\n",
      "Epoch [292/300], Step [103/172], Loss: 10.1939\n",
      "Epoch [292/300], Step [104/172], Loss: 17.0340\n",
      "Epoch [292/300], Step [105/172], Loss: 20.6158\n",
      "Epoch [292/300], Step [106/172], Loss: 13.5293\n",
      "Epoch [292/300], Step [107/172], Loss: 14.5484\n",
      "Epoch [292/300], Step [108/172], Loss: 13.4922\n",
      "Epoch [292/300], Step [109/172], Loss: 14.1871\n",
      "Epoch [292/300], Step [110/172], Loss: 15.5356\n",
      "Epoch [292/300], Step [111/172], Loss: 17.1032\n",
      "Epoch [292/300], Step [112/172], Loss: 13.9276\n",
      "Epoch [292/300], Step [113/172], Loss: 12.2152\n",
      "Epoch [292/300], Step [114/172], Loss: 14.1049\n",
      "Epoch [292/300], Step [115/172], Loss: 18.5455\n",
      "Epoch [292/300], Step [116/172], Loss: 12.7004\n",
      "Epoch [292/300], Step [117/172], Loss: 11.5344\n",
      "Epoch [292/300], Step [118/172], Loss: 12.8448\n",
      "Epoch [292/300], Step [119/172], Loss: 17.3096\n",
      "Epoch [292/300], Step [120/172], Loss: 9.8856\n",
      "Epoch [292/300], Step [121/172], Loss: 8.4546\n",
      "Epoch [292/300], Step [122/172], Loss: 12.2256\n",
      "Epoch [292/300], Step [123/172], Loss: 10.4856\n",
      "Epoch [292/300], Step [124/172], Loss: 6.9342\n",
      "Epoch [292/300], Step [125/172], Loss: 11.1774\n",
      "Epoch [292/300], Step [126/172], Loss: 10.9773\n",
      "Epoch [292/300], Step [127/172], Loss: 10.1279\n",
      "Epoch [292/300], Step [128/172], Loss: 9.2378\n",
      "Epoch [292/300], Step [129/172], Loss: 8.2303\n",
      "Epoch [292/300], Step [130/172], Loss: 11.5607\n",
      "Epoch [292/300], Step [131/172], Loss: 7.2787\n",
      "Epoch [292/300], Step [132/172], Loss: 9.1536\n",
      "Epoch [292/300], Step [133/172], Loss: 9.2774\n",
      "Epoch [292/300], Step [134/172], Loss: 10.1551\n",
      "Epoch [292/300], Step [135/172], Loss: 9.1417\n",
      "Epoch [292/300], Step [136/172], Loss: 7.3390\n",
      "Epoch [292/300], Step [137/172], Loss: 7.8406\n",
      "Epoch [292/300], Step [138/172], Loss: 6.4351\n",
      "Epoch [292/300], Step [139/172], Loss: 10.9943\n",
      "Epoch [292/300], Step [140/172], Loss: 10.6095\n",
      "Epoch [292/300], Step [141/172], Loss: 8.0668\n",
      "Epoch [292/300], Step [142/172], Loss: 14.1892\n",
      "Epoch [292/300], Step [143/172], Loss: 11.0563\n",
      "Epoch [292/300], Step [144/172], Loss: 8.6308\n",
      "Epoch [292/300], Step [145/172], Loss: 10.8526\n",
      "Epoch [292/300], Step [146/172], Loss: 10.0785\n",
      "Epoch [292/300], Step [147/172], Loss: 5.3017\n",
      "Epoch [292/300], Step [148/172], Loss: 6.0746\n",
      "Epoch [292/300], Step [149/172], Loss: 5.6937\n",
      "Epoch [292/300], Step [150/172], Loss: 5.1113\n",
      "Epoch [292/300], Step [151/172], Loss: 4.8736\n",
      "Epoch [292/300], Step [152/172], Loss: 7.6396\n",
      "Epoch [292/300], Step [153/172], Loss: 6.1066\n",
      "Epoch [292/300], Step [154/172], Loss: 6.1848\n",
      "Epoch [292/300], Step [155/172], Loss: 6.6881\n",
      "Epoch [292/300], Step [156/172], Loss: 13.2407\n",
      "Epoch [292/300], Step [157/172], Loss: 8.3568\n",
      "Epoch [292/300], Step [158/172], Loss: 6.5905\n",
      "Epoch [292/300], Step [159/172], Loss: 9.6594\n",
      "Epoch [292/300], Step [160/172], Loss: 8.9779\n",
      "Epoch [292/300], Step [161/172], Loss: 5.4647\n",
      "Epoch [292/300], Step [162/172], Loss: 4.1884\n",
      "Epoch [292/300], Step [163/172], Loss: 5.9479\n",
      "Epoch [292/300], Step [164/172], Loss: 8.6973\n",
      "Epoch [292/300], Step [165/172], Loss: 6.6825\n",
      "Epoch [292/300], Step [166/172], Loss: 6.3106\n",
      "Epoch [292/300], Step [167/172], Loss: 10.8675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [292/300], Step [168/172], Loss: 5.9119\n",
      "Epoch [292/300], Step [169/172], Loss: 6.9524\n",
      "Epoch [292/300], Step [170/172], Loss: 5.0581\n",
      "Epoch [292/300], Step [171/172], Loss: 8.5051\n",
      "Epoch [292/300], Step [172/172], Loss: 5.6005\n",
      "Epoch [293/300], Step [1/172], Loss: 39.2983\n",
      "Epoch [293/300], Step [2/172], Loss: 44.8730\n",
      "Epoch [293/300], Step [3/172], Loss: 40.5941\n",
      "Epoch [293/300], Step [4/172], Loss: 18.5926\n",
      "Epoch [293/300], Step [5/172], Loss: 35.9051\n",
      "Epoch [293/300], Step [6/172], Loss: 20.5047\n",
      "Epoch [293/300], Step [7/172], Loss: 25.6070\n",
      "Epoch [293/300], Step [8/172], Loss: 3.7637\n",
      "Epoch [293/300], Step [9/172], Loss: 23.3964\n",
      "Epoch [293/300], Step [10/172], Loss: 35.8995\n",
      "Epoch [293/300], Step [11/172], Loss: 48.7644\n",
      "Epoch [293/300], Step [12/172], Loss: 48.1431\n",
      "Epoch [293/300], Step [13/172], Loss: 32.9534\n",
      "Epoch [293/300], Step [14/172], Loss: 54.5691\n",
      "Epoch [293/300], Step [15/172], Loss: 47.2565\n",
      "Epoch [293/300], Step [16/172], Loss: 9.2099\n",
      "Epoch [293/300], Step [17/172], Loss: 35.0626\n",
      "Epoch [293/300], Step [18/172], Loss: 48.6590\n",
      "Epoch [293/300], Step [19/172], Loss: 64.5650\n",
      "Epoch [293/300], Step [20/172], Loss: 27.3306\n",
      "Epoch [293/300], Step [21/172], Loss: 71.9283\n",
      "Epoch [293/300], Step [22/172], Loss: 47.2400\n",
      "Epoch [293/300], Step [23/172], Loss: 1.9279\n",
      "Epoch [293/300], Step [24/172], Loss: 45.3460\n",
      "Epoch [293/300], Step [25/172], Loss: 30.0725\n",
      "Epoch [293/300], Step [26/172], Loss: 41.8407\n",
      "Epoch [293/300], Step [27/172], Loss: 53.5153\n",
      "Epoch [293/300], Step [28/172], Loss: 15.3956\n",
      "Epoch [293/300], Step [29/172], Loss: 13.8867\n",
      "Epoch [293/300], Step [30/172], Loss: 43.8016\n",
      "Epoch [293/300], Step [31/172], Loss: 27.8369\n",
      "Epoch [293/300], Step [32/172], Loss: 42.4396\n",
      "Epoch [293/300], Step [33/172], Loss: 61.4635\n",
      "Epoch [293/300], Step [34/172], Loss: 1.2387\n",
      "Epoch [293/300], Step [35/172], Loss: 14.1342\n",
      "Epoch [293/300], Step [36/172], Loss: 14.5873\n",
      "Epoch [293/300], Step [37/172], Loss: 14.4313\n",
      "Epoch [293/300], Step [38/172], Loss: 32.1293\n",
      "Epoch [293/300], Step [39/172], Loss: 34.1728\n",
      "Epoch [293/300], Step [40/172], Loss: 21.2917\n",
      "Epoch [293/300], Step [41/172], Loss: 31.3515\n",
      "Epoch [293/300], Step [42/172], Loss: 37.9481\n",
      "Epoch [293/300], Step [43/172], Loss: 27.5664\n",
      "Epoch [293/300], Step [44/172], Loss: 21.2173\n",
      "Epoch [293/300], Step [45/172], Loss: 30.1325\n",
      "Epoch [293/300], Step [46/172], Loss: 15.0935\n",
      "Epoch [293/300], Step [47/172], Loss: 47.1536\n",
      "Epoch [293/300], Step [48/172], Loss: 60.3629\n",
      "Epoch [293/300], Step [49/172], Loss: 22.3109\n",
      "Epoch [293/300], Step [50/172], Loss: 47.7065\n",
      "Epoch [293/300], Step [51/172], Loss: 7.5630\n",
      "Epoch [293/300], Step [52/172], Loss: 19.4770\n",
      "Epoch [293/300], Step [53/172], Loss: 21.8968\n",
      "Epoch [293/300], Step [54/172], Loss: 12.5061\n",
      "Epoch [293/300], Step [55/172], Loss: 12.8402\n",
      "Epoch [293/300], Step [56/172], Loss: 16.8848\n",
      "Epoch [293/300], Step [57/172], Loss: 17.0256\n",
      "Epoch [293/300], Step [58/172], Loss: 10.9604\n",
      "Epoch [293/300], Step [59/172], Loss: 25.6053\n",
      "Epoch [293/300], Step [60/172], Loss: 22.3085\n",
      "Epoch [293/300], Step [61/172], Loss: 4.1927\n",
      "Epoch [293/300], Step [62/172], Loss: 14.5207\n",
      "Epoch [293/300], Step [63/172], Loss: 7.3444\n",
      "Epoch [293/300], Step [64/172], Loss: 9.6149\n",
      "Epoch [293/300], Step [65/172], Loss: 15.3856\n",
      "Epoch [293/300], Step [66/172], Loss: 5.8469\n",
      "Epoch [293/300], Step [67/172], Loss: 24.1713\n",
      "Epoch [293/300], Step [68/172], Loss: 2.5808\n",
      "Epoch [293/300], Step [69/172], Loss: 29.7568\n",
      "Epoch [293/300], Step [70/172], Loss: 29.3997\n",
      "Epoch [293/300], Step [71/172], Loss: 32.5278\n",
      "Epoch [293/300], Step [72/172], Loss: 30.6862\n",
      "Epoch [293/300], Step [73/172], Loss: 40.5548\n",
      "Epoch [293/300], Step [74/172], Loss: 20.7835\n",
      "Epoch [293/300], Step [75/172], Loss: 18.8746\n",
      "Epoch [293/300], Step [76/172], Loss: 20.9472\n",
      "Epoch [293/300], Step [77/172], Loss: 40.0682\n",
      "Epoch [293/300], Step [78/172], Loss: 27.7961\n",
      "Epoch [293/300], Step [79/172], Loss: 25.4067\n",
      "Epoch [293/300], Step [80/172], Loss: 42.4709\n",
      "Epoch [293/300], Step [81/172], Loss: 23.8352\n",
      "Epoch [293/300], Step [82/172], Loss: 31.6141\n",
      "Epoch [293/300], Step [83/172], Loss: 35.7251\n",
      "Epoch [293/300], Step [84/172], Loss: 25.7631\n",
      "Epoch [293/300], Step [85/172], Loss: 31.4662\n",
      "Epoch [293/300], Step [86/172], Loss: 26.3259\n",
      "Epoch [293/300], Step [87/172], Loss: 19.3704\n",
      "Epoch [293/300], Step [88/172], Loss: 18.1822\n",
      "Epoch [293/300], Step [89/172], Loss: 23.1331\n",
      "Epoch [293/300], Step [90/172], Loss: 15.7197\n",
      "Epoch [293/300], Step [91/172], Loss: 21.9555\n",
      "Epoch [293/300], Step [92/172], Loss: 16.1791\n",
      "Epoch [293/300], Step [93/172], Loss: 15.7362\n",
      "Epoch [293/300], Step [94/172], Loss: 21.9736\n",
      "Epoch [293/300], Step [95/172], Loss: 16.3590\n",
      "Epoch [293/300], Step [96/172], Loss: 18.1506\n",
      "Epoch [293/300], Step [97/172], Loss: 25.4856\n",
      "Epoch [293/300], Step [98/172], Loss: 15.5763\n",
      "Epoch [293/300], Step [99/172], Loss: 16.3538\n",
      "Epoch [293/300], Step [100/172], Loss: 14.7804\n",
      "Epoch [293/300], Step [101/172], Loss: 16.3935\n",
      "Epoch [293/300], Step [102/172], Loss: 15.8905\n",
      "Epoch [293/300], Step [103/172], Loss: 10.3021\n",
      "Epoch [293/300], Step [104/172], Loss: 17.0055\n",
      "Epoch [293/300], Step [105/172], Loss: 20.5063\n",
      "Epoch [293/300], Step [106/172], Loss: 13.5610\n",
      "Epoch [293/300], Step [107/172], Loss: 14.5289\n",
      "Epoch [293/300], Step [108/172], Loss: 13.5999\n",
      "Epoch [293/300], Step [109/172], Loss: 14.0672\n",
      "Epoch [293/300], Step [110/172], Loss: 15.5317\n",
      "Epoch [293/300], Step [111/172], Loss: 17.1742\n",
      "Epoch [293/300], Step [112/172], Loss: 13.7569\n",
      "Epoch [293/300], Step [113/172], Loss: 12.4991\n",
      "Epoch [293/300], Step [114/172], Loss: 13.9688\n",
      "Epoch [293/300], Step [115/172], Loss: 18.1203\n",
      "Epoch [293/300], Step [116/172], Loss: 12.7362\n",
      "Epoch [293/300], Step [117/172], Loss: 11.4342\n",
      "Epoch [293/300], Step [118/172], Loss: 12.5517\n",
      "Epoch [293/300], Step [119/172], Loss: 17.3355\n",
      "Epoch [293/300], Step [120/172], Loss: 10.0077\n",
      "Epoch [293/300], Step [121/172], Loss: 8.5344\n",
      "Epoch [293/300], Step [122/172], Loss: 12.1792\n",
      "Epoch [293/300], Step [123/172], Loss: 10.6275\n",
      "Epoch [293/300], Step [124/172], Loss: 6.9018\n",
      "Epoch [293/300], Step [125/172], Loss: 11.1985\n",
      "Epoch [293/300], Step [126/172], Loss: 11.1272\n",
      "Epoch [293/300], Step [127/172], Loss: 10.0656\n",
      "Epoch [293/300], Step [128/172], Loss: 9.2621\n",
      "Epoch [293/300], Step [129/172], Loss: 8.2630\n",
      "Epoch [293/300], Step [130/172], Loss: 11.4806\n",
      "Epoch [293/300], Step [131/172], Loss: 7.3502\n",
      "Epoch [293/300], Step [132/172], Loss: 9.3218\n",
      "Epoch [293/300], Step [133/172], Loss: 9.3234\n",
      "Epoch [293/300], Step [134/172], Loss: 10.1688\n",
      "Epoch [293/300], Step [135/172], Loss: 9.0886\n",
      "Epoch [293/300], Step [136/172], Loss: 7.4043\n",
      "Epoch [293/300], Step [137/172], Loss: 7.8712\n",
      "Epoch [293/300], Step [138/172], Loss: 6.5390\n",
      "Epoch [293/300], Step [139/172], Loss: 10.9913\n",
      "Epoch [293/300], Step [140/172], Loss: 10.5969\n",
      "Epoch [293/300], Step [141/172], Loss: 8.1371\n",
      "Epoch [293/300], Step [142/172], Loss: 14.1294\n",
      "Epoch [293/300], Step [143/172], Loss: 10.8814\n",
      "Epoch [293/300], Step [144/172], Loss: 8.6599\n",
      "Epoch [293/300], Step [145/172], Loss: 10.7415\n",
      "Epoch [293/300], Step [146/172], Loss: 10.0440\n",
      "Epoch [293/300], Step [147/172], Loss: 5.3035\n",
      "Epoch [293/300], Step [148/172], Loss: 6.1086\n",
      "Epoch [293/300], Step [149/172], Loss: 5.7610\n",
      "Epoch [293/300], Step [150/172], Loss: 5.1787\n",
      "Epoch [293/300], Step [151/172], Loss: 4.9267\n",
      "Epoch [293/300], Step [152/172], Loss: 7.8435\n",
      "Epoch [293/300], Step [153/172], Loss: 6.0336\n",
      "Epoch [293/300], Step [154/172], Loss: 6.1748\n",
      "Epoch [293/300], Step [155/172], Loss: 6.7220\n",
      "Epoch [293/300], Step [156/172], Loss: 13.0681\n",
      "Epoch [293/300], Step [157/172], Loss: 7.9591\n",
      "Epoch [293/300], Step [158/172], Loss: 6.5370\n",
      "Epoch [293/300], Step [159/172], Loss: 9.3039\n",
      "Epoch [293/300], Step [160/172], Loss: 8.6699\n",
      "Epoch [293/300], Step [161/172], Loss: 5.5991\n",
      "Epoch [293/300], Step [162/172], Loss: 4.2330\n",
      "Epoch [293/300], Step [163/172], Loss: 5.7390\n",
      "Epoch [293/300], Step [164/172], Loss: 8.5292\n",
      "Epoch [293/300], Step [165/172], Loss: 6.6609\n",
      "Epoch [293/300], Step [166/172], Loss: 6.2297\n",
      "Epoch [293/300], Step [167/172], Loss: 10.6337\n",
      "Epoch [293/300], Step [168/172], Loss: 5.8886\n",
      "Epoch [293/300], Step [169/172], Loss: 6.6842\n",
      "Epoch [293/300], Step [170/172], Loss: 5.0046\n",
      "Epoch [293/300], Step [171/172], Loss: 8.4148\n",
      "Epoch [293/300], Step [172/172], Loss: 5.5049\n",
      "Epoch [294/300], Step [1/172], Loss: 39.2726\n",
      "Epoch [294/300], Step [2/172], Loss: 45.0964\n",
      "Epoch [294/300], Step [3/172], Loss: 42.2591\n",
      "Epoch [294/300], Step [4/172], Loss: 18.9889\n",
      "Epoch [294/300], Step [5/172], Loss: 36.8460\n",
      "Epoch [294/300], Step [6/172], Loss: 21.4566\n",
      "Epoch [294/300], Step [7/172], Loss: 26.8876\n",
      "Epoch [294/300], Step [8/172], Loss: 5.5112\n",
      "Epoch [294/300], Step [9/172], Loss: 24.1300\n",
      "Epoch [294/300], Step [10/172], Loss: 36.6489\n",
      "Epoch [294/300], Step [11/172], Loss: 48.9745\n",
      "Epoch [294/300], Step [12/172], Loss: 48.9957\n",
      "Epoch [294/300], Step [13/172], Loss: 33.6444\n",
      "Epoch [294/300], Step [14/172], Loss: 55.1148\n",
      "Epoch [294/300], Step [15/172], Loss: 47.4631\n",
      "Epoch [294/300], Step [16/172], Loss: 7.3286\n",
      "Epoch [294/300], Step [17/172], Loss: 36.2095\n",
      "Epoch [294/300], Step [18/172], Loss: 49.3200\n",
      "Epoch [294/300], Step [19/172], Loss: 65.3780\n",
      "Epoch [294/300], Step [20/172], Loss: 27.0111\n",
      "Epoch [294/300], Step [21/172], Loss: 73.2936\n",
      "Epoch [294/300], Step [22/172], Loss: 47.6038\n",
      "Epoch [294/300], Step [23/172], Loss: 1.7381\n",
      "Epoch [294/300], Step [24/172], Loss: 46.0743\n",
      "Epoch [294/300], Step [25/172], Loss: 30.4750\n",
      "Epoch [294/300], Step [26/172], Loss: 41.9951\n",
      "Epoch [294/300], Step [27/172], Loss: 52.6751\n",
      "Epoch [294/300], Step [28/172], Loss: 15.4479\n",
      "Epoch [294/300], Step [29/172], Loss: 14.3294\n",
      "Epoch [294/300], Step [30/172], Loss: 44.4863\n",
      "Epoch [294/300], Step [31/172], Loss: 27.9213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [294/300], Step [32/172], Loss: 42.6977\n",
      "Epoch [294/300], Step [33/172], Loss: 62.0615\n",
      "Epoch [294/300], Step [34/172], Loss: 1.4276\n",
      "Epoch [294/300], Step [35/172], Loss: 14.3251\n",
      "Epoch [294/300], Step [36/172], Loss: 14.9197\n",
      "Epoch [294/300], Step [37/172], Loss: 14.4751\n",
      "Epoch [294/300], Step [38/172], Loss: 32.4779\n",
      "Epoch [294/300], Step [39/172], Loss: 34.8820\n",
      "Epoch [294/300], Step [40/172], Loss: 22.2923\n",
      "Epoch [294/300], Step [41/172], Loss: 31.9719\n",
      "Epoch [294/300], Step [42/172], Loss: 38.8207\n",
      "Epoch [294/300], Step [43/172], Loss: 28.4295\n",
      "Epoch [294/300], Step [44/172], Loss: 22.0163\n",
      "Epoch [294/300], Step [45/172], Loss: 32.5456\n",
      "Epoch [294/300], Step [46/172], Loss: 15.7922\n",
      "Epoch [294/300], Step [47/172], Loss: 48.2192\n",
      "Epoch [294/300], Step [48/172], Loss: 61.4335\n",
      "Epoch [294/300], Step [49/172], Loss: 24.1301\n",
      "Epoch [294/300], Step [50/172], Loss: 47.0122\n",
      "Epoch [294/300], Step [51/172], Loss: 8.1591\n",
      "Epoch [294/300], Step [52/172], Loss: 20.1385\n",
      "Epoch [294/300], Step [53/172], Loss: 22.2267\n",
      "Epoch [294/300], Step [54/172], Loss: 13.4141\n",
      "Epoch [294/300], Step [55/172], Loss: 13.3649\n",
      "Epoch [294/300], Step [56/172], Loss: 17.2932\n",
      "Epoch [294/300], Step [57/172], Loss: 16.9097\n",
      "Epoch [294/300], Step [58/172], Loss: 10.7517\n",
      "Epoch [294/300], Step [59/172], Loss: 24.3985\n",
      "Epoch [294/300], Step [60/172], Loss: 21.3833\n",
      "Epoch [294/300], Step [61/172], Loss: 4.0942\n",
      "Epoch [294/300], Step [62/172], Loss: 13.7144\n",
      "Epoch [294/300], Step [63/172], Loss: 7.4639\n",
      "Epoch [294/300], Step [64/172], Loss: 9.6589\n",
      "Epoch [294/300], Step [65/172], Loss: 15.1959\n",
      "Epoch [294/300], Step [66/172], Loss: 5.5264\n",
      "Epoch [294/300], Step [67/172], Loss: 23.3423\n",
      "Epoch [294/300], Step [68/172], Loss: 2.2696\n",
      "Epoch [294/300], Step [69/172], Loss: 29.6908\n",
      "Epoch [294/300], Step [70/172], Loss: 29.7130\n",
      "Epoch [294/300], Step [71/172], Loss: 32.6329\n",
      "Epoch [294/300], Step [72/172], Loss: 30.8327\n",
      "Epoch [294/300], Step [73/172], Loss: 40.9530\n",
      "Epoch [294/300], Step [74/172], Loss: 20.9235\n",
      "Epoch [294/300], Step [75/172], Loss: 18.6856\n",
      "Epoch [294/300], Step [76/172], Loss: 21.0735\n",
      "Epoch [294/300], Step [77/172], Loss: 39.7121\n",
      "Epoch [294/300], Step [78/172], Loss: 27.7702\n",
      "Epoch [294/300], Step [79/172], Loss: 25.0667\n",
      "Epoch [294/300], Step [80/172], Loss: 42.4537\n",
      "Epoch [294/300], Step [81/172], Loss: 23.5111\n",
      "Epoch [294/300], Step [82/172], Loss: 32.0950\n",
      "Epoch [294/300], Step [83/172], Loss: 35.4004\n",
      "Epoch [294/300], Step [84/172], Loss: 25.5663\n",
      "Epoch [294/300], Step [85/172], Loss: 31.4116\n",
      "Epoch [294/300], Step [86/172], Loss: 26.3134\n",
      "Epoch [294/300], Step [87/172], Loss: 19.1120\n",
      "Epoch [294/300], Step [88/172], Loss: 18.1065\n",
      "Epoch [294/300], Step [89/172], Loss: 23.1977\n",
      "Epoch [294/300], Step [90/172], Loss: 15.7665\n",
      "Epoch [294/300], Step [91/172], Loss: 22.0292\n",
      "Epoch [294/300], Step [92/172], Loss: 16.1795\n",
      "Epoch [294/300], Step [93/172], Loss: 15.6959\n",
      "Epoch [294/300], Step [94/172], Loss: 22.0056\n",
      "Epoch [294/300], Step [95/172], Loss: 16.7413\n",
      "Epoch [294/300], Step [96/172], Loss: 18.2218\n",
      "Epoch [294/300], Step [97/172], Loss: 25.4020\n",
      "Epoch [294/300], Step [98/172], Loss: 15.7085\n",
      "Epoch [294/300], Step [99/172], Loss: 16.5581\n",
      "Epoch [294/300], Step [100/172], Loss: 15.2078\n",
      "Epoch [294/300], Step [101/172], Loss: 16.4359\n",
      "Epoch [294/300], Step [102/172], Loss: 16.2380\n",
      "Epoch [294/300], Step [103/172], Loss: 10.5011\n",
      "Epoch [294/300], Step [104/172], Loss: 17.0315\n",
      "Epoch [294/300], Step [105/172], Loss: 21.3341\n",
      "Epoch [294/300], Step [106/172], Loss: 13.9653\n",
      "Epoch [294/300], Step [107/172], Loss: 14.4985\n",
      "Epoch [294/300], Step [108/172], Loss: 13.8156\n",
      "Epoch [294/300], Step [109/172], Loss: 14.3774\n",
      "Epoch [294/300], Step [110/172], Loss: 15.8695\n",
      "Epoch [294/300], Step [111/172], Loss: 17.4446\n",
      "Epoch [294/300], Step [112/172], Loss: 13.7181\n",
      "Epoch [294/300], Step [113/172], Loss: 12.5738\n",
      "Epoch [294/300], Step [114/172], Loss: 13.8620\n",
      "Epoch [294/300], Step [115/172], Loss: 18.2479\n",
      "Epoch [294/300], Step [116/172], Loss: 12.8768\n",
      "Epoch [294/300], Step [117/172], Loss: 11.2868\n",
      "Epoch [294/300], Step [118/172], Loss: 12.3774\n",
      "Epoch [294/300], Step [119/172], Loss: 17.3337\n",
      "Epoch [294/300], Step [120/172], Loss: 10.2509\n",
      "Epoch [294/300], Step [121/172], Loss: 8.7320\n",
      "Epoch [294/300], Step [122/172], Loss: 12.3663\n",
      "Epoch [294/300], Step [123/172], Loss: 11.0652\n",
      "Epoch [294/300], Step [124/172], Loss: 6.9398\n",
      "Epoch [294/300], Step [125/172], Loss: 11.2753\n",
      "Epoch [294/300], Step [126/172], Loss: 11.3237\n",
      "Epoch [294/300], Step [127/172], Loss: 10.0461\n",
      "Epoch [294/300], Step [128/172], Loss: 9.3817\n",
      "Epoch [294/300], Step [129/172], Loss: 8.3399\n",
      "Epoch [294/300], Step [130/172], Loss: 11.3955\n",
      "Epoch [294/300], Step [131/172], Loss: 7.3814\n",
      "Epoch [294/300], Step [132/172], Loss: 9.3362\n",
      "Epoch [294/300], Step [133/172], Loss: 9.3832\n",
      "Epoch [294/300], Step [134/172], Loss: 10.0694\n",
      "Epoch [294/300], Step [135/172], Loss: 8.9430\n",
      "Epoch [294/300], Step [136/172], Loss: 7.6139\n",
      "Epoch [294/300], Step [137/172], Loss: 7.8745\n",
      "Epoch [294/300], Step [138/172], Loss: 6.8425\n",
      "Epoch [294/300], Step [139/172], Loss: 10.7783\n",
      "Epoch [294/300], Step [140/172], Loss: 10.4593\n",
      "Epoch [294/300], Step [141/172], Loss: 8.2831\n",
      "Epoch [294/300], Step [142/172], Loss: 14.0653\n",
      "Epoch [294/300], Step [143/172], Loss: 10.8567\n",
      "Epoch [294/300], Step [144/172], Loss: 8.6343\n",
      "Epoch [294/300], Step [145/172], Loss: 10.5106\n",
      "Epoch [294/300], Step [146/172], Loss: 10.1006\n",
      "Epoch [294/300], Step [147/172], Loss: 5.3855\n",
      "Epoch [294/300], Step [148/172], Loss: 6.1851\n",
      "Epoch [294/300], Step [149/172], Loss: 5.9055\n",
      "Epoch [294/300], Step [150/172], Loss: 5.3329\n",
      "Epoch [294/300], Step [151/172], Loss: 5.0343\n",
      "Epoch [294/300], Step [152/172], Loss: 8.0168\n",
      "Epoch [294/300], Step [153/172], Loss: 5.9804\n",
      "Epoch [294/300], Step [154/172], Loss: 6.1521\n",
      "Epoch [294/300], Step [155/172], Loss: 6.6371\n",
      "Epoch [294/300], Step [156/172], Loss: 12.9973\n",
      "Epoch [294/300], Step [157/172], Loss: 7.9209\n",
      "Epoch [294/300], Step [158/172], Loss: 6.4591\n",
      "Epoch [294/300], Step [159/172], Loss: 9.3233\n",
      "Epoch [294/300], Step [160/172], Loss: 8.2798\n",
      "Epoch [294/300], Step [161/172], Loss: 6.0062\n",
      "Epoch [294/300], Step [162/172], Loss: 4.2547\n",
      "Epoch [294/300], Step [163/172], Loss: 5.6766\n",
      "Epoch [294/300], Step [164/172], Loss: 8.8275\n",
      "Epoch [294/300], Step [165/172], Loss: 6.6040\n",
      "Epoch [294/300], Step [166/172], Loss: 6.1736\n",
      "Epoch [294/300], Step [167/172], Loss: 10.5240\n",
      "Epoch [294/300], Step [168/172], Loss: 5.9291\n",
      "Epoch [294/300], Step [169/172], Loss: 6.3010\n",
      "Epoch [294/300], Step [170/172], Loss: 4.9929\n",
      "Epoch [294/300], Step [171/172], Loss: 8.7477\n",
      "Epoch [294/300], Step [172/172], Loss: 5.5901\n",
      "Epoch [295/300], Step [1/172], Loss: 39.6474\n",
      "Epoch [295/300], Step [2/172], Loss: 44.4409\n",
      "Epoch [295/300], Step [3/172], Loss: 41.0672\n",
      "Epoch [295/300], Step [4/172], Loss: 19.6024\n",
      "Epoch [295/300], Step [5/172], Loss: 38.1362\n",
      "Epoch [295/300], Step [6/172], Loss: 22.4085\n",
      "Epoch [295/300], Step [7/172], Loss: 26.0901\n",
      "Epoch [295/300], Step [8/172], Loss: 4.1430\n",
      "Epoch [295/300], Step [9/172], Loss: 23.7936\n",
      "Epoch [295/300], Step [10/172], Loss: 36.9294\n",
      "Epoch [295/300], Step [11/172], Loss: 48.8220\n",
      "Epoch [295/300], Step [12/172], Loss: 49.1582\n",
      "Epoch [295/300], Step [13/172], Loss: 34.1854\n",
      "Epoch [295/300], Step [14/172], Loss: 54.9434\n",
      "Epoch [295/300], Step [15/172], Loss: 47.6224\n",
      "Epoch [295/300], Step [16/172], Loss: 8.0032\n",
      "Epoch [295/300], Step [17/172], Loss: 36.6982\n",
      "Epoch [295/300], Step [18/172], Loss: 49.6007\n",
      "Epoch [295/300], Step [19/172], Loss: 64.9650\n",
      "Epoch [295/300], Step [20/172], Loss: 26.6042\n",
      "Epoch [295/300], Step [21/172], Loss: 73.1027\n",
      "Epoch [295/300], Step [22/172], Loss: 48.7101\n",
      "Epoch [295/300], Step [23/172], Loss: 1.3038\n",
      "Epoch [295/300], Step [24/172], Loss: 46.5628\n",
      "Epoch [295/300], Step [25/172], Loss: 30.6776\n",
      "Epoch [295/300], Step [26/172], Loss: 42.3386\n",
      "Epoch [295/300], Step [27/172], Loss: 52.5197\n",
      "Epoch [295/300], Step [28/172], Loss: 15.6561\n",
      "Epoch [295/300], Step [29/172], Loss: 14.5210\n",
      "Epoch [295/300], Step [30/172], Loss: 45.3592\n",
      "Epoch [295/300], Step [31/172], Loss: 28.2762\n",
      "Epoch [295/300], Step [32/172], Loss: 42.9564\n",
      "Epoch [295/300], Step [33/172], Loss: 63.9355\n",
      "Epoch [295/300], Step [34/172], Loss: 1.5749\n",
      "Epoch [295/300], Step [35/172], Loss: 14.1586\n",
      "Epoch [295/300], Step [36/172], Loss: 14.7676\n",
      "Epoch [295/300], Step [37/172], Loss: 14.4113\n",
      "Epoch [295/300], Step [38/172], Loss: 32.1846\n",
      "Epoch [295/300], Step [39/172], Loss: 34.7198\n",
      "Epoch [295/300], Step [40/172], Loss: 22.1688\n",
      "Epoch [295/300], Step [41/172], Loss: 31.4597\n",
      "Epoch [295/300], Step [42/172], Loss: 37.9020\n",
      "Epoch [295/300], Step [43/172], Loss: 27.6024\n",
      "Epoch [295/300], Step [44/172], Loss: 22.5586\n",
      "Epoch [295/300], Step [45/172], Loss: 31.7143\n",
      "Epoch [295/300], Step [46/172], Loss: 16.1424\n",
      "Epoch [295/300], Step [47/172], Loss: 49.0960\n",
      "Epoch [295/300], Step [48/172], Loss: 63.2353\n",
      "Epoch [295/300], Step [49/172], Loss: 25.4828\n",
      "Epoch [295/300], Step [50/172], Loss: 47.6232\n",
      "Epoch [295/300], Step [51/172], Loss: 8.5247\n",
      "Epoch [295/300], Step [52/172], Loss: 21.2513\n",
      "Epoch [295/300], Step [53/172], Loss: 23.8024\n",
      "Epoch [295/300], Step [54/172], Loss: 14.0071\n",
      "Epoch [295/300], Step [55/172], Loss: 13.8165\n",
      "Epoch [295/300], Step [56/172], Loss: 17.9941\n",
      "Epoch [295/300], Step [57/172], Loss: 20.5765\n",
      "Epoch [295/300], Step [58/172], Loss: 10.8945\n",
      "Epoch [295/300], Step [59/172], Loss: 24.0797\n",
      "Epoch [295/300], Step [60/172], Loss: 21.7894\n",
      "Epoch [295/300], Step [61/172], Loss: 4.3432\n",
      "Epoch [295/300], Step [62/172], Loss: 13.3953\n",
      "Epoch [295/300], Step [63/172], Loss: 8.1978\n",
      "Epoch [295/300], Step [64/172], Loss: 10.1184\n",
      "Epoch [295/300], Step [65/172], Loss: 14.7608\n",
      "Epoch [295/300], Step [66/172], Loss: 5.2624\n",
      "Epoch [295/300], Step [67/172], Loss: 22.1763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [295/300], Step [68/172], Loss: 2.3887\n",
      "Epoch [295/300], Step [69/172], Loss: 27.7659\n",
      "Epoch [295/300], Step [70/172], Loss: 29.4052\n",
      "Epoch [295/300], Step [71/172], Loss: 32.5134\n",
      "Epoch [295/300], Step [72/172], Loss: 30.3543\n",
      "Epoch [295/300], Step [73/172], Loss: 40.8794\n",
      "Epoch [295/300], Step [74/172], Loss: 21.0928\n",
      "Epoch [295/300], Step [75/172], Loss: 18.7864\n",
      "Epoch [295/300], Step [76/172], Loss: 21.3972\n",
      "Epoch [295/300], Step [77/172], Loss: 40.2600\n",
      "Epoch [295/300], Step [78/172], Loss: 27.3736\n",
      "Epoch [295/300], Step [79/172], Loss: 25.1577\n",
      "Epoch [295/300], Step [80/172], Loss: 42.0244\n",
      "Epoch [295/300], Step [81/172], Loss: 23.7322\n",
      "Epoch [295/300], Step [82/172], Loss: 31.1084\n",
      "Epoch [295/300], Step [83/172], Loss: 35.5460\n",
      "Epoch [295/300], Step [84/172], Loss: 25.4973\n",
      "Epoch [295/300], Step [85/172], Loss: 30.5517\n",
      "Epoch [295/300], Step [86/172], Loss: 25.8951\n",
      "Epoch [295/300], Step [87/172], Loss: 19.5164\n",
      "Epoch [295/300], Step [88/172], Loss: 18.2899\n",
      "Epoch [295/300], Step [89/172], Loss: 23.1366\n",
      "Epoch [295/300], Step [90/172], Loss: 15.7806\n",
      "Epoch [295/300], Step [91/172], Loss: 21.7141\n",
      "Epoch [295/300], Step [92/172], Loss: 16.2368\n",
      "Epoch [295/300], Step [93/172], Loss: 15.8256\n",
      "Epoch [295/300], Step [94/172], Loss: 21.6714\n",
      "Epoch [295/300], Step [95/172], Loss: 16.4767\n",
      "Epoch [295/300], Step [96/172], Loss: 18.3319\n",
      "Epoch [295/300], Step [97/172], Loss: 25.1745\n",
      "Epoch [295/300], Step [98/172], Loss: 15.6631\n",
      "Epoch [295/300], Step [99/172], Loss: 16.4302\n",
      "Epoch [295/300], Step [100/172], Loss: 15.0794\n",
      "Epoch [295/300], Step [101/172], Loss: 16.3812\n",
      "Epoch [295/300], Step [102/172], Loss: 15.5413\n",
      "Epoch [295/300], Step [103/172], Loss: 10.5193\n",
      "Epoch [295/300], Step [104/172], Loss: 17.1112\n",
      "Epoch [295/300], Step [105/172], Loss: 20.7390\n",
      "Epoch [295/300], Step [106/172], Loss: 14.0013\n",
      "Epoch [295/300], Step [107/172], Loss: 14.2626\n",
      "Epoch [295/300], Step [108/172], Loss: 13.6983\n",
      "Epoch [295/300], Step [109/172], Loss: 14.1399\n",
      "Epoch [295/300], Step [110/172], Loss: 15.8099\n",
      "Epoch [295/300], Step [111/172], Loss: 17.4941\n",
      "Epoch [295/300], Step [112/172], Loss: 13.4615\n",
      "Epoch [295/300], Step [113/172], Loss: 13.1161\n",
      "Epoch [295/300], Step [114/172], Loss: 13.9985\n",
      "Epoch [295/300], Step [115/172], Loss: 17.9581\n",
      "Epoch [295/300], Step [116/172], Loss: 13.2922\n",
      "Epoch [295/300], Step [117/172], Loss: 11.2567\n",
      "Epoch [295/300], Step [118/172], Loss: 12.1238\n",
      "Epoch [295/300], Step [119/172], Loss: 17.2203\n",
      "Epoch [295/300], Step [120/172], Loss: 10.0106\n",
      "Epoch [295/300], Step [121/172], Loss: 8.8123\n",
      "Epoch [295/300], Step [122/172], Loss: 12.2619\n",
      "Epoch [295/300], Step [123/172], Loss: 10.8711\n",
      "Epoch [295/300], Step [124/172], Loss: 7.2161\n",
      "Epoch [295/300], Step [125/172], Loss: 11.6668\n",
      "Epoch [295/300], Step [126/172], Loss: 11.5849\n",
      "Epoch [295/300], Step [127/172], Loss: 10.3040\n",
      "Epoch [295/300], Step [128/172], Loss: 9.6529\n",
      "Epoch [295/300], Step [129/172], Loss: 8.5979\n",
      "Epoch [295/300], Step [130/172], Loss: 11.4778\n",
      "Epoch [295/300], Step [131/172], Loss: 7.4773\n",
      "Epoch [295/300], Step [132/172], Loss: 9.6113\n",
      "Epoch [295/300], Step [133/172], Loss: 9.3623\n",
      "Epoch [295/300], Step [134/172], Loss: 9.7650\n",
      "Epoch [295/300], Step [135/172], Loss: 8.9498\n",
      "Epoch [295/300], Step [136/172], Loss: 8.0545\n",
      "Epoch [295/300], Step [137/172], Loss: 8.1365\n",
      "Epoch [295/300], Step [138/172], Loss: 7.0443\n",
      "Epoch [295/300], Step [139/172], Loss: 10.9931\n",
      "Epoch [295/300], Step [140/172], Loss: 10.8614\n",
      "Epoch [295/300], Step [141/172], Loss: 8.4488\n",
      "Epoch [295/300], Step [142/172], Loss: 13.8322\n",
      "Epoch [295/300], Step [143/172], Loss: 11.0116\n",
      "Epoch [295/300], Step [144/172], Loss: 8.7584\n",
      "Epoch [295/300], Step [145/172], Loss: 10.5931\n",
      "Epoch [295/300], Step [146/172], Loss: 10.1526\n",
      "Epoch [295/300], Step [147/172], Loss: 5.5208\n",
      "Epoch [295/300], Step [148/172], Loss: 6.4278\n",
      "Epoch [295/300], Step [149/172], Loss: 6.1463\n",
      "Epoch [295/300], Step [150/172], Loss: 5.6057\n",
      "Epoch [295/300], Step [151/172], Loss: 5.2513\n",
      "Epoch [295/300], Step [152/172], Loss: 8.2279\n",
      "Epoch [295/300], Step [153/172], Loss: 6.1448\n",
      "Epoch [295/300], Step [154/172], Loss: 6.3230\n",
      "Epoch [295/300], Step [155/172], Loss: 7.0436\n",
      "Epoch [295/300], Step [156/172], Loss: 13.1964\n",
      "Epoch [295/300], Step [157/172], Loss: 7.8379\n",
      "Epoch [295/300], Step [158/172], Loss: 6.4817\n",
      "Epoch [295/300], Step [159/172], Loss: 9.0425\n",
      "Epoch [295/300], Step [160/172], Loss: 8.0542\n",
      "Epoch [295/300], Step [161/172], Loss: 6.5859\n",
      "Epoch [295/300], Step [162/172], Loss: 4.3806\n",
      "Epoch [295/300], Step [163/172], Loss: 5.7934\n",
      "Epoch [295/300], Step [164/172], Loss: 8.8682\n",
      "Epoch [295/300], Step [165/172], Loss: 6.8256\n",
      "Epoch [295/300], Step [166/172], Loss: 6.2968\n",
      "Epoch [295/300], Step [167/172], Loss: 10.2693\n",
      "Epoch [295/300], Step [168/172], Loss: 6.0813\n",
      "Epoch [295/300], Step [169/172], Loss: 6.3196\n",
      "Epoch [295/300], Step [170/172], Loss: 5.0554\n",
      "Epoch [295/300], Step [171/172], Loss: 8.4552\n",
      "Epoch [295/300], Step [172/172], Loss: 5.6886\n",
      "Epoch [296/300], Step [1/172], Loss: 39.5814\n",
      "Epoch [296/300], Step [2/172], Loss: 45.8936\n",
      "Epoch [296/300], Step [3/172], Loss: 43.7743\n",
      "Epoch [296/300], Step [4/172], Loss: 19.1132\n",
      "Epoch [296/300], Step [5/172], Loss: 39.4508\n",
      "Epoch [296/300], Step [6/172], Loss: 23.4523\n",
      "Epoch [296/300], Step [7/172], Loss: 28.6889\n",
      "Epoch [296/300], Step [8/172], Loss: 4.6600\n",
      "Epoch [296/300], Step [9/172], Loss: 23.4684\n",
      "Epoch [296/300], Step [10/172], Loss: 36.8578\n",
      "Epoch [296/300], Step [11/172], Loss: 48.0122\n",
      "Epoch [296/300], Step [12/172], Loss: 49.2170\n",
      "Epoch [296/300], Step [13/172], Loss: 34.3153\n",
      "Epoch [296/300], Step [14/172], Loss: 55.3029\n",
      "Epoch [296/300], Step [15/172], Loss: 46.7926\n",
      "Epoch [296/300], Step [16/172], Loss: 7.4227\n",
      "Epoch [296/300], Step [17/172], Loss: 37.5837\n",
      "Epoch [296/300], Step [18/172], Loss: 49.9473\n",
      "Epoch [296/300], Step [19/172], Loss: 66.0983\n",
      "Epoch [296/300], Step [20/172], Loss: 25.3779\n",
      "Epoch [296/300], Step [21/172], Loss: 75.1778\n",
      "Epoch [296/300], Step [22/172], Loss: 47.6337\n",
      "Epoch [296/300], Step [23/172], Loss: 0.9653\n",
      "Epoch [296/300], Step [24/172], Loss: 47.0644\n",
      "Epoch [296/300], Step [25/172], Loss: 30.8343\n",
      "Epoch [296/300], Step [26/172], Loss: 42.5877\n",
      "Epoch [296/300], Step [27/172], Loss: 51.9077\n",
      "Epoch [296/300], Step [28/172], Loss: 15.7614\n",
      "Epoch [296/300], Step [29/172], Loss: 14.5827\n",
      "Epoch [296/300], Step [30/172], Loss: 46.2617\n",
      "Epoch [296/300], Step [31/172], Loss: 28.8786\n",
      "Epoch [296/300], Step [32/172], Loss: 43.1718\n",
      "Epoch [296/300], Step [33/172], Loss: 63.5237\n",
      "Epoch [296/300], Step [34/172], Loss: 1.4422\n",
      "Epoch [296/300], Step [35/172], Loss: 14.2996\n",
      "Epoch [296/300], Step [36/172], Loss: 14.5433\n",
      "Epoch [296/300], Step [37/172], Loss: 14.5277\n",
      "Epoch [296/300], Step [38/172], Loss: 32.0167\n",
      "Epoch [296/300], Step [39/172], Loss: 34.3004\n",
      "Epoch [296/300], Step [40/172], Loss: 22.2766\n",
      "Epoch [296/300], Step [41/172], Loss: 31.1735\n",
      "Epoch [296/300], Step [42/172], Loss: 37.8388\n",
      "Epoch [296/300], Step [43/172], Loss: 27.5683\n",
      "Epoch [296/300], Step [44/172], Loss: 22.0678\n",
      "Epoch [296/300], Step [45/172], Loss: 32.1252\n",
      "Epoch [296/300], Step [46/172], Loss: 15.7572\n",
      "Epoch [296/300], Step [47/172], Loss: 48.2793\n",
      "Epoch [296/300], Step [48/172], Loss: 60.1584\n",
      "Epoch [296/300], Step [49/172], Loss: 25.4193\n",
      "Epoch [296/300], Step [50/172], Loss: 46.7438\n",
      "Epoch [296/300], Step [51/172], Loss: 8.9966\n",
      "Epoch [296/300], Step [52/172], Loss: 21.9940\n",
      "Epoch [296/300], Step [53/172], Loss: 23.9774\n",
      "Epoch [296/300], Step [54/172], Loss: 14.6903\n",
      "Epoch [296/300], Step [55/172], Loss: 14.5087\n",
      "Epoch [296/300], Step [56/172], Loss: 17.5035\n",
      "Epoch [296/300], Step [57/172], Loss: 19.0098\n",
      "Epoch [296/300], Step [58/172], Loss: 11.1544\n",
      "Epoch [296/300], Step [59/172], Loss: 23.1690\n",
      "Epoch [296/300], Step [60/172], Loss: 21.1935\n",
      "Epoch [296/300], Step [61/172], Loss: 4.4327\n",
      "Epoch [296/300], Step [62/172], Loss: 13.0552\n",
      "Epoch [296/300], Step [63/172], Loss: 8.8621\n",
      "Epoch [296/300], Step [64/172], Loss: 10.6951\n",
      "Epoch [296/300], Step [65/172], Loss: 15.1756\n",
      "Epoch [296/300], Step [66/172], Loss: 5.4287\n",
      "Epoch [296/300], Step [67/172], Loss: 23.0015\n",
      "Epoch [296/300], Step [68/172], Loss: 2.7308\n",
      "Epoch [296/300], Step [69/172], Loss: 28.0298\n",
      "Epoch [296/300], Step [70/172], Loss: 29.1362\n",
      "Epoch [296/300], Step [71/172], Loss: 32.0885\n",
      "Epoch [296/300], Step [72/172], Loss: 30.3200\n",
      "Epoch [296/300], Step [73/172], Loss: 40.6960\n",
      "Epoch [296/300], Step [74/172], Loss: 20.8218\n",
      "Epoch [296/300], Step [75/172], Loss: 18.4146\n",
      "Epoch [296/300], Step [76/172], Loss: 22.2618\n",
      "Epoch [296/300], Step [77/172], Loss: 39.8024\n",
      "Epoch [296/300], Step [78/172], Loss: 27.2844\n",
      "Epoch [296/300], Step [79/172], Loss: 25.4012\n",
      "Epoch [296/300], Step [80/172], Loss: 42.7207\n",
      "Epoch [296/300], Step [81/172], Loss: 23.7613\n",
      "Epoch [296/300], Step [82/172], Loss: 31.4985\n",
      "Epoch [296/300], Step [83/172], Loss: 36.3486\n",
      "Epoch [296/300], Step [84/172], Loss: 26.0188\n",
      "Epoch [296/300], Step [85/172], Loss: 31.5027\n",
      "Epoch [296/300], Step [86/172], Loss: 26.5617\n",
      "Epoch [296/300], Step [87/172], Loss: 19.6023\n",
      "Epoch [296/300], Step [88/172], Loss: 18.6989\n",
      "Epoch [296/300], Step [89/172], Loss: 23.5993\n",
      "Epoch [296/300], Step [90/172], Loss: 16.1821\n",
      "Epoch [296/300], Step [91/172], Loss: 22.1974\n",
      "Epoch [296/300], Step [92/172], Loss: 16.5194\n",
      "Epoch [296/300], Step [93/172], Loss: 16.0085\n",
      "Epoch [296/300], Step [94/172], Loss: 22.1191\n",
      "Epoch [296/300], Step [95/172], Loss: 16.9092\n",
      "Epoch [296/300], Step [96/172], Loss: 18.6477\n",
      "Epoch [296/300], Step [97/172], Loss: 25.6173\n",
      "Epoch [296/300], Step [98/172], Loss: 15.9832\n",
      "Epoch [296/300], Step [99/172], Loss: 16.7565\n",
      "Epoch [296/300], Step [100/172], Loss: 15.6902\n",
      "Epoch [296/300], Step [101/172], Loss: 16.6891\n",
      "Epoch [296/300], Step [102/172], Loss: 16.1181\n",
      "Epoch [296/300], Step [103/172], Loss: 10.6938\n",
      "Epoch [296/300], Step [104/172], Loss: 17.3287\n",
      "Epoch [296/300], Step [105/172], Loss: 21.6076\n",
      "Epoch [296/300], Step [106/172], Loss: 14.4846\n",
      "Epoch [296/300], Step [107/172], Loss: 14.3926\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [296/300], Step [108/172], Loss: 14.0745\n",
      "Epoch [296/300], Step [109/172], Loss: 14.4203\n",
      "Epoch [296/300], Step [110/172], Loss: 16.2480\n",
      "Epoch [296/300], Step [111/172], Loss: 17.5973\n",
      "Epoch [296/300], Step [112/172], Loss: 13.4873\n",
      "Epoch [296/300], Step [113/172], Loss: 13.0175\n",
      "Epoch [296/300], Step [114/172], Loss: 14.0832\n",
      "Epoch [296/300], Step [115/172], Loss: 17.8092\n",
      "Epoch [296/300], Step [116/172], Loss: 13.3802\n",
      "Epoch [296/300], Step [117/172], Loss: 11.1656\n",
      "Epoch [296/300], Step [118/172], Loss: 11.7581\n",
      "Epoch [296/300], Step [119/172], Loss: 17.2547\n",
      "Epoch [296/300], Step [120/172], Loss: 10.1809\n",
      "Epoch [296/300], Step [121/172], Loss: 8.8920\n",
      "Epoch [296/300], Step [122/172], Loss: 12.9885\n",
      "Epoch [296/300], Step [123/172], Loss: 10.8744\n",
      "Epoch [296/300], Step [124/172], Loss: 7.1773\n",
      "Epoch [296/300], Step [125/172], Loss: 11.7625\n",
      "Epoch [296/300], Step [126/172], Loss: 11.6472\n",
      "Epoch [296/300], Step [127/172], Loss: 10.3140\n",
      "Epoch [296/300], Step [128/172], Loss: 9.8064\n",
      "Epoch [296/300], Step [129/172], Loss: 8.6903\n",
      "Epoch [296/300], Step [130/172], Loss: 11.2651\n",
      "Epoch [296/300], Step [131/172], Loss: 7.5946\n",
      "Epoch [296/300], Step [132/172], Loss: 9.6056\n",
      "Epoch [296/300], Step [133/172], Loss: 9.6205\n",
      "Epoch [296/300], Step [134/172], Loss: 9.7227\n",
      "Epoch [296/300], Step [135/172], Loss: 8.9409\n",
      "Epoch [296/300], Step [136/172], Loss: 7.5716\n",
      "Epoch [296/300], Step [137/172], Loss: 8.0297\n",
      "Epoch [296/300], Step [138/172], Loss: 7.1789\n",
      "Epoch [296/300], Step [139/172], Loss: 10.7890\n",
      "Epoch [296/300], Step [140/172], Loss: 10.6268\n",
      "Epoch [296/300], Step [141/172], Loss: 8.4579\n",
      "Epoch [296/300], Step [142/172], Loss: 13.7529\n",
      "Epoch [296/300], Step [143/172], Loss: 10.8055\n",
      "Epoch [296/300], Step [144/172], Loss: 8.7051\n",
      "Epoch [296/300], Step [145/172], Loss: 10.4230\n",
      "Epoch [296/300], Step [146/172], Loss: 10.0359\n",
      "Epoch [296/300], Step [147/172], Loss: 5.5751\n",
      "Epoch [296/300], Step [148/172], Loss: 6.4781\n",
      "Epoch [296/300], Step [149/172], Loss: 6.1909\n",
      "Epoch [296/300], Step [150/172], Loss: 5.4271\n",
      "Epoch [296/300], Step [151/172], Loss: 5.3083\n",
      "Epoch [296/300], Step [152/172], Loss: 8.1566\n",
      "Epoch [296/300], Step [153/172], Loss: 6.0244\n",
      "Epoch [296/300], Step [154/172], Loss: 6.2966\n",
      "Epoch [296/300], Step [155/172], Loss: 6.9186\n",
      "Epoch [296/300], Step [156/172], Loss: 13.3275\n",
      "Epoch [296/300], Step [157/172], Loss: 7.6311\n",
      "Epoch [296/300], Step [158/172], Loss: 6.5682\n",
      "Epoch [296/300], Step [159/172], Loss: 8.6695\n",
      "Epoch [296/300], Step [160/172], Loss: 8.0216\n",
      "Epoch [296/300], Step [161/172], Loss: 6.6590\n",
      "Epoch [296/300], Step [162/172], Loss: 4.3532\n",
      "Epoch [296/300], Step [163/172], Loss: 5.6794\n",
      "Epoch [296/300], Step [164/172], Loss: 8.9839\n",
      "Epoch [296/300], Step [165/172], Loss: 6.8560\n",
      "Epoch [296/300], Step [166/172], Loss: 6.0400\n",
      "Epoch [296/300], Step [167/172], Loss: 10.0145\n",
      "Epoch [296/300], Step [168/172], Loss: 6.0848\n",
      "Epoch [296/300], Step [169/172], Loss: 6.2457\n",
      "Epoch [296/300], Step [170/172], Loss: 5.0935\n",
      "Epoch [296/300], Step [171/172], Loss: 8.3884\n",
      "Epoch [296/300], Step [172/172], Loss: 5.5845\n",
      "Epoch [297/300], Step [1/172], Loss: 39.9986\n",
      "Epoch [297/300], Step [2/172], Loss: 44.9504\n",
      "Epoch [297/300], Step [3/172], Loss: 53.7470\n",
      "Epoch [297/300], Step [4/172], Loss: 19.7797\n",
      "Epoch [297/300], Step [5/172], Loss: 40.4787\n",
      "Epoch [297/300], Step [6/172], Loss: 23.3486\n",
      "Epoch [297/300], Step [7/172], Loss: 26.7460\n",
      "Epoch [297/300], Step [8/172], Loss: 4.3773\n",
      "Epoch [297/300], Step [9/172], Loss: 23.4911\n",
      "Epoch [297/300], Step [10/172], Loss: 37.5705\n",
      "Epoch [297/300], Step [11/172], Loss: 47.8414\n",
      "Epoch [297/300], Step [12/172], Loss: 48.7425\n",
      "Epoch [297/300], Step [13/172], Loss: 34.9566\n",
      "Epoch [297/300], Step [14/172], Loss: 56.4397\n",
      "Epoch [297/300], Step [15/172], Loss: 45.5819\n",
      "Epoch [297/300], Step [16/172], Loss: 10.0482\n",
      "Epoch [297/300], Step [17/172], Loss: 37.2920\n",
      "Epoch [297/300], Step [18/172], Loss: 49.2176\n",
      "Epoch [297/300], Step [19/172], Loss: 66.2810\n",
      "Epoch [297/300], Step [20/172], Loss: 24.2769\n",
      "Epoch [297/300], Step [21/172], Loss: 77.1668\n",
      "Epoch [297/300], Step [22/172], Loss: 46.9769\n",
      "Epoch [297/300], Step [23/172], Loss: 1.7416\n",
      "Epoch [297/300], Step [24/172], Loss: 47.7894\n",
      "Epoch [297/300], Step [25/172], Loss: 31.8941\n",
      "Epoch [297/300], Step [26/172], Loss: 42.9242\n",
      "Epoch [297/300], Step [27/172], Loss: 50.7604\n",
      "Epoch [297/300], Step [28/172], Loss: 15.8705\n",
      "Epoch [297/300], Step [29/172], Loss: 14.5320\n",
      "Epoch [297/300], Step [30/172], Loss: 48.1619\n",
      "Epoch [297/300], Step [31/172], Loss: 29.9362\n",
      "Epoch [297/300], Step [32/172], Loss: 43.5377\n",
      "Epoch [297/300], Step [33/172], Loss: 63.7058\n",
      "Epoch [297/300], Step [34/172], Loss: 1.4073\n",
      "Epoch [297/300], Step [35/172], Loss: 14.2811\n",
      "Epoch [297/300], Step [36/172], Loss: 14.3039\n",
      "Epoch [297/300], Step [37/172], Loss: 14.7131\n",
      "Epoch [297/300], Step [38/172], Loss: 32.6971\n",
      "Epoch [297/300], Step [39/172], Loss: 34.0230\n",
      "Epoch [297/300], Step [40/172], Loss: 22.8920\n",
      "Epoch [297/300], Step [41/172], Loss: 33.5226\n",
      "Epoch [297/300], Step [42/172], Loss: 38.5254\n",
      "Epoch [297/300], Step [43/172], Loss: 28.6840\n",
      "Epoch [297/300], Step [44/172], Loss: 22.6433\n",
      "Epoch [297/300], Step [45/172], Loss: 33.3513\n",
      "Epoch [297/300], Step [46/172], Loss: 15.2764\n",
      "Epoch [297/300], Step [47/172], Loss: 48.2895\n",
      "Epoch [297/300], Step [48/172], Loss: 57.8741\n",
      "Epoch [297/300], Step [49/172], Loss: 25.1729\n",
      "Epoch [297/300], Step [50/172], Loss: 44.9123\n",
      "Epoch [297/300], Step [51/172], Loss: 9.0607\n",
      "Epoch [297/300], Step [52/172], Loss: 21.4439\n",
      "Epoch [297/300], Step [53/172], Loss: 23.4067\n",
      "Epoch [297/300], Step [54/172], Loss: 15.6246\n",
      "Epoch [297/300], Step [55/172], Loss: 14.7562\n",
      "Epoch [297/300], Step [56/172], Loss: 15.7465\n",
      "Epoch [297/300], Step [57/172], Loss: 16.7733\n",
      "Epoch [297/300], Step [58/172], Loss: 11.3187\n",
      "Epoch [297/300], Step [59/172], Loss: 23.4790\n",
      "Epoch [297/300], Step [60/172], Loss: 17.7495\n",
      "Epoch [297/300], Step [61/172], Loss: 4.4377\n",
      "Epoch [297/300], Step [62/172], Loss: 11.7970\n",
      "Epoch [297/300], Step [63/172], Loss: 8.3626\n",
      "Epoch [297/300], Step [64/172], Loss: 10.5431\n",
      "Epoch [297/300], Step [65/172], Loss: 15.1484\n",
      "Epoch [297/300], Step [66/172], Loss: 5.3887\n",
      "Epoch [297/300], Step [67/172], Loss: 21.5451\n",
      "Epoch [297/300], Step [68/172], Loss: 2.2715\n",
      "Epoch [297/300], Step [69/172], Loss: 27.5147\n",
      "Epoch [297/300], Step [70/172], Loss: 29.6303\n",
      "Epoch [297/300], Step [71/172], Loss: 33.4062\n",
      "Epoch [297/300], Step [72/172], Loss: 30.8238\n",
      "Epoch [297/300], Step [73/172], Loss: 40.7441\n",
      "Epoch [297/300], Step [74/172], Loss: 20.8400\n",
      "Epoch [297/300], Step [75/172], Loss: 18.7979\n",
      "Epoch [297/300], Step [76/172], Loss: 22.0813\n",
      "Epoch [297/300], Step [77/172], Loss: 40.4303\n",
      "Epoch [297/300], Step [78/172], Loss: 27.2414\n",
      "Epoch [297/300], Step [79/172], Loss: 25.3937\n",
      "Epoch [297/300], Step [80/172], Loss: 43.2880\n",
      "Epoch [297/300], Step [81/172], Loss: 23.4602\n",
      "Epoch [297/300], Step [82/172], Loss: 31.6585\n",
      "Epoch [297/300], Step [83/172], Loss: 35.8906\n",
      "Epoch [297/300], Step [84/172], Loss: 25.9654\n",
      "Epoch [297/300], Step [85/172], Loss: 31.4781\n",
      "Epoch [297/300], Step [86/172], Loss: 26.3270\n",
      "Epoch [297/300], Step [87/172], Loss: 19.3428\n",
      "Epoch [297/300], Step [88/172], Loss: 18.5045\n",
      "Epoch [297/300], Step [89/172], Loss: 23.6831\n",
      "Epoch [297/300], Step [90/172], Loss: 15.6246\n",
      "Epoch [297/300], Step [91/172], Loss: 22.4733\n",
      "Epoch [297/300], Step [92/172], Loss: 16.3192\n",
      "Epoch [297/300], Step [93/172], Loss: 15.8564\n",
      "Epoch [297/300], Step [94/172], Loss: 22.2312\n",
      "Epoch [297/300], Step [95/172], Loss: 16.6895\n",
      "Epoch [297/300], Step [96/172], Loss: 18.7404\n",
      "Epoch [297/300], Step [97/172], Loss: 25.6579\n",
      "Epoch [297/300], Step [98/172], Loss: 16.1294\n",
      "Epoch [297/300], Step [99/172], Loss: 17.0554\n",
      "Epoch [297/300], Step [100/172], Loss: 15.9229\n",
      "Epoch [297/300], Step [101/172], Loss: 16.7827\n",
      "Epoch [297/300], Step [102/172], Loss: 16.0860\n",
      "Epoch [297/300], Step [103/172], Loss: 10.9270\n",
      "Epoch [297/300], Step [104/172], Loss: 17.5312\n",
      "Epoch [297/300], Step [105/172], Loss: 22.0427\n",
      "Epoch [297/300], Step [106/172], Loss: 14.7384\n",
      "Epoch [297/300], Step [107/172], Loss: 14.6558\n",
      "Epoch [297/300], Step [108/172], Loss: 14.4121\n",
      "Epoch [297/300], Step [109/172], Loss: 14.6721\n",
      "Epoch [297/300], Step [110/172], Loss: 16.0902\n",
      "Epoch [297/300], Step [111/172], Loss: 17.9269\n",
      "Epoch [297/300], Step [112/172], Loss: 13.4401\n",
      "Epoch [297/300], Step [113/172], Loss: 13.2603\n",
      "Epoch [297/300], Step [114/172], Loss: 13.8187\n",
      "Epoch [297/300], Step [115/172], Loss: 17.8571\n",
      "Epoch [297/300], Step [116/172], Loss: 13.4130\n",
      "Epoch [297/300], Step [117/172], Loss: 11.0762\n",
      "Epoch [297/300], Step [118/172], Loss: 11.6701\n",
      "Epoch [297/300], Step [119/172], Loss: 17.4393\n",
      "Epoch [297/300], Step [120/172], Loss: 10.3860\n",
      "Epoch [297/300], Step [121/172], Loss: 9.1314\n",
      "Epoch [297/300], Step [122/172], Loss: 13.0770\n",
      "Epoch [297/300], Step [123/172], Loss: 11.4610\n",
      "Epoch [297/300], Step [124/172], Loss: 7.0911\n",
      "Epoch [297/300], Step [125/172], Loss: 11.6795\n",
      "Epoch [297/300], Step [126/172], Loss: 11.9046\n",
      "Epoch [297/300], Step [127/172], Loss: 10.3045\n",
      "Epoch [297/300], Step [128/172], Loss: 9.5706\n",
      "Epoch [297/300], Step [129/172], Loss: 8.5534\n",
      "Epoch [297/300], Step [130/172], Loss: 10.9578\n",
      "Epoch [297/300], Step [131/172], Loss: 7.6467\n",
      "Epoch [297/300], Step [132/172], Loss: 9.6557\n",
      "Epoch [297/300], Step [133/172], Loss: 9.7715\n",
      "Epoch [297/300], Step [134/172], Loss: 9.8280\n",
      "Epoch [297/300], Step [135/172], Loss: 9.0273\n",
      "Epoch [297/300], Step [136/172], Loss: 8.3289\n",
      "Epoch [297/300], Step [137/172], Loss: 8.0188\n",
      "Epoch [297/300], Step [138/172], Loss: 7.1637\n",
      "Epoch [297/300], Step [139/172], Loss: 10.7595\n",
      "Epoch [297/300], Step [140/172], Loss: 10.6933\n",
      "Epoch [297/300], Step [141/172], Loss: 8.5902\n",
      "Epoch [297/300], Step [142/172], Loss: 13.5821\n",
      "Epoch [297/300], Step [143/172], Loss: 10.6647\n",
      "Epoch [297/300], Step [144/172], Loss: 8.7741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [297/300], Step [145/172], Loss: 10.1662\n",
      "Epoch [297/300], Step [146/172], Loss: 10.0407\n",
      "Epoch [297/300], Step [147/172], Loss: 5.6493\n",
      "Epoch [297/300], Step [148/172], Loss: 6.5516\n",
      "Epoch [297/300], Step [149/172], Loss: 6.3415\n",
      "Epoch [297/300], Step [150/172], Loss: 5.6956\n",
      "Epoch [297/300], Step [151/172], Loss: 5.3264\n",
      "Epoch [297/300], Step [152/172], Loss: 8.4198\n",
      "Epoch [297/300], Step [153/172], Loss: 6.0614\n",
      "Epoch [297/300], Step [154/172], Loss: 6.3181\n",
      "Epoch [297/300], Step [155/172], Loss: 6.8690\n",
      "Epoch [297/300], Step [156/172], Loss: 13.4063\n",
      "Epoch [297/300], Step [157/172], Loss: 7.6373\n",
      "Epoch [297/300], Step [158/172], Loss: 6.6398\n",
      "Epoch [297/300], Step [159/172], Loss: 8.7771\n",
      "Epoch [297/300], Step [160/172], Loss: 8.0843\n",
      "Epoch [297/300], Step [161/172], Loss: 6.8474\n",
      "Epoch [297/300], Step [162/172], Loss: 4.5386\n",
      "Epoch [297/300], Step [163/172], Loss: 5.4928\n",
      "Epoch [297/300], Step [164/172], Loss: 8.9577\n",
      "Epoch [297/300], Step [165/172], Loss: 6.9428\n",
      "Epoch [297/300], Step [166/172], Loss: 6.0983\n",
      "Epoch [297/300], Step [167/172], Loss: 9.9364\n",
      "Epoch [297/300], Step [168/172], Loss: 6.0298\n",
      "Epoch [297/300], Step [169/172], Loss: 6.2493\n",
      "Epoch [297/300], Step [170/172], Loss: 5.1692\n",
      "Epoch [297/300], Step [171/172], Loss: 8.4665\n",
      "Epoch [297/300], Step [172/172], Loss: 5.5773\n",
      "Epoch [298/300], Step [1/172], Loss: 39.9422\n",
      "Epoch [298/300], Step [2/172], Loss: 44.5332\n",
      "Epoch [298/300], Step [3/172], Loss: 44.8271\n",
      "Epoch [298/300], Step [4/172], Loss: 19.3503\n",
      "Epoch [298/300], Step [5/172], Loss: 41.7964\n",
      "Epoch [298/300], Step [6/172], Loss: 23.3336\n",
      "Epoch [298/300], Step [7/172], Loss: 26.8748\n",
      "Epoch [298/300], Step [8/172], Loss: 4.7136\n",
      "Epoch [298/300], Step [9/172], Loss: 23.8613\n",
      "Epoch [298/300], Step [10/172], Loss: 38.3955\n",
      "Epoch [298/300], Step [11/172], Loss: 48.4889\n",
      "Epoch [298/300], Step [12/172], Loss: 49.0684\n",
      "Epoch [298/300], Step [13/172], Loss: 34.8367\n",
      "Epoch [298/300], Step [14/172], Loss: 56.5879\n",
      "Epoch [298/300], Step [15/172], Loss: 45.9715\n",
      "Epoch [298/300], Step [16/172], Loss: 8.6219\n",
      "Epoch [298/300], Step [17/172], Loss: 37.8826\n",
      "Epoch [298/300], Step [18/172], Loss: 49.2122\n",
      "Epoch [298/300], Step [19/172], Loss: 66.4748\n",
      "Epoch [298/300], Step [20/172], Loss: 23.9916\n",
      "Epoch [298/300], Step [21/172], Loss: 76.2434\n",
      "Epoch [298/300], Step [22/172], Loss: 47.6288\n",
      "Epoch [298/300], Step [23/172], Loss: 1.3712\n",
      "Epoch [298/300], Step [24/172], Loss: 47.9170\n",
      "Epoch [298/300], Step [25/172], Loss: 32.0675\n",
      "Epoch [298/300], Step [26/172], Loss: 43.0604\n",
      "Epoch [298/300], Step [27/172], Loss: 49.9026\n",
      "Epoch [298/300], Step [28/172], Loss: 16.1814\n",
      "Epoch [298/300], Step [29/172], Loss: 14.5857\n",
      "Epoch [298/300], Step [30/172], Loss: 47.7522\n",
      "Epoch [298/300], Step [31/172], Loss: 29.9455\n",
      "Epoch [298/300], Step [32/172], Loss: 43.6560\n",
      "Epoch [298/300], Step [33/172], Loss: 63.8177\n",
      "Epoch [298/300], Step [34/172], Loss: 1.4029\n",
      "Epoch [298/300], Step [35/172], Loss: 14.4476\n",
      "Epoch [298/300], Step [36/172], Loss: 14.2625\n",
      "Epoch [298/300], Step [37/172], Loss: 14.8388\n",
      "Epoch [298/300], Step [38/172], Loss: 32.4622\n",
      "Epoch [298/300], Step [39/172], Loss: 32.8113\n",
      "Epoch [298/300], Step [40/172], Loss: 22.5840\n",
      "Epoch [298/300], Step [41/172], Loss: 32.1924\n",
      "Epoch [298/300], Step [42/172], Loss: 37.4878\n",
      "Epoch [298/300], Step [43/172], Loss: 27.8096\n",
      "Epoch [298/300], Step [44/172], Loss: 22.3952\n",
      "Epoch [298/300], Step [45/172], Loss: 33.5432\n",
      "Epoch [298/300], Step [46/172], Loss: 14.9325\n",
      "Epoch [298/300], Step [47/172], Loss: 47.4320\n",
      "Epoch [298/300], Step [48/172], Loss: 56.8742\n",
      "Epoch [298/300], Step [49/172], Loss: 25.3186\n",
      "Epoch [298/300], Step [50/172], Loss: 44.2466\n",
      "Epoch [298/300], Step [51/172], Loss: 9.3444\n",
      "Epoch [298/300], Step [52/172], Loss: 21.8583\n",
      "Epoch [298/300], Step [53/172], Loss: 23.5004\n",
      "Epoch [298/300], Step [54/172], Loss: 16.5614\n",
      "Epoch [298/300], Step [55/172], Loss: 15.6995\n",
      "Epoch [298/300], Step [56/172], Loss: 17.6308\n",
      "Epoch [298/300], Step [57/172], Loss: 16.2675\n",
      "Epoch [298/300], Step [58/172], Loss: 12.1722\n",
      "Epoch [298/300], Step [59/172], Loss: 23.2590\n",
      "Epoch [298/300], Step [60/172], Loss: 17.2920\n",
      "Epoch [298/300], Step [61/172], Loss: 4.6270\n",
      "Epoch [298/300], Step [62/172], Loss: 11.7591\n",
      "Epoch [298/300], Step [63/172], Loss: 9.1104\n",
      "Epoch [298/300], Step [64/172], Loss: 10.9612\n",
      "Epoch [298/300], Step [65/172], Loss: 15.0530\n",
      "Epoch [298/300], Step [66/172], Loss: 5.4161\n",
      "Epoch [298/300], Step [67/172], Loss: 20.9750\n",
      "Epoch [298/300], Step [68/172], Loss: 2.4511\n",
      "Epoch [298/300], Step [69/172], Loss: 27.1542\n",
      "Epoch [298/300], Step [70/172], Loss: 29.1702\n",
      "Epoch [298/300], Step [71/172], Loss: 32.8753\n",
      "Epoch [298/300], Step [72/172], Loss: 30.7135\n",
      "Epoch [298/300], Step [73/172], Loss: 40.3430\n",
      "Epoch [298/300], Step [74/172], Loss: 20.5627\n",
      "Epoch [298/300], Step [75/172], Loss: 18.0833\n",
      "Epoch [298/300], Step [76/172], Loss: 22.1094\n",
      "Epoch [298/300], Step [77/172], Loss: 40.9273\n",
      "Epoch [298/300], Step [78/172], Loss: 27.4772\n",
      "Epoch [298/300], Step [79/172], Loss: 25.5865\n",
      "Epoch [298/300], Step [80/172], Loss: 43.7880\n",
      "Epoch [298/300], Step [81/172], Loss: 23.7729\n",
      "Epoch [298/300], Step [82/172], Loss: 32.3847\n",
      "Epoch [298/300], Step [83/172], Loss: 36.1312\n",
      "Epoch [298/300], Step [84/172], Loss: 26.4519\n",
      "Epoch [298/300], Step [85/172], Loss: 31.5546\n",
      "Epoch [298/300], Step [86/172], Loss: 26.7304\n",
      "Epoch [298/300], Step [87/172], Loss: 19.5951\n",
      "Epoch [298/300], Step [88/172], Loss: 18.5551\n",
      "Epoch [298/300], Step [89/172], Loss: 24.0380\n",
      "Epoch [298/300], Step [90/172], Loss: 15.6579\n",
      "Epoch [298/300], Step [91/172], Loss: 22.4814\n",
      "Epoch [298/300], Step [92/172], Loss: 16.3572\n",
      "Epoch [298/300], Step [93/172], Loss: 15.6685\n",
      "Epoch [298/300], Step [94/172], Loss: 21.9999\n",
      "Epoch [298/300], Step [95/172], Loss: 16.7346\n",
      "Epoch [298/300], Step [96/172], Loss: 18.3929\n",
      "Epoch [298/300], Step [97/172], Loss: 25.6218\n",
      "Epoch [298/300], Step [98/172], Loss: 16.0500\n",
      "Epoch [298/300], Step [99/172], Loss: 16.8932\n",
      "Epoch [298/300], Step [100/172], Loss: 15.9331\n",
      "Epoch [298/300], Step [101/172], Loss: 16.6964\n",
      "Epoch [298/300], Step [102/172], Loss: 16.2559\n",
      "Epoch [298/300], Step [103/172], Loss: 10.8003\n",
      "Epoch [298/300], Step [104/172], Loss: 17.3075\n",
      "Epoch [298/300], Step [105/172], Loss: 22.0760\n",
      "Epoch [298/300], Step [106/172], Loss: 14.7273\n",
      "Epoch [298/300], Step [107/172], Loss: 14.7546\n",
      "Epoch [298/300], Step [108/172], Loss: 14.4557\n",
      "Epoch [298/300], Step [109/172], Loss: 14.7186\n",
      "Epoch [298/300], Step [110/172], Loss: 16.2508\n",
      "Epoch [298/300], Step [111/172], Loss: 17.9907\n",
      "Epoch [298/300], Step [112/172], Loss: 13.4903\n",
      "Epoch [298/300], Step [113/172], Loss: 13.2282\n",
      "Epoch [298/300], Step [114/172], Loss: 13.7089\n",
      "Epoch [298/300], Step [115/172], Loss: 18.1572\n",
      "Epoch [298/300], Step [116/172], Loss: 13.4657\n",
      "Epoch [298/300], Step [117/172], Loss: 11.1201\n",
      "Epoch [298/300], Step [118/172], Loss: 11.9014\n",
      "Epoch [298/300], Step [119/172], Loss: 17.5539\n",
      "Epoch [298/300], Step [120/172], Loss: 10.5544\n",
      "Epoch [298/300], Step [121/172], Loss: 9.1072\n",
      "Epoch [298/300], Step [122/172], Loss: 13.3121\n",
      "Epoch [298/300], Step [123/172], Loss: 11.7616\n",
      "Epoch [298/300], Step [124/172], Loss: 6.9394\n",
      "Epoch [298/300], Step [125/172], Loss: 11.3930\n",
      "Epoch [298/300], Step [126/172], Loss: 11.9126\n",
      "Epoch [298/300], Step [127/172], Loss: 10.4676\n",
      "Epoch [298/300], Step [128/172], Loss: 9.6170\n",
      "Epoch [298/300], Step [129/172], Loss: 8.5020\n",
      "Epoch [298/300], Step [130/172], Loss: 11.1012\n",
      "Epoch [298/300], Step [131/172], Loss: 7.6513\n",
      "Epoch [298/300], Step [132/172], Loss: 9.7017\n",
      "Epoch [298/300], Step [133/172], Loss: 10.0050\n",
      "Epoch [298/300], Step [134/172], Loss: 9.8913\n",
      "Epoch [298/300], Step [135/172], Loss: 9.0107\n",
      "Epoch [298/300], Step [136/172], Loss: 8.8075\n",
      "Epoch [298/300], Step [137/172], Loss: 8.0053\n",
      "Epoch [298/300], Step [138/172], Loss: 7.0963\n",
      "Epoch [298/300], Step [139/172], Loss: 10.5024\n",
      "Epoch [298/300], Step [140/172], Loss: 10.9041\n",
      "Epoch [298/300], Step [141/172], Loss: 8.7615\n",
      "Epoch [298/300], Step [142/172], Loss: 13.6369\n",
      "Epoch [298/300], Step [143/172], Loss: 10.7762\n",
      "Epoch [298/300], Step [144/172], Loss: 8.8603\n",
      "Epoch [298/300], Step [145/172], Loss: 10.1134\n",
      "Epoch [298/300], Step [146/172], Loss: 10.1811\n",
      "Epoch [298/300], Step [147/172], Loss: 5.6363\n",
      "Epoch [298/300], Step [148/172], Loss: 6.5472\n",
      "Epoch [298/300], Step [149/172], Loss: 6.4080\n",
      "Epoch [298/300], Step [150/172], Loss: 5.8393\n",
      "Epoch [298/300], Step [151/172], Loss: 5.3073\n",
      "Epoch [298/300], Step [152/172], Loss: 8.4760\n",
      "Epoch [298/300], Step [153/172], Loss: 5.9705\n",
      "Epoch [298/300], Step [154/172], Loss: 6.4138\n",
      "Epoch [298/300], Step [155/172], Loss: 6.7559\n",
      "Epoch [298/300], Step [156/172], Loss: 13.5348\n",
      "Epoch [298/300], Step [157/172], Loss: 7.8355\n",
      "Epoch [298/300], Step [158/172], Loss: 6.7444\n",
      "Epoch [298/300], Step [159/172], Loss: 8.7475\n",
      "Epoch [298/300], Step [160/172], Loss: 8.2033\n",
      "Epoch [298/300], Step [161/172], Loss: 6.9292\n",
      "Epoch [298/300], Step [162/172], Loss: 4.6677\n",
      "Epoch [298/300], Step [163/172], Loss: 5.4681\n",
      "Epoch [298/300], Step [164/172], Loss: 8.9842\n",
      "Epoch [298/300], Step [165/172], Loss: 6.8868\n",
      "Epoch [298/300], Step [166/172], Loss: 6.0904\n",
      "Epoch [298/300], Step [167/172], Loss: 9.7158\n",
      "Epoch [298/300], Step [168/172], Loss: 5.9760\n",
      "Epoch [298/300], Step [169/172], Loss: 6.3309\n",
      "Epoch [298/300], Step [170/172], Loss: 5.1684\n",
      "Epoch [298/300], Step [171/172], Loss: 8.0708\n",
      "Epoch [298/300], Step [172/172], Loss: 5.5265\n",
      "Epoch [299/300], Step [1/172], Loss: 39.9854\n",
      "Epoch [299/300], Step [2/172], Loss: 43.4484\n",
      "Epoch [299/300], Step [3/172], Loss: 42.9649\n",
      "Epoch [299/300], Step [4/172], Loss: 18.7374\n",
      "Epoch [299/300], Step [5/172], Loss: 43.0569\n",
      "Epoch [299/300], Step [6/172], Loss: 22.0962\n",
      "Epoch [299/300], Step [7/172], Loss: 25.4332\n",
      "Epoch [299/300], Step [8/172], Loss: 4.6397\n",
      "Epoch [299/300], Step [9/172], Loss: 23.7152\n",
      "Epoch [299/300], Step [10/172], Loss: 38.3233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [299/300], Step [11/172], Loss: 48.8266\n",
      "Epoch [299/300], Step [12/172], Loss: 49.0109\n",
      "Epoch [299/300], Step [13/172], Loss: 34.7851\n",
      "Epoch [299/300], Step [14/172], Loss: 56.7936\n",
      "Epoch [299/300], Step [15/172], Loss: 46.0528\n",
      "Epoch [299/300], Step [16/172], Loss: 8.3973\n",
      "Epoch [299/300], Step [17/172], Loss: 37.7778\n",
      "Epoch [299/300], Step [18/172], Loss: 49.3384\n",
      "Epoch [299/300], Step [19/172], Loss: 66.4478\n",
      "Epoch [299/300], Step [20/172], Loss: 24.6563\n",
      "Epoch [299/300], Step [21/172], Loss: 73.8911\n",
      "Epoch [299/300], Step [22/172], Loss: 48.7551\n",
      "Epoch [299/300], Step [23/172], Loss: 1.2645\n",
      "Epoch [299/300], Step [24/172], Loss: 47.9234\n",
      "Epoch [299/300], Step [25/172], Loss: 32.2787\n",
      "Epoch [299/300], Step [26/172], Loss: 43.9093\n",
      "Epoch [299/300], Step [27/172], Loss: 50.8480\n",
      "Epoch [299/300], Step [28/172], Loss: 16.4700\n",
      "Epoch [299/300], Step [29/172], Loss: 14.6545\n",
      "Epoch [299/300], Step [30/172], Loss: 48.0758\n",
      "Epoch [299/300], Step [31/172], Loss: 30.3594\n",
      "Epoch [299/300], Step [32/172], Loss: 43.6765\n",
      "Epoch [299/300], Step [33/172], Loss: 64.6510\n",
      "Epoch [299/300], Step [34/172], Loss: 1.4536\n",
      "Epoch [299/300], Step [35/172], Loss: 14.9433\n",
      "Epoch [299/300], Step [36/172], Loss: 14.4517\n",
      "Epoch [299/300], Step [37/172], Loss: 15.0566\n",
      "Epoch [299/300], Step [38/172], Loss: 32.8282\n",
      "Epoch [299/300], Step [39/172], Loss: 33.6084\n",
      "Epoch [299/300], Step [40/172], Loss: 22.8953\n",
      "Epoch [299/300], Step [41/172], Loss: 32.7597\n",
      "Epoch [299/300], Step [42/172], Loss: 37.7137\n",
      "Epoch [299/300], Step [43/172], Loss: 28.5843\n",
      "Epoch [299/300], Step [44/172], Loss: 23.4609\n",
      "Epoch [299/300], Step [45/172], Loss: 34.8663\n",
      "Epoch [299/300], Step [46/172], Loss: 14.9286\n",
      "Epoch [299/300], Step [47/172], Loss: 48.3549\n",
      "Epoch [299/300], Step [48/172], Loss: 56.2629\n",
      "Epoch [299/300], Step [49/172], Loss: 26.3745\n",
      "Epoch [299/300], Step [50/172], Loss: 45.3137\n",
      "Epoch [299/300], Step [51/172], Loss: 9.7489\n",
      "Epoch [299/300], Step [52/172], Loss: 22.8030\n",
      "Epoch [299/300], Step [53/172], Loss: 24.5359\n",
      "Epoch [299/300], Step [54/172], Loss: 17.5836\n",
      "Epoch [299/300], Step [55/172], Loss: 16.2933\n",
      "Epoch [299/300], Step [56/172], Loss: 18.5915\n",
      "Epoch [299/300], Step [57/172], Loss: 17.4617\n",
      "Epoch [299/300], Step [58/172], Loss: 12.7286\n",
      "Epoch [299/300], Step [59/172], Loss: 23.3520\n",
      "Epoch [299/300], Step [60/172], Loss: 17.0212\n",
      "Epoch [299/300], Step [61/172], Loss: 5.0269\n",
      "Epoch [299/300], Step [62/172], Loss: 12.1617\n",
      "Epoch [299/300], Step [63/172], Loss: 9.8663\n",
      "Epoch [299/300], Step [64/172], Loss: 12.0718\n",
      "Epoch [299/300], Step [65/172], Loss: 15.8099\n",
      "Epoch [299/300], Step [66/172], Loss: 5.4902\n",
      "Epoch [299/300], Step [67/172], Loss: 21.1921\n",
      "Epoch [299/300], Step [68/172], Loss: 3.3433\n",
      "Epoch [299/300], Step [69/172], Loss: 26.5487\n",
      "Epoch [299/300], Step [70/172], Loss: 28.1685\n",
      "Epoch [299/300], Step [71/172], Loss: 31.8894\n",
      "Epoch [299/300], Step [72/172], Loss: 29.9911\n",
      "Epoch [299/300], Step [73/172], Loss: 39.2884\n",
      "Epoch [299/300], Step [74/172], Loss: 20.2369\n",
      "Epoch [299/300], Step [75/172], Loss: 17.8822\n",
      "Epoch [299/300], Step [76/172], Loss: 22.1555\n",
      "Epoch [299/300], Step [77/172], Loss: 40.2204\n",
      "Epoch [299/300], Step [78/172], Loss: 27.1646\n",
      "Epoch [299/300], Step [79/172], Loss: 25.4462\n",
      "Epoch [299/300], Step [80/172], Loss: 43.6177\n",
      "Epoch [299/300], Step [81/172], Loss: 24.0451\n",
      "Epoch [299/300], Step [82/172], Loss: 32.6071\n",
      "Epoch [299/300], Step [83/172], Loss: 36.6605\n",
      "Epoch [299/300], Step [84/172], Loss: 27.0426\n",
      "Epoch [299/300], Step [85/172], Loss: 31.9200\n",
      "Epoch [299/300], Step [86/172], Loss: 27.2308\n",
      "Epoch [299/300], Step [87/172], Loss: 20.0313\n",
      "Epoch [299/300], Step [88/172], Loss: 19.1047\n",
      "Epoch [299/300], Step [89/172], Loss: 24.5866\n",
      "Epoch [299/300], Step [90/172], Loss: 15.9711\n",
      "Epoch [299/300], Step [91/172], Loss: 22.9402\n",
      "Epoch [299/300], Step [92/172], Loss: 16.5532\n",
      "Epoch [299/300], Step [93/172], Loss: 15.8681\n",
      "Epoch [299/300], Step [94/172], Loss: 22.2503\n",
      "Epoch [299/300], Step [95/172], Loss: 17.3200\n",
      "Epoch [299/300], Step [96/172], Loss: 18.7560\n",
      "Epoch [299/300], Step [97/172], Loss: 26.0913\n",
      "Epoch [299/300], Step [98/172], Loss: 16.3725\n",
      "Epoch [299/300], Step [99/172], Loss: 17.2148\n",
      "Epoch [299/300], Step [100/172], Loss: 16.4454\n",
      "Epoch [299/300], Step [101/172], Loss: 17.0409\n",
      "Epoch [299/300], Step [102/172], Loss: 16.4795\n",
      "Epoch [299/300], Step [103/172], Loss: 11.1127\n",
      "Epoch [299/300], Step [104/172], Loss: 17.7298\n",
      "Epoch [299/300], Step [105/172], Loss: 22.6537\n",
      "Epoch [299/300], Step [106/172], Loss: 15.1913\n",
      "Epoch [299/300], Step [107/172], Loss: 14.9372\n",
      "Epoch [299/300], Step [108/172], Loss: 14.9119\n",
      "Epoch [299/300], Step [109/172], Loss: 14.7507\n",
      "Epoch [299/300], Step [110/172], Loss: 16.4798\n",
      "Epoch [299/300], Step [111/172], Loss: 18.2718\n",
      "Epoch [299/300], Step [112/172], Loss: 13.5065\n",
      "Epoch [299/300], Step [113/172], Loss: 13.6577\n",
      "Epoch [299/300], Step [114/172], Loss: 13.9732\n",
      "Epoch [299/300], Step [115/172], Loss: 18.1452\n",
      "Epoch [299/300], Step [116/172], Loss: 13.7690\n",
      "Epoch [299/300], Step [117/172], Loss: 11.4153\n",
      "Epoch [299/300], Step [118/172], Loss: 11.8528\n",
      "Epoch [299/300], Step [119/172], Loss: 17.7328\n",
      "Epoch [299/300], Step [120/172], Loss: 10.5892\n",
      "Epoch [299/300], Step [121/172], Loss: 9.2002\n",
      "Epoch [299/300], Step [122/172], Loss: 13.4732\n",
      "Epoch [299/300], Step [123/172], Loss: 12.1571\n",
      "Epoch [299/300], Step [124/172], Loss: 7.0538\n",
      "Epoch [299/300], Step [125/172], Loss: 11.6077\n",
      "Epoch [299/300], Step [126/172], Loss: 12.2603\n",
      "Epoch [299/300], Step [127/172], Loss: 10.5718\n",
      "Epoch [299/300], Step [128/172], Loss: 9.7204\n",
      "Epoch [299/300], Step [129/172], Loss: 8.6358\n",
      "Epoch [299/300], Step [130/172], Loss: 11.6486\n",
      "Epoch [299/300], Step [131/172], Loss: 7.8147\n",
      "Epoch [299/300], Step [132/172], Loss: 9.9493\n",
      "Epoch [299/300], Step [133/172], Loss: 10.1728\n",
      "Epoch [299/300], Step [134/172], Loss: 9.9208\n",
      "Epoch [299/300], Step [135/172], Loss: 9.2516\n",
      "Epoch [299/300], Step [136/172], Loss: 9.4205\n",
      "Epoch [299/300], Step [137/172], Loss: 8.1817\n",
      "Epoch [299/300], Step [138/172], Loss: 7.2206\n",
      "Epoch [299/300], Step [139/172], Loss: 10.7457\n",
      "Epoch [299/300], Step [140/172], Loss: 11.2282\n",
      "Epoch [299/300], Step [141/172], Loss: 9.0392\n",
      "Epoch [299/300], Step [142/172], Loss: 13.8354\n",
      "Epoch [299/300], Step [143/172], Loss: 11.0673\n",
      "Epoch [299/300], Step [144/172], Loss: 9.0780\n",
      "Epoch [299/300], Step [145/172], Loss: 10.4114\n",
      "Epoch [299/300], Step [146/172], Loss: 10.3942\n",
      "Epoch [299/300], Step [147/172], Loss: 5.7902\n",
      "Epoch [299/300], Step [148/172], Loss: 6.7457\n",
      "Epoch [299/300], Step [149/172], Loss: 6.7083\n",
      "Epoch [299/300], Step [150/172], Loss: 6.0894\n",
      "Epoch [299/300], Step [151/172], Loss: 5.6847\n",
      "Epoch [299/300], Step [152/172], Loss: 8.7791\n",
      "Epoch [299/300], Step [153/172], Loss: 6.0442\n",
      "Epoch [299/300], Step [154/172], Loss: 6.6293\n",
      "Epoch [299/300], Step [155/172], Loss: 7.0411\n",
      "Epoch [299/300], Step [156/172], Loss: 13.9483\n",
      "Epoch [299/300], Step [157/172], Loss: 8.1581\n",
      "Epoch [299/300], Step [158/172], Loss: 6.9526\n",
      "Epoch [299/300], Step [159/172], Loss: 9.1248\n",
      "Epoch [299/300], Step [160/172], Loss: 8.3952\n",
      "Epoch [299/300], Step [161/172], Loss: 7.5651\n",
      "Epoch [299/300], Step [162/172], Loss: 4.8115\n",
      "Epoch [299/300], Step [163/172], Loss: 5.9928\n",
      "Epoch [299/300], Step [164/172], Loss: 9.3138\n",
      "Epoch [299/300], Step [165/172], Loss: 7.2374\n",
      "Epoch [299/300], Step [166/172], Loss: 6.2715\n",
      "Epoch [299/300], Step [167/172], Loss: 10.1925\n",
      "Epoch [299/300], Step [168/172], Loss: 6.2086\n",
      "Epoch [299/300], Step [169/172], Loss: 6.6121\n",
      "Epoch [299/300], Step [170/172], Loss: 5.5718\n",
      "Epoch [299/300], Step [171/172], Loss: 8.3852\n",
      "Epoch [299/300], Step [172/172], Loss: 5.7539\n",
      "Epoch [300/300], Step [1/172], Loss: 39.2405\n",
      "Epoch [300/300], Step [2/172], Loss: 42.6252\n",
      "Epoch [300/300], Step [3/172], Loss: 40.2826\n",
      "Epoch [300/300], Step [4/172], Loss: 18.1013\n",
      "Epoch [300/300], Step [5/172], Loss: 40.0844\n",
      "Epoch [300/300], Step [6/172], Loss: 21.9953\n",
      "Epoch [300/300], Step [7/172], Loss: 26.4124\n",
      "Epoch [300/300], Step [8/172], Loss: 4.1535\n",
      "Epoch [300/300], Step [9/172], Loss: 23.0356\n",
      "Epoch [300/300], Step [10/172], Loss: 37.8843\n",
      "Epoch [300/300], Step [11/172], Loss: 48.5248\n",
      "Epoch [300/300], Step [12/172], Loss: 48.2077\n",
      "Epoch [300/300], Step [13/172], Loss: 34.3256\n",
      "Epoch [300/300], Step [14/172], Loss: 56.0899\n",
      "Epoch [300/300], Step [15/172], Loss: 44.6689\n",
      "Epoch [300/300], Step [16/172], Loss: 9.2560\n",
      "Epoch [300/300], Step [17/172], Loss: 37.7057\n",
      "Epoch [300/300], Step [18/172], Loss: 49.2056\n",
      "Epoch [300/300], Step [19/172], Loss: 66.0754\n",
      "Epoch [300/300], Step [20/172], Loss: 23.8946\n",
      "Epoch [300/300], Step [21/172], Loss: 75.9436\n",
      "Epoch [300/300], Step [22/172], Loss: 48.1173\n",
      "Epoch [300/300], Step [23/172], Loss: 1.5253\n",
      "Epoch [300/300], Step [24/172], Loss: 47.8790\n",
      "Epoch [300/300], Step [25/172], Loss: 32.1477\n",
      "Epoch [300/300], Step [26/172], Loss: 43.5238\n",
      "Epoch [300/300], Step [27/172], Loss: 50.6089\n",
      "Epoch [300/300], Step [28/172], Loss: 16.6808\n",
      "Epoch [300/300], Step [29/172], Loss: 14.6277\n",
      "Epoch [300/300], Step [30/172], Loss: 47.8659\n",
      "Epoch [300/300], Step [31/172], Loss: 30.6098\n",
      "Epoch [300/300], Step [32/172], Loss: 43.5781\n",
      "Epoch [300/300], Step [33/172], Loss: 63.8132\n",
      "Epoch [300/300], Step [34/172], Loss: 1.4755\n",
      "Epoch [300/300], Step [35/172], Loss: 14.8427\n",
      "Epoch [300/300], Step [36/172], Loss: 14.4542\n",
      "Epoch [300/300], Step [37/172], Loss: 15.2945\n",
      "Epoch [300/300], Step [38/172], Loss: 33.4933\n",
      "Epoch [300/300], Step [39/172], Loss: 33.0009\n",
      "Epoch [300/300], Step [40/172], Loss: 22.7938\n",
      "Epoch [300/300], Step [41/172], Loss: 33.4891\n",
      "Epoch [300/300], Step [42/172], Loss: 38.3755\n",
      "Epoch [300/300], Step [43/172], Loss: 29.4403\n",
      "Epoch [300/300], Step [44/172], Loss: 23.0312\n",
      "Epoch [300/300], Step [45/172], Loss: 34.5188\n",
      "Epoch [300/300], Step [46/172], Loss: 15.1320\n",
      "Epoch [300/300], Step [47/172], Loss: 48.4053\n",
      "Epoch [300/300], Step [48/172], Loss: 56.8684\n",
      "Epoch [300/300], Step [49/172], Loss: 26.0730\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300], Step [50/172], Loss: 43.8786\n",
      "Epoch [300/300], Step [51/172], Loss: 9.5994\n",
      "Epoch [300/300], Step [52/172], Loss: 22.6742\n",
      "Epoch [300/300], Step [53/172], Loss: 25.1297\n",
      "Epoch [300/300], Step [54/172], Loss: 17.7283\n",
      "Epoch [300/300], Step [55/172], Loss: 17.0084\n",
      "Epoch [300/300], Step [56/172], Loss: 17.3921\n",
      "Epoch [300/300], Step [57/172], Loss: 17.2901\n",
      "Epoch [300/300], Step [58/172], Loss: 13.1750\n",
      "Epoch [300/300], Step [59/172], Loss: 24.5842\n",
      "Epoch [300/300], Step [60/172], Loss: 17.4232\n",
      "Epoch [300/300], Step [61/172], Loss: 5.3995\n",
      "Epoch [300/300], Step [62/172], Loss: 12.6860\n",
      "Epoch [300/300], Step [63/172], Loss: 9.9874\n",
      "Epoch [300/300], Step [64/172], Loss: 11.9294\n",
      "Epoch [300/300], Step [65/172], Loss: 16.3520\n",
      "Epoch [300/300], Step [66/172], Loss: 5.5050\n",
      "Epoch [300/300], Step [67/172], Loss: 21.5335\n",
      "Epoch [300/300], Step [68/172], Loss: 3.4372\n",
      "Epoch [300/300], Step [69/172], Loss: 26.2652\n",
      "Epoch [300/300], Step [70/172], Loss: 27.6259\n",
      "Epoch [300/300], Step [71/172], Loss: 31.6287\n",
      "Epoch [300/300], Step [72/172], Loss: 29.7414\n",
      "Epoch [300/300], Step [73/172], Loss: 38.7138\n",
      "Epoch [300/300], Step [74/172], Loss: 20.0030\n",
      "Epoch [300/300], Step [75/172], Loss: 17.9851\n",
      "Epoch [300/300], Step [76/172], Loss: 22.4382\n",
      "Epoch [300/300], Step [77/172], Loss: 40.4903\n",
      "Epoch [300/300], Step [78/172], Loss: 27.2463\n",
      "Epoch [300/300], Step [79/172], Loss: 25.6648\n",
      "Epoch [300/300], Step [80/172], Loss: 43.7300\n",
      "Epoch [300/300], Step [81/172], Loss: 23.9854\n",
      "Epoch [300/300], Step [82/172], Loss: 32.3615\n",
      "Epoch [300/300], Step [83/172], Loss: 37.1497\n",
      "Epoch [300/300], Step [84/172], Loss: 27.4277\n",
      "Epoch [300/300], Step [85/172], Loss: 32.2148\n",
      "Epoch [300/300], Step [86/172], Loss: 27.5254\n",
      "Epoch [300/300], Step [87/172], Loss: 20.1800\n",
      "Epoch [300/300], Step [88/172], Loss: 19.3534\n",
      "Epoch [300/300], Step [89/172], Loss: 24.7739\n",
      "Epoch [300/300], Step [90/172], Loss: 16.1807\n",
      "Epoch [300/300], Step [91/172], Loss: 23.2584\n",
      "Epoch [300/300], Step [92/172], Loss: 16.7698\n",
      "Epoch [300/300], Step [93/172], Loss: 15.9857\n",
      "Epoch [300/300], Step [94/172], Loss: 22.6522\n",
      "Epoch [300/300], Step [95/172], Loss: 17.4168\n",
      "Epoch [300/300], Step [96/172], Loss: 18.9692\n",
      "Epoch [300/300], Step [97/172], Loss: 26.5709\n",
      "Epoch [300/300], Step [98/172], Loss: 16.7646\n",
      "Epoch [300/300], Step [99/172], Loss: 17.6218\n",
      "Epoch [300/300], Step [100/172], Loss: 17.0622\n",
      "Epoch [300/300], Step [101/172], Loss: 17.4658\n",
      "Epoch [300/300], Step [102/172], Loss: 16.5211\n",
      "Epoch [300/300], Step [103/172], Loss: 11.3559\n",
      "Epoch [300/300], Step [104/172], Loss: 18.2038\n",
      "Epoch [300/300], Step [105/172], Loss: 23.1727\n",
      "Epoch [300/300], Step [106/172], Loss: 15.5911\n",
      "Epoch [300/300], Step [107/172], Loss: 15.0726\n",
      "Epoch [300/300], Step [108/172], Loss: 15.2055\n",
      "Epoch [300/300], Step [109/172], Loss: 14.9002\n",
      "Epoch [300/300], Step [110/172], Loss: 16.7362\n",
      "Epoch [300/300], Step [111/172], Loss: 18.3546\n",
      "Epoch [300/300], Step [112/172], Loss: 13.3522\n",
      "Epoch [300/300], Step [113/172], Loss: 13.8748\n",
      "Epoch [300/300], Step [114/172], Loss: 13.9731\n",
      "Epoch [300/300], Step [115/172], Loss: 18.1090\n",
      "Epoch [300/300], Step [116/172], Loss: 13.7495\n",
      "Epoch [300/300], Step [117/172], Loss: 11.3688\n",
      "Epoch [300/300], Step [118/172], Loss: 11.6684\n",
      "Epoch [300/300], Step [119/172], Loss: 17.8568\n",
      "Epoch [300/300], Step [120/172], Loss: 10.7025\n",
      "Epoch [300/300], Step [121/172], Loss: 9.1839\n",
      "Epoch [300/300], Step [122/172], Loss: 13.6974\n",
      "Epoch [300/300], Step [123/172], Loss: 12.2105\n",
      "Epoch [300/300], Step [124/172], Loss: 7.0533\n",
      "Epoch [300/300], Step [125/172], Loss: 11.6621\n",
      "Epoch [300/300], Step [126/172], Loss: 12.5039\n",
      "Epoch [300/300], Step [127/172], Loss: 10.7171\n",
      "Epoch [300/300], Step [128/172], Loss: 9.7526\n",
      "Epoch [300/300], Step [129/172], Loss: 8.6255\n",
      "Epoch [300/300], Step [130/172], Loss: 11.6599\n",
      "Epoch [300/300], Step [131/172], Loss: 7.8854\n",
      "Epoch [300/300], Step [132/172], Loss: 9.9545\n",
      "Epoch [300/300], Step [133/172], Loss: 10.2752\n",
      "Epoch [300/300], Step [134/172], Loss: 10.0559\n",
      "Epoch [300/300], Step [135/172], Loss: 9.3161\n",
      "Epoch [300/300], Step [136/172], Loss: 9.3707\n",
      "Epoch [300/300], Step [137/172], Loss: 8.2081\n",
      "Epoch [300/300], Step [138/172], Loss: 7.4060\n",
      "Epoch [300/300], Step [139/172], Loss: 11.0028\n",
      "Epoch [300/300], Step [140/172], Loss: 11.2887\n",
      "Epoch [300/300], Step [141/172], Loss: 9.1311\n",
      "Epoch [300/300], Step [142/172], Loss: 13.7595\n",
      "Epoch [300/300], Step [143/172], Loss: 10.9949\n",
      "Epoch [300/300], Step [144/172], Loss: 9.1051\n",
      "Epoch [300/300], Step [145/172], Loss: 10.3866\n",
      "Epoch [300/300], Step [146/172], Loss: 10.5383\n",
      "Epoch [300/300], Step [147/172], Loss: 5.8637\n",
      "Epoch [300/300], Step [148/172], Loss: 6.8347\n",
      "Epoch [300/300], Step [149/172], Loss: 6.8103\n",
      "Epoch [300/300], Step [150/172], Loss: 6.0406\n",
      "Epoch [300/300], Step [151/172], Loss: 5.7181\n",
      "Epoch [300/300], Step [152/172], Loss: 8.8647\n",
      "Epoch [300/300], Step [153/172], Loss: 6.0362\n",
      "Epoch [300/300], Step [154/172], Loss: 6.6022\n",
      "Epoch [300/300], Step [155/172], Loss: 7.1884\n",
      "Epoch [300/300], Step [156/172], Loss: 14.0994\n",
      "Epoch [300/300], Step [157/172], Loss: 8.2042\n",
      "Epoch [300/300], Step [158/172], Loss: 7.0373\n",
      "Epoch [300/300], Step [159/172], Loss: 9.1732\n",
      "Epoch [300/300], Step [160/172], Loss: 8.6562\n",
      "Epoch [300/300], Step [161/172], Loss: 7.6892\n",
      "Epoch [300/300], Step [162/172], Loss: 4.8690\n",
      "Epoch [300/300], Step [163/172], Loss: 5.9257\n",
      "Epoch [300/300], Step [164/172], Loss: 9.2327\n",
      "Epoch [300/300], Step [165/172], Loss: 7.2559\n",
      "Epoch [300/300], Step [166/172], Loss: 6.2857\n",
      "Epoch [300/300], Step [167/172], Loss: 10.3929\n",
      "Epoch [300/300], Step [168/172], Loss: 6.2936\n",
      "Epoch [300/300], Step [169/172], Loss: 6.7967\n",
      "Epoch [300/300], Step [170/172], Loss: 5.5785\n",
      "Epoch [300/300], Step [171/172], Loss: 8.3496\n",
      "Epoch [300/300], Step [172/172], Loss: 5.8896\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "training_loss = train(NUM_EPOCHS, train_dataloader, model, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "140c00f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f70c2f15c90>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(training_loss, label='train_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23edb2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    pred_prob, pred_label = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            pred_prob.append(pred)\n",
    "            pred_label.append(pred.argmax(1))\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return pred_prob,pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d939b199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 63.0%, Avg loss: 102.466583 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_prob, pred_label = test(test_dataloader,model,loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56ad0926",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
       "         1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1]),\n",
       " tensor([0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0]),\n",
       " tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " tensor([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 2, 2, 2, 2,\n",
       "         2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 2, 2,\n",
       "         0, 2, 2, 2, 2, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0]),\n",
       " tensor([1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "         1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 2, 2, 2, 2]),\n",
       " tensor([2, 2, 2, 2, 2, 2, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "         1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0]),\n",
       " tensor([0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1,\n",
       "         0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0,\n",
       "         0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 0, 2, 2, 2, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1]),\n",
       " tensor([1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "         1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0]),\n",
       " tensor([1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "         0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "         1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 2, 2, 2,\n",
       "         0, 1, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0]),\n",
       " tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "         1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " tensor([0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0]),\n",
       " tensor([0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "         0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1]),\n",
       " tensor([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]),\n",
       " tensor([0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "         0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1,\n",
       "         0, 1, 1, 2, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1]),\n",
       " tensor([1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "         1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]),\n",
       " tensor([0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0]),\n",
       " tensor([0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]),\n",
       " tensor([1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "         0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0]),\n",
       " tensor([0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       " tensor([0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1]),\n",
       " tensor([1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "         1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0]),\n",
       " tensor([1, 0, 1, 0, 0, 0, 2, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "         1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0]),\n",
       " tensor([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "         1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0]),\n",
       " tensor([1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1]),\n",
       " tensor([0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "         1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0]),\n",
       " tensor([0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "         0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0]),\n",
       " tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2,\n",
       "         2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]),\n",
       " tensor([0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 2, 2, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 0, 2, 2, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 2, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "         2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0]),\n",
       " tensor([0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor([0, 0])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ed2afff1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  1.7631,  -1.5696,  -3.9310],\n",
       "         [  1.7694,  -1.5822,  -3.9668],\n",
       "         [  1.7652,  -1.5639,  -3.9675],\n",
       "         [  1.7560,  -1.6016,  -3.9419],\n",
       "         [  2.3852,  -8.2395,   1.1595],\n",
       "         [  2.2667,  -8.1510,   1.2629],\n",
       "         [  2.5017,  -8.3407,   1.0773],\n",
       "         [  2.3849,  -8.1588,   1.0112],\n",
       "         [  2.2528,  -8.0697,   1.0174],\n",
       "         [  2.6101,  -8.4172,   0.9553],\n",
       "         [  2.5711,  -8.5538,   1.5427],\n",
       "         [  3.4842,  -7.4991,  -4.1867],\n",
       "         [  2.5413,  -8.3032,   0.4310],\n",
       "         [  2.5029,  -8.2677,   0.8133],\n",
       "         [  2.3773,  -8.2131,   1.0746],\n",
       "         [  2.2815,  -8.2311,   1.3641],\n",
       "         [  2.4182,  -8.2106,   0.9095],\n",
       "         [  2.3193,  -8.1692,   1.1631],\n",
       "         [  2.3690,  -8.1802,   1.0567],\n",
       "         [  2.3819,  -8.2126,   1.0365],\n",
       "         [  2.4499,  -8.2981,   1.1276],\n",
       "         [  2.5312,  -8.3538,   1.0893],\n",
       "         [  2.4502,  -8.2836,   1.0493],\n",
       "         [  2.5750,  -8.4353,   1.0958],\n",
       "         [  2.3476,  -8.1274,   0.9079],\n",
       "         [  2.5029,  -8.3228,   0.9513],\n",
       "         [  2.3999,  -8.2777,   1.2259],\n",
       "         [  2.3280,  -8.2550,   1.2789],\n",
       "         [  2.4772,  -8.2885,   0.9879],\n",
       "         [  2.4307,  -8.2662,   1.0803],\n",
       "         [  2.3699,  -8.1120,   0.8049],\n",
       "         [  2.4848,  -8.2880,   0.8786],\n",
       "         [  2.3646,  -8.2398,   1.1067],\n",
       "         [  2.5377,  -8.3815,   1.0493],\n",
       "         [  2.4168,  -8.2426,   0.9823],\n",
       "         [  2.3926,  -8.2150,   1.0402],\n",
       "         [  2.5818,  -8.4205,   0.9687],\n",
       "         [  2.7424,  -8.5801,   1.0354],\n",
       "         [  2.4672,  -8.3885,   1.3644],\n",
       "         [  2.3422,  -8.2328,   1.2695],\n",
       "         [  2.3437,  -8.2029,   1.1689],\n",
       "         [  3.1287,  -7.1657,  -3.9056],\n",
       "         [  2.4901,  -8.3621,   1.1692],\n",
       "         [  2.6113,  -8.6300,   1.4350],\n",
       "         [  2.4744,  -8.4577,   1.3506],\n",
       "         [  2.2913,  -8.1389,   1.1043],\n",
       "         [  2.3342,  -8.1779,   1.0613],\n",
       "         [  2.9687,  -8.5552,  -0.2608],\n",
       "         [  2.3422,  -8.2328,   1.2695],\n",
       "         [  2.4490,  -8.4914,   1.2317],\n",
       "         [  2.4250,  -8.2419,   0.9634],\n",
       "         [  2.5618,  -8.4369,   1.1413],\n",
       "         [  2.8937,  -8.7666,   1.1176],\n",
       "         [  2.5070,  -8.3474,   1.0967],\n",
       "         [  2.3801,  -8.1299,   0.8676],\n",
       "         [  2.3498,  -8.1635,   1.0074],\n",
       "         [  2.4430,  -8.2333,   0.9342],\n",
       "         [  2.4112,  -8.2941,   1.0976],\n",
       "         [  2.9716,  -3.9827, -15.3151],\n",
       "         [  2.4795,  -8.2938,   1.0606],\n",
       "         [  2.3252,  -8.1297,   1.0058],\n",
       "         [  2.3280,  -8.2550,   1.2789],\n",
       "         [  2.4559,  -8.2903,   0.9838],\n",
       "         [  2.3937,  -8.2247,   1.0798]]),\n",
       " tensor([[  2.2940,  -8.1759,   1.1664],\n",
       "         [  2.3445,  -8.1640,   1.0375],\n",
       "         [  2.6126,  -8.4598,   1.0453],\n",
       "         [  2.5794,  -8.4714,   1.1447],\n",
       "         [  2.3656,  -8.2340,   1.2085],\n",
       "         [  2.4763,  -8.2301,   0.7676],\n",
       "         [  2.4675,  -8.3348,   1.2411],\n",
       "         [  2.3999,  -8.2050,   1.0042],\n",
       "         [ -2.1822,  -5.8673,  -7.4224],\n",
       "         [  2.3555,  -8.3112,   1.4016],\n",
       "         [  2.4172,  -8.2300,   1.0395],\n",
       "         [  2.4953,  -8.2331,   0.8590],\n",
       "         [ -0.0560,  -2.3918, -13.9255],\n",
       "         [  3.5349, -10.0764,   1.2673],\n",
       "         [  4.5020, -10.9311,   0.6324],\n",
       "         [  0.6575,  -0.0695,  -5.1149],\n",
       "         [  1.4014,  -0.4235,  -7.4012],\n",
       "         [  1.5109,  -2.0247,  -2.6088],\n",
       "         [  1.5190,  -2.0440,  -2.6119],\n",
       "         [  1.5180,  -2.0192,  -2.6444],\n",
       "         [  1.5221,  -2.0554,  -2.6003],\n",
       "         [  1.5162,  -2.0111,  -2.6639],\n",
       "         [  1.5145,  -2.0254,  -2.6259],\n",
       "         [  1.5188,  -2.0561,  -2.5899],\n",
       "         [  1.5141,  -1.9757,  -2.7123],\n",
       "         [  1.5199,  -2.0391,  -2.6237],\n",
       "         [  1.5241,  -2.0536,  -2.6009],\n",
       "         [  1.5151,  -2.0444,  -2.6041],\n",
       "         [  1.5199,  -2.0403,  -2.6201],\n",
       "         [  1.5181,  -2.0658,  -2.5742],\n",
       "         [  1.5192,  -2.0689,  -2.4961],\n",
       "         [  1.5050,  -1.9486,  -2.7454],\n",
       "         [  1.5204,  -2.0483,  -2.6087],\n",
       "         [  1.5237,  -2.0591,  -2.5875],\n",
       "         [  1.5149,  -1.9940,  -2.6819],\n",
       "         [  1.5250,  -2.0656,  -2.5813],\n",
       "         [  1.5253,  -2.0404,  -2.6349],\n",
       "         [  1.5228,  -2.0698,  -2.5711],\n",
       "         [  1.5144,  -2.0211,  -2.6264],\n",
       "         [  1.5047,  -1.9575,  -2.7348],\n",
       "         [  1.5153,  -2.0598,  -2.5669],\n",
       "         [  1.5107,  -2.0316,  -2.6195],\n",
       "         [  1.5203,  -2.0652,  -2.5618],\n",
       "         [  1.5151,  -2.0269,  -2.6228],\n",
       "         [  1.5136,  -2.0427,  -2.5920],\n",
       "         [  1.5242,  -2.0675,  -2.5505],\n",
       "         [  1.5224,  -2.0348,  -2.6341],\n",
       "         [  1.5165,  -2.0353,  -2.6192],\n",
       "         [  1.5158,  -2.0530,  -2.5797],\n",
       "         [  1.5158,  -2.0428,  -2.5915],\n",
       "         [  1.5087,  -1.9698,  -2.7145],\n",
       "         [  1.5145,  -1.9739,  -2.7116],\n",
       "         [  1.5137,  -2.0609,  -2.5652],\n",
       "         [ -4.5076,   0.6882,  -8.3816],\n",
       "         [  1.5202,  -2.0497,  -2.6104],\n",
       "         [  1.5195,  -1.9944,  -2.6889],\n",
       "         [  1.5223,  -2.0873,  -2.5267],\n",
       "         [  1.5157,  -1.9978,  -2.6903],\n",
       "         [  1.5138,  -2.0250,  -2.6319],\n",
       "         [  1.5007,  -2.0121,  -2.6367],\n",
       "         [  1.5231,  -2.1160,  -2.4917],\n",
       "         [  1.5183,  -2.0727,  -2.5595],\n",
       "         [  1.5253,  -2.0404,  -2.6349],\n",
       "         [  1.5237,  -2.0591,  -2.5875]]),\n",
       " tensor([[ 1.5190e+00, -2.0257e+00, -2.6242e+00],\n",
       "         [ 1.5191e+00, -2.0607e+00, -2.5756e+00],\n",
       "         [ 1.5217e+00, -2.0241e+00, -2.6584e+00],\n",
       "         [ 1.5276e+00, -2.0735e+00, -2.5664e+00],\n",
       "         [ 1.5079e+00, -2.0203e+00, -2.6206e+00],\n",
       "         [ 1.5221e+00, -2.0554e+00, -2.6003e+00],\n",
       "         [ 1.5098e+00, -2.0257e+00, -2.6162e+00],\n",
       "         [ 1.5240e+00, -2.0584e+00, -2.5815e+00],\n",
       "         [ 4.5600e+00, -9.0309e-01, -1.3026e+01],\n",
       "         [-1.1378e+00,  4.9760e+00, -1.7325e+01],\n",
       "         [ 4.4502e+00, -7.9931e-01, -1.2879e+01],\n",
       "         [ 4.5464e+00, -8.8094e-01, -1.3059e+01],\n",
       "         [ 4.4461e+00, -4.8637e-01, -1.3930e+01],\n",
       "         [ 2.9482e+00,  1.3597e+00, -1.4688e+01],\n",
       "         [ 4.5549e+00, -9.0946e-01, -1.2989e+01],\n",
       "         [ 4.5123e+00, -8.8080e-01, -1.2968e+01],\n",
       "         [ 4.4829e+00, -6.5301e-01, -1.3751e+01],\n",
       "         [ 4.4967e+00, -6.4680e-01, -1.3868e+01],\n",
       "         [ 4.5436e+00, -8.8292e-01, -1.3032e+01],\n",
       "         [ 4.5524e+00, -8.8839e-01, -1.3050e+01],\n",
       "         [ 2.3975e+00,  1.9350e+00, -1.4798e+01],\n",
       "         [ 1.5557e-02,  4.0308e+00, -1.5051e+01],\n",
       "         [ 4.5504e+00, -9.0037e-01, -1.3008e+01],\n",
       "         [ 1.6442e+00,  1.0795e+00, -8.7938e+00],\n",
       "         [ 2.6545e+00, -8.0232e-01, -8.3914e+00],\n",
       "         [ 1.0501e+00, -8.3699e-01, -3.2155e+00],\n",
       "         [ 1.0559e+00, -8.1700e-01, -3.2708e+00],\n",
       "         [ 1.0505e+00, -8.1737e-01, -3.2489e+00],\n",
       "         [ 1.0458e+00, -7.6914e-01, -3.3050e+00],\n",
       "         [ 1.0388e+00, -7.2717e-01, -3.3424e+00],\n",
       "         [ 1.0463e+00, -7.7121e-01, -3.2848e+00],\n",
       "         [ 1.0537e+00, -8.4279e-01, -3.2360e+00],\n",
       "         [ 9.3051e-02,  2.0635e+00, -1.0434e+01],\n",
       "         [-1.0910e+00,  1.6922e+00, -2.0335e+01],\n",
       "         [-4.5848e+00,  1.7687e+00, -2.3613e+01],\n",
       "         [ 4.4717e-01,  4.4953e-01, -6.0552e+00],\n",
       "         [ 5.0007e-01,  4.5819e-01, -6.1084e+00],\n",
       "         [-4.1687e+00, -3.5647e-01, -1.7163e+01],\n",
       "         [ 4.0656e-01,  4.2894e-01, -5.9741e+00],\n",
       "         [ 5.3183e-01,  5.2914e-01, -6.2690e+00],\n",
       "         [ 5.1916e-01,  3.2013e-01, -5.8903e+00],\n",
       "         [ 4.2015e-01,  6.0359e-01, -6.2890e+00],\n",
       "         [ 4.4417e-01,  6.4673e-01, -6.4188e+00],\n",
       "         [ 5.3464e-01,  4.9265e-01, -6.2119e+00],\n",
       "         [-3.3122e+00, -1.6293e+00, -1.8185e+01],\n",
       "         [-9.4649e-01,  1.6037e+00, -1.9098e+01],\n",
       "         [ 1.3545e+00,  5.1628e-01, -1.4100e+01],\n",
       "         [-2.7449e+00,  1.8897e+00, -2.2634e+01],\n",
       "         [-4.4058e+00, -6.0037e-01, -1.5682e+01],\n",
       "         [ 4.5780e-01,  5.2032e-01, -6.1856e+00],\n",
       "         [-4.8187e-01,  1.3939e+00, -1.5744e+01],\n",
       "         [ 4.3314e-01,  6.6998e-01, -6.4514e+00],\n",
       "         [ 4.4560e-01,  4.7225e-01, -6.0593e+00],\n",
       "         [-4.2374e-01,  1.3876e+00, -1.6605e+01],\n",
       "         [ 4.2904e-01,  5.0792e-01, -6.1370e+00],\n",
       "         [ 1.7471e+00,  2.2252e-02, -7.5932e+00],\n",
       "         [ 2.4678e+00, -8.4162e-01, -1.0421e+01],\n",
       "         [ 1.2402e+00, -3.0896e+00, -1.6063e+00],\n",
       "         [ 1.2017e+00, -3.1282e+00, -1.4822e+00],\n",
       "         [ 1.2391e+00, -3.0586e+00, -1.6706e+00],\n",
       "         [ 1.2376e+00, -3.0557e+00, -1.6729e+00],\n",
       "         [ 1.2145e+00, -3.0136e+00, -1.7030e+00],\n",
       "         [ 1.2459e+00, -3.1219e+00, -1.5686e+00],\n",
       "         [ 1.2208e+00, -2.9960e+00, -1.7416e+00]]),\n",
       " tensor([[ 1.1579e+00, -2.9629e+00, -1.7083e+00],\n",
       "         [-1.0443e+00,  7.1287e-01, -1.1137e+01],\n",
       "         [ 1.1126e+00, -1.4232e+00, -4.6163e+00],\n",
       "         [ 9.9202e-01, -4.2567e+00,  1.3146e+00],\n",
       "         [ 2.0237e+00, -3.2114e-02, -8.0864e+00],\n",
       "         [ 2.0557e+00, -9.7534e-02, -7.9408e+00],\n",
       "         [ 2.0560e+00,  1.8459e-03, -8.3346e+00],\n",
       "         [-5.9556e+00,  1.8405e+00, -1.2029e+01],\n",
       "         [ 2.0640e+00, -5.8201e-02, -8.1390e+00],\n",
       "         [ 2.1479e+00, -2.3124e+00, -4.1190e+00],\n",
       "         [-3.8034e-01,  2.0087e+00, -1.0548e+01],\n",
       "         [-5.1105e+00, -1.4040e+00, -8.7256e+00],\n",
       "         [-4.6301e+00, -6.5297e-01, -5.2864e+00],\n",
       "         [-5.6574e+00, -1.5401e-01, -1.0449e+01],\n",
       "         [-4.6807e+00, -6.6915e-01, -5.4248e+00],\n",
       "         [ 1.5866e+00, -1.5413e+00, -5.0796e+00],\n",
       "         [ 9.7543e-01, -1.3918e+00, -3.8687e+00],\n",
       "         [-4.9987e+00, -9.3572e-01, -9.0222e+00],\n",
       "         [-5.5371e-01,  2.0801e+00, -1.2511e+01],\n",
       "         [ 1.5568e+00, -1.1345e+00, -5.8950e+00],\n",
       "         [-4.6234e+00, -5.8035e-01, -5.2363e+00],\n",
       "         [ 1.5741e+00, -1.4709e+00, -5.2171e+00],\n",
       "         [ 1.5680e+00, -1.3297e+00, -5.5145e+00],\n",
       "         [ 1.6448e+00, -3.5336e+00, -1.5460e+00],\n",
       "         [ 4.2797e+00, -7.0399e-01, -1.0802e+01],\n",
       "         [ 3.2692e+00, -8.5760e-02, -8.7337e+00],\n",
       "         [ 2.5846e+00,  1.9516e-01, -7.6569e+00],\n",
       "         [ 7.4716e-01,  1.1697e+00, -1.0352e+01],\n",
       "         [ 2.1432e+00, -1.3013e+00, -6.5460e+00],\n",
       "         [ 2.0720e+00, -1.3678e+00, -6.1800e+00],\n",
       "         [ 1.7870e+00, -4.7934e-01, -7.4590e+00],\n",
       "         [ 2.1246e+00, -1.1194e+00, -6.7744e+00],\n",
       "         [ 2.0519e+00, -2.5920e-01, -8.8758e+00],\n",
       "         [-1.2683e+00,  3.8835e+00, -1.7398e+01],\n",
       "         [ 1.8426e+00,  3.3670e-01, -1.0751e+01],\n",
       "         [-5.3160e+00,  2.2821e+00, -1.7972e+01],\n",
       "         [-6.2673e-02,  2.3615e+00, -1.2877e+01],\n",
       "         [ 2.1022e+00, -1.0892e+00, -6.8272e+00],\n",
       "         [ 5.7203e-01,  1.4423e+00, -1.0606e+01],\n",
       "         [ 2.0505e+00, -8.9315e-02, -9.6226e+00],\n",
       "         [ 4.8431e-02,  2.2803e+00, -1.2726e+01],\n",
       "         [ 2.0758e+00, -1.3532e+00, -6.2187e+00],\n",
       "         [ 7.5821e-01,  1.4608e+00, -1.1258e+01],\n",
       "         [ 1.8101e+00,  4.1303e-01, -1.1026e+01],\n",
       "         [ 9.0718e-01,  1.0499e+00, -1.0430e+01],\n",
       "         [ 2.0992e+00, -1.2688e+00, -6.4098e+00],\n",
       "         [ 9.9776e-01,  1.3578e+00, -1.1875e+01],\n",
       "         [ 2.1365e+00, -1.3470e+00, -6.4611e+00],\n",
       "         [ 2.0777e+00, -1.6615e-01, -9.2885e+00],\n",
       "         [ 2.1349e+00, -1.2121e+00, -6.6314e+00],\n",
       "         [ 2.1391e+00, -1.3075e+00, -6.5360e+00],\n",
       "         [ 2.1040e+00, -1.2903e+00, -6.4127e+00],\n",
       "         [ 6.2608e-01,  1.4728e+00, -1.0784e+01],\n",
       "         [ 2.1333e+00, -1.2744e+00, -6.5486e+00],\n",
       "         [ 2.0490e+00, -1.4818e-01, -9.2196e+00],\n",
       "         [ 8.4196e-01,  1.1304e+00, -1.0497e+01],\n",
       "         [ 1.7822e+00,  4.6021e-01, -1.1119e+01],\n",
       "         [ 1.9098e+00,  2.2558e-01, -1.0585e+01],\n",
       "         [-1.1752e+00,  4.0995e+00, -1.8607e+01],\n",
       "         [ 7.2066e-01,  1.5104e+00, -1.1189e+01],\n",
       "         [-5.0671e+00,  1.9234e+00, -1.6987e+01],\n",
       "         [ 2.0040e+00,  1.6015e-02, -9.8724e+00],\n",
       "         [ 2.0693e+00, -2.3568e-01, -9.0425e+00],\n",
       "         [ 7.2066e-01,  1.5104e+00, -1.1189e+01]]),\n",
       " tensor([[ 2.0884e+00, -1.2953e+00, -6.3350e+00],\n",
       "         [ 2.0903e+00, -1.2948e+00, -6.3436e+00],\n",
       "         [ 2.0681e+00, -1.5396e+00, -5.9578e+00],\n",
       "         [ 2.0684e+00, -1.5318e+00, -5.9726e+00],\n",
       "         [ 2.0672e+00, -1.5284e+00, -5.9661e+00],\n",
       "         [ 2.1103e+00, -1.2880e+00, -6.4449e+00],\n",
       "         [ 2.0668e+00, -1.5216e+00, -5.9732e+00],\n",
       "         [ 7.5870e-02,  2.1238e+00, -1.2057e+01],\n",
       "         [ 2.0102e+00,  4.5873e-02, -1.0078e+01],\n",
       "         [ 2.0364e+00, -2.4692e-02, -9.8852e+00],\n",
       "         [ 2.0305e+00, -2.2648e-01, -8.8331e+00],\n",
       "         [ 2.1407e+00, -1.2954e+00, -6.5183e+00],\n",
       "         [ 2.0729e+00, -1.9547e-01, -9.5251e+00],\n",
       "         [ 2.0670e+00, -1.5235e+00, -5.9678e+00],\n",
       "         [ 1.8459e+00, -4.7539e-01, -7.7347e+00],\n",
       "         [ 2.0601e+00, -6.1593e-02, -9.7024e+00],\n",
       "         [-4.8499e+00,  1.6839e+00, -1.6821e+01],\n",
       "         [ 1.4914e+00,  7.1843e-01, -1.0872e+01],\n",
       "         [ 1.9193e+00, -2.9337e-01, -8.3480e+00],\n",
       "         [-3.8037e+00,  5.3420e+00, -2.7343e+01],\n",
       "         [ 1.8928e+00, -3.5715e-01, -8.1406e+00],\n",
       "         [ 2.1061e+00, -1.2965e+00, -6.4219e+00],\n",
       "         [ 2.1319e+00, -1.2911e+00, -6.5061e+00],\n",
       "         [ 1.8604e+00, -4.7320e-01, -7.7889e+00],\n",
       "         [ 1.8738e+00, -4.6789e-01, -7.8274e+00],\n",
       "         [ 1.8451e+00, -4.6918e-01, -7.7462e+00],\n",
       "         [ 1.8477e+00, -4.9066e-01, -7.6825e+00],\n",
       "         [-4.7026e+00,  4.8705e-01, -1.1579e+01],\n",
       "         [ 2.1173e+00, -1.1324e+00, -6.7633e+00],\n",
       "         [ 6.8853e-01,  1.5432e+00, -1.1224e+01],\n",
       "         [ 3.8999e-02,  2.2258e+00, -1.2402e+01],\n",
       "         [ 1.6381e+00, -2.8837e-01, -7.6842e+00],\n",
       "         [ 2.0615e+00, -1.4929e+00, -5.9715e+00],\n",
       "         [ 1.6540e+00, -2.4240e-01, -7.7476e+00],\n",
       "         [ 1.6751e+00, -2.5347e-01, -7.7287e+00],\n",
       "         [ 1.6751e+00, -2.5347e-01, -7.7287e+00],\n",
       "         [ 5.0734e-01,  1.5931e+00, -1.0840e+01],\n",
       "         [ 1.7062e+00, -4.4215e-01, -7.3604e+00],\n",
       "         [ 2.1211e+00, -1.3472e+00, -6.4283e+00],\n",
       "         [ 2.0417e+00, -2.4853e-01, -8.9261e+00],\n",
       "         [ 2.0417e+00, -2.4853e-01, -8.9261e+00],\n",
       "         [ 2.0695e+00, -1.5299e+00, -5.9616e+00],\n",
       "         [ 2.0417e+00, -2.4853e-01, -8.9261e+00],\n",
       "         [ 2.0952e+00, -1.0272e+00, -7.0068e+00],\n",
       "         [ 2.1097e+00, -1.0773e+00, -6.8948e+00],\n",
       "         [ 2.1091e+00, -1.0911e+00, -6.8543e+00],\n",
       "         [ 8.3793e-01,  1.3588e+00, -1.1152e+01],\n",
       "         [ 2.0793e+00, -1.3482e+00, -6.2394e+00],\n",
       "         [-2.2123e+00,  5.3398e+00, -2.6081e+01],\n",
       "         [ 2.1349e+00, -1.2121e+00, -6.6314e+00],\n",
       "         [ 2.1397e+00, -1.3183e+00, -6.4834e+00],\n",
       "         [-3.6995e+00,  5.0244e+00, -2.5103e+01],\n",
       "         [ 2.0660e+00, -1.3971e+00, -6.1154e+00],\n",
       "         [ 7.2561e-01,  1.5639e+00, -1.1456e+01],\n",
       "         [ 2.1287e+00, -1.2788e+00, -6.5177e+00],\n",
       "         [ 2.0677e+00, -1.5351e+00, -5.9564e+00],\n",
       "         [ 2.1347e+00, -1.2462e+00, -6.5954e+00],\n",
       "         [ 2.0654e+00, -1.5155e+00, -5.9668e+00],\n",
       "         [ 1.9938e+00,  8.1101e-02, -1.0198e+01],\n",
       "         [ 1.9694e+00,  8.5927e-02, -1.0143e+01],\n",
       "         [ 2.0709e+00, -1.5280e+00, -5.9839e+00],\n",
       "         [ 2.0637e+00, -6.0620e-02, -9.6821e+00],\n",
       "         [-3.4891e+00,  4.8974e+00, -2.4022e+01],\n",
       "         [ 2.1238e+00, -1.3328e+00, -6.4572e+00]]),\n",
       " tensor([[ 2.1188e+00, -1.2925e+00, -6.4818e+00],\n",
       "         [ 2.1124e+00, -1.2976e+00, -6.4435e+00],\n",
       "         [ 2.0635e+00, -1.5426e+00, -5.9298e+00],\n",
       "         [ 2.1051e+00, -1.2918e+00, -6.4203e+00],\n",
       "         [ 2.1013e+00, -1.3054e+00, -6.3853e+00],\n",
       "         [ 2.1020e+00, -1.2889e+00, -6.4027e+00],\n",
       "         [ 2.0674e+00, -1.5461e+00, -5.9441e+00],\n",
       "         [-3.4514e+00,  4.5142e+00, -2.3176e+01],\n",
       "         [ 2.0992e+00, -1.2688e+00, -6.4098e+00],\n",
       "         [ 2.0992e+00, -1.2688e+00, -6.4098e+00],\n",
       "         [ 2.0508e+00, -1.3933e+00, -6.0425e+00],\n",
       "         [ 2.0884e+00, -1.2953e+00, -6.3350e+00],\n",
       "         [ 2.1246e+00, -1.1194e+00, -6.7744e+00],\n",
       "         [ 1.1874e+00,  1.0452e+00, -1.1142e+01],\n",
       "         [ 2.0713e+00, -1.3801e+00, -6.1620e+00],\n",
       "         [ 2.0669e+00, -1.4038e+00, -6.1118e+00],\n",
       "         [ 2.0670e+00, -1.5228e+00, -5.9699e+00],\n",
       "         [ 1.8961e+00,  2.3524e-01, -1.0556e+01],\n",
       "         [ 1.6795e+00, -2.7876e-01, -7.7006e+00],\n",
       "         [ 2.0576e+00, -1.5194e+00, -5.9349e+00],\n",
       "         [ 2.0638e+00, -1.5355e+00, -5.9427e+00],\n",
       "         [ 2.1373e+00, -1.3226e+00, -6.5014e+00],\n",
       "         [-1.3697e+00,  4.4813e+00, -2.0396e+01],\n",
       "         [-1.4757e+00,  4.6187e+00, -2.0945e+01],\n",
       "         [-1.3697e+00,  4.4813e+00, -2.0396e+01],\n",
       "         [ 3.7806e-01,  1.6697e+00, -1.0716e+01],\n",
       "         [-1.7650e-03,  2.2489e+00, -1.2445e+01],\n",
       "         [ 7.2066e-01,  1.5104e+00, -1.1189e+01],\n",
       "         [ 2.0637e+00, -1.5435e+00, -5.9252e+00],\n",
       "         [ 2.0640e+00, -1.4630e+00, -6.0202e+00],\n",
       "         [ 2.0605e+00, -1.5523e+00, -5.8955e+00],\n",
       "         [ 2.0678e+00, -1.5545e+00, -5.9335e+00],\n",
       "         [ 2.0697e+00, -1.5141e+00, -5.9961e+00],\n",
       "         [ 8.9353e-01,  1.2880e+00, -1.1082e+01],\n",
       "         [ 2.0659e+00, -1.5370e+00, -5.9520e+00],\n",
       "         [ 1.8492e+00,  3.5966e-01, -1.0889e+01],\n",
       "         [ 2.0657e+00, -1.5446e+00, -5.9358e+00],\n",
       "         [ 2.0685e+00, -1.5335e+00, -5.9675e+00],\n",
       "         [ 1.1066e+00,  1.2227e+00, -1.1773e+01],\n",
       "         [ 2.0665e+00, -1.5348e+00, -5.9502e+00],\n",
       "         [ 2.0673e+00, -1.5383e+00, -5.9614e+00],\n",
       "         [ 2.1108e+00, -1.2982e+00, -6.4398e+00],\n",
       "         [ 1.2240e-01,  1.9677e+00, -1.1371e+01],\n",
       "         [ 2.0675e+00, -1.5392e+00, -5.9574e+00],\n",
       "         [ 2.0661e+00, -1.5296e+00, -5.9570e+00],\n",
       "         [-1.0390e+00,  3.4875e+00, -1.6248e+01],\n",
       "         [ 3.9935e-01,  1.8109e+00, -1.1337e+01],\n",
       "         [ 2.1341e+00, -1.3084e+00, -6.5138e+00],\n",
       "         [-4.6706e+00,  1.5434e+00, -1.6251e+01],\n",
       "         [ 6.2884e-01,  1.3559e+00, -1.0494e+01],\n",
       "         [-4.3743e+00,  1.0948e-01, -1.3779e+01],\n",
       "         [ 2.0668e+00, -1.5527e+00, -5.9319e+00],\n",
       "         [ 1.9264e+00, -1.5359e-01, -8.7101e+00],\n",
       "         [-4.6706e+00,  1.5434e+00, -1.6251e+01],\n",
       "         [ 2.0682e+00, -1.5235e+00, -5.9722e+00],\n",
       "         [-6.2674e-02,  2.3615e+00, -1.2877e+01],\n",
       "         [-1.1245e-01,  2.3892e+00, -1.2914e+01],\n",
       "         [ 2.1333e+00, -1.2789e+00, -6.5452e+00],\n",
       "         [ 1.2207e-01,  2.1127e+00, -1.2119e+01],\n",
       "         [ 1.5222e-01,  2.0264e+00, -1.1733e+01],\n",
       "         [ 2.0681e+00, -1.5102e+00, -5.9919e+00],\n",
       "         [ 2.0671e+00, -1.5415e+00, -5.9467e+00],\n",
       "         [ 2.0673e+00, -1.5363e+00, -5.9604e+00],\n",
       "         [ 7.7924e-01,  1.2091e+00, -1.0610e+01]]),\n",
       " tensor([[  2.0695,  -1.3775,  -6.1576],\n",
       "         [ -3.3914,   0.5228, -14.7863],\n",
       "         [  2.0671,  -1.5177,  -5.9765],\n",
       "         [  2.0636,  -0.0645,  -9.6859],\n",
       "         [  2.0579,  -1.5821,  -5.8497],\n",
       "         [  2.1108,  -1.2982,  -6.4398],\n",
       "         [  0.3994,   1.8109, -11.3375],\n",
       "         [ -5.4224,   1.5885, -17.7444],\n",
       "         [  0.7507,   1.3287, -11.0345],\n",
       "         [  2.0665,  -1.5212,  -5.9758],\n",
       "         [  2.0758,  -1.3532,  -6.2187],\n",
       "         [ -5.3817,   2.2886, -18.3365],\n",
       "         [  2.0709,  -1.5280,  -5.9839],\n",
       "         [  2.0620,  -1.5025,  -5.9660],\n",
       "         [  2.1439,  -1.3284,  -6.5059],\n",
       "         [  2.1352,  -1.2404,  -6.6073],\n",
       "         [  2.0656,  -1.5219,  -5.9654],\n",
       "         [  2.0642,  -1.5327,  -5.9453],\n",
       "         [  2.0644,  -1.5363,  -5.9437],\n",
       "         [  1.4914,   0.7184, -10.8717],\n",
       "         [  1.6447,   0.5725, -10.8792],\n",
       "         [  2.0637,  -1.5412,  -5.9367],\n",
       "         [  2.0068,   0.0431, -10.0676],\n",
       "         [  2.1280,  -1.2840,  -6.5226],\n",
       "         [  1.7739,   0.4680, -11.1191],\n",
       "         [  1.8552,   0.3444, -10.8438],\n",
       "         [  1.4914,   0.7184, -10.8717],\n",
       "         [  2.0665,  -1.5372,  -5.9456],\n",
       "         [  0.3994,   1.8109, -11.3375],\n",
       "         [  2.0644,  -1.5262,  -5.9586],\n",
       "         [  1.8255,  -0.4497,  -7.7423],\n",
       "         [  1.7591,  -0.4185,  -7.5071],\n",
       "         [  1.9582,   0.0749, -10.0459],\n",
       "         [  0.7712,   1.2293, -10.6374],\n",
       "         [  0.8008,   1.3898, -11.1252],\n",
       "         [  1.8492,   0.3597, -10.8889],\n",
       "         [  2.1110,  -1.2996,  -6.4424],\n",
       "         [  2.0644,  -1.4829,  -6.0022],\n",
       "         [  1.8815,   0.3003, -10.7772],\n",
       "         [ -2.0782,  -0.1664,  -3.8873],\n",
       "         [ -1.5243,   0.4438,  -5.0876],\n",
       "         [ -2.0954,   0.3509,  -5.8272],\n",
       "         [ -3.4332,  -1.6271,  -8.8850],\n",
       "         [ -1.4071,   0.4799,  -5.2283],\n",
       "         [  1.1669,  -2.8625,  -1.0510],\n",
       "         [  1.1583,  -2.8867,  -0.9590],\n",
       "         [  1.1619,  -2.8570,  -1.0510],\n",
       "         [  1.1562,  -2.8552,  -0.9808],\n",
       "         [  1.1589,  -2.8510,  -1.0373],\n",
       "         [  1.1532,  -2.8764,  -0.9793],\n",
       "         [ -3.9205,   2.0715, -16.0277],\n",
       "         [  1.1391,  -2.8294,  -1.0235],\n",
       "         [  1.1546,  -2.8527,  -1.0374],\n",
       "         [  0.4824,   0.0893,  -5.0392],\n",
       "         [ -4.0375,   0.6709,  -7.3985],\n",
       "         [  1.1317,  -2.8175,  -1.0557],\n",
       "         [ -0.9686,   2.0256,  -8.8617],\n",
       "         [ -4.0375,   0.6709,  -7.3985],\n",
       "         [  1.1466,  -2.7707,  -1.1406],\n",
       "         [ -4.0089,   1.5683, -10.7227],\n",
       "         [  1.1621,  -2.8830,  -0.9933],\n",
       "         [  1.1535,  -2.8603,  -1.0233],\n",
       "         [  0.8465,  -0.6198,  -4.3301],\n",
       "         [  1.1508,  -2.8544,  -1.0922]]),\n",
       " tensor([[ -4.4243,   2.2001, -13.9133],\n",
       "         [  1.1467,  -2.8262,  -1.0585],\n",
       "         [  1.1595,  -2.8618,  -1.0387],\n",
       "         [  1.1519,  -2.8614,  -1.0194],\n",
       "         [  1.1559,  -2.8688,  -1.0322],\n",
       "         [  1.1540,  -2.8862,  -0.9231],\n",
       "         [  1.1572,  -2.8454,  -1.0436],\n",
       "         [  1.1511,  -2.8719,  -1.0008],\n",
       "         [  1.1540,  -2.8862,  -0.9231],\n",
       "         [  1.1449,  -2.8131,  -1.0488],\n",
       "         [  1.1604,  -2.8828,  -0.9987],\n",
       "         [  0.2110,   0.5823,  -5.4459],\n",
       "         [  1.1546,  -2.8174,  -1.0320],\n",
       "         [  1.0327,  -0.3844,  -4.8869],\n",
       "         [  0.8242,  -0.4174,  -4.6695],\n",
       "         [  1.1500,  -2.8578,  -1.0073],\n",
       "         [  1.1411,  -2.8380,  -0.9988],\n",
       "         [ -5.2090,   6.8989, -17.3778],\n",
       "         [  2.1369,  -0.8566,  -5.2323],\n",
       "         [  0.5912,   1.2019,  -9.2858],\n",
       "         [  2.0254,  -0.4846,  -7.7851],\n",
       "         [  2.0203,  -0.4904,  -7.7639],\n",
       "         [  1.2327,  -0.5876,  -4.7845],\n",
       "         [  1.1552,  -0.1948,  -5.4713],\n",
       "         [  1.5002,   0.2974,  -8.3606],\n",
       "         [  1.5320,   0.2369,  -8.2651],\n",
       "         [  2.2174,  -0.5999,  -6.3158],\n",
       "         [  2.2519,  -0.6180,  -6.3690],\n",
       "         [  2.2104,  -0.5935,  -6.3241],\n",
       "         [  1.8344,  -0.6946,  -5.9606],\n",
       "         [  1.5924,  -0.6838,  -5.0412],\n",
       "         [  1.6697,  -0.9477,  -4.5739],\n",
       "         [  1.6787,  -0.9612,  -4.5808],\n",
       "         [  1.6843,  -0.9891,  -4.5528],\n",
       "         [  2.9825,  -0.6419,  -8.4170],\n",
       "         [  0.0786,  -1.4352,   0.0791],\n",
       "         [  0.0775,  -1.4414,   0.0849],\n",
       "         [  0.0812,  -1.4625,   0.1149],\n",
       "         [  0.0764,  -1.4226,   0.0475],\n",
       "         [  0.1001,  -0.7409,  -1.7917],\n",
       "         [  0.1168,  -0.7305,  -1.8491],\n",
       "         [  0.1045,  -0.7360,  -1.8132],\n",
       "         [  1.8424,  -4.0060,  -0.7075],\n",
       "         [  1.8206,  -4.0044,  -0.6301],\n",
       "         [  1.8565,  -4.0253,  -0.7063],\n",
       "         [  1.7944,  -3.8993,  -0.7606],\n",
       "         [  1.8188,  -4.0335,  -0.5954],\n",
       "         [  1.9436,   0.5738, -10.6987],\n",
       "         [  1.9544,   0.5272, -10.5557],\n",
       "         [ -3.4922,   4.6771, -15.2023],\n",
       "         [ -7.0465,   3.9130, -13.2087],\n",
       "         [  1.7579,   1.3790, -12.4688],\n",
       "         [  1.9521,   0.5767, -10.7108],\n",
       "         [  1.4125,  -0.1434,  -6.7499],\n",
       "         [ -1.4037,  -1.7942, -16.6766],\n",
       "         [  1.5875,  -1.4535,  -4.6250],\n",
       "         [  1.5852,  -1.4804,  -4.5502],\n",
       "         [  1.5856,  -1.4475,  -4.6231],\n",
       "         [  2.8874,  -4.3102,  -2.7001],\n",
       "         [  2.8826,  -4.3237,  -2.6698],\n",
       "         [  2.8815,  -4.3025,  -2.6925],\n",
       "         [  3.0433,  -3.7654,  -3.7645],\n",
       "         [  3.0465,  -3.7289,  -3.8696],\n",
       "         [ -2.5619,   1.2180,  -5.0779]]),\n",
       " tensor([[ 2.9554e+00, -1.5833e+00, -6.7073e+00],\n",
       "         [ 3.3428e+00, -3.3617e+00, -5.2961e+00],\n",
       "         [ 1.4765e+00, -1.4590e+00, -4.1126e+00],\n",
       "         [ 9.6214e-02,  1.0791e+00, -8.4446e+00],\n",
       "         [ 7.3739e-01, -1.1638e+00, -3.1566e+00],\n",
       "         [ 1.3309e+00,  2.1285e-01, -1.1187e+01],\n",
       "         [ 7.5804e-01, -1.1372e+00, -3.2566e+00],\n",
       "         [ 1.5812e+00,  5.7287e-03, -1.1903e+01],\n",
       "         [ 9.6645e-02,  9.9265e-01, -8.0255e+00],\n",
       "         [ 5.1730e-01, -5.9902e-01, -3.9191e+00],\n",
       "         [ 7.4304e-01, -1.1791e+00, -3.1318e+00],\n",
       "         [ 7.4064e-01, -1.1417e+00, -3.2010e+00],\n",
       "         [ 3.7468e-01,  3.8882e-01, -5.6953e+00],\n",
       "         [ 6.4178e-01, -5.5095e-01, -4.1455e+00],\n",
       "         [ 1.7974e-01,  1.0670e+00, -7.8895e+00],\n",
       "         [ 1.4754e+00,  9.8479e-02, -1.0509e+01],\n",
       "         [ 1.8663e+00, -4.1254e+00, -4.6244e-01],\n",
       "         [ 1.8655e+00, -4.1382e+00, -4.2311e-01],\n",
       "         [ 2.8339e-01, -9.6219e-01, -2.8662e+00],\n",
       "         [ 1.8817e+00, -4.1275e+00, -4.9631e-01],\n",
       "         [ 1.3930e+00, -1.9123e+00, -3.3660e+00],\n",
       "         [ 1.8677e+00, -4.1423e+00, -4.2943e-01],\n",
       "         [ 1.7477e-01, -1.1120e+00, -2.4358e+00],\n",
       "         [ 1.8281e+00, -4.0008e+00, -5.9945e-01],\n",
       "         [ 1.8511e+00, -4.0828e+00, -4.9269e-01],\n",
       "         [ 1.8583e+00, -4.0881e+00, -5.0605e-01],\n",
       "         [ 1.8120e+00, -4.0104e+00, -5.2592e-01],\n",
       "         [ 1.4825e+00, -2.0357e+00, -3.4903e+00],\n",
       "         [ 1.8648e+00, -4.0876e+00, -5.0126e-01],\n",
       "         [ 1.8547e+00, -4.1572e+00, -4.0975e-01],\n",
       "         [ 1.8246e+00, -4.0324e+00, -5.1755e-01],\n",
       "         [ 1.8605e+00, -4.0625e+00, -5.7288e-01],\n",
       "         [ 2.6643e+00, -1.1726e+00, -6.0175e+00],\n",
       "         [ 2.7772e+00, -1.2044e+00, -6.2540e+00],\n",
       "         [ 2.3695e+00, -9.2363e-01, -5.9495e+00],\n",
       "         [ 2.5791e+00, -1.0487e+00, -6.1233e+00],\n",
       "         [-1.9619e+00,  2.4613e+00, -7.7308e+00],\n",
       "         [ 8.8957e-01,  4.7653e-01, -4.7628e+00],\n",
       "         [ 2.0147e+00, -4.5643e-01, -5.8576e+00],\n",
       "         [-1.9161e+00,  2.4998e+00, -7.5760e+00],\n",
       "         [ 2.7535e+00, -1.2801e+00, -6.0145e+00],\n",
       "         [ 2.7639e+00, -1.2796e+00, -6.0434e+00],\n",
       "         [ 2.7655e+00, -1.2779e+00, -6.0479e+00],\n",
       "         [ 2.7931e+00, -1.2309e+00, -6.2257e+00],\n",
       "         [ 2.6096e+00, -1.1075e+00, -6.0366e+00],\n",
       "         [-2.4141e+00,  2.0848e+00, -1.2401e+01],\n",
       "         [-2.1014e+00,  2.3887e+00, -7.0901e+00],\n",
       "         [-1.0373e+00,  1.9956e+00, -6.2170e+00],\n",
       "         [-1.5847e+00,  1.3208e+00, -1.5780e+01],\n",
       "         [ 2.2380e+00, -7.1013e-01, -6.0255e+00],\n",
       "         [ 1.9975e+00, -5.7601e-01, -5.6504e+00],\n",
       "         [ 2.1703e+00, -6.3501e-01, -6.0384e+00],\n",
       "         [ 1.1872e+00,  8.1258e-01, -1.2614e+01],\n",
       "         [ 1.0627e+00,  8.8491e-01, -1.2532e+01],\n",
       "         [ 1.2006e+00,  8.0637e-01, -1.2967e+01],\n",
       "         [ 1.2482e+00,  7.7989e-01, -1.2697e+01],\n",
       "         [ 1.1408e+00,  8.4741e-01, -1.2529e+01],\n",
       "         [ 1.0702e+00,  9.1738e-01, -1.2673e+01],\n",
       "         [ 1.2661e+00,  7.4907e-01, -1.2849e+01],\n",
       "         [ 1.2019e+00,  8.0349e-01, -1.2651e+01],\n",
       "         [ 1.2122e+00,  8.2370e-01, -1.2569e+01],\n",
       "         [ 1.1334e+00,  7.8576e-01, -1.3321e+01],\n",
       "         [ 1.1408e+00,  8.4741e-01, -1.2529e+01],\n",
       "         [ 1.1093e+00,  8.6162e-01, -1.2596e+01]]),\n",
       " tensor([[ 1.0004e+00,  9.3659e-01, -1.1914e+01],\n",
       "         [ 1.1540e+00,  8.3192e-01, -1.2508e+01],\n",
       "         [ 1.0272e+00,  9.0187e-01, -1.2224e+01],\n",
       "         [ 1.0738e+00,  8.9092e-01, -1.2576e+01],\n",
       "         [ 1.6490e+00, -1.6003e+00, -7.4611e+00],\n",
       "         [ 1.6803e+00, -1.5856e+00, -7.5308e+00],\n",
       "         [ 1.7124e+00, -1.7564e+00, -7.2507e+00],\n",
       "         [ 1.1877e+00,  2.3395e-01, -1.0944e+01],\n",
       "         [ 1.3149e+00,  1.9152e-01, -1.1336e+01],\n",
       "         [ 1.3455e+00,  2.0920e-01, -1.1699e+01],\n",
       "         [ 1.2911e+00,  2.0690e-01, -1.1235e+01],\n",
       "         [-5.9267e+00,  1.5192e-01, -1.2162e+01],\n",
       "         [ 5.8096e-01, -2.9626e+00,  1.0409e+00],\n",
       "         [ 2.2931e+00, -2.1472e-03, -9.2422e+00],\n",
       "         [ 1.5537e+00,  2.0753e+00, -1.5165e+01],\n",
       "         [ 2.3143e+00,  4.9541e-03, -9.3369e+00],\n",
       "         [ 2.2149e+00,  2.6523e-02, -9.2957e+00],\n",
       "         [ 2.3022e+00, -4.1725e-02, -9.0953e+00],\n",
       "         [ 2.3181e+00, -3.2418e-02, -9.2206e+00],\n",
       "         [ 1.7704e+00,  1.7066e+00, -1.4624e+01],\n",
       "         [ 2.3151e+00, -2.5568e-02, -9.2383e+00],\n",
       "         [ 2.3160e+00, -1.4159e-02, -9.2690e+00],\n",
       "         [ 2.3016e+00,  2.6891e-03, -9.3215e+00],\n",
       "         [ 2.3126e+00, -5.7624e-03, -9.3128e+00],\n",
       "         [ 2.3124e+00, -9.5324e-03, -9.2659e+00],\n",
       "         [ 2.3101e+00, -1.1824e-02, -9.2990e+00],\n",
       "         [ 2.3126e+00, -1.1142e-02, -9.2769e+00],\n",
       "         [ 2.3143e+00, -1.9252e-02, -9.2612e+00],\n",
       "         [ 2.3062e+00, -1.4128e-02, -9.2297e+00],\n",
       "         [ 2.3169e+00, -2.3036e-02, -9.2340e+00],\n",
       "         [ 2.3020e+00,  1.6289e-02, -9.4059e+00],\n",
       "         [ 1.7846e+00,  2.0924e+00, -1.6587e+01],\n",
       "         [ 2.3128e+00, -1.8809e-02, -9.2585e+00],\n",
       "         [ 2.1210e+00,  2.0377e-01, -9.9041e+00],\n",
       "         [-4.5440e+00,  1.9842e+00, -1.0246e+01],\n",
       "         [ 2.3145e+00, -1.8091e-02, -9.2481e+00],\n",
       "         [ 2.3116e+00, -4.3052e-03, -9.3010e+00],\n",
       "         [ 1.8977e+00,  1.8040e+00, -1.5835e+01],\n",
       "         [ 2.3151e+00, -1.1727e-02, -9.3039e+00],\n",
       "         [ 1.4420e+00, -2.3041e-01, -6.0763e+00],\n",
       "         [-5.1286e+00,  3.2571e+00, -1.7463e+01],\n",
       "         [-5.1211e+00,  2.8685e+00, -1.5096e+01],\n",
       "         [ 1.5179e+00, -4.4806e-01, -5.6657e+00],\n",
       "         [ 1.5196e+00, -5.0411e-01, -5.5743e+00],\n",
       "         [ 1.5157e+00, -4.6800e-01, -5.6233e+00],\n",
       "         [ 1.5117e+00, -4.5475e-01, -5.6412e+00],\n",
       "         [ 1.5191e+00, -5.0761e-01, -5.5682e+00],\n",
       "         [ 1.5211e+00, -4.8887e-01, -5.6111e+00],\n",
       "         [-4.3057e+00,  7.6412e-01, -3.7734e+00],\n",
       "         [ 2.5120e+00, -2.5668e-02, -8.8511e+00],\n",
       "         [-3.9107e+00,  1.1028e+00, -6.8724e+00],\n",
       "         [ 2.5238e+00, -3.9357e-02, -8.8381e+00],\n",
       "         [-1.0703e+00,  4.3626e+00, -1.0782e+01],\n",
       "         [ 5.6920e+00, -2.4423e+00, -1.2231e+01],\n",
       "         [-2.3830e+00,  3.6819e+00, -1.1722e+01],\n",
       "         [ 2.0783e+00, -1.0157e-02, -1.1560e+01],\n",
       "         [ 2.1964e+00, -1.7349e-01, -1.1510e+01],\n",
       "         [ 2.2501e+00, -1.9062e-01, -1.1679e+01],\n",
       "         [ 2.1946e+00, -1.4028e-01, -1.1601e+01],\n",
       "         [ 2.2746e+00, -2.5250e-01, -1.1576e+01],\n",
       "         [ 2.2369e+00, -2.3958e-01, -1.1396e+01],\n",
       "         [ 2.1750e+00, -1.3876e-01, -1.1493e+01],\n",
       "         [ 2.0787e+00, -1.6107e-02, -1.1456e+01],\n",
       "         [ 2.1438e+00, -1.2100e-01, -1.1366e+01]]),\n",
       " tensor([[ 2.2249e+00, -1.8198e-01, -1.1636e+01],\n",
       "         [ 2.4043e+00, -4.3230e-01, -1.1289e+01],\n",
       "         [ 2.2160e+00, -1.8641e-01, -1.1562e+01],\n",
       "         [ 2.0787e+00, -1.6107e-02, -1.1456e+01],\n",
       "         [ 2.2746e+00, -2.5250e-01, -1.1576e+01],\n",
       "         [ 2.1836e+00, -1.3152e-01, -1.1520e+01],\n",
       "         [ 2.2532e+00, -1.9561e-01, -1.1658e+01],\n",
       "         [ 2.2150e+00, -1.3726e-01, -1.1776e+01],\n",
       "         [ 2.2447e+00, -2.1748e-01, -1.1528e+01],\n",
       "         [ 2.2910e+00, -2.5762e-01, -1.1672e+01],\n",
       "         [ 2.1282e+00, -1.0901e-01, -1.1405e+01],\n",
       "         [ 2.1139e+00, -8.4398e-02, -1.1406e+01],\n",
       "         [ 2.2127e+00, -1.6757e-01, -1.1600e+01],\n",
       "         [ 2.2652e+00, -2.3320e-01, -1.1532e+01],\n",
       "         [ 2.0495e+00, -2.2220e-02, -1.1240e+01],\n",
       "         [ 2.2061e+00, -1.6362e-01, -1.1588e+01],\n",
       "         [ 2.1711e+00, -1.3630e-01, -1.1433e+01],\n",
       "         [ 2.1730e+00, -1.3434e-01, -1.1513e+01],\n",
       "         [ 2.1128e+00, -9.4178e-02, -1.1346e+01],\n",
       "         [ 2.3396e+00, -2.7455e-01, -1.1724e+01],\n",
       "         [ 2.4681e+00, -4.2964e-01, -1.1791e+01],\n",
       "         [ 2.2509e+00, -1.7329e-01, -1.1745e+01],\n",
       "         [ 2.1770e+00, -1.7064e-01, -1.1377e+01],\n",
       "         [ 2.1826e+00, -1.6521e-01, -1.1376e+01],\n",
       "         [ 2.4625e+00, -4.5544e-01, -1.1710e+01],\n",
       "         [ 2.2439e+00, -2.2274e-01, -1.1532e+01],\n",
       "         [ 2.1578e+00, -1.0056e-01, -1.1570e+01],\n",
       "         [ 2.2858e+00, -2.3859e-01, -1.1650e+01],\n",
       "         [ 3.2271e+00, -1.1777e+00, -1.3529e+01],\n",
       "         [ 2.2504e+00, -1.9367e-01, -1.1645e+01],\n",
       "         [ 2.1640e+00, -1.2067e-01, -1.1519e+01],\n",
       "         [ 2.4325e+00, -4.1183e-01, -1.1761e+01],\n",
       "         [ 2.2982e+00, -2.5610e-01, -1.1614e+01],\n",
       "         [ 2.2767e+00, -2.2667e-01, -1.1625e+01],\n",
       "         [ 1.4957e+00,  7.8976e-02, -6.4392e+00],\n",
       "         [ 1.6218e+00,  5.8896e-03, -6.5998e+00],\n",
       "         [ 1.2957e+00,  1.7278e-01, -5.9415e+00],\n",
       "         [ 1.6969e+00,  1.1483e+00, -1.1966e+01],\n",
       "         [-5.0808e+00,  4.4970e+00, -8.9141e+00],\n",
       "         [-1.0783e+00,  1.8618e+00, -2.3025e+00],\n",
       "         [ 3.1754e+00, -2.6636e-01, -7.0538e+00],\n",
       "         [ 3.0560e+00, -2.2940e-01, -6.6669e+00],\n",
       "         [-5.9681e-01,  1.8230e+00, -2.8147e+00],\n",
       "         [-1.4071e+00,  2.1477e+00, -3.2228e+00],\n",
       "         [ 1.8866e+00, -3.2926e-01, -6.4001e+00],\n",
       "         [ 2.1973e+00, -2.1873e+00, -5.7730e+00],\n",
       "         [ 2.1969e+00, -2.2265e+00, -5.7306e+00],\n",
       "         [ 2.2043e+00, -2.2105e+00, -5.7725e+00],\n",
       "         [ 2.1979e+00, -2.2212e+00, -5.7373e+00],\n",
       "         [ 2.1914e+00, -2.2263e+00, -5.7090e+00],\n",
       "         [ 2.1930e+00, -2.2263e+00, -5.7138e+00],\n",
       "         [ 2.2001e+00, -2.2348e+00, -5.7423e+00],\n",
       "         [ 2.1921e+00, -2.2280e+00, -5.7232e+00],\n",
       "         [ 2.3905e+00, -1.5837e+00, -6.7637e+00],\n",
       "         [ 2.3283e-01,  1.4036e+00, -8.1732e+00],\n",
       "         [ 2.1669e+00, -1.1180e+00, -6.7545e+00],\n",
       "         [ 2.3132e+00, -1.8005e+00, -6.3671e+00],\n",
       "         [ 2.2020e+00, -2.2212e+00, -5.7481e+00],\n",
       "         [ 2.1967e+00, -2.2272e+00, -5.7280e+00],\n",
       "         [ 2.1991e+00, -2.2210e+00, -5.7454e+00],\n",
       "         [-3.2047e+00, -6.8001e-01, -8.5055e+00],\n",
       "         [ 2.1980e+00, -2.2288e+00, -5.7234e+00],\n",
       "         [ 2.2420e+00, -1.2406e+00, -6.7288e+00],\n",
       "         [ 2.3649e+00, -1.6087e+00, -6.6949e+00]]),\n",
       " tensor([[  2.1943,  -2.1961,  -5.7488],\n",
       "         [  1.6867,  -1.2647,  -4.1696],\n",
       "         [  1.6639,  -1.2146,  -4.1874],\n",
       "         [  1.6829,  -1.2260,  -4.2170],\n",
       "         [  1.6757,  -1.2068,  -4.2361],\n",
       "         [  1.6725,  -1.2208,  -4.2009],\n",
       "         [  1.6473,  -1.1913,  -4.1754],\n",
       "         [  1.6801,  -1.2277,  -4.2099],\n",
       "         [  1.6715,  -1.2220,  -4.2024],\n",
       "         [  1.6867,  -1.2405,  -4.2032],\n",
       "         [  1.6556,  -1.1753,  -4.2216],\n",
       "         [  1.6808,  -1.2725,  -4.1449],\n",
       "         [  1.6865,  -1.2481,  -4.1955],\n",
       "         [  1.6773,  -1.2504,  -4.1713],\n",
       "         [  1.7167,  -0.8164,  -5.0193],\n",
       "         [ -3.1386,   2.9655,  -8.9584],\n",
       "         [  1.6883,  -1.2363,  -4.2273],\n",
       "         [  1.6557,  -1.1913,  -4.2018],\n",
       "         [  1.6779,  -1.2441,  -4.1781],\n",
       "         [  1.5780,  -0.9800,  -4.3327],\n",
       "         [  0.5907,  -3.3816,   1.3076],\n",
       "         [  0.5899,  -3.3703,   1.3014],\n",
       "         [  0.4390,  -3.1700,   1.3138],\n",
       "         [  0.4175,  -3.0460,   1.1672],\n",
       "         [  0.5925,  -3.3879,   1.3090],\n",
       "         [ -3.0221,  -0.8085,  -7.1935],\n",
       "         [  4.1440,  -1.0319,  -9.9218],\n",
       "         [  4.1763,  -1.0364,  -9.9922],\n",
       "         [  4.1935,  -1.0375, -10.0359],\n",
       "         [  4.1671,  -1.0451,  -9.9448],\n",
       "         [  4.1610,  -1.0348,  -9.9497],\n",
       "         [  4.1854,  -1.0435,  -9.9978],\n",
       "         [  4.1771,  -1.0285, -10.0132],\n",
       "         [  4.1626,  -1.0459,  -9.9276],\n",
       "         [  4.1734,  -1.0312, -10.0003],\n",
       "         [  4.1642,  -1.0381,  -9.9569],\n",
       "         [  4.2133,  -1.0314, -10.0985],\n",
       "         [  4.1584,  -1.0518,  -9.8993],\n",
       "         [  4.1735,  -1.0368,  -9.9845],\n",
       "         [  4.1487,  -1.0393,  -9.9122],\n",
       "         [  2.7863,  -1.4812,  -4.7740],\n",
       "         [  1.7801,  -1.4134,  -1.7228],\n",
       "         [ -1.4620,   1.0562,  -1.4403],\n",
       "         [  1.4407,   1.0104,  -9.9572],\n",
       "         [  1.3362,  -1.8299,  -3.0520],\n",
       "         [ -0.7940,   2.0453,  -2.8658],\n",
       "         [  0.8761,   1.3016,  -5.3856],\n",
       "         [ -1.8443,   2.6772,  -8.3844],\n",
       "         [ -2.1996,   2.1734,  -6.4058],\n",
       "         [ -0.7366,   1.8213,  -2.3663],\n",
       "         [ -0.6503,   0.6484,  -4.1677],\n",
       "         [  1.3225,  -1.2876,  -4.0570],\n",
       "         [ -3.1359,   2.9274, -14.2457],\n",
       "         [  1.2739,  -1.1866,  -4.3176],\n",
       "         [  1.6193,  -2.4633,  -2.2389],\n",
       "         [  1.0167,   0.4020,  -6.7514],\n",
       "         [  1.6417,  -2.4256,  -2.3271],\n",
       "         [  1.6307,  -2.4341,  -2.3164],\n",
       "         [  1.6227,  -2.4870,  -2.2200],\n",
       "         [  0.8307,   0.6390,  -7.0635],\n",
       "         [ -0.3464,   0.6286,  -6.0676],\n",
       "         [  2.1235,  -2.4456,  -5.7298],\n",
       "         [  2.2176,  -2.3213,  -5.6985],\n",
       "         [  1.9852,  -1.5761,  -5.6452]]),\n",
       " tensor([[ 1.1408e+00, -1.7573e+00, -2.6515e+00],\n",
       "         [ 1.1287e+00, -1.7084e+00, -2.7119e+00],\n",
       "         [ 2.7013e-01,  2.3071e-02, -4.2570e+00],\n",
       "         [ 1.1383e+00, -1.7573e+00, -2.6582e+00],\n",
       "         [ 1.3066e+00, -5.5939e-01, -4.3851e+00],\n",
       "         [ 1.2664e+00, -2.3472e-01, -5.2871e+00],\n",
       "         [ 1.2644e+00, -2.3393e-01, -5.2742e+00],\n",
       "         [ 1.2613e+00, -2.1879e-01, -5.3096e+00],\n",
       "         [ 1.2670e+00, -2.3385e-01, -5.2743e+00],\n",
       "         [ 1.2691e+00, -2.2459e-01, -5.3093e+00],\n",
       "         [-3.3051e+00,  1.4829e+00, -1.6152e+01],\n",
       "         [-3.7246e+00,  1.8451e+00, -1.4896e+01],\n",
       "         [ 1.2655e+00, -2.5202e-01, -5.1980e+00],\n",
       "         [-2.0012e+00,  2.3487e+00, -9.4661e+00],\n",
       "         [ 1.6259e+00, -2.4724e+00, -3.0528e+00],\n",
       "         [-2.0796e+00,  2.4018e+00, -9.7573e+00],\n",
       "         [ 1.5445e+00, -5.5606e-01, -6.3219e+00],\n",
       "         [ 9.0827e-02,  4.9293e-01, -1.6232e+00],\n",
       "         [ 1.4415e+00, -4.4303e-01, -4.3350e+00],\n",
       "         [ 1.8271e+00, -5.8094e+00,  1.5636e+00],\n",
       "         [ 1.7954e+00, -5.5782e+00,  1.4506e+00],\n",
       "         [ 1.8616e+00, -5.8161e+00,  1.5250e+00],\n",
       "         [ 1.6793e+00, -5.9050e+00,  2.1387e+00],\n",
       "         [ 1.6475e+00, -5.7313e+00,  1.8709e+00],\n",
       "         [ 1.7464e+00, -5.7470e+00,  1.6180e+00],\n",
       "         [ 1.6605e+00, -5.6676e+00,  1.7315e+00],\n",
       "         [ 4.7991e-01, -2.7577e+00,  1.0552e+00],\n",
       "         [ 4.5693e-01, -2.7189e+00,  1.0595e+00],\n",
       "         [ 4.5785e-01, -2.7117e+00,  1.0405e+00],\n",
       "         [ 4.5813e-01, -2.7230e+00,  1.0621e+00],\n",
       "         [-4.2457e+00,  5.5157e+00, -1.4209e+01],\n",
       "         [-4.0670e+00,  5.3704e+00, -1.3907e+01],\n",
       "         [ 1.7037e+00,  5.4862e-01, -2.0028e+01],\n",
       "         [ 4.1326e-01, -6.9999e-01, -3.7778e+00],\n",
       "         [ 1.7346e+00, -7.5225e-01, -8.2816e+00],\n",
       "         [ 2.6818e+00, -1.3295e-01, -1.1607e+01],\n",
       "         [ 3.2806e+00, -3.7705e-01, -1.0539e+01],\n",
       "         [ 1.2867e+00, -3.0718e+00, -5.5423e-01],\n",
       "         [ 7.4310e-01, -2.2660e+00, -3.8668e-01],\n",
       "         [ 6.2655e-01, -2.1590e+00, -3.0359e-01],\n",
       "         [ 1.2835e+00, -3.0508e+00, -5.6237e-01],\n",
       "         [ 1.3847e+00, -1.1487e+00, -3.6674e+00],\n",
       "         [-4.0224e+00,  6.9256e-01, -1.1967e+01],\n",
       "         [-3.0806e+00,  2.6127e+00, -7.7598e+00],\n",
       "         [ 1.4264e+00, -1.5920e+00, -3.2861e+00],\n",
       "         [-1.6353e+00,  1.8826e+00, -6.2577e+00],\n",
       "         [-3.3300e+00,  2.6442e+00, -7.9927e+00],\n",
       "         [ 1.0902e+00, -3.3947e-01, -4.7526e+00],\n",
       "         [-3.4774e+00,  1.3981e-01, -1.2940e+01],\n",
       "         [-3.4143e+00,  1.4701e-02, -1.2825e+01],\n",
       "         [-3.9209e+00,  6.3541e-01, -1.2848e+01],\n",
       "         [-3.8708e+00,  7.6376e-01, -1.0991e+01],\n",
       "         [-3.9007e+00,  6.7149e-01, -1.1136e+01],\n",
       "         [-3.8722e+00,  6.5822e-01, -1.0961e+01],\n",
       "         [-2.7435e+00,  2.5709e+00, -7.5341e+00],\n",
       "         [-4.1905e+00,  2.6734e+00, -9.9802e+00],\n",
       "         [ 1.3952e+00, -9.9496e-01, -3.9148e+00],\n",
       "         [-3.9831e+00,  6.8781e-01, -1.1883e+01],\n",
       "         [-3.4311e+00,  2.6133e+00, -8.0497e+00],\n",
       "         [-3.8895e+00,  6.9559e-01, -1.1184e+01],\n",
       "         [-2.2803e+00,  2.3638e+00, -7.3494e+00],\n",
       "         [-2.9572e+00,  2.5567e+00, -7.6204e+00],\n",
       "         [-4.1449e+00,  7.7623e-01, -1.3249e+01],\n",
       "         [ 4.6186e-01,  3.2528e-01, -5.1309e+00]]),\n",
       " tensor([[ -4.2971,   2.7157, -10.5841],\n",
       "         [ -4.3279,   2.6234, -10.8291],\n",
       "         [ -3.9035,   0.7712, -10.4918],\n",
       "         [ -1.7205,   1.9191,  -6.2188],\n",
       "         [ -4.1331,   2.6604,  -9.7616],\n",
       "         [  1.3899,  -1.0990,  -3.7404],\n",
       "         [  1.3874,  -0.9740,  -3.9330],\n",
       "         [  1.4084,  -1.5787,  -3.2390],\n",
       "         [ -4.3343,   2.5213, -11.1969],\n",
       "         [  1.3874,  -0.9740,  -3.9330],\n",
       "         [  1.3846,  -1.0895,  -3.7457],\n",
       "         [  1.4141,  -1.4991,  -3.3424],\n",
       "         [ -4.1812,   0.9732, -10.2104],\n",
       "         [  0.5578,   0.2401,  -5.0960],\n",
       "         [ -3.9867,   0.6814, -12.5036],\n",
       "         [ -2.8760,   2.5462,  -7.5616],\n",
       "         [ -0.0187,   0.7876,  -5.3845],\n",
       "         [ -3.8799,   0.6798, -11.1122],\n",
       "         [ -3.9625,   0.6644, -11.9360],\n",
       "         [ -4.3279,   2.6234, -10.8291],\n",
       "         [  1.4035,  -1.6104,  -3.1912],\n",
       "         [  1.4122,  -1.4668,  -3.3718],\n",
       "         [ -1.6495,   1.9070,  -6.2846],\n",
       "         [ -3.9867,   0.6814, -12.5036],\n",
       "         [ -4.3198,   1.0805, -12.0704],\n",
       "         [ -2.4218,   2.4459,  -7.4607],\n",
       "         [  1.3959,  -1.0865,  -3.7614],\n",
       "         [ -2.3182,   2.3035,  -7.0369],\n",
       "         [ -2.8841,   2.5485,  -7.5574],\n",
       "         [  1.1731,  -0.4819,  -4.5814],\n",
       "         [  0.5516,   0.2431,  -5.0903],\n",
       "         [  0.0855,   0.6574,  -5.2221],\n",
       "         [ -1.4987,   1.7764,  -6.0485],\n",
       "         [ -1.4995,   1.8126,  -6.2186],\n",
       "         [ -3.5589,   0.2884, -11.2295],\n",
       "         [ -2.6721,   2.5313,  -7.4681],\n",
       "         [  1.3880,  -1.1199,  -3.7037],\n",
       "         [ -2.3942,   2.3813,  -7.2344],\n",
       "         [ -1.5875,   1.8341,  -6.1301],\n",
       "         [  1.4614,  -1.7823,  -3.1794],\n",
       "         [ -3.2887,   2.5345,  -7.8891],\n",
       "         [ -3.7525,   2.6894,  -9.0455],\n",
       "         [ -3.8386,   0.2433, -13.5190],\n",
       "         [  1.4084,  -1.5787,  -3.2390],\n",
       "         [  1.4024,  -1.5821,  -3.2089],\n",
       "         [ -2.9289,   2.5529,  -7.6734],\n",
       "         [ -3.4235,   0.0482, -12.7332],\n",
       "         [ -1.4787,   1.7695,  -6.0718],\n",
       "         [  1.4730,  -1.7621,  -3.2385],\n",
       "         [  1.4180,  -1.4861,  -3.3726],\n",
       "         [  1.6835,  -2.9537,  -1.2228],\n",
       "         [  1.6808,  -2.9590,  -1.2062],\n",
       "         [  1.6859,  -2.9524,  -1.2280],\n",
       "         [  1.6875,  -2.9663,  -1.2152],\n",
       "         [  1.6786,  -2.9538,  -1.2073],\n",
       "         [  1.6801,  -2.9696,  -1.1919],\n",
       "         [  1.7783,  -2.3583,  -2.2720],\n",
       "         [  1.0819,  -1.7135,  -1.8236],\n",
       "         [  1.7664,  -2.3318,  -2.2602],\n",
       "         [  2.0951,  -0.3099,  -7.2214],\n",
       "         [  1.7919,  -0.7909,  -4.4302],\n",
       "         [  1.8265,  -0.5619,  -4.8023],\n",
       "         [  1.7746,  -0.6149,  -4.6284],\n",
       "         [  1.7988,  -0.5194,  -4.7799]]),\n",
       " tensor([[ 1.2915e+00,  8.8374e-02, -7.0413e+00],\n",
       "         [ 1.3181e+00,  3.1739e-02, -6.9550e+00],\n",
       "         [ 1.3011e+00,  5.2673e-02, -6.9421e+00],\n",
       "         [ 9.9643e-01, -3.8696e+00,  1.0228e+00],\n",
       "         [ 1.0265e+00, -3.7783e+00,  8.1595e-01],\n",
       "         [ 1.3139e+00,  2.0261e-02, -9.6627e+00],\n",
       "         [ 1.2757e+00,  7.1884e-02, -9.6420e+00],\n",
       "         [ 1.3528e+00, -6.0674e-03, -9.7277e+00],\n",
       "         [ 1.3200e+00,  2.7568e-02, -9.7005e+00],\n",
       "         [ 1.2912e+00,  5.4726e-02, -9.6838e+00],\n",
       "         [ 1.3290e+00,  1.3475e-02, -9.7212e+00],\n",
       "         [ 1.3279e+00,  2.0256e-02, -9.7166e+00],\n",
       "         [ 1.3283e+00,  2.3796e-02, -9.7546e+00],\n",
       "         [ 1.3258e+00,  2.6562e-02, -9.7609e+00],\n",
       "         [ 1.3065e+00,  3.1843e-02, -9.6987e+00],\n",
       "         [ 1.3281e+00,  1.0031e-02, -9.7175e+00],\n",
       "         [ 1.3282e+00,  1.4910e-02, -9.7270e+00],\n",
       "         [ 1.3136e+00,  1.4681e-02, -9.6329e+00],\n",
       "         [ 1.3241e+00,  1.0962e-02, -9.7013e+00],\n",
       "         [ 1.2732e+00,  6.1094e-02, -9.6282e+00],\n",
       "         [ 1.3376e+00,  1.5492e-02, -9.7800e+00],\n",
       "         [ 1.3107e+00,  3.6914e-02, -9.7204e+00],\n",
       "         [ 1.3376e+00,  1.5492e-02, -9.7800e+00],\n",
       "         [ 1.3119e+00,  3.0157e-02, -9.7001e+00],\n",
       "         [ 1.3286e+00,  2.0421e-02, -9.7306e+00],\n",
       "         [ 1.3259e+00,  1.5646e-02, -9.7244e+00],\n",
       "         [ 1.4164e+00, -8.1068e-02, -9.8181e+00],\n",
       "         [ 1.3502e+00, -5.0656e-03, -9.7597e+00],\n",
       "         [ 1.3708e+00, -3.9667e-02, -9.7034e+00],\n",
       "         [ 1.3569e+00, -1.3525e-03, -9.8150e+00],\n",
       "         [ 1.3497e+00,  2.3279e-04, -9.7723e+00],\n",
       "         [ 1.3697e+00, -4.9558e-02, -9.6815e+00],\n",
       "         [ 1.3828e+00, -3.5306e-02, -9.8217e+00],\n",
       "         [ 1.4652e+00, -1.2516e-01, -9.8939e+00],\n",
       "         [ 1.3216e+00,  1.7790e-02, -9.7042e+00],\n",
       "         [ 1.3142e+00,  2.6364e-02, -9.7048e+00],\n",
       "         [ 1.2785e+00,  4.1038e-02, -9.5761e+00],\n",
       "         [ 1.3529e+00, -2.1084e-02, -9.7082e+00],\n",
       "         [ 1.3314e+00, -8.3563e-03, -9.6397e+00],\n",
       "         [-1.4637e+00,  8.2622e-01, -1.3808e+01],\n",
       "         [-1.8091e-01, -1.4421e+00, -5.9032e+00],\n",
       "         [ 9.4337e-01, -2.2223e+00, -5.2974e-01],\n",
       "         [ 9.4966e-01, -2.2147e+00, -5.5064e-01],\n",
       "         [ 9.3695e-01, -2.2402e+00, -5.0125e-01],\n",
       "         [ 2.2032e-01, -1.5212e+00, -1.9497e-01],\n",
       "         [-2.1562e+00, -4.4977e-01, -7.4782e+00],\n",
       "         [-1.2337e+00,  8.6454e-01, -5.6762e+00],\n",
       "         [ 9.3192e-01, -2.2300e+00, -5.0407e-01],\n",
       "         [ 9.4931e-01, -2.2472e+00, -5.3014e-01],\n",
       "         [ 9.4827e-01, -2.1880e+00, -5.6882e-01],\n",
       "         [ 9.3794e-01, -2.2346e+00, -5.0924e-01],\n",
       "         [-1.3615e+00,  6.7708e-01, -5.4994e+00],\n",
       "         [ 9.4598e-01, -2.2395e+00, -5.1922e-01],\n",
       "         [ 9.4772e-01, -2.2367e+00, -5.2371e-01],\n",
       "         [ 9.3448e-01, -2.2398e+00, -4.9764e-01],\n",
       "         [ 9.4062e-01, -2.2376e+00, -5.1629e-01],\n",
       "         [ 8.2908e-01, -2.0950e+00, -3.7493e-02],\n",
       "         [-6.0668e-02, -1.4137e+00,  2.1387e-01],\n",
       "         [ 9.4726e-01, -2.2210e+00, -5.4260e-01],\n",
       "         [ 4.6890e-01, -2.2768e+00,  6.9922e-01],\n",
       "         [ 4.7304e-01, -2.2942e+00,  6.8459e-01],\n",
       "         [ 4.7322e-01, -2.2914e+00,  6.8384e-01],\n",
       "         [ 4.8324e-01, -2.2908e+00,  6.4751e-01],\n",
       "         [ 5.4846e-01, -2.3562e+00,  7.5308e-01]]),\n",
       " tensor([[  0.5304,  -2.3019,   0.6251],\n",
       "         [  0.4832,  -2.2908,   0.6475],\n",
       "         [  0.4602,  -2.2936,   0.7440],\n",
       "         [  0.4692,  -2.2972,   0.7117],\n",
       "         [  0.4629,  -2.2848,   0.6867],\n",
       "         [  0.5006,  -2.3336,   0.8260],\n",
       "         [  1.5127,  -2.0456,  -2.9994],\n",
       "         [  1.5124,  -2.0550,  -2.9841],\n",
       "         [ -1.6084,   1.5169,  -6.2090],\n",
       "         [  1.4306,  -1.9006,  -2.6278],\n",
       "         [ -1.0836,   1.7810,  -7.1914],\n",
       "         [  1.3122,  -0.8007,  -4.0024],\n",
       "         [ -2.3080,   1.3538,  -9.7539],\n",
       "         [  1.3725,   0.1599,  -6.3186],\n",
       "         [  1.3590,   0.1744,  -6.3403],\n",
       "         [  1.3766,   0.1783,  -6.4073],\n",
       "         [ -0.8130,   2.4624, -12.8883],\n",
       "         [  0.8982,   0.6063,  -7.6380],\n",
       "         [ -0.9170,   2.4945, -12.8222],\n",
       "         [  0.9111,   0.5671,  -7.4942],\n",
       "         [ -4.1267,   0.4308, -15.9306],\n",
       "         [  0.9047,   0.5830,  -7.5523],\n",
       "         [  0.9283,   0.5427,  -7.4601],\n",
       "         [  2.5819,  -0.8771,  -3.9387],\n",
       "         [  2.4410,  -0.7426,  -3.7897],\n",
       "         [  2.4030,  -0.6685,  -3.7718],\n",
       "         [  2.5095,  -0.7730,  -3.9965],\n",
       "         [  2.4476,  -0.7184,  -3.8542],\n",
       "         [  2.3627,  -6.6860,   1.8847],\n",
       "         [  2.2462,  -6.6260,   2.0709],\n",
       "         [  2.3627,  -6.6860,   1.8847],\n",
       "         [  2.1277,  -6.6130,   2.3190],\n",
       "         [  1.4594,  -3.9839,  -0.2175],\n",
       "         [  1.4915,  -3.9812,  -0.2880],\n",
       "         [  1.4485,  -3.9030,  -0.3134],\n",
       "         [  3.1653,  -1.6320,  -7.1968],\n",
       "         [  3.1665,  -1.6246,  -7.2454],\n",
       "         [  2.6788,  -0.6726,  -8.3524],\n",
       "         [  2.0541,  -5.4664,   0.6836],\n",
       "         [  2.0887,  -5.4745,   0.6327],\n",
       "         [  2.0917,  -5.4809,   0.6335],\n",
       "         [  2.0313,  -5.4608,   0.7147],\n",
       "         [  1.9895,  -4.7580,  -0.4163],\n",
       "         [  2.0788,  -5.4782,   0.6573],\n",
       "         [  2.0047,  -5.4490,   0.7674],\n",
       "         [  2.0586,  -5.4767,   0.6896],\n",
       "         [  2.0585,  -5.5150,   0.7460],\n",
       "         [  2.1295,  -5.2705,   0.2036],\n",
       "         [  2.0532,  -5.4539,   0.6661],\n",
       "         [  3.0881,  -6.8865,   0.6429],\n",
       "         [  3.1274,  -6.8906,   0.5717],\n",
       "         [  3.0833,  -6.8713,   0.6329],\n",
       "         [  3.1046,  -6.9017,   0.6340],\n",
       "         [  3.1028,  -6.8832,   0.6077],\n",
       "         [  3.1007,  -6.9004,   0.6352],\n",
       "         [  3.1010,  -6.8669,   0.5878],\n",
       "         [  3.1010,  -6.9046,   0.6447],\n",
       "         [  3.1422,  -6.9285,   0.5970],\n",
       "         [  3.1144,  -6.8953,   0.6061],\n",
       "         [  3.0952,  -6.8838,   0.6233],\n",
       "         [  3.0980,  -6.9148,   0.6641],\n",
       "         [  3.1333,  -6.9026,   0.5836],\n",
       "         [  3.0840,  -6.8886,   0.6561],\n",
       "         [  3.0766,  -6.8970,   0.6768]]),\n",
       " tensor([[ 3.1160e+00, -6.9060e+00,  6.1753e-01],\n",
       "         [ 3.1160e+00, -6.9194e+00,  6.3503e-01],\n",
       "         [ 3.1096e+00, -6.8756e+00,  5.8469e-01],\n",
       "         [ 3.1108e+00, -6.8829e+00,  6.0343e-01],\n",
       "         [ 3.1278e+00, -6.9047e+00,  5.9804e-01],\n",
       "         [ 3.1382e+00, -6.8792e+00,  5.4840e-01],\n",
       "         [ 3.1166e+00, -6.9054e+00,  6.1180e-01],\n",
       "         [ 3.1149e+00, -6.9087e+00,  6.2616e-01],\n",
       "         [ 3.1870e+00, -6.5963e+00,  2.2293e-02],\n",
       "         [ 3.0501e+00, -6.8978e+00,  7.3991e-01],\n",
       "         [ 3.1142e+00, -6.9253e+00,  6.4819e-01],\n",
       "         [ 3.0973e+00, -6.8790e+00,  6.1441e-01],\n",
       "         [ 3.0435e+00, -6.9015e+00,  7.5076e-01],\n",
       "         [ 3.1016e+00, -6.8948e+00,  6.3415e-01],\n",
       "         [ 3.1047e+00, -6.8778e+00,  5.9960e-01],\n",
       "         [ 2.9073e+00, -6.2275e+00, -7.1542e-02],\n",
       "         [ 3.1172e+00, -6.8995e+00,  6.1334e-01],\n",
       "         [ 3.1162e+00, -6.8924e+00,  6.0559e-01],\n",
       "         [ 3.1028e+00, -6.8968e+00,  6.2819e-01],\n",
       "         [ 3.1068e+00, -6.9030e+00,  6.3141e-01],\n",
       "         [ 3.1041e+00, -6.8970e+00,  6.2485e-01],\n",
       "         [ 3.1268e+00, -6.8941e+00,  5.8211e-01],\n",
       "         [ 3.0856e+00, -6.8774e+00,  6.3619e-01],\n",
       "         [ 3.1006e+00, -6.8878e+00,  6.2304e-01],\n",
       "         [ 3.0999e+00, -6.9016e+00,  6.4540e-01],\n",
       "         [ 3.1081e+00, -6.9139e+00,  6.3777e-01],\n",
       "         [ 3.0970e+00, -6.9111e+00,  6.5498e-01],\n",
       "         [ 3.0079e+00, -6.9382e+00,  8.3087e-01],\n",
       "         [ 3.0907e+00, -6.8855e+00,  6.3509e-01],\n",
       "         [ 3.1173e+00, -6.8873e+00,  5.9416e-01],\n",
       "         [ 3.0845e+00, -6.8855e+00,  6.4758e-01],\n",
       "         [ 3.1064e+00, -6.9073e+00,  6.3011e-01],\n",
       "         [ 3.1101e+00, -6.9023e+00,  6.3281e-01],\n",
       "         [ 3.1189e+00, -6.9025e+00,  6.1217e-01],\n",
       "         [-8.4995e+00,  3.5083e+00, -1.2457e+01],\n",
       "         [-6.5467e+00,  3.9627e+00, -1.9772e+01],\n",
       "         [ 2.2326e+00,  5.8125e-01, -1.1232e+01],\n",
       "         [-9.0564e-02,  2.7591e+00, -1.4800e+01],\n",
       "         [-2.5455e-01,  2.9342e+00, -1.6338e+01],\n",
       "         [-2.5455e-01,  2.9342e+00, -1.6338e+01],\n",
       "         [-5.5411e+00,  6.9027e+00, -2.1699e+01],\n",
       "         [ 2.2419e+00,  5.5587e-01, -1.1216e+01],\n",
       "         [-5.0756e+00,  6.3478e+00, -2.4803e+01],\n",
       "         [-8.1153e+00,  3.0840e+00, -1.0789e+01],\n",
       "         [ 2.2450e+00,  5.7660e-01, -1.1358e+01],\n",
       "         [ 2.2183e+00,  6.1735e-01, -1.1343e+01],\n",
       "         [-2.6810e+00,  5.0242e+00, -1.9864e+01],\n",
       "         [ 6.0517e-01,  2.0690e+00, -1.3743e+01],\n",
       "         [-2.0419e-01,  2.7735e+00, -1.4659e+01],\n",
       "         [-3.8204e+00,  5.8691e+00, -2.0105e+01],\n",
       "         [-5.2387e+00,  6.5853e+00, -2.4232e+01],\n",
       "         [ 2.2257e+00,  5.6699e-01, -1.1249e+01],\n",
       "         [-6.0404e-01,  3.1969e+00, -1.6662e+01],\n",
       "         [-6.3809e+00,  5.3290e+00, -2.1453e+01],\n",
       "         [ 2.2337e+00,  5.6695e-01, -1.1215e+01],\n",
       "         [ 2.2423e+00,  5.8562e-01, -1.1310e+01],\n",
       "         [-6.9481e-01,  3.2602e+00, -1.6441e+01],\n",
       "         [-8.8661e+00,  2.8961e+00, -1.0648e+01],\n",
       "         [-6.9481e-01,  3.2602e+00, -1.6441e+01],\n",
       "         [ 1.8294e+00,  1.1629e+00, -1.1452e+01],\n",
       "         [ 9.8032e-01,  1.7892e+00, -1.3741e+01],\n",
       "         [ 6.1914e+00, -3.0729e+00, -1.2551e+01],\n",
       "         [ 6.1833e+00, -3.0513e+00, -1.2557e+01],\n",
       "         [ 6.1157e+00, -3.0008e+00, -1.2395e+01]]),\n",
       " tensor([[  6.1284,  -2.9904, -12.4450],\n",
       "         [  1.2847,  -1.2153,  -4.5208],\n",
       "         [  3.5891,  -1.5385,  -6.6464],\n",
       "         [  3.8623,  -1.0329,  -8.1194],\n",
       "         [ -1.9137,   2.0547, -11.3164],\n",
       "         [  0.8876,  -1.7506,  -1.5703],\n",
       "         [  0.9355,  -1.5530,  -1.9776],\n",
       "         [  0.8862,  -1.7613,  -1.5542],\n",
       "         [  0.9543,  -1.2269,  -2.8719],\n",
       "         [  0.4033,  -1.1515,  -1.7599],\n",
       "         [ -0.7735,   1.2921,  -6.1199],\n",
       "         [ -6.7311,   3.0840, -10.5966],\n",
       "         [ -0.7981,   1.8981, -14.3543],\n",
       "         [  1.4545,  -0.7161,  -5.5968],\n",
       "         [  1.5651,  -1.4669,  -4.5788],\n",
       "         [  1.5639,  -1.4288,  -4.6322],\n",
       "         [  1.1475,   0.1642,  -7.0100],\n",
       "         [  1.1692,   0.1908,  -7.3424],\n",
       "         [  1.0657,   0.2396,  -6.9817],\n",
       "         [  0.5925,   0.7616,  -8.0299],\n",
       "         [  0.6083,   0.7356,  -7.9986],\n",
       "         [  2.4401,  -6.7217,   1.4560],\n",
       "         [  1.7433,  -1.3740,  -4.5066],\n",
       "         [  1.7495,  -1.4090,  -4.4801],\n",
       "         [  1.7444,  -1.3982,  -4.4871],\n",
       "         [  1.7469,  -1.3919,  -4.5000],\n",
       "         [  1.7403,  -1.4207,  -4.4344],\n",
       "         [  1.7289,  -0.6635,  -5.4299],\n",
       "         [  1.7475,  -1.3683,  -4.5397],\n",
       "         [  1.7442,  -1.3841,  -4.5025],\n",
       "         [  1.7398,  -1.3768,  -4.5075],\n",
       "         [  1.5244,  -0.9566,  -3.9239],\n",
       "         [  1.7535,  -1.3131,  -4.6438],\n",
       "         [  1.7469,  -1.4102,  -4.4708],\n",
       "         [  1.7472,  -1.3980,  -4.4960],\n",
       "         [  1.7271,  -0.6158,  -5.4993],\n",
       "         [  1.7530,  -1.3382,  -4.6058],\n",
       "         [  1.7432,  -1.3558,  -4.5390],\n",
       "         [  1.7461,  -1.3680,  -4.5326],\n",
       "         [  2.0854,  -2.0209,  -4.5344],\n",
       "         [  2.0919,  -2.0416,  -4.5094],\n",
       "         [  2.0902,  -2.0321,  -4.5200],\n",
       "         [  2.0842,  -1.9980,  -4.5720],\n",
       "         [  2.0489,  -1.6638,  -5.1763],\n",
       "         [ -1.3439,   2.3307, -12.6558],\n",
       "         [  2.0801,  -1.9785,  -4.6019],\n",
       "         [  2.0835,  -2.0259,  -4.5166],\n",
       "         [  2.0955,  -2.0156,  -4.5709],\n",
       "         [  2.0780,  -2.0038,  -4.5594],\n",
       "         [  2.0946,  -2.0278,  -4.5449],\n",
       "         [ -2.8320,   2.9958, -12.6191],\n",
       "         [ -2.2295,   3.4184, -12.4788],\n",
       "         [ -3.3733,   3.3328, -13.0328],\n",
       "         [ -2.1872,   2.8959, -12.4277],\n",
       "         [  2.0894,  -2.0075,  -4.5570],\n",
       "         [ -1.8796,   2.6128, -13.3482],\n",
       "         [ -0.4531,   1.1458, -18.2034],\n",
       "         [ -1.1918,   1.6014, -13.0461],\n",
       "         [  2.0846,  -2.0149,  -4.5433],\n",
       "         [ -1.4774,   1.6380, -12.6534],\n",
       "         [  1.0515,   1.0851, -10.3616],\n",
       "         [  2.0909,  -2.0270,  -4.5348],\n",
       "         [ -1.3367,   1.8155, -14.8161],\n",
       "         [  2.0871,  -2.0205,  -4.5401]]),\n",
       " tensor([[ 2.0905e+00, -2.0433e+00, -4.5090e+00],\n",
       "         [-2.3604e+00,  3.6923e+00, -1.4717e+01],\n",
       "         [-1.8790e+00,  2.6728e+00, -1.2928e+01],\n",
       "         [ 2.0955e+00, -2.0318e+00, -4.5418e+00],\n",
       "         [-2.4988e-01,  2.2968e+00, -1.1232e+01],\n",
       "         [ 1.9060e+00, -1.3711e+00, -5.6169e+00],\n",
       "         [ 2.0895e+00, -2.0107e+00, -4.5607e+00],\n",
       "         [-1.2185e+00,  1.5860e+00, -1.3694e+01],\n",
       "         [ 2.0984e+00, -2.0348e+00, -4.5417e+00],\n",
       "         [ 2.0839e+00, -2.0311e+00, -4.5239e+00],\n",
       "         [ 2.0900e+00, -2.0173e+00, -4.5555e+00],\n",
       "         [ 2.0919e+00, -2.0148e+00, -4.5577e+00],\n",
       "         [ 1.0224e+00,  2.5152e-01, -5.5620e+00],\n",
       "         [ 1.0786e+00, -3.7632e-02, -5.2488e+00],\n",
       "         [ 1.1098e+00, -4.1683e-02, -5.3753e+00],\n",
       "         [ 1.0739e+00, -9.7776e-03, -5.3112e+00],\n",
       "         [ 9.7950e-01,  2.9415e-01, -5.5785e+00],\n",
       "         [ 1.0928e+00,  4.3836e-02, -5.4039e+00],\n",
       "         [ 1.1016e+00, -8.3685e-02, -5.2388e+00],\n",
       "         [-3.7580e+00,  1.6816e+00, -1.6350e+01],\n",
       "         [ 1.1274e+00, -5.3784e-02, -5.0265e+00],\n",
       "         [ 1.1398e+00, -1.0303e-01, -4.9295e+00],\n",
       "         [ 1.1380e+00,  7.6041e-02, -5.3116e+00],\n",
       "         [ 1.0219e+00,  2.3502e-01, -5.2359e+00],\n",
       "         [ 1.0495e+00,  8.7154e-02, -5.0231e+00],\n",
       "         [ 1.1467e+00, -3.2895e-01, -4.5239e+00],\n",
       "         [ 1.1328e+00,  1.8362e-01, -5.5324e+00],\n",
       "         [-3.8595e+00,  2.2079e-01, -4.8713e+00],\n",
       "         [-1.4403e+00,  2.8821e+00, -1.0141e+01],\n",
       "         [-1.5516e+00,  2.8126e+00, -9.4808e+00],\n",
       "         [-1.0013e+00,  2.3801e+00, -8.5685e+00],\n",
       "         [-8.4369e-01,  2.3378e+00, -8.7155e+00],\n",
       "         [-1.0050e+00,  2.5230e+00, -9.2192e+00],\n",
       "         [ 7.6762e-01,  6.4083e-01, -5.4941e+00],\n",
       "         [-4.8784e+00,  3.7750e+00, -1.7754e+01],\n",
       "         [-4.0517e+00,  3.5757e+00, -1.7261e+01],\n",
       "         [ 1.9151e+00, -1.5505e+00, -4.0259e+00],\n",
       "         [ 1.9141e+00, -1.5631e+00, -4.0254e+00],\n",
       "         [ 1.8845e+00, -1.5514e+00, -4.0112e+00],\n",
       "         [ 1.9364e+00, -1.5524e+00, -4.0648e+00],\n",
       "         [ 1.9005e+00, -1.5267e+00, -4.0454e+00],\n",
       "         [ 1.8786e+00, -1.5439e+00, -4.0252e+00],\n",
       "         [ 1.9041e+00, -1.5646e+00, -4.0199e+00],\n",
       "         [ 1.8994e+00, -1.5585e+00, -4.0335e+00],\n",
       "         [ 1.9027e+00, -1.5667e+00, -4.0295e+00],\n",
       "         [ 1.9009e+00, -1.5468e+00, -4.0243e+00],\n",
       "         [ 1.8956e+00, -1.5565e+00, -4.0168e+00],\n",
       "         [ 1.8831e+00, -1.5094e+00, -4.0935e+00],\n",
       "         [ 1.9300e+00, -1.5526e+00, -4.0517e+00],\n",
       "         [ 1.8871e+00, -1.3951e+00, -4.1817e+00],\n",
       "         [ 1.9116e+00, -1.4974e+00, -4.1099e+00],\n",
       "         [ 1.9209e+00, -1.5550e+00, -4.0536e+00],\n",
       "         [ 1.9087e+00, -1.5384e+00, -4.0576e+00],\n",
       "         [ 1.9070e+00, -1.5435e+00, -4.0425e+00],\n",
       "         [ 1.8823e+00, -1.5635e+00, -3.9949e+00],\n",
       "         [ 1.9011e+00, -1.5635e+00, -4.0292e+00],\n",
       "         [ 1.9067e+00, -1.5714e+00, -4.0161e+00],\n",
       "         [ 1.9072e+00, -1.5543e+00, -4.0327e+00],\n",
       "         [ 1.8842e+00, -1.5444e+00, -4.0163e+00],\n",
       "         [-1.4035e+00,  1.1834e+00, -3.3870e+00],\n",
       "         [ 1.8971e+00, -1.5525e+00, -4.0262e+00],\n",
       "         [ 1.9084e+00, -1.5465e+00, -4.0417e+00],\n",
       "         [ 1.8864e+00, -1.5476e+00, -4.0265e+00],\n",
       "         [ 1.8958e+00, -1.5713e+00, -3.9669e+00]]),\n",
       " tensor([[ 1.9004e+00, -1.5295e+00, -4.0496e+00],\n",
       "         [ 1.9066e+00, -1.5617e+00, -4.0483e+00],\n",
       "         [ 1.9099e+00, -1.5563e+00, -4.0341e+00],\n",
       "         [-1.0498e-01,  9.1549e-01, -4.7380e+00],\n",
       "         [ 1.6012e+00, -5.6370e-01, -5.0087e+00],\n",
       "         [ 1.1626e+00,  1.7374e-01, -5.1852e+00],\n",
       "         [-3.0210e+00,  5.6819e-01, -9.9769e+00],\n",
       "         [ 1.5941e+00, -6.2276e-01, -4.9102e+00],\n",
       "         [ 1.6086e+00, -7.1680e-02, -6.1512e+00],\n",
       "         [ 1.5718e+00, -1.1455e-01, -5.6881e+00],\n",
       "         [ 5.0142e-01,  6.3195e-01, -4.9889e+00],\n",
       "         [ 1.8256e+00, -3.5129e-01, -6.2260e+00],\n",
       "         [-2.1951e+00,  1.0823e+00, -1.0123e+01],\n",
       "         [ 3.3753e+00, -6.7316e-02, -1.1811e+01],\n",
       "         [ 3.0521e+00,  7.1037e-01, -1.3519e+01],\n",
       "         [ 3.0032e+00,  1.0042e+00, -1.4489e+01],\n",
       "         [ 3.8170e+00, -4.0965e-01, -1.1801e+01],\n",
       "         [ 3.8105e+00, -4.1724e-01, -1.1745e+01],\n",
       "         [ 3.8658e+00, -3.5602e-01, -1.2092e+01],\n",
       "         [ 1.2648e+00,  9.6106e-01, -8.5769e+00],\n",
       "         [ 3.1388e-01,  1.4484e+00, -7.9080e+00],\n",
       "         [ 1.1077e+00,  1.1329e+00, -8.4251e+00],\n",
       "         [ 7.1459e-02,  1.5972e+00, -8.4712e+00],\n",
       "         [ 1.2866e+00,  9.5226e-01, -8.6116e+00],\n",
       "         [-1.6871e+00,  2.2551e+00, -1.2454e+01],\n",
       "         [-8.2019e-01,  1.9302e+00, -9.9456e+00],\n",
       "         [ 7.5246e-02,  1.6174e+00, -8.6247e+00],\n",
       "         [-3.3021e-01,  1.8058e+00, -8.4564e+00],\n",
       "         [-1.3766e-02,  1.6069e+00, -8.4220e+00],\n",
       "         [-1.2367e+00,  2.2079e+00, -9.3045e+00],\n",
       "         [ 3.0720e-02,  1.5703e+00, -8.2974e+00],\n",
       "         [ 8.8253e-03,  1.7124e+00, -8.9897e+00],\n",
       "         [-3.9831e+00,  3.1417e+00, -1.3080e+01],\n",
       "         [ 3.9787e-01,  1.2925e+00, -6.7926e+00],\n",
       "         [-5.7456e+00,  5.2302e+00, -1.7141e+01],\n",
       "         [-6.1025e+00,  5.3401e+00, -1.8187e+01],\n",
       "         [ 2.1778e+00,  4.3739e-01, -1.1137e+01],\n",
       "         [-6.3011e+00,  5.3281e+00, -1.5914e+01],\n",
       "         [ 2.5116e+00,  7.5313e-01, -1.3627e+01],\n",
       "         [-1.2818e+00,  1.7441e+00, -5.0825e+00],\n",
       "         [-1.5141e+00,  1.6143e+00, -4.0316e+00],\n",
       "         [-1.2818e+00,  1.7441e+00, -5.0825e+00],\n",
       "         [ 1.9806e+00, -3.0464e+00, -2.2086e+00],\n",
       "         [ 1.9712e+00, -3.0432e+00, -2.1872e+00],\n",
       "         [ 1.9736e+00, -3.0611e+00, -2.1621e+00],\n",
       "         [ 1.9082e+00, -1.3618e+00, -4.5845e+00],\n",
       "         [ 1.8861e+00, -1.2835e+00, -4.5165e+00],\n",
       "         [-5.0319e-03,  1.1553e+00, -2.2506e+00],\n",
       "         [ 4.9759e-01, -1.6442e+00,  4.9171e-01],\n",
       "         [-2.2778e-02,  1.2175e+00, -2.1544e+00],\n",
       "         [ 1.9614e+00, -3.2373e+00, -2.3321e+00],\n",
       "         [ 2.9488e+00,  3.8953e-02, -9.8252e+00],\n",
       "         [ 3.0071e+00, -4.8671e-02, -9.8459e+00],\n",
       "         [ 3.0273e+00, -5.6686e-02, -9.9121e+00],\n",
       "         [ 3.0314e+00, -5.8609e-02, -9.9199e+00],\n",
       "         [ 4.1473e+00, -1.4705e+00, -1.0894e+01],\n",
       "         [ 1.5875e+00,  5.4031e-01, -8.1494e+00],\n",
       "         [ 1.3174e+00,  6.0141e-01, -6.9064e+00],\n",
       "         [ 1.7568e+00, -1.3380e+00, -4.7792e+00],\n",
       "         [ 1.1254e+00,  1.0066e+00, -7.5087e+00],\n",
       "         [ 1.4383e+00,  7.4408e-01, -8.2308e+00],\n",
       "         [ 1.3467e+00,  3.4279e-01, -6.4031e+00],\n",
       "         [ 1.7358e+00, -1.3563e+00, -4.6131e+00],\n",
       "         [ 1.7700e+00, -1.3047e+00, -4.9051e+00]]),\n",
       " tensor([[ -1.1916,   0.2635,  -7.3428],\n",
       "         [  1.7492,  -1.3037,  -4.8157],\n",
       "         [  1.4507,   0.6079,  -7.6355],\n",
       "         [  1.4708,   0.7850,  -8.6425],\n",
       "         [  2.8902,  -0.9908,  -7.0647],\n",
       "         [ -1.3756,   1.2542,  -8.9945],\n",
       "         [ -5.0863,   1.7182,  -6.4555],\n",
       "         [  2.0282,  -2.6688,  -2.8598],\n",
       "         [  2.0388,  -2.5548,  -3.0691],\n",
       "         [  2.0265,  -2.6101,  -2.9501],\n",
       "         [  2.9128,  -0.8845,  -7.8815],\n",
       "         [  0.4895,  -0.9654,  -0.8958],\n",
       "         [  1.9131,  -1.4297,  -5.1943],\n",
       "         [  1.9198,  -1.4250,  -5.2354],\n",
       "         [  1.9130,  -1.4421,  -5.1726],\n",
       "         [  1.9097,  -1.4500,  -5.1339],\n",
       "         [  1.8686,  -1.3442,  -5.2171],\n",
       "         [  1.6522,  -1.1040,  -4.8192],\n",
       "         [  1.3881,  -0.1788,  -6.2109],\n",
       "         [  1.1220,   0.1176,  -6.7582],\n",
       "         [  1.4608,  -0.1880,  -6.5070],\n",
       "         [ -4.2130,   2.8973, -11.2923],\n",
       "         [  1.4774,  -0.3445,  -5.8014],\n",
       "         [ -0.9037,   1.8341,  -7.7560],\n",
       "         [ -0.9655,   1.8201,  -7.5801],\n",
       "         [ -5.0645,   2.3405, -14.2029],\n",
       "         [ -1.3915,   1.8974,  -7.1217],\n",
       "         [ -3.1155,  -0.5884, -12.5664],\n",
       "         [  1.3958,  -0.2259,  -6.0385],\n",
       "         [  1.4165,  -0.2605,  -5.9326],\n",
       "         [  1.4759,  -2.3795,  -1.1714],\n",
       "         [  1.4698,  -2.3913,  -1.1391],\n",
       "         [ -0.6065,   0.8207,  -8.1395],\n",
       "         [  1.4678,  -2.3770,  -1.1506],\n",
       "         [  1.4706,  -2.3633,  -1.1768],\n",
       "         [  1.4733,  -2.3880,  -1.1197],\n",
       "         [  0.1935,  -0.8308,  -0.4009],\n",
       "         [  0.3225,  -1.0831,  -0.1840],\n",
       "         [  1.4778,  -2.3678,  -1.1856],\n",
       "         [  1.4750,  -2.3725,  -1.1933],\n",
       "         [  1.4681,  -2.3605,  -1.2083],\n",
       "         [  1.4865,  -2.3750,  -1.2069],\n",
       "         [  1.4886,  -2.3832,  -1.1901],\n",
       "         [  1.4769,  -2.3916,  -1.1566],\n",
       "         [  1.4275,  -2.2628,  -1.2879],\n",
       "         [  1.4782,  -2.4053,  -1.1383],\n",
       "         [  1.4816,  -2.3773,  -1.1711],\n",
       "         [ -0.4913,  -0.1412,  -2.2120],\n",
       "         [  1.4813,  -2.3719,  -1.1893],\n",
       "         [  1.3817,  -2.2136,  -1.1808],\n",
       "         [  1.4753,  -2.3380,  -1.2479],\n",
       "         [  1.4978,  -2.4107,  -1.1737],\n",
       "         [  1.5110,  -2.3636,  -1.2977],\n",
       "         [ -3.8658,   0.8038,  -6.9820],\n",
       "         [  1.4715,  -2.3484,  -1.2286],\n",
       "         [  1.4920,  -2.3937,  -1.1850],\n",
       "         [  1.4834,  -2.3649,  -1.2226],\n",
       "         [  1.4874,  -2.4046,  -1.1515],\n",
       "         [  1.4759,  -2.3952,  -1.1624],\n",
       "         [  0.4432,  -1.1615,  -0.2863],\n",
       "         [  1.4768,  -2.3743,  -1.1850],\n",
       "         [  1.4594,  -2.3545,  -1.2058],\n",
       "         [  1.4807,  -2.3823,  -1.1871],\n",
       "         [  1.4666,  -2.3888,  -1.1269]]),\n",
       " tensor([[  1.4788,  -2.3882,  -1.1742],\n",
       "         [ -0.0644,  -0.2305,  -1.6826],\n",
       "         [ -0.6846,   0.0967,  -2.4900],\n",
       "         [  1.4958,  -2.3796,  -1.2082],\n",
       "         [  1.4692,  -2.3660,  -1.1627],\n",
       "         [  1.4641,  -2.3724,  -1.1242],\n",
       "         [ -0.8333,   0.2498,  -2.6422],\n",
       "         [  1.4676,  -2.3643,  -1.1688],\n",
       "         [  1.4886,  -2.3861,  -1.1876],\n",
       "         [  1.4666,  -2.4034,  -1.1119],\n",
       "         [ -0.9994,   1.0196, -11.3141],\n",
       "         [  1.4777,  -2.3878,  -1.1699],\n",
       "         [  1.4693,  -2.3780,  -1.1385],\n",
       "         [ -0.0550,  -0.1985,  -1.7675],\n",
       "         [  1.4639,  -2.3572,  -1.2151],\n",
       "         [  1.4877,  -2.3659,  -1.2037],\n",
       "         [  1.4744,  -2.3887,  -1.1438],\n",
       "         [  1.4575,  -2.3415,  -1.1877],\n",
       "         [  1.4618,  -2.3662,  -1.1689],\n",
       "         [  0.1935,  -0.8308,  -0.4009],\n",
       "         [  0.5865,  -1.3236,  -0.2925],\n",
       "         [  1.4646,  -2.3717,  -1.1597],\n",
       "         [  0.7390,  -1.5059,  -0.2934],\n",
       "         [  1.4719,  -2.3652,  -1.1814],\n",
       "         [  1.4820,  -2.3673,  -1.2054],\n",
       "         [  1.4758,  -2.3674,  -1.1931],\n",
       "         [  1.4969,  -2.3911,  -1.2175],\n",
       "         [  1.4848,  -2.3918,  -1.1810],\n",
       "         [  1.4937,  -2.3984,  -1.1975],\n",
       "         [  1.4939,  -2.3673,  -1.2478],\n",
       "         [  1.4668,  -2.3727,  -1.1644],\n",
       "         [  1.4142,  -2.3232,  -1.1210],\n",
       "         [  1.4837,  -2.3947,  -1.1713],\n",
       "         [ -0.5698,   0.6587,  -4.6462],\n",
       "         [  1.4697,  -2.3652,  -1.1958],\n",
       "         [  1.4823,  -2.3725,  -1.1982],\n",
       "         [  1.4739,  -2.3697,  -1.2104],\n",
       "         [  1.5073,  -2.3963,  -1.2318],\n",
       "         [  1.4989,  -2.4134,  -1.1631],\n",
       "         [  1.4976,  -2.3867,  -1.2109],\n",
       "         [  1.4737,  -2.3661,  -1.1755],\n",
       "         [ -0.5186,   0.6158,  -4.5738],\n",
       "         [  1.4615,  -2.2918,  -1.2854],\n",
       "         [  1.4768,  -2.3798,  -1.1710],\n",
       "         [  1.4822,  -2.3922,  -1.1788],\n",
       "         [  1.4835,  -2.4309,  -1.1267],\n",
       "         [  1.4830,  -2.3983,  -1.1801],\n",
       "         [  1.4633,  -2.3872,  -1.1373],\n",
       "         [  1.4621,  -2.3776,  -1.1406],\n",
       "         [  1.4809,  -2.3744,  -1.1859],\n",
       "         [  1.3817,  -2.2136,  -1.1808],\n",
       "         [  1.4807,  -2.3915,  -1.1689],\n",
       "         [  1.4890,  -2.3798,  -1.2083],\n",
       "         [  1.4918,  -2.4069,  -1.1853],\n",
       "         [  1.4752,  -2.3736,  -1.1694],\n",
       "         [  1.4906,  -2.3892,  -1.2056],\n",
       "         [  1.4659,  -2.3357,  -1.2500],\n",
       "         [  1.4811,  -2.3842,  -1.1828],\n",
       "         [  1.4788,  -2.3719,  -1.1939],\n",
       "         [ -0.7714,   0.2151,  -3.1041],\n",
       "         [  1.4768,  -2.3678,  -1.1909],\n",
       "         [  1.4716,  -2.3956,  -1.1342],\n",
       "         [  1.4761,  -2.3586,  -1.2103],\n",
       "         [  1.4878,  -2.3720,  -1.2139]]),\n",
       " tensor([[  1.4833,  -2.3806,  -1.1844],\n",
       "         [  1.4868,  -2.3867,  -1.1798],\n",
       "         [  1.4821,  -2.3872,  -1.1974],\n",
       "         [ -0.4997,   0.8083,  -7.2105],\n",
       "         [  1.4824,  -2.3708,  -1.2247],\n",
       "         [  0.1919,  -0.5623,  -0.9014],\n",
       "         [ -0.7806,   0.1897,  -2.2827],\n",
       "         [ -0.7806,   0.1897,  -2.2827],\n",
       "         [  1.4809,  -2.3787,  -1.2009],\n",
       "         [  1.3761,  -1.5270,  -2.8025],\n",
       "         [  1.6052,  -0.1185,  -5.6449],\n",
       "         [  1.6213,  -0.1481,  -5.6357],\n",
       "         [  1.6094,  -0.1218,  -5.7426],\n",
       "         [ -2.2996,   2.3716,  -6.7288],\n",
       "         [  1.6054,  -0.1365,  -5.6452],\n",
       "         [  1.6196,  -0.1136,  -5.6067],\n",
       "         [  1.6179,  -0.1530,  -5.5943],\n",
       "         [  1.5940,  -0.1305,  -5.5793],\n",
       "         [  1.5808,   0.0360,  -6.2207],\n",
       "         [ -4.3188,   4.1917, -13.6178],\n",
       "         [ -2.4653,   2.4937,  -6.9579],\n",
       "         [ -5.8907,   3.5423, -10.8145],\n",
       "         [  4.6983,  -0.7823, -12.7953],\n",
       "         [ -2.1484,   5.5377, -13.9762],\n",
       "         [ -0.8530,   3.9072, -18.5514],\n",
       "         [  2.5157,  -0.2974,  -8.3766],\n",
       "         [  2.6652,  -0.4521,  -8.2425],\n",
       "         [ -0.7813,   2.1177, -11.4066],\n",
       "         [  1.2321,  -0.9147,  -4.3925],\n",
       "         [  1.2285,  -0.9036,  -4.4100],\n",
       "         [  1.2319,  -0.9764,  -4.2833],\n",
       "         [  1.2333,  -0.9463,  -4.3363],\n",
       "         [  1.2305,  -0.8791,  -4.4588],\n",
       "         [ -0.3867,   1.9920, -11.7577],\n",
       "         [  1.2352,  -0.9183,  -4.3983],\n",
       "         [ -4.6359,  -0.5756, -10.0148],\n",
       "         [  1.2383,  -0.9466,  -4.3495],\n",
       "         [  1.9048,  -4.5120,   0.1270],\n",
       "         [  1.8767,  -4.4924,   0.1606],\n",
       "         [  1.8884,  -4.4785,   0.1169],\n",
       "         [  3.0342,  -3.6324,  -3.8022],\n",
       "         [  3.0260,  -3.5682,  -3.9158],\n",
       "         [  3.0370,  -3.3756,  -4.3430],\n",
       "         [  3.0364,  -3.6161,  -3.8556],\n",
       "         [  3.0328,  -3.4572,  -4.1342],\n",
       "         [  3.0272,  -3.4851,  -4.0773],\n",
       "         [  3.0455,  -3.5009,  -4.0771],\n",
       "         [  3.0321,  -3.4674,  -4.1190],\n",
       "         [  3.0273,  -3.4791,  -4.0951],\n",
       "         [  1.9557,  -1.9253,  -3.4685],\n",
       "         [  1.8880,  -1.7611,  -3.6664],\n",
       "         [  1.6875,  -0.2250,  -6.0697],\n",
       "         [ -0.7496,   2.4279, -10.6527],\n",
       "         [  1.9509,  -1.8986,  -3.5558],\n",
       "         [  1.9030,  -1.8244,  -3.6021],\n",
       "         [  1.8908,  -1.7210,  -3.7327],\n",
       "         [  1.7905,  -1.6399,  -3.7799],\n",
       "         [  2.0593,  -2.2879,  -3.2377],\n",
       "         [  2.1140,  -0.5489,  -7.5711],\n",
       "         [ -4.1714,   2.8337, -15.7120],\n",
       "         [  2.0481,  -0.3367,  -7.8293],\n",
       "         [ -1.9269,   4.2971, -17.4120],\n",
       "         [  1.8871,   0.1734,  -9.2946],\n",
       "         [  2.1072,  -0.6154,  -7.4446]]),\n",
       " tensor([[ 2.1110e+00, -5.9236e-01, -7.5042e+00],\n",
       "         [ 2.1033e+00, -6.0479e-01, -7.4338e+00],\n",
       "         [ 2.1107e+00, -5.3585e-01, -7.6072e+00],\n",
       "         [ 1.6772e+00,  4.9073e-01, -9.9966e+00],\n",
       "         [-2.0392e+00,  4.3487e+00, -1.9602e+01],\n",
       "         [-2.1806e+00,  4.2773e+00, -1.8941e+01],\n",
       "         [ 1.9915e+00,  2.0751e-01, -9.7506e+00],\n",
       "         [ 2.0900e+00, -4.8356e-01, -7.6253e+00],\n",
       "         [ 2.7372e+00, -4.0480e-01, -8.0001e+00],\n",
       "         [ 2.7438e+00, -4.0077e-01, -8.0270e+00],\n",
       "         [ 2.7348e+00, -4.1049e-01, -7.9704e+00],\n",
       "         [ 2.3461e+00, -3.2105e+00, -4.1428e+00],\n",
       "         [ 2.7516e-01, -2.2179e-03, -4.0668e+00],\n",
       "         [-3.4445e+00,  2.5840e+00, -1.3940e+01],\n",
       "         [ 1.5824e+00, -1.1635e+00, -4.5879e+00],\n",
       "         [ 7.8653e-02, -9.5100e-02, -3.8763e+00],\n",
       "         [-5.2570e+00,  2.0214e+00, -1.4432e+01],\n",
       "         [ 1.8964e+00, -1.9996e+00, -3.8867e+00],\n",
       "         [ 1.9096e+00, -2.0050e+00, -3.9103e+00],\n",
       "         [ 1.9080e+00, -2.0106e+00, -3.8995e+00],\n",
       "         [ 1.2064e+00, -4.8500e-02, -5.4436e+00],\n",
       "         [-4.8423e+00,  2.9029e+00, -1.6993e+01],\n",
       "         [ 1.3441e+00,  4.0883e-01, -1.2458e+01],\n",
       "         [ 1.2149e+00,  1.6232e-01, -7.3831e+00],\n",
       "         [ 2.0219e+00, -2.2489e+00, -3.8419e+00],\n",
       "         [ 2.4123e+00, -3.3551e+00, -3.0249e+00],\n",
       "         [ 2.4577e+00, -3.3846e+00, -3.1139e+00],\n",
       "         [ 1.6252e+00, -1.9498e+00, -3.5790e+00],\n",
       "         [ 2.0330e+00, -2.3560e+00, -3.7116e+00],\n",
       "         [ 6.2045e+00, -1.8721e+00, -1.5178e+01],\n",
       "         [ 5.4129e+00, -1.9884e+00, -1.3095e+01],\n",
       "         [ 6.0334e+00, -1.5763e+00, -1.4977e+01],\n",
       "         [ 9.0959e-01, -2.4373e+00,  8.0972e-02],\n",
       "         [ 8.8356e-01, -2.3830e+00, -1.2485e-01],\n",
       "         [ 1.3710e+00, -3.4318e-01, -4.8413e+00],\n",
       "         [ 1.3799e+00, -3.4805e-01, -4.8466e+00],\n",
       "         [ 2.1685e+00, -2.5837e+00, -4.0747e+00],\n",
       "         [ 2.1770e+00, -2.6016e+00, -4.0467e+00],\n",
       "         [ 2.1707e+00, -2.5839e+00, -4.0822e+00],\n",
       "         [ 2.1702e+00, -2.5906e+00, -4.0772e+00],\n",
       "         [ 2.1494e+00, -2.5078e+00, -4.1785e+00],\n",
       "         [ 2.1799e+00, -2.6057e+00, -4.0454e+00],\n",
       "         [ 2.1696e+00, -2.5995e+00, -4.0372e+00],\n",
       "         [ 2.1732e+00, -2.6045e+00, -4.0549e+00],\n",
       "         [ 2.1822e+00, -2.6426e+00, -3.9571e+00],\n",
       "         [ 2.1698e+00, -2.5891e+00, -4.0729e+00],\n",
       "         [ 2.1696e+00, -2.5850e+00, -4.0852e+00],\n",
       "         [ 2.1802e+00, -2.6422e+00, -3.9799e+00],\n",
       "         [ 1.8413e+00, -2.5842e-01, -7.1156e+00],\n",
       "         [ 1.8501e+00, -3.4194e-01, -6.8348e+00],\n",
       "         [ 1.4351e+00, -2.6256e-01, -6.2640e+00],\n",
       "         [ 1.4368e+00, -2.6079e-01, -6.2881e+00],\n",
       "         [ 1.4881e+00, -6.9054e-03, -7.4297e+00],\n",
       "         [ 1.4297e+00, -2.5651e-01, -6.2680e+00],\n",
       "         [ 1.8225e+00, -3.9339e+00, -6.3054e-01],\n",
       "         [ 1.8084e+00, -3.8561e+00, -7.3395e-01],\n",
       "         [ 1.8093e+00, -3.8634e+00, -7.3220e-01],\n",
       "         [ 1.8113e+00, -3.9379e+00, -6.2169e-01],\n",
       "         [ 1.7950e+00, -3.8570e+00, -6.8162e-01],\n",
       "         [ 1.1183e+00, -1.6379e+00, -4.5943e-01],\n",
       "         [ 2.6829e+00, -1.8720e+00, -3.1977e+00],\n",
       "         [ 1.8219e+00,  1.0809e+00, -1.1885e+01],\n",
       "         [ 1.5455e+00, -1.0568e+00, -5.2329e+00],\n",
       "         [ 1.5665e+00, -1.0393e+00, -5.3056e+00]]),\n",
       " tensor([[  1.5500,  -0.9954,  -5.3339],\n",
       "         [  1.5449,  -1.0197,  -5.2932],\n",
       "         [  1.5544,  -1.0044,  -5.3307],\n",
       "         [  1.2688,   0.2813,  -6.6565],\n",
       "         [ -0.7432,   2.1360,  -9.7445],\n",
       "         [ -2.5421,   2.5801, -13.4957],\n",
       "         [  1.0359,   1.4794, -11.4912],\n",
       "         [  1.1278,   1.4629, -11.8005],\n",
       "         [  1.0465,   1.5884, -12.0347],\n",
       "         [ -2.0987,   2.7202, -11.0119],\n",
       "         [ -2.4080,   1.9440, -20.3638],\n",
       "         [  1.7367,   0.4014, -10.0577],\n",
       "         [  1.7384,   0.3439,  -9.8162],\n",
       "         [  1.7157,   0.3387,  -9.4422],\n",
       "         [ -2.0325,   3.4067, -19.2887],\n",
       "         [  1.6636,   0.5463, -10.2581],\n",
       "         [  0.6405,   2.1736, -13.0875],\n",
       "         [  1.7302,   0.3664,  -9.8554],\n",
       "         [  1.7309,   0.4120, -10.0814],\n",
       "         [ -0.5099,   2.5638, -12.3661],\n",
       "         [  0.4651,   1.4828,  -9.9120],\n",
       "         [ -2.7589,   2.5408, -19.9377],\n",
       "         [  1.7297,   0.4133, -10.0440],\n",
       "         [  1.7195,   0.3685,  -9.8175],\n",
       "         [  1.7395,   0.7512, -11.4439],\n",
       "         [ -3.5119,   3.0723, -19.6651],\n",
       "         [ -3.0918,   2.7567, -19.7882],\n",
       "         [  1.7293,   0.3333,  -9.7081],\n",
       "         [ -3.6035,   3.3041, -21.0379],\n",
       "         [  1.7338,   0.4094, -10.0848],\n",
       "         [ -3.5057,   0.9849, -20.2168],\n",
       "         [ -0.7026,   2.6242, -12.2235],\n",
       "         [ -3.1727,   2.9147, -19.9163],\n",
       "         [ -2.4939,   0.0710, -19.9783],\n",
       "         [ -1.9918,  -0.1657, -19.2526],\n",
       "         [  1.7260,   0.3823,  -9.8811],\n",
       "         [  2.3309,  -2.0539,  -4.3239],\n",
       "         [  2.3206,  -2.0823,  -4.2546],\n",
       "         [  2.3267,  -2.0621,  -4.3058],\n",
       "         [  1.3431,  -6.0339,   1.9251],\n",
       "         [  1.3721,  -6.0582,   1.9045],\n",
       "         [  1.3861,  -6.0612,   1.8884],\n",
       "         [  1.3809,  -6.0573,   1.8927],\n",
       "         [  1.3903,  -6.0553,   1.8795],\n",
       "         [  1.3579,  -6.0622,   1.9242],\n",
       "         [  1.3812,  -6.0604,   1.8942],\n",
       "         [  1.3875,  -6.0589,   1.8854],\n",
       "         [  1.3801,  -6.0502,   1.8884],\n",
       "         [  1.3820,  -6.0506,   1.8872],\n",
       "         [  1.3917,  -6.0687,   1.8860],\n",
       "         [  2.7811,  -0.4933,  -7.9445],\n",
       "         [  1.4881,  -5.7662,   1.5801],\n",
       "         [  1.4757,  -5.7654,   1.5950],\n",
       "         [  1.4962,  -5.7629,   1.5676],\n",
       "         [  1.7109,  -0.1469,  -7.3219],\n",
       "         [  1.2212,   1.2368, -11.6502],\n",
       "         [  1.7004,  -0.1388,  -7.3047],\n",
       "         [  1.7062,  -0.1328,  -7.3610],\n",
       "         [  1.6996,  -0.1279,  -7.3570],\n",
       "         [ -1.0281,   2.7951, -11.9137],\n",
       "         [ -2.6730,   4.2921, -17.9208],\n",
       "         [ -1.0449,   3.7073, -16.3963],\n",
       "         [  2.3228,   0.2447, -11.3205],\n",
       "         [  0.3109,   2.0061, -11.4618]]),\n",
       " tensor([[ -2.4156,   4.3293, -18.0693],\n",
       "         [  1.9643,   0.5677, -11.2801],\n",
       "         [ -1.1699,   2.9157, -12.1977],\n",
       "         [  2.3373,   0.2530, -11.4458],\n",
       "         [ -2.0736,   3.5945, -13.6712],\n",
       "         [  2.3168,   0.2583, -11.3504],\n",
       "         [ -2.6730,   4.2921, -17.9208],\n",
       "         [  2.3627,   0.0430, -10.7043],\n",
       "         [ -2.8099,   2.1983,  -7.7163],\n",
       "         [  2.1763,  -0.3826,  -7.1548],\n",
       "         [ -2.6073,   1.9818,  -7.0633],\n",
       "         [ -2.1799,   2.1553,  -6.8465],\n",
       "         [  2.1753,  -0.3963,  -7.0882],\n",
       "         [ -5.3995,   2.0648,  -8.9531],\n",
       "         [ -2.4050,   1.9288,  -6.7101],\n",
       "         [  2.0189,  -0.3865,  -6.7784],\n",
       "         [  2.1566,  -0.3835,  -7.0663],\n",
       "         [  2.0671,  -0.4371,  -6.8074],\n",
       "         [  2.1957,  -0.3823,  -7.2305],\n",
       "         [  2.0189,  -0.3865,  -6.7784],\n",
       "         [ -0.7244,   1.6563,  -6.1982],\n",
       "         [  2.1771,  -0.4008,  -7.0895],\n",
       "         [ -1.2591,   1.9288,  -6.3886],\n",
       "         [ -2.4112,   2.2122,  -7.1593],\n",
       "         [ -1.7769,   2.0697,  -6.7165],\n",
       "         [  2.0426,  -0.3806,  -6.8570],\n",
       "         [  2.1934,  -0.4330,  -7.0656],\n",
       "         [ -0.4032,   1.4054,  -5.7267],\n",
       "         [  1.9505,  -0.0758,  -7.5555],\n",
       "         [  2.1914,  -0.4053,  -7.1198],\n",
       "         [  1.5597,   0.2098,  -6.8167],\n",
       "         [  2.0671,  -0.4371,  -6.8074],\n",
       "         [  2.1356,  -0.5566,  -6.5895],\n",
       "         [  2.1237,  -0.5576,  -6.5499],\n",
       "         [  0.0997,   2.0755,  -8.2724],\n",
       "         [  1.5487,   0.6159,  -7.4955],\n",
       "         [  2.1262,  -0.5354,  -6.6215],\n",
       "         [  2.0985,  -0.5044,  -6.5297],\n",
       "         [  2.1235,  -0.5872,  -6.4961],\n",
       "         [  2.2174,   0.2384,  -9.9456],\n",
       "         [ -4.2118,   3.8653, -22.1795],\n",
       "         [  2.9027,  -1.4212,  -6.5261],\n",
       "         [  2.8946,  -1.4206,  -6.5120],\n",
       "         [  2.8901,  -1.3420,  -6.6042],\n",
       "         [  2.2442,  -2.0553,  -4.2085],\n",
       "         [  2.0977,  -1.4491,  -4.8599],\n",
       "         [  1.9593,  -0.6573,  -6.1369],\n",
       "         [  2.2358,  -2.0821,  -4.1073],\n",
       "         [  1.8682,  -0.3323,  -6.6983],\n",
       "         [ -0.2038,   1.7378,  -7.9751],\n",
       "         [ -3.5285,   5.4427, -27.2815],\n",
       "         [  1.9260,  -2.6350,  -2.7797],\n",
       "         [  1.1318,   0.6861,  -8.2479],\n",
       "         [  1.3579,  -0.2918,  -6.3975],\n",
       "         [  2.0074,   0.4560, -10.2626],\n",
       "         [ -2.7441,  -0.3098, -10.4928],\n",
       "         [  4.1058,  -0.6387, -11.7444],\n",
       "         [  4.5256,  -0.8554, -12.3825],\n",
       "         [  4.5532,  -0.8627, -12.4781],\n",
       "         [  4.5382,  -0.8368, -12.4976],\n",
       "         [  4.5844,  -0.8650, -12.5751],\n",
       "         [  4.4970,  -0.8250, -12.3996],\n",
       "         [  1.6646,  -2.2434,  -2.0797],\n",
       "         [  1.7067,  -2.2454,  -2.1095]]),\n",
       " tensor([[  1.6687,  -2.2753,  -2.0401],\n",
       "         [  1.6737,  -2.2327,  -2.0851],\n",
       "         [  1.6697,  -2.2451,  -2.0735],\n",
       "         [  1.6740,  -2.2396,  -2.0948],\n",
       "         [  1.6901,  -2.2491,  -2.0860],\n",
       "         [  1.6718,  -2.2376,  -2.0991],\n",
       "         [  1.6746,  -2.2517,  -2.0919],\n",
       "         [ -5.3427,   1.2514,  -9.3345],\n",
       "         [ -5.4225,   3.1509,  -6.3232],\n",
       "         [  2.6741,  -1.6297,  -5.6872],\n",
       "         [  2.6249,  -1.1928,  -6.0415],\n",
       "         [ -7.0372,   5.2936, -12.8677],\n",
       "         [ -5.7309,   1.1594,  -8.6912],\n",
       "         [  2.6671,  -0.9813,  -6.7719],\n",
       "         [  2.5954,  -0.9049,  -6.7119],\n",
       "         [ -5.2517,   3.1816,  -5.8220],\n",
       "         [ -4.9820,   1.6158, -10.8508],\n",
       "         [  2.6007,  -0.8950,  -6.7585],\n",
       "         [ -5.1632,   3.3677,  -7.0369],\n",
       "         [ -5.0727,   3.7789,  -9.1571],\n",
       "         [  2.6604,  -0.9465,  -6.8331],\n",
       "         [ -5.0785,   3.6457,  -8.4835],\n",
       "         [ -5.1186,   3.6914,  -8.6657],\n",
       "         [ -5.0713,   3.4413,  -7.6380],\n",
       "         [ -5.3874,   1.3290,  -9.6241],\n",
       "         [ -0.9781,   1.9717,  -7.5724],\n",
       "         [ -5.6902,   1.3281,  -9.1635],\n",
       "         [ -6.3217,   4.5401, -13.0183],\n",
       "         [ -1.4843,   2.2520,  -7.8255],\n",
       "         [  1.9830,  -0.2543,  -6.7716],\n",
       "         [  2.2883,  -0.0846, -10.6945],\n",
       "         [  1.4565,   0.5380,  -7.2158],\n",
       "         [  2.1348,  -1.7505,  -8.6774],\n",
       "         [  2.3027,  -1.7292,  -9.2128],\n",
       "         [  2.2741,  -1.7936,  -8.8761],\n",
       "         [  2.2741,  -1.7936,  -8.8761],\n",
       "         [  2.1375,  -1.7016,  -8.8543],\n",
       "         [  1.8297,  -1.9694,  -4.7651],\n",
       "         [  1.8268,  -1.9888,  -4.7322],\n",
       "         [  1.8279,  -1.9608,  -4.7715],\n",
       "         [  0.5298,   0.7559,  -8.5660],\n",
       "         [  1.3989,   0.1987,  -5.9647],\n",
       "         [  1.1860,   0.3943,  -5.6407],\n",
       "         [  1.3554,   0.1819,  -5.7558],\n",
       "         [  1.2236,   0.2897,  -5.4312],\n",
       "         [  1.3262,   0.2416,  -5.8111],\n",
       "         [  1.3456,   0.2325,  -5.8697],\n",
       "         [  1.0337,  -2.3546,  -1.3900],\n",
       "         [ -3.8909,   3.3666, -17.8574],\n",
       "         [  1.9838,  -2.0940,  -4.8539],\n",
       "         [  1.9844,  -2.1018,  -4.8364],\n",
       "         [  1.0702,  -3.5488,   0.4130],\n",
       "         [  0.9857,  -3.5658,   0.5887],\n",
       "         [  1.4092,  -3.5822,  -0.2784],\n",
       "         [  3.2239,  -0.7172,  -9.7572],\n",
       "         [  3.2078,  -0.7090,  -9.7012],\n",
       "         [  3.2136,  -0.7341,  -9.6682],\n",
       "         [  1.5337,  -0.5947,  -5.9750],\n",
       "         [ -2.9095,   2.7505, -12.9749],\n",
       "         [ -5.8810,   3.0997, -16.0167],\n",
       "         [  0.7575,   0.8964,  -7.3502],\n",
       "         [  1.0445,   0.5888,  -7.8526],\n",
       "         [  0.6278,   0.9833,  -7.0716],\n",
       "         [  1.2805,  -1.7595,  -3.8321]]),\n",
       " tensor([[ -4.4067,   1.5063, -15.7644],\n",
       "         [ -5.1551,   0.8250, -12.6915],\n",
       "         [  1.3460,  -2.1138,  -3.4048],\n",
       "         [  1.3769,  -2.1375,  -3.5523],\n",
       "         [ -4.9678,   1.4864, -16.4639],\n",
       "         [  1.2992,   0.1990,  -7.1217],\n",
       "         [  1.3114,  -0.0238,  -6.2358],\n",
       "         [  2.1096,  -2.7470,  -3.3317],\n",
       "         [  2.1023,  -2.7445,  -3.3045],\n",
       "         [  2.1051,  -2.7420,  -3.3184],\n",
       "         [  2.1153,  -2.7631,  -3.3232],\n",
       "         [  2.1089,  -2.7554,  -3.3204],\n",
       "         [  2.1023,  -2.7484,  -3.3046],\n",
       "         [  1.6181,  -1.9613,  -3.4887],\n",
       "         [  0.2532,  -0.4446,  -3.3569],\n",
       "         [  1.7784,  -0.3180,  -6.8603],\n",
       "         [  1.7426,  -0.3042,  -6.6916],\n",
       "         [  1.7141,   0.0492,  -8.3721],\n",
       "         [  1.8048,  -0.3770,  -7.0942],\n",
       "         [ -2.7006,   3.2396, -11.4797],\n",
       "         [  1.5364,   0.1755,  -6.6874],\n",
       "         [ -0.4552,   1.9362,  -8.8654],\n",
       "         [  1.1731,  -0.4157,  -5.3773],\n",
       "         [  1.2202,  -0.5485,  -5.2544],\n",
       "         [  1.2020,  -0.5455,  -5.2299],\n",
       "         [  0.0974,   1.2984,  -7.7831],\n",
       "         [  1.0479,   0.6881,  -7.8721],\n",
       "         [ -3.7717,   2.7148, -16.0221],\n",
       "         [  0.4220,   1.3748,  -8.3759],\n",
       "         [ -2.3099,   2.9098, -11.0294],\n",
       "         [  0.0803,   1.4889,  -8.7761],\n",
       "         [  0.9963,   0.7009,  -7.6665],\n",
       "         [  2.2143,  -1.0831,  -6.6524],\n",
       "         [  0.5088,  -1.8631,  -0.7376],\n",
       "         [ -4.5809,   4.8613, -23.5010],\n",
       "         [ -4.1325,   3.9639,  -9.4103],\n",
       "         [ -4.0539,   3.8966,  -9.0357],\n",
       "         [ -4.8662,   4.3842, -10.7781],\n",
       "         [  3.0336,  -2.1237,  -7.0109],\n",
       "         [  3.0169,  -2.0654,  -7.0166],\n",
       "         [  2.6064,  -0.2384,  -9.5435],\n",
       "         [  0.1031,   1.7988,  -9.2989],\n",
       "         [ -4.3029,   4.1011,  -9.5283],\n",
       "         [ -4.9404,   4.3460, -10.9691],\n",
       "         [  3.0040,  -1.4215,  -8.1461],\n",
       "         [ -0.1278,   3.3753, -14.6915],\n",
       "         [  2.3777,   0.5507, -11.6968],\n",
       "         [ -4.6356,   3.4593, -12.2600],\n",
       "         [ -4.0796,   4.8374, -15.5790],\n",
       "         [ -4.6724,   5.3164, -15.2111],\n",
       "         [  0.8007,  -2.0848,  -3.5913],\n",
       "         [ -0.0435,  -0.4306,  -5.8880],\n",
       "         [ -6.5569,   1.4068, -12.3517],\n",
       "         [  1.2910,  -2.9109,  -2.6442],\n",
       "         [  1.3176,  -3.0140,  -2.5118],\n",
       "         [  1.2747,  -2.5847,  -3.0673],\n",
       "         [ -3.6027,   3.3067, -10.0023],\n",
       "         [  1.4596,  -3.4788,  -2.4689],\n",
       "         [  1.3303,  -3.0335,  -2.5106],\n",
       "         [  1.3536,  -3.0860,  -2.4697],\n",
       "         [  0.7932,  -2.3920,  -2.9967],\n",
       "         [  1.4503,  -3.4500,  -2.4786],\n",
       "         [  1.3986,  -3.4357,  -2.3634],\n",
       "         [  1.3086,  -3.0652,  -2.5791]]),\n",
       " tensor([[ 1.7197e+00, -9.4676e-02, -6.8472e+00],\n",
       "         [ 1.7111e+00, -1.1157e-01, -6.7483e+00],\n",
       "         [ 1.7253e+00, -9.6889e-02, -6.8594e+00],\n",
       "         [ 1.6325e+00, -1.0289e-01, -6.4377e+00],\n",
       "         [-3.2906e+00,  2.6576e+00, -9.9050e+00],\n",
       "         [ 1.4233e+00,  4.1946e-02, -6.6818e+00],\n",
       "         [-3.6111e+00,  2.6435e+00, -8.3291e+00],\n",
       "         [ 1.8628e+00, -1.9452e+00, -4.0050e+00],\n",
       "         [ 1.9270e+00, -1.8244e+00, -4.9603e+00],\n",
       "         [ 1.9242e+00, -1.8420e+00, -4.9397e+00],\n",
       "         [ 1.9235e+00, -1.7888e+00, -5.0447e+00],\n",
       "         [-2.8942e+00,  3.0809e+00, -1.0645e+01],\n",
       "         [ 1.1189e+00,  6.8334e-01, -9.4745e+00],\n",
       "         [-1.0406e+00,  2.3600e+00, -1.3475e+01],\n",
       "         [ 2.9370e-01,  1.6468e+00, -1.0831e+01],\n",
       "         [ 1.1091e+00,  6.8751e-01, -9.4535e+00],\n",
       "         [ 1.1309e+00,  6.4566e-01, -9.3667e+00],\n",
       "         [-1.6800e-01,  2.5306e+00, -1.4956e+01],\n",
       "         [-1.3712e-01,  2.2443e+00, -1.2987e+01],\n",
       "         [-1.8422e-01,  2.6001e+00, -1.5853e+01],\n",
       "         [-4.8675e-01,  2.2439e+00, -1.2883e+01],\n",
       "         [-1.7062e-03,  1.9652e+00, -1.1444e+01],\n",
       "         [-1.8277e-03,  1.8967e+00, -1.1087e+01],\n",
       "         [ 2.7920e-01,  1.6492e+00, -1.0783e+01],\n",
       "         [ 6.1650e-01,  1.1545e+00, -9.8237e+00],\n",
       "         [ 5.5077e-01,  1.2522e+00, -1.0006e+01],\n",
       "         [-2.0968e+00,  2.1248e+00, -2.3204e+01],\n",
       "         [-3.0596e-01,  2.3237e+00, -1.3325e+01],\n",
       "         [-1.2539e+00,  2.2643e+00, -1.3359e+01],\n",
       "         [-1.6024e-01,  2.5500e+00, -1.5662e+01],\n",
       "         [-2.3280e-01,  2.4727e+00, -1.5174e+01],\n",
       "         [-4.9298e-01,  2.2274e+00, -1.3138e+01],\n",
       "         [ 5.3414e-01,  1.2743e+00, -1.0046e+01],\n",
       "         [-1.4260e-01,  2.5924e+00, -1.6163e+01],\n",
       "         [ 4.7952e+00, -6.0121e-01, -1.3162e+01],\n",
       "         [-6.5446e-01,  4.0753e+00, -2.0468e+01],\n",
       "         [ 1.5779e+00, -1.2842e+00, -4.9412e+00],\n",
       "         [ 1.5769e+00,  1.7910e-01, -8.5427e+00],\n",
       "         [-7.2617e-01,  3.0071e+00, -1.2725e+01],\n",
       "         [ 1.9067e+00,  4.8919e-02, -8.1076e+00],\n",
       "         [-3.2162e+00,  2.7110e+00, -8.1495e+00],\n",
       "         [ 2.0026e+00, -9.8561e-01, -5.7706e+00],\n",
       "         [ 2.0034e+00, -9.9114e-01, -5.7188e+00],\n",
       "         [ 1.3569e+00, -1.6985e+00, -4.2404e+00],\n",
       "         [ 1.3511e+00,  1.6197e+00, -1.8622e+01],\n",
       "         [ 2.8546e+00, -1.0065e+01,  3.1972e+00],\n",
       "         [ 2.7753e+00, -1.0074e+01,  3.3067e+00],\n",
       "         [ 2.8439e+00, -1.0066e+01,  3.2107e+00],\n",
       "         [ 3.3554e+00, -6.6405e+00, -6.8675e+00],\n",
       "         [-3.1315e+00,  1.3856e+00, -4.3431e+00],\n",
       "         [ 1.1011e+00, -1.8471e+00, -1.3566e+00],\n",
       "         [ 1.1123e+00, -1.8617e+00, -1.3827e+00],\n",
       "         [-9.5077e-01, -7.1973e-01,  4.0445e-01],\n",
       "         [ 1.5435e+00, -7.7038e-01, -5.0858e+00],\n",
       "         [ 4.9409e+00, -4.7629e-01, -1.3779e+01],\n",
       "         [ 1.5177e+00,  8.7808e-01, -9.7561e+00],\n",
       "         [-3.1426e+00,  1.5373e+00, -1.6892e+01],\n",
       "         [ 1.0848e+00, -2.7031e+00, -8.7104e-01],\n",
       "         [ 1.0901e+00, -2.6928e+00, -8.9686e-01],\n",
       "         [ 1.0892e+00, -2.6942e+00, -8.9340e-01],\n",
       "         [ 1.0924e+00, -2.6818e+00, -9.1028e-01],\n",
       "         [ 1.0885e+00, -2.6712e+00, -8.9985e-01],\n",
       "         [ 1.1209e+00, -2.3092e+00, -1.2868e+00],\n",
       "         [ 1.8587e+00, -2.0723e+00, -3.8233e+00]]),\n",
       " tensor([[  1.8454,  -1.9709,  -3.9347],\n",
       "         [  1.8661,  -2.0550,  -3.8741],\n",
       "         [  1.8617,  -2.0805,  -3.8099],\n",
       "         [ -1.0430,   3.1704, -13.0475],\n",
       "         [ -6.1856,   3.2546, -12.5000],\n",
       "         [  2.1043,   0.3932, -10.7788],\n",
       "         [  1.6394,   0.6413,  -9.6682],\n",
       "         [ -6.4983,   2.7299, -14.4722],\n",
       "         [  2.1470,   0.2711, -10.4748],\n",
       "         [ -0.9160,   2.9422, -12.3342],\n",
       "         [  1.4221,   0.8973, -10.0219],\n",
       "         [ -1.0000,   3.1357, -12.9882],\n",
       "         [  2.1736,   0.2828, -10.6136],\n",
       "         [ -1.9167,   3.6171, -13.2021],\n",
       "         [  1.3197,   1.0390, -10.2792],\n",
       "         [ -4.1108,   4.4778, -16.8430],\n",
       "         [  2.1753,   0.2555, -10.5027],\n",
       "         [ -4.1108,   4.4778, -16.8430],\n",
       "         [  1.7214,  -0.9057,  -7.0253],\n",
       "         [ -1.3897,   1.7624,  -9.5833],\n",
       "         [ -3.6560,   1.8089, -10.5205],\n",
       "         [ -4.9150,   0.1890,  -9.2748],\n",
       "         [  1.6425,  -1.3050,  -4.6560],\n",
       "         [  1.6287,  -1.2831,  -4.6422],\n",
       "         [  1.6330,  -1.3245,  -4.5876],\n",
       "         [ -5.5013,  -1.4274, -10.7055],\n",
       "         [ -1.3537,  -0.3821,  -4.4939],\n",
       "         [  2.6719,  -0.4239,  -8.5423],\n",
       "         [  2.6689,  -0.4182,  -8.5714],\n",
       "         [  2.5040,  -0.2940,  -8.6689],\n",
       "         [ -1.1857,   4.2957, -16.8931],\n",
       "         [  2.7026,  -0.4066,  -8.6772],\n",
       "         [  2.6700,  -0.4199,  -8.5665],\n",
       "         [ -4.0763,   3.9376, -10.3531],\n",
       "         [  2.5000,  -1.2319,  -6.8530],\n",
       "         [ -3.6920,   3.3204, -10.3529],\n",
       "         [  0.7719,   1.4802,  -9.1250],\n",
       "         [  2.4789,  -1.0999,  -7.0440],\n",
       "         [  2.1262,  -1.5421,  -5.6607],\n",
       "         [  2.1350,  -1.5767,  -5.6096],\n",
       "         [  0.2610,  -0.6626,  -2.8687],\n",
       "         [  2.1375,  -1.6420,  -5.5133],\n",
       "         [  2.1313,  -1.4598,  -5.8066],\n",
       "         [  2.1177,  -1.5565,  -5.6180],\n",
       "         [ -1.2616,   2.2380,  -7.8151],\n",
       "         [ -4.2003,   3.7230,  -9.3280],\n",
       "         [  1.9672,  -2.0452,  -3.4813],\n",
       "         [  0.3414,  -1.0934,  -6.0285],\n",
       "         [ -5.3163,   3.2512, -10.7404],\n",
       "         [ -5.2981,   3.5439,  -8.9850],\n",
       "         [ -1.1405,   2.1480,  -8.6499],\n",
       "         [ -0.3636,   1.1070,  -6.0835],\n",
       "         [ -2.9267,   1.2453, -10.2511],\n",
       "         [ -2.9267,   1.2453, -10.2511],\n",
       "         [ -1.4499,  -1.1992, -14.9186],\n",
       "         [  1.7719,  -0.1528,  -7.6532],\n",
       "         [ -2.8541,   1.1461, -10.1956],\n",
       "         [ -2.7011,   1.2993,  -7.6806],\n",
       "         [  1.8093,  -0.2135,  -7.6628],\n",
       "         [ -2.8037,   1.0596, -10.6071],\n",
       "         [  1.6232,  -0.0307,  -7.2843],\n",
       "         [ -1.1900,  -1.4993, -13.7298],\n",
       "         [ -2.7219,   1.3160,  -7.7221],\n",
       "         [  1.9684,  -1.3510,  -5.3475]]),\n",
       " tensor([[  1.9798,  -1.3395,  -5.3996],\n",
       "         [ -2.2851,   0.8547,  -2.1160],\n",
       "         [  1.4114,   0.0470,  -7.4863],\n",
       "         [  1.2625,  -0.1651,  -5.9312],\n",
       "         [  1.7828,  -1.5612,  -4.6824],\n",
       "         [  2.7342,   0.4420, -11.3596],\n",
       "         [  2.7088,   0.4354, -11.2996],\n",
       "         [  2.7445,   0.4355, -11.3725],\n",
       "         [  1.3329,  -0.1904,  -3.4400],\n",
       "         [  1.3884,  -0.2771,  -3.4564],\n",
       "         [  1.0536,  -1.7649,  -1.9047],\n",
       "         [  1.6258,  -0.0509,  -6.9725],\n",
       "         [ -0.2494,   1.7526,  -7.4275],\n",
       "         [  1.6790,   0.0616,  -7.5714],\n",
       "         [  1.1886,   0.7468,  -8.1002],\n",
       "         [  0.5777,   2.5889, -14.5963],\n",
       "         [  2.1064,  -2.9340,  -2.7969],\n",
       "         [  2.0980,  -2.9191,  -2.7946],\n",
       "         [  2.1034,  -2.9339,  -2.7893],\n",
       "         [  2.1025,  -2.9205,  -2.8001],\n",
       "         [  1.6180,  -0.7609,  -5.6286],\n",
       "         [  1.6142,  -0.7578,  -5.6270],\n",
       "         [  1.6172,  -0.7419,  -5.6516],\n",
       "         [  1.6166,  -0.7587,  -5.6217],\n",
       "         [ -4.0495,  -0.9446,  -6.4553],\n",
       "         [ -0.2487,   0.3018,  -4.8589],\n",
       "         [  1.5701,  -2.3422,  -2.5270],\n",
       "         [ -3.0439,   0.3712,  -3.4230],\n",
       "         [  1.0558,  -1.1993,  -3.6253],\n",
       "         [  1.5743,  -2.3458,  -2.5217],\n",
       "         [  0.8181,  -0.8703,  -3.9467],\n",
       "         [  1.5760,  -2.3248,  -2.5353],\n",
       "         [ -0.7268,  -0.4626,  -3.1903],\n",
       "         [  1.4438,  -1.8282,  -3.0906],\n",
       "         [ -4.5074,  -0.2810,  -7.3852],\n",
       "         [  1.8571,  -0.9642,  -5.9005],\n",
       "         [  1.8555,  -0.9172,  -5.9605],\n",
       "         [  1.8855,  -0.9817,  -5.9799],\n",
       "         [  1.8589,  -0.8810,  -6.0296],\n",
       "         [  1.8886,  -1.0163,  -5.9284],\n",
       "         [ -2.9495,   4.4343, -14.7510],\n",
       "         [  2.0556,  -2.6828,  -2.8340],\n",
       "         [  2.0513,  -2.5806,  -2.7460],\n",
       "         [ -5.9450,   2.2902,  -9.0950],\n",
       "         [  1.9170,  -1.5069,  -5.0408],\n",
       "         [  2.0196,  -2.5568,  -2.7567],\n",
       "         [  2.0532,  -2.6060,  -2.7083],\n",
       "         [ -5.9667,   2.2201,  -8.1890],\n",
       "         [ -3.0652,  -0.2443,  -8.2787],\n",
       "         [  1.6122,   1.1954, -12.3023],\n",
       "         [  1.5131,   1.2757, -12.3988],\n",
       "         [ -6.1288,   2.2854,  -8.4661],\n",
       "         [  1.9550,  -2.5696,  -2.7533],\n",
       "         [  2.0211,  -2.5788,  -2.7200],\n",
       "         [  1.4024,   1.6916, -13.6422],\n",
       "         [  1.9104,  -1.2655,  -5.4633],\n",
       "         [ -6.1902,   2.3202,  -8.3340],\n",
       "         [  1.8558,  -1.9511,  -3.8838],\n",
       "         [  1.9187,  -1.4548,  -5.0111],\n",
       "         [  1.8690,  -1.8995,  -3.9876],\n",
       "         [  2.0243,  -2.5976,  -2.7033],\n",
       "         [  1.5527,   1.0761, -11.7359],\n",
       "         [  1.6022,   1.1226, -11.9888],\n",
       "         [  1.8171,  -0.0154,  -8.1616]]),\n",
       " tensor([[  1.9020,  -1.5117,  -4.8533],\n",
       "         [  1.3049,   1.4653, -12.2755],\n",
       "         [ -5.8752,   2.3996,  -7.6644],\n",
       "         [ -6.0829,   2.6435,  -8.9386],\n",
       "         [  1.4396,   1.6266, -13.5247],\n",
       "         [  1.5729,   1.0937, -11.8036],\n",
       "         [  1.8416,  -2.0442,  -3.6523],\n",
       "         [  1.8429,  -2.0713,  -3.6494],\n",
       "         [  1.8690,  -1.8995,  -3.9876],\n",
       "         [  2.0358,  -2.5715,  -2.7367],\n",
       "         [ -6.0193,   2.6416,  -8.9169],\n",
       "         [  1.8201,   0.3267,  -9.3535],\n",
       "         [  1.9152,  -2.0861,  -3.6592],\n",
       "         [  1.5729,   1.0937, -11.8036],\n",
       "         [ -3.0652,  -0.2443,  -8.2787],\n",
       "         [  1.8690,  -1.8995,  -3.9876],\n",
       "         [  1.9153,  -1.5086,  -4.8924],\n",
       "         [  1.9798,  -2.5974,  -2.6949],\n",
       "         [  1.8837,  -1.9364,  -4.0102],\n",
       "         [  2.0178,  -2.6363,  -2.8082],\n",
       "         [  2.0447,  -2.6749,  -2.8022],\n",
       "         [  1.7862,   0.2746,  -9.0889],\n",
       "         [  1.9232,  -1.2365,  -5.5067],\n",
       "         [ -6.3988,   2.2633,  -9.8810],\n",
       "         [  1.7895,   0.2707,  -9.1195],\n",
       "         [  1.9152,  -2.0861,  -3.6592],\n",
       "         [  1.9778,  -2.2878,  -3.2526],\n",
       "         [  1.5347,   1.1356, -11.8993],\n",
       "         [  1.7914,   0.4080,  -9.6051],\n",
       "         [  1.2173,   1.8986, -13.6607],\n",
       "         [ -1.5222,   3.8685, -14.7902],\n",
       "         [ -1.9374,   3.2443, -11.1166],\n",
       "         [  1.9928,  -0.7247,  -5.3850],\n",
       "         [  1.9218,  -0.6610,  -5.4275],\n",
       "         [  1.9578,  -0.6906,  -5.4099],\n",
       "         [  1.9682,  -0.6992,  -5.4219],\n",
       "         [  2.0214,  -0.7245,  -5.4693],\n",
       "         [  1.9455,  -0.6829,  -5.4116],\n",
       "         [  2.0063,  -0.7295,  -5.4136],\n",
       "         [  1.9656,  -0.6877,  -5.4315],\n",
       "         [  1.9647,  -0.6908,  -5.4188],\n",
       "         [  1.9498,  -0.6875,  -5.4161],\n",
       "         [ -3.8750,   4.0397, -12.3405],\n",
       "         [ -0.5150,   2.3783,  -9.7621],\n",
       "         [  1.9421,  -2.4642,  -2.9279],\n",
       "         [  1.9271,  -2.4169,  -2.9331],\n",
       "         [  1.9344,  -2.4657,  -2.9027],\n",
       "         [  1.9215,  -2.4029,  -2.9425],\n",
       "         [  1.9200,  -2.4136,  -2.9109],\n",
       "         [ -6.2252,   2.0284,  -9.4285],\n",
       "         [ -6.5142,   3.0522, -12.7155],\n",
       "         [  0.7083,  -1.5683,  -2.5705],\n",
       "         [  0.7097,  -1.4461,  -2.7812],\n",
       "         [ -1.7949,   1.1026,  -5.2189],\n",
       "         [  0.7099,  -1.5722,  -2.5778],\n",
       "         [  1.6908,  -0.4423,  -5.6229],\n",
       "         [  1.6699,  -0.3867,  -5.6575],\n",
       "         [  1.7894,  -3.3738,  -0.9699],\n",
       "         [  1.7892,  -3.3734,  -0.9596],\n",
       "         [  1.7724,  -3.3543,  -0.9464],\n",
       "         [  1.7858,  -3.3796,  -0.9506],\n",
       "         [  1.7724,  -3.3543,  -0.9464],\n",
       "         [  2.7503,  -1.8523,  -5.9398],\n",
       "         [ -4.3539,   4.0396,  -8.7205]]),\n",
       " tensor([[  2.0147,  -0.8049,  -6.8373],\n",
       "         [  1.9745,  -2.3594,  -3.4683],\n",
       "         [ -2.5139,   2.4312,  -9.2583],\n",
       "         [ -2.7580,   2.4705, -11.0889],\n",
       "         [ -2.6063,   2.4882,  -9.5997],\n",
       "         [  1.8845,  -0.4977,  -6.0345],\n",
       "         [  1.8760,  -1.8916,  -3.7535],\n",
       "         [  1.9310,  -0.8274,  -5.4743],\n",
       "         [  1.9577,  -2.3243,  -3.4515],\n",
       "         [  1.9662,  -2.3474,  -3.4546],\n",
       "         [  1.8910,  -1.0709,  -4.9791],\n",
       "         [  1.8987,  -1.1478,  -4.8461],\n",
       "         [ -2.7523,   2.4900, -10.9497],\n",
       "         [  1.9408,  -2.3173,  -3.4106],\n",
       "         [  1.9630,  -2.3276,  -3.4718],\n",
       "         [  1.9764,  -1.6682,  -4.4137],\n",
       "         [ -2.8320,   2.4472, -12.6685],\n",
       "         [  1.9645,  -2.3658,  -3.4239],\n",
       "         [  1.9478,  -2.2961,  -3.4596],\n",
       "         [  1.9416,  -2.2878,  -3.4500],\n",
       "         [  1.9632,  -2.3882,  -3.3887],\n",
       "         [  1.9442,  -2.3022,  -3.4380],\n",
       "         [  1.9559,  -2.3300,  -3.4429],\n",
       "         [  1.9666,  -2.3707,  -3.4307],\n",
       "         [ -4.6879,  -0.1193, -14.7383],\n",
       "         [ -2.2856,   2.3411,  -6.9374],\n",
       "         [  1.6750,  -0.3070,  -5.7621],\n",
       "         [  1.9209,  -1.3737,  -4.6060],\n",
       "         [  1.9658,  -2.3786,  -3.4105],\n",
       "         [  1.9017,  -0.7960,  -5.4557],\n",
       "         [  1.9595,  -2.3311,  -3.4562],\n",
       "         [  1.9478,  -2.2961,  -3.4596],\n",
       "         [  1.9234,  -1.0760,  -5.0625],\n",
       "         [  1.9463,  -2.3475,  -3.3868],\n",
       "         [ -2.7150,   2.5001,  -9.7045],\n",
       "         [  1.9587,  -2.2883,  -3.5086],\n",
       "         [  1.8526,  -0.7304,  -5.4005],\n",
       "         [  1.9626,  -2.3352,  -3.4556],\n",
       "         [  1.9194,  -1.0433,  -5.0991],\n",
       "         [  1.9590,  -2.3333,  -3.4432],\n",
       "         [  1.9669,  -2.3862,  -3.4082],\n",
       "         [  1.2933,   0.1021,  -5.5742],\n",
       "         [  3.0605,  -0.7313,  -8.0772],\n",
       "         [  3.0609,  -0.7270,  -8.0926],\n",
       "         [  3.0571,  -0.7335,  -8.0598],\n",
       "         [  3.0535,  -0.7435,  -8.0172],\n",
       "         [  1.4421,  -2.6736,  -2.2164],\n",
       "         [ -2.6080,   3.3837, -15.5037],\n",
       "         [  1.5089,  -2.9103,  -2.1378],\n",
       "         [  1.5146,  -2.8833,  -2.1868],\n",
       "         [  3.0922,  -1.0566,  -7.4219],\n",
       "         [  1.1854,  -0.2795,  -4.2447],\n",
       "         [  1.1867,  -0.2911,  -4.2129],\n",
       "         [  1.1909,  -0.2915,  -4.2116],\n",
       "         [ -1.8346,   1.7578,  -3.6171],\n",
       "         [ -2.3313,   2.2061,  -4.6074],\n",
       "         [ -2.1256,   2.5742, -14.3475],\n",
       "         [  2.2742,  -0.8622,  -8.2540],\n",
       "         [  2.2736,  -0.8329,  -8.3263],\n",
       "         [ -5.3524,   4.1460, -12.8130],\n",
       "         [  2.1520,  -0.4095,  -8.8305],\n",
       "         [  2.2679,  -0.8087,  -8.3373],\n",
       "         [ -1.0078,   2.5636, -10.0610],\n",
       "         [  1.8008,   0.3328,  -9.3364]]),\n",
       " tensor([[ 2.2658e+00, -7.8844e-01, -8.3673e+00],\n",
       "         [ 1.7950e+00, -2.0725e+00, -5.8126e+00],\n",
       "         [ 1.7323e+00, -1.9394e+00, -5.9995e+00],\n",
       "         [ 1.7323e+00, -1.9394e+00, -5.9995e+00],\n",
       "         [ 1.7763e+00, -1.9738e+00, -6.0178e+00],\n",
       "         [ 2.0907e+00, -1.4524e+00, -4.7199e+00],\n",
       "         [ 2.0898e+00, -1.4524e+00, -4.7288e+00],\n",
       "         [ 2.0922e+00, -1.4746e+00, -4.6674e+00],\n",
       "         [ 2.0973e+00, -1.4600e+00, -4.7098e+00],\n",
       "         [ 2.0917e+00, -1.4429e+00, -4.7362e+00],\n",
       "         [ 2.0954e+00, -1.4602e+00, -4.7139e+00],\n",
       "         [ 1.2247e+00, -1.2116e+00, -4.0760e+00],\n",
       "         [ 1.2282e+00, -1.2795e+00, -3.9592e+00],\n",
       "         [ 1.2205e+00, -1.2290e+00, -3.9974e+00],\n",
       "         [-4.3508e-01,  9.8274e-01, -7.5578e+00],\n",
       "         [-3.7274e+00,  2.4222e+00, -1.4080e+01],\n",
       "         [-4.8912e+00,  2.2800e+00, -1.5655e+01],\n",
       "         [ 1.0475e+00, -8.4479e-01, -5.5112e+00],\n",
       "         [ 1.0377e+00, -9.9483e-01, -5.1037e+00],\n",
       "         [ 1.1127e+00, -9.4205e-01, -5.4870e+00],\n",
       "         [ 1.0670e+00, -9.0707e-01, -5.4648e+00],\n",
       "         [ 4.0097e-01, -5.3482e-01, -1.2494e+00],\n",
       "         [ 1.7856e+00, -1.4481e-01, -7.0870e+00],\n",
       "         [ 1.9024e+00, -4.8522e-01, -5.6251e+00],\n",
       "         [ 1.9200e+00, -4.8445e-01, -5.8648e+00],\n",
       "         [ 1.9132e+00, -4.9351e-01, -5.6260e+00],\n",
       "         [ 1.7882e+00, -1.5938e-01, -6.9579e+00],\n",
       "         [ 1.4784e+00, -2.0959e-01, -6.3893e+00],\n",
       "         [-1.3190e+00,  2.9927e+00, -1.6849e+01],\n",
       "         [ 1.4848e+00, -2.1816e-01, -6.3826e+00],\n",
       "         [ 2.2076e+00, -3.4835e-03, -9.2960e+00],\n",
       "         [-6.6942e+00,  4.7803e+00, -1.7183e+01],\n",
       "         [-4.3860e+00, -4.6393e-01, -2.3053e+00],\n",
       "         [ 1.8184e+00, -1.9535e+00, -2.6591e+00],\n",
       "         [ 1.8630e+00, -2.1051e+00, -2.5743e+00],\n",
       "         [ 1.8746e+00, -2.1120e+00, -2.5884e+00],\n",
       "         [ 1.8766e+00, -2.1402e+00, -2.5606e+00],\n",
       "         [ 1.8679e+00, -2.1129e+00, -2.5875e+00],\n",
       "         [ 1.9270e+00, -5.3503e-01, -6.2337e+00],\n",
       "         [-4.1270e+00,  2.5009e+00, -9.4659e+00],\n",
       "         [ 2.1782e+00, -2.0364e+00, -4.1339e+00],\n",
       "         [ 2.1661e+00, -2.0497e+00, -4.0801e+00],\n",
       "         [-3.1728e+00,  3.1778e+00, -7.3637e+00],\n",
       "         [ 1.8900e+00,  2.0190e-02, -7.3749e+00],\n",
       "         [ 1.8986e+00,  9.5480e-03, -7.3928e+00],\n",
       "         [ 1.8859e+00,  2.4207e-02, -7.3999e+00],\n",
       "         [ 1.8897e+00,  1.5756e-02, -7.3765e+00],\n",
       "         [ 1.8900e+00,  2.0508e-02, -7.4163e+00],\n",
       "         [ 1.8891e+00,  1.3502e-02, -7.3590e+00],\n",
       "         [ 1.8336e+00,  1.6860e-01, -7.9042e+00],\n",
       "         [ 1.8956e+00, -4.9768e-03, -7.3069e+00],\n",
       "         [ 1.9517e+00, -2.0086e-01, -6.4280e+00],\n",
       "         [ 1.9708e+00, -2.0481e-01, -6.4743e+00],\n",
       "         [-3.2271e+00, -5.4471e-01, -1.0582e+00],\n",
       "         [-2.5058e-01,  8.4566e-01, -4.8041e+00],\n",
       "         [ 1.6338e+00, -2.1180e+00, -3.2292e+00],\n",
       "         [-4.8889e+00,  2.7982e+00, -2.1158e+01],\n",
       "         [ 1.6962e+00, -2.6123e+00, -2.7782e+00],\n",
       "         [-2.5058e-01,  8.4566e-01, -4.8041e+00],\n",
       "         [ 1.7016e+00, -2.6255e+00, -2.7875e+00],\n",
       "         [-2.4090e+00,  2.6318e+00, -1.7682e+01],\n",
       "         [ 1.7149e+00, -2.6799e+00, -2.7569e+00],\n",
       "         [ 1.6933e+00, -2.5974e+00, -2.7844e+00],\n",
       "         [ 1.6991e+00, -2.6481e+00, -2.7390e+00]]),\n",
       " tensor([[  1.7090,  -2.6483,  -2.7831],\n",
       "         [ -1.1594,   1.9179,  -7.5907],\n",
       "         [ -4.7680,   1.5067, -19.0063],\n",
       "         [ -0.0873,   0.8325,  -4.7416],\n",
       "         [  0.2438,   0.3270,  -4.1980],\n",
       "         [ -3.1305,   0.2578,  -7.3193],\n",
       "         [ -3.6029,   1.6278,  -5.2387],\n",
       "         [ -1.1636,  -0.9992, -12.1691],\n",
       "         [ -4.4135,   2.1565,  -4.0150],\n",
       "         [  1.9297,  -1.9126,  -3.2683],\n",
       "         [  1.9366,  -1.9919,  -3.1513],\n",
       "         [  0.5951,  -0.1575,  -2.7340],\n",
       "         [ -3.4931,   1.5450,  -5.0153],\n",
       "         [ -3.9937,   1.7981,  -6.7225],\n",
       "         [ -3.8610,   2.0047,  -6.4343],\n",
       "         [ -3.4931,   1.5450,  -5.0153],\n",
       "         [  1.6736,  -0.9341,  -3.4056],\n",
       "         [ -1.1399,  -1.3359,  -6.1543],\n",
       "         [ -4.0769,   1.8092,  -7.1206],\n",
       "         [  1.2438,  -0.9512,  -3.9026],\n",
       "         [ -2.3300,   1.4888, -17.1358],\n",
       "         [ -1.8603,   1.1239, -14.0761],\n",
       "         [  1.2611,  -0.9102,  -4.0358],\n",
       "         [ -3.8864,  -0.4700,  -9.3181],\n",
       "         [  1.3206,   0.0403,  -6.6524],\n",
       "         [  0.9555,   0.5165,  -7.3871],\n",
       "         [  1.5911,  -1.3322,  -4.0899],\n",
       "         [  0.9272,   1.5031,  -9.0040],\n",
       "         [  1.7833,   0.1857,  -7.4378],\n",
       "         [ -3.0098,   2.6139, -15.2221],\n",
       "         [  1.8206,   0.0923,  -7.2145],\n",
       "         [  1.8858,   0.0664,  -7.4477],\n",
       "         [  1.8091,   0.1388,  -7.3726],\n",
       "         [  2.0276,  -0.4586,  -7.2948],\n",
       "         [  0.5868,   0.7172,  -6.7209],\n",
       "         [  1.0834,  -0.4529,  -4.9229],\n",
       "         [  1.0863,  -0.4070,  -5.0404],\n",
       "         [  1.1742,  -1.3637,  -4.0448],\n",
       "         [  1.0545,  -0.4890,  -5.3135],\n",
       "         [  1.1759,  -1.3581,  -4.0383],\n",
       "         [  1.1645,  -1.3377,  -4.0773],\n",
       "         [  1.0205,  -0.6697,  -4.5416],\n",
       "         [  1.1811,  -1.3632,  -4.0204],\n",
       "         [  1.1740,  -1.3498,  -4.0499],\n",
       "         [ -1.6216,   0.8753,  -4.4774],\n",
       "         [  1.1357,  -1.3406,  -4.0097],\n",
       "         [  1.1796,  -1.3984,  -3.9709],\n",
       "         [  1.1698,  -1.3810,  -3.9766],\n",
       "         [  1.1491,  -1.3139,  -4.0951],\n",
       "         [  1.1117,  -1.3576,  -3.9814],\n",
       "         [  1.1567,  -1.3734,  -3.9938],\n",
       "         [  1.1669,  -1.3430,  -4.0638],\n",
       "         [  1.1718,  -1.3663,  -4.0126],\n",
       "         [  1.1097,  -0.5759,  -5.0649],\n",
       "         [  1.1731,  -1.3794,  -3.9856],\n",
       "         [  1.1993,  -1.4133,  -3.9644],\n",
       "         [  1.1396,  -1.3138,  -4.0760],\n",
       "         [  1.7069,   0.2730,  -8.6398],\n",
       "         [  2.3185,  -1.7989,  -5.3380],\n",
       "         [  2.3074,  -1.7524,  -5.3743],\n",
       "         [  1.9353,  -1.2103,  -4.4072],\n",
       "         [  1.9434,  -1.1972,  -4.4446],\n",
       "         [  1.9405,  -1.1937,  -4.4464],\n",
       "         [  1.9347,  -1.1877,  -4.4410]]),\n",
       " tensor([[  1.9360,  -1.1621,  -4.4808],\n",
       "         [  1.9297,  -1.1660,  -4.4531],\n",
       "         [  1.9472,  -1.1417,  -4.5369],\n",
       "         [  1.9191,  -1.0820,  -4.5429],\n",
       "         [  1.9408,  -1.1919,  -4.4521],\n",
       "         [  1.9349,  -1.1358,  -4.4940],\n",
       "         [  1.9371,  -1.1857,  -4.4496],\n",
       "         [  1.9340,  -1.1928,  -4.4288],\n",
       "         [  1.9300,  -1.1670,  -4.4321],\n",
       "         [  1.9293,  -1.1780,  -4.4268],\n",
       "         [  1.9395,  -1.1477,  -4.5078],\n",
       "         [  1.9387,  -1.1986,  -4.4349],\n",
       "         [  1.9374,  -1.2153,  -4.4008],\n",
       "         [  3.8736,  -0.8286, -10.4330],\n",
       "         [  0.8768,  -0.1904,  -4.0652],\n",
       "         [  0.9758,  -0.3628,  -3.9373],\n",
       "         [ -4.2199,   0.7649, -12.3447],\n",
       "         [  0.7426,   0.8466,  -7.6939],\n",
       "         [  0.6882,   0.8347,  -7.4606],\n",
       "         [  0.5971,   1.6053, -10.3412],\n",
       "         [  1.5823,  -0.4198,  -6.0393],\n",
       "         [  1.0725,  -2.0435,  -2.6335],\n",
       "         [  1.0341,  -1.7719,  -3.0090],\n",
       "         [  1.0750,  -2.0085,  -2.7103],\n",
       "         [ -6.1009,   1.8311,  -9.8907],\n",
       "         [ -0.2098,   1.3220,  -6.9829],\n",
       "         [ -2.4859,   3.4851, -13.4158],\n",
       "         [ -6.6409,   2.9721, -15.4725],\n",
       "         [ -0.5371,   2.1471,  -9.8699],\n",
       "         [  1.2601,   0.4986,  -7.9945],\n",
       "         [ -0.2574,   1.3062,  -6.7246],\n",
       "         [ -1.0766,   2.7042, -11.2512],\n",
       "         [  1.9333,  -0.9795,  -5.9910],\n",
       "         [ -1.2624,   2.8736, -11.7499],\n",
       "         [  1.6668,   1.0244, -11.3578],\n",
       "         [  1.9038,   0.4821,  -9.4953],\n",
       "         [  1.3044,   0.1535,  -6.5946],\n",
       "         [  1.1764,   0.0946,  -5.6559],\n",
       "         [  1.6995,  -0.5259,  -5.1961],\n",
       "         [  1.7095,  -0.5435,  -5.1754],\n",
       "         [  1.9036,  -0.9039,  -6.3017],\n",
       "         [  2.0012,  -2.8390,  -2.2360],\n",
       "         [  2.0529,  -3.2416,  -2.1195],\n",
       "         [ -1.1825,  -1.4329,   1.7172],\n",
       "         [  1.9444,  -0.0892,  -7.9575],\n",
       "         [  1.5077,   0.2822,  -9.2235],\n",
       "         [  1.5242,   0.1963,  -8.9254],\n",
       "         [  1.4545,   0.3695,  -9.4202],\n",
       "         [ -6.0292,   1.8110, -15.6042],\n",
       "         [ -2.9914,   4.1428, -18.5495],\n",
       "         [  1.5365,   0.2393,  -9.1298],\n",
       "         [  1.4812,   0.3480,  -9.4532],\n",
       "         [  1.8687,  -0.5331,  -5.6353],\n",
       "         [ -3.4805,   2.9382, -11.9858],\n",
       "         [ -2.0932,   3.5860, -13.9835],\n",
       "         [  1.8766,  -0.5371,  -5.6694],\n",
       "         [  2.5426,  -0.1707, -10.0375],\n",
       "         [  2.5718,  -0.2966,  -9.7430],\n",
       "         [  2.5732,  -0.2855,  -9.7825],\n",
       "         [  2.5843,  -0.7403,  -6.6266],\n",
       "         [  2.7201,  -1.4710,  -5.5613],\n",
       "         [  2.5986,  -0.7553,  -6.6421],\n",
       "         [ -3.9845,   1.4330,  -9.9950],\n",
       "         [  0.6269,   1.0253,  -7.6709]]),\n",
       " tensor([[ -3.0123,  -0.8836,  -4.1054],\n",
       "         [  0.3656,   1.2476,  -7.8822],\n",
       "         [  0.1888,   1.7108, -10.1980],\n",
       "         [ -1.8063,   2.2150, -11.7109],\n",
       "         [ -0.5410,   1.6428,  -7.3466],\n",
       "         [  1.4995,  -3.2072,  -0.5227],\n",
       "         [  1.5203,  -3.2566,  -0.5362],\n",
       "         [  1.5214,  -3.2607,  -0.5232],\n",
       "         [  1.5271,  -3.2648,  -0.5431],\n",
       "         [  1.5344,  -3.2682,  -0.5572],\n",
       "         [  1.5160,  -3.2520,  -0.5178],\n",
       "         [  1.5241,  -3.2677,  -0.5274],\n",
       "         [  1.5198,  -3.2497,  -0.5358],\n",
       "         [  2.4105,  -0.5827,  -6.9876],\n",
       "         [ -0.2015,   0.8900,  -3.7781],\n",
       "         [  1.0492,  -2.5682,  -1.0733],\n",
       "         [  1.0583,  -2.5657,  -1.0866],\n",
       "         [  1.0577,  -2.5686,  -1.1044],\n",
       "         [  1.0652,  -2.5548,  -1.1285],\n",
       "         [  1.0618,  -2.5656,  -1.1158],\n",
       "         [  1.0503,  -2.5749,  -1.0775],\n",
       "         [  1.0617,  -2.5772,  -1.1205],\n",
       "         [  1.0602,  -2.5648,  -1.1035],\n",
       "         [  1.0547,  -2.5774,  -1.0822],\n",
       "         [  1.0590,  -2.5701,  -1.1168],\n",
       "         [  1.0548,  -2.5651,  -1.0944],\n",
       "         [  1.0536,  -2.5734,  -1.0808],\n",
       "         [  1.0588,  -2.5747,  -1.0911],\n",
       "         [  1.8364,  -0.6832,  -6.0774],\n",
       "         [ -4.4968,   2.1383, -10.4596],\n",
       "         [  1.0385,   0.5673,  -7.5736],\n",
       "         [ -4.4337,   2.0786, -10.7544],\n",
       "         [ -4.4337,   2.0786, -10.7544],\n",
       "         [  1.1571,   0.5327,  -7.7746],\n",
       "         [  0.8461,   1.6490, -10.0863],\n",
       "         [  2.4332,  -0.5545,  -8.6066],\n",
       "         [  0.7346,   1.6913,  -9.8760],\n",
       "         [  2.4386,  -0.5613,  -8.5759],\n",
       "         [ -2.2021,   3.7275, -11.3541],\n",
       "         [  2.1378,   0.6653, -11.0211],\n",
       "         [  2.4368,  -0.5278,  -8.6934],\n",
       "         [  2.1138,   0.6566, -10.9809],\n",
       "         [ -2.5003,   4.7741, -15.8789],\n",
       "         [ -1.9721,   4.2585, -14.5103],\n",
       "         [  1.5180,   1.1645, -10.4541],\n",
       "         [  1.5009,   1.1373, -10.3048],\n",
       "         [  2.4589,  -0.5635,  -8.6149],\n",
       "         [  2.4550,  -0.5541,  -8.6554],\n",
       "         [  1.6289,  -2.1852,  -2.7384],\n",
       "         [  1.9313,   0.5811,  -9.6471],\n",
       "         [  1.1854,   1.5180,  -9.6911],\n",
       "         [ -0.3034,   3.4395, -14.3967],\n",
       "         [  1.7871,   0.9165, -10.0842],\n",
       "         [  2.7513,  -1.5181,  -5.8214],\n",
       "         [  1.7291,  -3.6303,  -0.6212],\n",
       "         [  1.7303,  -3.6448,  -0.6161],\n",
       "         [  1.7292,  -3.6281,  -0.6307],\n",
       "         [  1.5372,  -1.3455,  -3.9133],\n",
       "         [ -3.4019,   4.6935, -13.9333],\n",
       "         [ -3.3693,   4.5529, -11.8241],\n",
       "         [ -3.5373,   4.8474, -14.2425],\n",
       "         [ -5.7245,   2.9294,  -7.6419],\n",
       "         [  1.3824,  -0.6864,  -5.5470],\n",
       "         [  1.9177,  -0.9067,  -4.2676]]),\n",
       " tensor([[ 1.5742e+00, -1.2888e+00, -3.8851e+00],\n",
       "         [ 1.8596e+00,  1.3568e+00, -1.2020e+01],\n",
       "         [-4.3846e+00,  4.9822e+00, -1.6872e+01],\n",
       "         [ 1.3924e+00, -3.0245e+00, -1.1262e+00],\n",
       "         [-2.5348e-01,  4.4380e-02, -4.8419e+00],\n",
       "         [ 1.3743e+00, -3.0389e+00, -1.0337e+00],\n",
       "         [ 1.4091e+00, -7.2130e-02, -5.7414e+00],\n",
       "         [ 1.4483e+00, -1.4468e-01, -5.5961e+00],\n",
       "         [ 1.4318e+00, -1.0807e-01, -5.6537e+00],\n",
       "         [ 1.4437e+00, -1.3688e-01, -5.5937e+00],\n",
       "         [ 9.0911e-01, -3.2396e+00,  2.4365e-01],\n",
       "         [ 9.1150e-01, -3.2424e+00,  2.4037e-01],\n",
       "         [ 9.1038e-01, -3.2455e+00,  2.4786e-01],\n",
       "         [ 2.1534e+00,  1.3216e-01, -9.6744e+00],\n",
       "         [ 1.1312e+00,  7.5705e-01, -8.6310e+00],\n",
       "         [-1.2429e+00,  2.8066e+00, -1.0932e+01],\n",
       "         [-8.7949e-01,  2.6111e+00, -1.0810e+01],\n",
       "         [ 1.1654e+00,  7.3950e-01, -8.6104e+00],\n",
       "         [-2.3143e+00,  3.0790e+00, -1.2606e+01],\n",
       "         [-5.7154e+00, -1.8810e+00, -2.8461e+00],\n",
       "         [ 1.1269e+00,  6.9217e-01, -8.1765e+00],\n",
       "         [ 2.2984e+00, -1.0918e+00, -5.7097e+00],\n",
       "         [ 2.2771e+00, -1.1225e+00, -5.5917e+00],\n",
       "         [ 2.2890e+00, -1.1102e+00, -5.6628e+00],\n",
       "         [ 2.2790e+00, -1.0678e+00, -5.6867e+00],\n",
       "         [ 2.2557e+00, -1.1780e+00, -5.4509e+00],\n",
       "         [ 2.5679e+00,  2.1905e-01, -8.7340e+00],\n",
       "         [ 1.9712e-01,  2.6039e+00, -1.1756e+01],\n",
       "         [ 2.8038e+00,  1.7512e-01, -9.4256e+00],\n",
       "         [ 2.7156e+00,  1.0124e-01, -9.0814e+00],\n",
       "         [ 2.6476e+00, -1.0976e+00, -7.3070e+00],\n",
       "         [ 2.6480e+00, -1.0982e+00, -7.2971e+00],\n",
       "         [ 2.9942e-01,  1.1660e+00, -6.1279e+00],\n",
       "         [-3.0800e+00,  2.4765e+00, -2.2148e+01],\n",
       "         [-3.5116e+00,  2.5073e+00, -2.2668e+01],\n",
       "         [-5.2595e-01,  1.4729e+00, -6.3888e+00],\n",
       "         [ 2.1874e+00, -5.5529e-01, -7.0346e+00],\n",
       "         [ 2.1908e+00, -5.4401e-01, -7.0847e+00],\n",
       "         [ 2.1860e+00, -5.4273e-01, -7.0661e+00],\n",
       "         [ 1.4507e-01,  1.6885e+00, -1.0874e+01],\n",
       "         [ 1.4969e+00,  2.8966e-01, -6.8680e+00],\n",
       "         [-5.1362e-01,  1.9695e+00, -1.2899e+01],\n",
       "         [ 1.6713e-02,  1.2653e+00, -6.1130e+00],\n",
       "         [ 2.1771e+00, -5.7359e-01, -6.9321e+00],\n",
       "         [-4.0094e-01,  1.5295e+00, -6.9954e+00],\n",
       "         [-2.4189e+00, -1.2892e+00, -7.4892e+00],\n",
       "         [-2.0975e+00, -1.5456e+00, -6.7696e+00],\n",
       "         [ 1.6713e-02,  1.2653e+00, -6.1130e+00],\n",
       "         [ 2.1753e+00, -5.5807e-01, -6.9850e+00],\n",
       "         [ 2.1856e+00, -5.4938e-01, -7.0447e+00],\n",
       "         [ 2.1915e+00, -5.4647e-01, -7.0734e+00],\n",
       "         [ 2.1728e+00, -5.6491e-01, -6.9566e+00],\n",
       "         [ 1.0869e-01,  1.7150e+00, -1.1400e+01],\n",
       "         [-4.0259e+00,  2.5204e+00, -2.3059e+01],\n",
       "         [-5.6070e+00,  1.1491e+00, -7.5529e+00],\n",
       "         [ 1.3095e-01,  1.7156e+00, -1.1192e+01],\n",
       "         [ 2.0198e+00, -8.5678e-01, -6.3240e+00],\n",
       "         [ 2.0827e+00, -8.4191e-01, -6.3794e+00],\n",
       "         [ 2.0993e+00, -8.0295e-01, -6.4772e+00],\n",
       "         [ 9.2393e-01, -1.5800e+00, -3.4742e+00],\n",
       "         [ 9.2443e-01, -1.5920e+00, -3.4531e+00],\n",
       "         [ 9.1563e-01, -1.5471e+00, -3.5105e+00],\n",
       "         [ 9.1190e-01, -1.6095e+00, -3.3934e+00],\n",
       "         [ 9.2138e-01, -1.5460e+00, -3.5291e+00]]),\n",
       " tensor([[  2.6055,  -2.0268,  -5.2777],\n",
       "         [  2.5944,  -1.3292,  -6.6054],\n",
       "         [  2.5703,  -1.2564,  -6.3123],\n",
       "         [  2.5997,  -1.9999,  -5.2983],\n",
       "         [  2.6031,  -2.0253,  -5.2744],\n",
       "         [  1.5580,   0.3035,  -8.2554],\n",
       "         [  1.5553,   0.2815,  -8.1489],\n",
       "         [  1.5277,   0.3281,  -8.2442],\n",
       "         [  1.5608,   0.2800,  -8.1812],\n",
       "         [  1.5077,   0.9049, -10.6728],\n",
       "         [  2.4078,  -0.2580,  -9.5582],\n",
       "         [ -4.7018,   3.2024, -15.8913],\n",
       "         [  1.6469,  -0.0379,  -8.4759],\n",
       "         [  1.6473,  -0.0253,  -8.5343],\n",
       "         [  1.6501,  -0.0438,  -8.4605],\n",
       "         [  1.5066,  -0.6802,  -5.0871],\n",
       "         [  1.5107,  -0.7066,  -5.0396],\n",
       "         [  1.5109,  -0.6858,  -5.0826],\n",
       "         [  0.5404,   1.0704,  -7.7563],\n",
       "         [ -2.1866,   2.4539, -16.7145],\n",
       "         [  1.2120,  -0.1369,  -6.4914],\n",
       "         [  1.3579,  -1.7338,  -3.9382],\n",
       "         [  0.1176,   1.4291,  -8.2597],\n",
       "         [  1.3561,  -1.6978,  -4.0082],\n",
       "         [  1.3452,  -1.6916,  -3.9821],\n",
       "         [  1.3520,  -1.6611,  -4.0634],\n",
       "         [  1.2672,  -0.5429,  -5.9247],\n",
       "         [  1.3511,  -1.7195,  -3.9462],\n",
       "         [  1.3428,  -1.6965,  -3.9508],\n",
       "         [  1.3447,  -1.6667,  -4.0206],\n",
       "         [  1.3509,  -1.6909,  -4.0005],\n",
       "         [  1.3424,  -1.7472,  -3.8599],\n",
       "         [  1.3315,  -1.7865,  -3.7092],\n",
       "         [  1.3484,  -1.6978,  -3.9783],\n",
       "         [  1.3419,  -1.6423,  -4.0659],\n",
       "         [  1.3449,  -1.6716,  -4.0132],\n",
       "         [  1.3482,  -1.6935,  -3.9824],\n",
       "         [  1.3368,  -1.5230,  -4.2738],\n",
       "         [  1.3463,  -1.6997,  -3.9665],\n",
       "         [  1.3489,  -1.7410,  -3.8929],\n",
       "         [  1.3483,  -1.7150,  -3.9424],\n",
       "         [  1.3457,  -1.6898,  -3.9816],\n",
       "         [  1.3493,  -1.7481,  -3.8731],\n",
       "         [  1.3399,  -1.7209,  -3.9027],\n",
       "         [  1.3470,  -1.7177,  -3.9375],\n",
       "         [  2.7421,  -0.9453,  -7.0170],\n",
       "         [  0.1975,   3.0198, -15.0852],\n",
       "         [  2.0300,   0.2598,  -9.8946],\n",
       "         [  2.7575,  -0.8056,  -7.1482],\n",
       "         [  2.7436,  -0.9045,  -7.0937],\n",
       "         [ -5.5983,   1.7900,  -8.8180],\n",
       "         [  1.7356,   1.0811, -12.3659],\n",
       "         [  2.7492,  -0.9328,  -7.0475],\n",
       "         [  3.6364,  -0.6778, -10.8930],\n",
       "         [  3.5613,  -0.4920, -11.1890],\n",
       "         [  3.7988,  -1.0242, -10.0332],\n",
       "         [  3.4185,  -0.0514, -11.8984],\n",
       "         [  3.8138,  -1.0418,  -9.8269],\n",
       "         [  3.8208,  -1.0397,  -9.9207],\n",
       "         [  3.7792,  -1.0060, -10.0456],\n",
       "         [  3.7903,  -0.9189, -10.3765],\n",
       "         [ -2.3603,   3.5391, -19.4887],\n",
       "         [  3.7273,  -0.9770,  -9.7424],\n",
       "         [ -2.1743,   4.3013, -18.7032]]),\n",
       " tensor([[ 3.8406e+00, -1.0358e+00, -1.0066e+01],\n",
       "         [ 3.5440e+00, -4.5685e-01, -1.1249e+01],\n",
       "         [ 3.8512e+00, -1.1053e+00, -9.7041e+00],\n",
       "         [ 3.8304e+00, -1.2570e+00, -9.2189e+00],\n",
       "         [ 3.8344e+00, -1.0368e+00, -9.9880e+00],\n",
       "         [ 3.8396e+00, -1.2097e+00, -9.4037e+00],\n",
       "         [ 3.8384e+00, -1.2165e+00, -9.3806e+00],\n",
       "         [-3.8259e+00,  7.7348e-01, -7.6851e+00],\n",
       "         [ 3.8343e+00, -1.0220e+00, -1.0048e+01],\n",
       "         [ 3.5302e+00,  1.5386e-02, -1.2726e+01],\n",
       "         [ 1.6137e+00,  1.7842e+00, -1.1269e+01],\n",
       "         [ 3.7849e+00, -9.4020e-01, -1.0237e+01],\n",
       "         [ 3.8355e+00, -1.2508e+00, -9.2501e+00],\n",
       "         [ 3.7823e+00, -9.1866e-01, -1.0358e+01],\n",
       "         [ 3.7208e+00, -7.7420e-01, -1.0760e+01],\n",
       "         [ 2.7244e+00,  4.1558e-01, -1.0482e+01],\n",
       "         [-2.6781e+00,  3.3351e+00, -1.9289e+01],\n",
       "         [ 3.8494e+00, -1.0890e+00, -9.7497e+00],\n",
       "         [ 3.7168e+00, -7.6293e-01, -1.0779e+01],\n",
       "         [ 3.8061e+00, -9.5865e-01, -1.0297e+01],\n",
       "         [ 3.8483e+00, -1.1287e+00, -9.6916e+00],\n",
       "         [ 3.5792e+00, -5.4958e-01, -1.1077e+01],\n",
       "         [ 3.7717e+00, -9.7775e-01, -9.9051e+00],\n",
       "         [-2.4070e+00,  4.2029e+00, -1.6137e+01],\n",
       "         [ 2.6129e+00, -2.9428e+00, -4.0412e+00],\n",
       "         [ 2.6388e+00, -2.8514e+00, -4.3014e+00],\n",
       "         [ 1.7652e+00, -7.1538e-01, -6.6256e+00],\n",
       "         [ 2.0900e+00, -4.9318e-01, -7.4829e+00],\n",
       "         [-2.3305e+00,  1.8645e+00, -1.0880e+01],\n",
       "         [-2.4531e+00,  1.7746e+00, -1.0934e+01],\n",
       "         [-4.7360e-01,  2.2513e+00, -1.1214e+01],\n",
       "         [ 1.3218e+00,  4.8409e-01, -8.1357e+00],\n",
       "         [-4.0123e+00,  6.2925e-01, -6.4078e+00],\n",
       "         [ 2.6225e+00, -1.9901e+00, -3.4405e+00],\n",
       "         [ 2.6361e+00, -2.0652e+00, -3.3796e+00],\n",
       "         [ 2.6361e+00, -2.0593e+00, -3.3978e+00],\n",
       "         [ 7.5977e-01, -1.1093e+00, -3.3878e-01],\n",
       "         [-2.9577e+00, -1.7900e-01, -3.8151e+00],\n",
       "         [ 2.9540e+00, -1.6987e+00, -4.6838e+00],\n",
       "         [ 2.9750e+00, -1.6648e+00, -4.7772e+00],\n",
       "         [ 2.9470e+00, -1.6533e+00, -4.7346e+00],\n",
       "         [ 2.9676e+00, -1.6584e+00, -4.7625e+00],\n",
       "         [ 1.9657e+00, -2.4801e+00, -3.2522e+00],\n",
       "         [ 1.9893e+00, -2.6199e+00, -3.1453e+00],\n",
       "         [-2.3024e-01, -1.4539e+00,  2.0384e-01],\n",
       "         [ 1.6148e+00, -2.2141e+00, -2.4033e+00],\n",
       "         [ 7.8492e-01, -1.6127e+00, -1.3883e+00],\n",
       "         [-3.0974e+00, -1.9662e+00, -4.6329e+00],\n",
       "         [ 1.9323e+00, -2.6188e+00, -2.9636e+00],\n",
       "         [-5.8364e+00,  3.6659e-01, -3.3609e+00],\n",
       "         [-3.7240e+00, -1.8296e+00, -3.8641e+00],\n",
       "         [-1.0265e+00, -2.1427e+00,  2.1784e+00],\n",
       "         [ 1.9233e+00, -2.3826e+00, -2.7435e+00],\n",
       "         [ 1.6787e+00, -1.5657e+00, -4.9763e+00],\n",
       "         [ 1.7032e+00, -1.4289e+00, -5.3500e+00],\n",
       "         [ 1.6974e+00, -1.0387e+00, -5.9435e+00],\n",
       "         [-9.4932e-01,  2.6406e+00, -1.1356e+01],\n",
       "         [ 1.6880e+00, -1.5796e+00, -5.0241e+00],\n",
       "         [-3.5002e+00,  4.7519e+00, -1.6768e+01],\n",
       "         [ 1.6851e+00, -1.5679e+00, -5.0207e+00],\n",
       "         [-4.0263e+00,  5.3751e-01, -5.5580e+00],\n",
       "         [-4.1197e+00,  5.6789e-01, -5.5368e+00],\n",
       "         [ 7.9298e-01, -6.8353e-01, -3.8800e+00],\n",
       "         [ 7.5800e-01, -3.4401e-01, -4.1467e+00]]),\n",
       " tensor([[ 6.3539e-01, -2.6487e-01, -4.1758e+00],\n",
       "         [ 1.8706e+00, -8.5291e-01, -5.5586e+00],\n",
       "         [ 1.1924e+00, -2.3106e-01, -5.2263e+00],\n",
       "         [ 1.1883e+00, -8.4801e-01, -4.4757e+00],\n",
       "         [-4.3517e+00,  4.0931e+00, -1.7563e+01],\n",
       "         [ 9.8011e-01,  1.4011e-01, -5.6886e+00],\n",
       "         [ 1.2060e+00, -5.8840e-01, -4.9550e+00],\n",
       "         [ 1.2066e+00, -2.3632e-01, -5.3081e+00],\n",
       "         [ 1.1777e+00, -7.8920e-01, -4.5024e+00],\n",
       "         [ 1.1808e+00, -8.6821e-01, -4.4485e+00],\n",
       "         [ 1.1875e+00, -8.6436e-01, -4.4679e+00],\n",
       "         [ 1.1864e+00, -8.6222e-01, -4.4638e+00],\n",
       "         [ 1.1968e+00, -6.4419e-01, -4.8508e+00],\n",
       "         [ 1.1893e+00, -8.6467e-01, -4.4772e+00],\n",
       "         [ 1.1865e+00, -8.6323e-01, -4.4548e+00],\n",
       "         [ 1.1807e+00, -2.3158e-01, -5.1889e+00],\n",
       "         [ 1.0485e+00, -1.0897e-02, -5.3691e+00],\n",
       "         [ 1.1863e+00, -8.6305e-01, -4.4555e+00],\n",
       "         [ 1.1862e+00, -8.6392e-01, -4.4525e+00],\n",
       "         [ 1.1820e+00, -8.7864e-01, -4.4278e+00],\n",
       "         [ 1.1826e+00, -8.6625e-01, -4.4234e+00],\n",
       "         [ 1.2129e+00, -5.2411e-01, -5.0762e+00],\n",
       "         [ 1.1926e+00, -8.3129e-01, -4.5269e+00],\n",
       "         [-2.4639e+00,  2.2449e+00, -1.3429e+01],\n",
       "         [ 1.2010e+00, -5.5077e-01, -4.9772e+00],\n",
       "         [ 1.1878e+00, -8.7718e-01, -4.4479e+00],\n",
       "         [ 1.1868e+00, -8.5793e-01, -4.4592e+00],\n",
       "         [ 1.2105e+00, -5.2715e-01, -5.0448e+00],\n",
       "         [ 1.1878e+00, -8.3565e-01, -4.4953e+00],\n",
       "         [ 1.1848e+00, -8.6000e-01, -4.4517e+00],\n",
       "         [ 1.1895e+00, -8.5333e-01, -4.4797e+00],\n",
       "         [ 1.1868e+00, -8.6033e-01, -4.4643e+00],\n",
       "         [ 1.1461e+00, -1.3833e-01, -5.3821e+00],\n",
       "         [-1.6114e+00,  2.0045e+00, -1.0814e+01],\n",
       "         [ 1.2112e+00, -2.3015e-01, -5.3431e+00],\n",
       "         [-4.4227e+00,  3.7621e+00, -1.6310e+01],\n",
       "         [ 1.2069e+00, -2.1413e-01, -5.4302e+00],\n",
       "         [ 1.2181e+00, -2.2081e-01, -5.4173e+00],\n",
       "         [ 1.0795e+00, -1.7361e-02, -5.5024e+00],\n",
       "         [ 1.2092e+00, -2.1670e-01, -5.4266e+00],\n",
       "         [ 1.2102e+00, -2.3716e-01, -5.3581e+00],\n",
       "         [-1.5043e+00,  1.9491e+00, -1.0641e+01],\n",
       "         [ 1.1884e+00, -8.5602e-01, -4.4870e+00],\n",
       "         [ 3.0724e+00, -7.8497e-01, -8.9796e+00],\n",
       "         [ 2.2236e+00,  4.9458e-01, -1.0071e+01],\n",
       "         [ 3.0395e+00, -7.9768e-01, -8.8637e+00],\n",
       "         [ 3.0234e+00, -7.7803e-01, -8.8581e+00],\n",
       "         [ 3.0414e+00, -7.9036e-01, -8.8825e+00],\n",
       "         [ 3.0181e+00, -7.7929e-01, -8.8391e+00],\n",
       "         [ 1.7928e+00, -6.6992e-01, -4.1569e+00],\n",
       "         [ 1.3310e+00,  4.3612e-01, -8.3951e+00],\n",
       "         [ 1.3698e+00,  4.3736e-01, -8.5124e+00],\n",
       "         [ 1.2125e+00,  5.8968e-01, -8.6127e+00],\n",
       "         [ 1.7600e+00,  2.8280e-01, -8.8406e+00],\n",
       "         [-2.8739e+00,  4.0216e+00, -1.9600e+01],\n",
       "         [ 1.7595e+00,  3.1803e-01, -9.0029e+00],\n",
       "         [ 1.7848e+00,  1.1878e-01, -8.2451e+00],\n",
       "         [ 1.7529e+00,  5.7629e-01, -1.0041e+01],\n",
       "         [ 1.6265e+00,  1.5023e+00, -1.3335e+01],\n",
       "         [ 1.7593e+00,  6.2403e-01, -1.0266e+01],\n",
       "         [-2.2534e+00,  2.9350e+00, -1.1231e+01],\n",
       "         [-5.5442e+00,  2.1973e+00, -1.5377e+01],\n",
       "         [ 8.9947e-01,  2.0276e+00, -1.3584e+01],\n",
       "         [-9.1789e-01,  3.6294e+00, -1.5939e+01]]),\n",
       " tensor([[ -2.4580,   4.9825, -22.2099],\n",
       "         [  1.6803,   1.0925, -12.2228],\n",
       "         [ -0.6877,   3.6645, -16.5748],\n",
       "         [  1.8138,   0.9182, -12.0481],\n",
       "         [  0.5352,   2.4860, -14.2272],\n",
       "         [ -3.5519,   5.2033, -24.9227],\n",
       "         [  0.7708,   2.1861, -13.8516],\n",
       "         [  0.3291,   2.6600, -14.1130],\n",
       "         [ -2.2860,   5.1451, -20.6905],\n",
       "         [ -2.9535,   5.4504, -24.7054],\n",
       "         [ -2.1723,   5.1545, -22.8889],\n",
       "         [ -5.5536,   3.6955, -24.1511],\n",
       "         [ -3.2552,  -0.3448, -11.9685],\n",
       "         [ -7.3777,   1.5494,  -5.0707],\n",
       "         [  1.6772,   0.4828,  -8.6067],\n",
       "         [  1.6584,   0.4731,  -8.5449],\n",
       "         [ -0.0621,   1.5696,  -7.7020],\n",
       "         [ -0.9347,   2.6901,  -9.4282],\n",
       "         [ -4.6737,   3.4546,  -6.9818],\n",
       "         [ -7.2690,   1.8209,  -5.4272],\n",
       "         [  2.2704,  -2.3191,  -4.6518],\n",
       "         [  2.2796,  -2.2937,  -4.7233],\n",
       "         [  2.2711,  -2.3238,  -4.6510],\n",
       "         [  1.8907,  -0.0908,  -7.6937],\n",
       "         [  1.9953,  -3.1122,  -2.3272],\n",
       "         [  1.9958,  -3.0734,  -2.3874],\n",
       "         [  1.4785,  -1.0335,  -5.5033],\n",
       "         [  1.9992,  -3.1152,  -2.3378],\n",
       "         [  1.9908,  -3.1062,  -2.3456],\n",
       "         [  2.0008,  -3.2101,  -2.1920],\n",
       "         [  1.5154,   0.4454,  -7.6969],\n",
       "         [ -2.2358,  -6.3732,  -1.8079],\n",
       "         [  1.9907,  -3.0778,  -2.3631],\n",
       "         [  1.6333,  -1.3778,  -4.9766],\n",
       "         [  1.9909,  -3.0972,  -2.3514],\n",
       "         [  1.9942,  -3.1206,  -2.3275],\n",
       "         [  1.9885,  -3.0325,  -2.4545],\n",
       "         [ -0.3104,   0.4550,  -7.1766],\n",
       "         [ -3.1518,   4.0657, -18.6472],\n",
       "         [  1.6822,   0.5398, -10.1120],\n",
       "         [  1.7421,   0.3184,  -9.4943],\n",
       "         [  1.7757,   0.0650,  -8.6723],\n",
       "         [ -0.9753,   3.7219, -16.2050],\n",
       "         [ -2.5404,   1.8165, -17.8284],\n",
       "         [ -4.9633,   1.6945, -11.3907],\n",
       "         [ -0.0390,   1.3200,  -5.9312],\n",
       "         [  1.9023,  -0.0448,  -6.8045],\n",
       "         [  2.2197,  -3.7180,  -1.9706],\n",
       "         [  1.2964,  -2.5248,  -1.5485],\n",
       "         [  1.2532,  -2.4950,  -1.5027],\n",
       "         [  1.0846,   0.5678,  -8.5060],\n",
       "         [  0.8375,   1.2407, -10.8434],\n",
       "         [  2.5690,  -3.1234,  -3.4767],\n",
       "         [  2.0068,  -4.3226,  -0.6394],\n",
       "         [  1.9892,  -4.3494,  -0.5710],\n",
       "         [  1.9760,  -4.3326,  -0.5550],\n",
       "         [  2.0072,  -4.3336,  -0.6319],\n",
       "         [ -3.4395,   0.9294, -19.1316],\n",
       "         [  1.9222,  -1.0131,  -6.2780],\n",
       "         [ -1.5430,   2.1510,  -7.2533],\n",
       "         [ -5.7382,   3.1172, -12.5165],\n",
       "         [  2.7730,  -0.6589,  -7.7005],\n",
       "         [  2.7618,  -0.7122,  -7.5046],\n",
       "         [  1.4025,   0.6899,  -9.1601]]),\n",
       " tensor([[  1.4778,   0.4816,  -8.4963],\n",
       "         [  1.4556,   0.5348,  -8.6525],\n",
       "         [  1.4693,   0.5139,  -8.6000],\n",
       "         [  1.3351,   0.6214,  -8.6091],\n",
       "         [  1.4398,   0.5709,  -8.7404],\n",
       "         [  1.4179,   0.6111,  -8.8291],\n",
       "         [  0.9339,   1.1644,  -9.6429],\n",
       "         [  1.7580,  -1.4298,  -4.1019],\n",
       "         [  1.7563,  -1.3000,  -4.2917],\n",
       "         [  1.8729,  -2.5509,  -3.2503],\n",
       "         [  0.9389,  -1.0137,  -3.8756],\n",
       "         [ -3.1200,   0.5021,  -9.2849],\n",
       "         [ -1.9475,   1.7585,  -5.0152],\n",
       "         [ -2.7197,   2.1608,  -5.5127],\n",
       "         [ -2.9610,   0.4043,  -9.1883],\n",
       "         [ -3.9289,   1.3700, -10.9700],\n",
       "         [ -2.7843,   2.2686,  -5.7842],\n",
       "         [ -2.9610,   0.4043,  -9.1883],\n",
       "         [  1.2806,  -1.0896,  -4.3873],\n",
       "         [  1.7081,  -1.4680,  -4.6759],\n",
       "         [  1.1185,  -1.0659,  -4.0443],\n",
       "         [  1.8603,  -2.4341,  -3.3645],\n",
       "         [ -0.4306,   0.3991,  -3.8572],\n",
       "         [  1.0653,  -1.1128,  -3.9022],\n",
       "         [  1.8542,  -2.5005,  -3.2608],\n",
       "         [  1.8880,  -2.5692,  -3.2659],\n",
       "         [ -3.1673,   2.1660,  -7.1648],\n",
       "         [ -1.3436,   1.2309,  -4.3668],\n",
       "         [ -0.9570,   1.0013,  -4.2296],\n",
       "         [ -3.3270,   2.1749,  -8.0614],\n",
       "         [ -3.3160,   2.1523,  -8.0505],\n",
       "         [ -3.1528,   1.9343,  -7.2937],\n",
       "         [ -2.9610,   0.4043,  -9.1883],\n",
       "         [ -2.0262,  -1.4879, -14.4844],\n",
       "         [ -3.9140,   1.3328, -10.8586],\n",
       "         [ -2.1823,  -1.4107, -15.3389],\n",
       "         [ -3.1673,   2.1660,  -7.1648],\n",
       "         [ -1.4504,   1.3157,  -4.4370],\n",
       "         [  1.8940,  -2.6044,  -3.2358],\n",
       "         [ -4.1976,   1.2641, -12.1186],\n",
       "         [ -3.2872,   2.3887,  -6.6337],\n",
       "         [  1.8810,  -2.5128,  -3.3526],\n",
       "         [ -1.3416,   2.3514,  -8.8210],\n",
       "         [ -5.8349,   1.6016, -12.9328],\n",
       "         [  2.2844,  -2.0431,  -4.3617],\n",
       "         [  2.2801,  -1.9876,  -4.4556],\n",
       "         [  2.0820,  -1.1793,  -5.8859],\n",
       "         [  2.0421,  -0.3812,  -7.3832],\n",
       "         [ -3.2293,   2.8236, -14.6952],\n",
       "         [ -3.0725,   3.3050,  -8.8233],\n",
       "         [  2.0894,  -0.0600,  -6.8030],\n",
       "         [ -2.9204,   3.3831, -12.9695],\n",
       "         [ -3.1456,   3.5457,  -7.9458],\n",
       "         [  1.2798,  -0.2422,  -5.2151],\n",
       "         [ -3.6132,   2.8159,  -9.6072],\n",
       "         [  1.3001,  -0.2686,  -5.2422],\n",
       "         [ -3.6180,   2.6833,  -9.3179],\n",
       "         [  1.3116,  -0.2651,  -5.2886],\n",
       "         [  1.2928,  -0.2407,  -5.3037],\n",
       "         [  1.2839,  -0.2697,  -5.1670],\n",
       "         [  0.9327,   0.1466,  -5.3912],\n",
       "         [  0.9433,   0.1674,  -5.5148],\n",
       "         [ -3.7286,   3.0403, -10.4736],\n",
       "         [  1.2461,  -0.2114,  -5.1382]]),\n",
       " tensor([[  1.2903,  -0.2710,  -5.2187],\n",
       "         [ -0.6147,   1.5391,  -6.7167],\n",
       "         [ -3.7827,   3.2965, -13.0506],\n",
       "         [ -3.4893,   2.6561, -11.7175],\n",
       "         [ -2.3355,   0.8402, -17.3658],\n",
       "         [ -5.5233,  -0.5738,  -7.1496],\n",
       "         [  1.5026,  -0.5104,  -5.7484],\n",
       "         [ -1.1967,   4.0844, -19.8391],\n",
       "         [ -1.1996,   4.2751, -21.3128],\n",
       "         [  0.3417,   2.3022, -13.1319],\n",
       "         [  0.1093,   2.0408, -11.6985],\n",
       "         [  1.5878,  -1.6824,  -3.7217],\n",
       "         [  1.5787,  -1.6112,  -3.8303],\n",
       "         [  1.5860,  -1.6724,  -3.7472],\n",
       "         [  1.2403,   0.6876,  -9.0771],\n",
       "         [  1.3886,   0.1287,  -7.2256],\n",
       "         [ -1.3847,   4.0462, -22.6886],\n",
       "         [ -0.0231,   1.9002,  -9.8827],\n",
       "         [ -6.1560,   4.6368, -22.5922],\n",
       "         [  2.1295,   0.1220,  -9.7214],\n",
       "         [ -6.5401,   4.5265, -15.3500],\n",
       "         [ -6.3565,   4.6864, -21.7571],\n",
       "         [  1.1084,   0.1359,  -6.3944],\n",
       "         [  1.1178,   0.1526,  -6.5202],\n",
       "         [  1.1455,   0.1005,  -6.4218],\n",
       "         [  1.1026,   0.1547,  -6.4623],\n",
       "         [  0.2067,   3.3327, -16.1165],\n",
       "         [  2.0326,   0.1415,  -8.1984],\n",
       "         [  0.2186,   2.5939, -12.2148],\n",
       "         [  1.6346,  -0.1574,  -7.4863],\n",
       "         [  1.6327,   0.3772,  -9.5940],\n",
       "         [  1.6387,  -0.1787,  -7.4436],\n",
       "         [ -4.7146,   0.3955, -15.6637],\n",
       "         [  2.0368,  -0.7403,  -7.5292],\n",
       "         [ -3.8132,   2.8692, -22.3901],\n",
       "         [  0.3113,   1.3642,  -8.1948],\n",
       "         [ -0.7035,   2.2992, -15.1294],\n",
       "         [ -1.6110,   2.4490, -15.5662],\n",
       "         [  1.4073,  -0.3744,  -5.4057],\n",
       "         [  0.9112,   0.7794,  -8.1678],\n",
       "         [  0.8421,   0.8760,  -8.3450],\n",
       "         [  1.5606,  -0.8254,  -5.2788],\n",
       "         [  2.4054,  -3.3046,  -3.3553],\n",
       "         [  2.4854,  -3.2859,  -3.3554],\n",
       "         [  2.3942,  -3.2691,  -3.1709],\n",
       "         [ -5.7220,   5.0184, -13.6589],\n",
       "         [  1.5446,  -2.1572,  -2.8701],\n",
       "         [  1.5447,  -2.1465,  -2.8836],\n",
       "         [  1.5393,  -2.1429,  -2.8791],\n",
       "         [  1.5457,  -2.1631,  -2.8691],\n",
       "         [ -1.5599,   0.8814,  -3.3972],\n",
       "         [ -5.5217,   1.4532,  -6.7985],\n",
       "         [ -4.8648,   2.3286,  -7.1196],\n",
       "         [ -4.4891,  -1.1776,  -7.0594],\n",
       "         [ -5.4353,   1.1043,  -6.5534],\n",
       "         [ -5.3631,   1.6223,  -8.1619],\n",
       "         [ -5.2496,   1.7907,  -5.7910],\n",
       "         [ -4.8488,   1.9660,  -5.6959],\n",
       "         [  0.4562,   0.2858,  -5.0964],\n",
       "         [  2.0292,  -0.8506,  -5.7265],\n",
       "         [ -5.4827,   1.4008,  -6.7082],\n",
       "         [ -2.0575,   1.3465,  -4.3409],\n",
       "         [ -1.8169,   1.2661,  -4.3741],\n",
       "         [  2.0344,  -0.6659,  -7.6768]]),\n",
       " tensor([[ 2.3042e+00, -4.9386e-01, -8.0104e+00],\n",
       "         [-2.4455e+00,  3.6326e+00, -1.6422e+01],\n",
       "         [-1.4935e+00,  8.4071e-01, -2.1035e+01],\n",
       "         [ 2.2878e+00, -4.8349e-01, -7.9797e+00],\n",
       "         [ 2.2985e+00, -4.8326e-01, -8.0219e+00],\n",
       "         [ 8.8731e-01,  5.7397e-01, -6.2863e+00],\n",
       "         [ 8.6004e-01,  5.4095e-01, -6.0199e+00],\n",
       "         [ 8.6912e-01,  5.8139e-01, -6.2569e+00],\n",
       "         [ 7.3208e-01,  7.8750e-01, -6.5729e+00],\n",
       "         [ 8.3428e-01,  6.1969e-01, -6.3141e+00],\n",
       "         [ 9.0527e-01,  5.2625e-01, -6.1320e+00],\n",
       "         [-1.4994e+00,  3.3376e+00, -1.0438e+01],\n",
       "         [ 7.3720e-01,  7.6331e-01, -6.5553e+00],\n",
       "         [ 7.1818e-01,  7.7277e-01, -6.5270e+00],\n",
       "         [ 7.0640e-01,  7.9070e-01, -6.5637e+00],\n",
       "         [ 4.4940e-01,  1.1906e+00, -7.3336e+00],\n",
       "         [ 9.7650e-01,  5.0949e-01, -6.5003e+00],\n",
       "         [ 4.6036e-01,  1.0598e+00, -6.7903e+00],\n",
       "         [-1.7348e+00,  3.4895e+00, -1.0846e+01],\n",
       "         [-1.8703e+00,  3.5917e+00, -1.1224e+01],\n",
       "         [-1.7168e+00,  3.4940e+00, -1.0788e+01],\n",
       "         [ 8.2293e-01,  6.8216e-01, -6.4032e+00],\n",
       "         [ 8.5852e-01,  5.8799e-01, -6.2023e+00],\n",
       "         [ 8.6063e-01,  6.1522e-01, -6.3528e+00],\n",
       "         [ 8.1315e-01,  6.3122e-01, -6.2679e+00],\n",
       "         [-1.3326e+00,  2.8601e+00, -1.4132e+01],\n",
       "         [ 7.4409e-01,  8.2433e-01, -6.7683e+00],\n",
       "         [ 9.1107e-01,  5.0606e-01, -6.1107e+00],\n",
       "         [ 8.0944e-01,  6.8207e-01, -6.4350e+00],\n",
       "         [ 7.4129e-01,  7.6919e-01, -6.5552e+00],\n",
       "         [ 9.3233e-01,  5.0182e-01, -6.1409e+00],\n",
       "         [ 8.9103e-01,  5.5832e-01, -6.2257e+00],\n",
       "         [-2.8024e-01,  1.6181e+00, -8.3769e+00],\n",
       "         [ 6.8303e-02,  1.2575e+00, -6.9946e+00],\n",
       "         [ 9.4671e-01,  4.9034e-01, -6.1387e+00],\n",
       "         [ 9.1226e-01,  5.2720e-01, -6.1693e+00],\n",
       "         [ 4.2268e-01,  1.0258e+00, -6.4961e+00],\n",
       "         [ 8.3454e-01,  6.5487e-01, -6.4024e+00],\n",
       "         [ 7.2324e-01,  7.4814e-01, -6.3298e+00],\n",
       "         [ 3.6920e-01,  1.1043e+00, -6.7481e+00],\n",
       "         [ 5.2922e-01,  9.5629e-01, -6.4890e+00],\n",
       "         [ 4.0487e-01,  1.0754e+00, -6.6685e+00],\n",
       "         [ 8.0034e-01,  7.1672e-01, -6.5122e+00],\n",
       "         [-1.3326e+00,  2.8601e+00, -1.4132e+01],\n",
       "         [-3.4565e-03,  1.4743e+00, -7.9764e+00],\n",
       "         [ 9.0527e-01,  5.2625e-01, -6.1320e+00],\n",
       "         [-1.5267e-01,  1.3247e+00, -7.1700e+00],\n",
       "         [ 5.1193e-01,  9.5895e-01, -6.5388e+00],\n",
       "         [ 6.9866e-01,  7.9754e-01, -6.5901e+00],\n",
       "         [ 1.0355e+00,  4.2635e-01, -6.3016e+00],\n",
       "         [ 9.2911e-01,  4.8913e-01, -6.1135e+00],\n",
       "         [ 8.3386e-01,  6.2380e-01, -6.3468e+00],\n",
       "         [ 5.6888e-01,  8.4519e-01, -6.2181e+00],\n",
       "         [-1.7596e-01,  1.3243e+00, -6.8296e+00],\n",
       "         [ 2.8859e-01,  1.1710e+00, -7.0217e+00],\n",
       "         [ 5.5475e-01,  9.7802e-01, -6.8473e+00],\n",
       "         [ 6.7006e-01,  8.7143e-01, -6.9923e+00],\n",
       "         [ 5.9259e-01,  8.8086e-01, -6.5035e+00],\n",
       "         [-1.5252e+00,  3.1646e+00, -9.6224e+00],\n",
       "         [ 7.1084e-01,  7.6864e-01, -6.4988e+00],\n",
       "         [ 7.4129e-01,  7.6919e-01, -6.5552e+00],\n",
       "         [ 5.8326e-01,  8.3335e-01, -6.0792e+00],\n",
       "         [ 4.5720e-01,  1.1697e+00, -7.2439e+00],\n",
       "         [ 9.0388e-01,  5.5820e-01, -6.2966e+00]]),\n",
       " tensor([[ 7.2552e-01,  7.8222e-01, -6.5924e+00],\n",
       "         [ 9.0572e-01,  5.4906e-01, -6.2578e+00],\n",
       "         [ 8.4149e-01,  6.5376e-01, -6.4429e+00],\n",
       "         [ 5.9382e-01,  9.3257e-01, -6.6942e+00],\n",
       "         [ 8.1937e-01,  6.7383e-01, -6.4267e+00],\n",
       "         [ 1.8939e+00, -1.5172e-01, -8.0854e+00],\n",
       "         [ 1.7449e+00,  3.6130e-02, -8.1004e+00],\n",
       "         [-1.6187e+00,  1.6960e+00, -5.6386e+00],\n",
       "         [ 1.7572e+00, -7.8215e-01, -4.7865e+00],\n",
       "         [ 9.3836e-01, -2.2333e+00, -9.3866e-01],\n",
       "         [-1.9045e-01,  3.0145e-01, -3.0788e+00],\n",
       "         [ 6.6326e-01, -9.3979e-01, -2.4758e+00],\n",
       "         [ 9.3391e-01, -2.2071e+00, -9.9785e-01],\n",
       "         [ 8.2092e-01, -1.1808e+00, -2.3666e+00],\n",
       "         [ 9.3751e-01, -2.2274e+00, -9.8585e-01],\n",
       "         [ 9.3467e-01, -2.2413e+00, -9.4747e-01],\n",
       "         [ 4.4268e-01, -5.9148e-01, -2.6532e+00],\n",
       "         [-4.5378e-01,  8.9310e-01, -4.0655e+00],\n",
       "         [-5.2394e+00,  2.1377e+00, -9.6793e+00],\n",
       "         [-5.6427e+00,  2.0668e+00, -9.6471e+00],\n",
       "         [-5.9640e+00,  2.2878e+00, -9.9710e+00],\n",
       "         [ 9.3787e-01, -1.7921e+00, -1.5501e+00],\n",
       "         [-2.5808e-01,  5.9918e-01, -3.6198e+00],\n",
       "         [ 9.6331e-01, -1.5807e+00, -1.9090e+00],\n",
       "         [-5.9856e-01,  1.0998e+00, -4.4602e+00],\n",
       "         [-5.2742e+00,  2.4062e+00, -1.1015e+01],\n",
       "         [-3.5281e+00, -2.5941e-01, -9.1771e+00],\n",
       "         [ 9.3709e-01, -2.2058e+00, -1.0145e+00],\n",
       "         [-4.7642e+00,  1.9940e+00, -9.0029e+00],\n",
       "         [-1.1538e-02,  3.0955e-02, -2.8827e+00],\n",
       "         [ 7.8413e-01, -1.1195e+00, -2.4064e+00],\n",
       "         [ 9.5568e-01, -1.6055e+00, -1.8511e+00],\n",
       "         [ 2.2219e-01, -3.6712e-01, -2.6335e+00],\n",
       "         [ 9.3409e-01, -2.2298e+00, -9.6586e-01],\n",
       "         [ 9.3361e-01, -2.2330e+00, -9.6002e-01],\n",
       "         [-5.0477e-01,  1.0130e+00, -4.2729e+00],\n",
       "         [ 8.3632e-01, -1.2011e+00, -2.3653e+00],\n",
       "         [-4.4885e+00,  1.6522e+00, -8.3507e+00],\n",
       "         [-4.7266e+00,  1.9612e+00, -8.9310e+00],\n",
       "         [ 2.3894e-02,  1.4056e-02, -2.9098e+00],\n",
       "         [ 9.3279e-01, -2.2070e+00, -9.7951e-01],\n",
       "         [-5.0682e+00,  2.1656e+00, -1.0562e+01],\n",
       "         [-8.2980e-01,  1.1468e+00, -5.2322e+00],\n",
       "         [-5.7574e+00,  2.1118e+00, -1.0084e+01],\n",
       "         [-7.5528e-01,  1.2123e+00, -5.6234e+00],\n",
       "         [-5.1864e+00,  2.3877e+00, -1.1072e+01],\n",
       "         [ 9.3668e-01, -2.2321e+00, -9.5754e-01],\n",
       "         [-3.1488e+00,  3.2026e+00, -2.1726e+01],\n",
       "         [ 1.8880e+00,  1.8450e-01, -9.1736e+00],\n",
       "         [ 1.6444e+00, -2.8835e-01, -6.4919e+00],\n",
       "         [ 1.6556e+00, -3.1197e-01, -6.4553e+00],\n",
       "         [-4.9728e+00,  2.3498e+00, -8.7395e+00],\n",
       "         [ 1.6562e+00, -2.9658e-01, -6.5076e+00],\n",
       "         [ 1.6559e+00, -3.1282e-01, -6.4485e+00],\n",
       "         [-5.6039e+00,  1.2421e+00, -1.2447e+01],\n",
       "         [ 1.8448e+00, -2.4474e+00, -3.7418e+00],\n",
       "         [ 1.8531e+00, -2.4001e+00, -3.8609e+00],\n",
       "         [ 1.8532e+00, -2.3795e+00, -3.8828e+00],\n",
       "         [ 1.8512e+00, -2.4067e+00, -3.8230e+00],\n",
       "         [ 1.8220e+00, -2.0750e+00, -4.3056e+00],\n",
       "         [ 1.4949e+00, -1.5504e-02, -5.8423e+00],\n",
       "         [ 1.5017e+00, -1.6147e-02, -5.8633e+00],\n",
       "         [ 2.9598e+00, -7.6832e-01, -8.0700e+00],\n",
       "         [ 1.8449e+00, -2.1650e+00, -3.4526e+00]]),\n",
       " tensor([[  1.8547,  -2.1781,  -3.4695],\n",
       "         [  0.8959,   1.3759, -11.3372],\n",
       "         [  1.8507,  -2.1515,  -3.4857],\n",
       "         [  1.3944,   0.1307,  -7.1073],\n",
       "         [  1.5365,  -1.2323,  -3.9094],\n",
       "         [  1.8409,  -2.1445,  -3.4590],\n",
       "         [  1.5352,  -1.2604,  -3.8791],\n",
       "         [  1.6726,  -1.7584,  -3.5428],\n",
       "         [  2.5153,  -0.2546,  -8.2352],\n",
       "         [ -4.0056,   3.9899, -17.1917],\n",
       "         [ -3.9856,   4.0422, -15.1691],\n",
       "         [ -0.2440,   1.0856, -12.6628],\n",
       "         [ -0.6994,   1.1227, -14.1218],\n",
       "         [  0.3115,   0.9709,  -6.2590],\n",
       "         [ -0.2265,   1.1402, -14.6186],\n",
       "         [ -0.3110,   1.2950, -15.6990],\n",
       "         [  0.5216,   0.7783,  -6.2854],\n",
       "         [  0.4181,   0.8518,  -6.1793],\n",
       "         [ -0.7651,   1.5679, -18.6690],\n",
       "         [ -0.6534,   1.2922, -15.4657],\n",
       "         [  0.5113,   0.7775,  -9.2661],\n",
       "         [ -3.1219,  -0.4520, -14.7127],\n",
       "         [  0.2346,   0.9693,  -6.0349],\n",
       "         [  0.3591,   0.8550,  -6.2815],\n",
       "         [  0.2954,   0.9797,  -6.1929],\n",
       "         [  0.2735,   0.9362,  -5.8910],\n",
       "         [  0.3488,   0.9306,  -6.1938],\n",
       "         [ -0.3768,   1.4609, -16.0266],\n",
       "         [  0.5387,   0.6233,  -8.3343],\n",
       "         [  0.5039,   0.8339,  -8.6097],\n",
       "         [ -1.1582,   1.4464, -17.5168],\n",
       "         [  0.3011,   0.8235,  -9.8521],\n",
       "         [  0.2862,   1.0244,  -6.3569],\n",
       "         [  0.3758,   0.8963,  -6.1833],\n",
       "         [  0.5387,   0.6842,  -8.4188],\n",
       "         [ -0.6968,   1.1533, -14.2712],\n",
       "         [  1.5255,  -2.3492,  -3.0998],\n",
       "         [  1.7206,  -0.4865,  -5.4232],\n",
       "         [  1.7064,  -0.4229,  -5.4934],\n",
       "         [  1.6641,  -0.3648,  -5.5257],\n",
       "         [  1.3988,  -1.5987,  -3.1390],\n",
       "         [ -1.7696,   2.5852,  -5.6635],\n",
       "         [ -2.1335,   3.0536,  -6.8796],\n",
       "         [  1.3738,  -1.8341,  -3.3462],\n",
       "         [  0.8316,   0.3839,  -6.6530],\n",
       "         [ -3.8726,   1.7961, -13.2847],\n",
       "         [  1.0044,   0.1602,  -6.3561],\n",
       "         [  1.4614,   0.9508, -10.3969],\n",
       "         [  1.6999,  -2.3835,  -2.4049],\n",
       "         [  1.5392,  -0.4095,  -5.6918],\n",
       "         [  1.5847,  -0.6154,  -6.1673],\n",
       "         [  1.5621,  -0.3573,  -6.5961],\n",
       "         [  0.7225,   0.4826,  -6.7930],\n",
       "         [ -4.7683,   1.7434, -12.1340],\n",
       "         [ -1.5887,   1.9643,  -7.4652],\n",
       "         [  0.8303,   0.4309,  -6.9954],\n",
       "         [  1.5303,  -0.2014,  -6.7592],\n",
       "         [ -1.4682,   1.9062,  -7.2745],\n",
       "         [  0.2219,   0.8939,  -6.8988],\n",
       "         [  0.7624,   0.4209,  -6.6765],\n",
       "         [  0.8148,   0.3654,  -6.6355],\n",
       "         [ -1.8278,   1.9943,  -6.9168],\n",
       "         [ -2.8258,   3.0390, -11.4207],\n",
       "         [  1.6725,  -1.5207,  -5.0000]]),\n",
       " tensor([[-4.5863e+00,  1.6882e+00, -1.2465e+01],\n",
       "         [-7.4879e-01,  1.7273e+00, -8.0466e+00],\n",
       "         [ 4.1937e-01,  6.4989e-01, -6.5401e+00],\n",
       "         [-3.9790e+00,  2.9207e+00, -1.6997e+01],\n",
       "         [-1.6712e+00,  2.1261e+00, -8.1483e+00],\n",
       "         [-2.9723e+00,  3.0875e+00, -1.2713e+01],\n",
       "         [-1.7767e+00,  1.9265e+00, -6.6355e+00],\n",
       "         [ 8.6238e-01,  3.5573e-01, -6.7702e+00],\n",
       "         [-1.5941e+00,  2.0076e+00, -7.6600e+00],\n",
       "         [-1.2146e+00,  1.8768e+00, -7.6715e+00],\n",
       "         [-1.7115e+00,  2.1726e+00, -8.2392e+00],\n",
       "         [-1.6325e+00,  2.1532e+00, -8.4382e+00],\n",
       "         [-8.0486e-01,  1.6545e+00, -7.6347e+00],\n",
       "         [ 1.2238e+00,  7.2082e-02, -6.9785e+00],\n",
       "         [ 1.5472e+00, -3.5321e-01, -6.5467e+00],\n",
       "         [ 1.5565e+00, -6.7144e-03, -7.4955e+00],\n",
       "         [ 1.5652e+00, -2.3843e-02, -7.4785e+00],\n",
       "         [ 1.5642e+00, -3.8680e-02, -7.4036e+00],\n",
       "         [ 1.5658e+00, -7.5196e-04, -7.5557e+00],\n",
       "         [ 1.5583e+00,  1.0224e-02, -7.5604e+00],\n",
       "         [ 1.5625e+00, -7.0577e-03, -7.5253e+00],\n",
       "         [-2.1699e+00,  3.7173e+00, -2.0134e+01],\n",
       "         [ 5.2845e+00,  1.0175e+00, -2.0084e+01],\n",
       "         [ 6.2671e+00, -1.4922e+00, -1.8870e+01],\n",
       "         [ 1.3763e+00, -1.1392e+00, -8.7596e+00],\n",
       "         [-9.2382e-01,  1.9247e+00, -1.0346e+01],\n",
       "         [ 1.2496e+00, -8.9936e-02, -6.0212e+00],\n",
       "         [ 1.2502e+00, -8.4470e-02, -6.0486e+00],\n",
       "         [ 1.2480e+00, -1.1132e-01, -5.9261e+00],\n",
       "         [ 1.2449e+00, -9.5402e-02, -5.9801e+00],\n",
       "         [-1.1556e+00,  1.9885e+00, -1.0471e+01],\n",
       "         [ 1.2552e+00, -1.0507e-01, -5.9849e+00],\n",
       "         [ 1.2527e+00, -9.5119e-02, -6.0196e+00],\n",
       "         [ 1.0717e+00,  4.3560e-01, -7.6054e+00],\n",
       "         [-1.3192e+00,  2.0191e+00, -1.2523e+01],\n",
       "         [-1.3425e+00,  2.0691e+00, -1.0312e+01],\n",
       "         [-4.5838e+00,  3.2911e-03, -1.2548e+01],\n",
       "         [ 2.0217e+00, -4.1092e-01, -6.4940e+00],\n",
       "         [ 2.0225e+00, -4.1691e-01, -6.4786e+00],\n",
       "         [ 1.5669e+00, -1.0771e+00, -5.2034e+00],\n",
       "         [ 2.4428e+00, -1.8066e+00, -5.1186e+00],\n",
       "         [ 3.6633e+00, -1.5875e-01, -1.0659e+01],\n",
       "         [ 3.9132e+00, -5.6658e-01, -1.0165e+01],\n",
       "         [ 1.3877e+00, -2.1443e-01, -5.6228e+00],\n",
       "         [ 1.3793e+00, -2.3797e-02, -6.5127e+00],\n",
       "         [ 1.4011e+00, -2.8332e+00, -1.0757e+00],\n",
       "         [ 1.4004e+00, -2.8406e+00, -1.0696e+00],\n",
       "         [ 1.7700e+00, -1.0825e-01, -7.8615e+00],\n",
       "         [ 1.0579e+00, -4.0762e-01, -3.3019e+00],\n",
       "         [-2.0540e+00,  2.4341e+00, -7.0295e+00],\n",
       "         [-3.5810e+00,  3.2481e+00, -1.3472e+01],\n",
       "         [ 2.0317e+00, -6.3704e-01, -5.9623e+00],\n",
       "         [-2.0724e+00,  2.4148e+00, -6.9936e+00],\n",
       "         [-3.4432e+00,  3.2319e+00, -1.4331e+01],\n",
       "         [ 1.3137e+00, -3.3378e+00, -2.6041e-01],\n",
       "         [ 1.2859e+00, -3.2944e+00, -2.4041e-01],\n",
       "         [ 1.3005e+00, -3.3547e+00, -2.1036e-01],\n",
       "         [ 1.3722e+00, -3.1951e+00, -5.6682e-01],\n",
       "         [-1.0130e+00,  2.1954e+00, -9.8146e+00],\n",
       "         [ 1.7370e+00, -7.7749e-01, -6.8697e+00],\n",
       "         [-1.3485e+00,  2.2419e+00, -9.0669e+00],\n",
       "         [-2.6719e+00,  2.8027e+00, -8.9588e+00],\n",
       "         [-4.8160e+00,  1.9501e+00, -1.1588e+01],\n",
       "         [ 1.3812e+00, -2.5997e-01, -5.7654e+00]]),\n",
       " tensor([[ 1.1029e+00,  8.0306e-02, -5.6542e+00],\n",
       "         [-5.9626e+00,  2.9130e+00, -1.0837e+01],\n",
       "         [ 1.4776e+00, -2.3434e+00, -3.6053e+00],\n",
       "         [ 1.6558e+00, -2.9667e-01, -7.7414e+00],\n",
       "         [ 1.1220e+00, -7.9645e-03, -5.7330e+00],\n",
       "         [ 9.6631e-01,  2.4104e-01, -6.0454e+00],\n",
       "         [ 1.1066e+00,  3.7476e-02, -5.7500e+00],\n",
       "         [-4.9076e+00,  2.7945e+00, -6.8336e+00],\n",
       "         [ 2.7359e+00, -9.9669e-01, -6.3662e+00],\n",
       "         [ 2.4968e+00, -8.7122e-01, -6.1101e+00],\n",
       "         [ 1.4120e+00, -1.4047e+00, -3.4754e+00],\n",
       "         [ 2.3297e+00, -2.2131e+00, -4.9281e+00],\n",
       "         [ 2.3202e+00, -2.2064e+00, -4.8852e+00],\n",
       "         [ 2.2900e+00, -2.1969e+00, -4.8465e+00],\n",
       "         [ 7.6864e-02,  1.1801e+00, -6.6315e+00],\n",
       "         [-3.4369e+00,  4.6553e-01, -1.4809e+01],\n",
       "         [ 2.3087e+00, -2.1873e+00, -4.8683e+00],\n",
       "         [ 2.3157e+00, -2.1364e+00, -4.9671e+00],\n",
       "         [ 2.2871e+00, -2.2175e+00, -4.7820e+00],\n",
       "         [-5.2441e+00,  4.0843e+00, -1.5523e+01],\n",
       "         [ 9.5386e-01, -2.5849e-01, -4.0929e+00],\n",
       "         [ 2.0671e+00,  1.1581e-01, -8.6636e+00],\n",
       "         [ 1.9344e+00,  1.0105e+00, -1.2332e+01],\n",
       "         [ 1.9579e+00,  5.7044e-01, -1.0588e+01],\n",
       "         [ 2.3092e+00, -1.4171e+00, -4.9439e+00],\n",
       "         [ 2.2958e+00, -1.4481e+00, -4.8615e+00],\n",
       "         [ 1.7920e+00,  1.0974e+00, -1.1976e+01],\n",
       "         [ 1.8262e+00, -2.3182e+00, -3.0916e+00],\n",
       "         [ 1.8274e+00, -2.3972e+00, -2.9250e+00],\n",
       "         [ 2.8731e+00, -3.2705e-01, -9.1199e+00],\n",
       "         [ 1.6120e+00,  1.1371e-01, -7.6230e+00],\n",
       "         [ 7.9720e-01,  5.4926e-01, -7.0273e+00],\n",
       "         [ 7.9900e-01,  5.4642e-01, -7.0194e+00],\n",
       "         [ 7.6717e-01,  9.8264e-01, -8.8314e+00],\n",
       "         [-4.0875e+00,  1.5217e+00, -1.4667e+01],\n",
       "         [-1.0124e+00,  2.2976e+00, -1.2347e+01],\n",
       "         [ 4.8583e-01,  1.5645e+00, -1.0362e+01],\n",
       "         [ 2.4132e+00,  6.8636e-01, -1.2676e+01],\n",
       "         [-6.2212e+00,  5.0652e+00, -1.6866e+01],\n",
       "         [-5.4550e+00,  3.9015e+00, -1.4417e+01],\n",
       "         [ 2.7577e+00, -4.6702e-01, -8.9196e+00],\n",
       "         [-5.5150e+00,  4.7809e+00, -1.7930e+01],\n",
       "         [ 2.7445e+00, -4.5800e-01, -8.9195e+00],\n",
       "         [ 2.4013e+00,  6.3164e-01, -1.2455e+01],\n",
       "         [-2.7004e+00,  3.8393e+00, -1.2584e+01],\n",
       "         [ 2.4342e+00,  4.2257e-01, -1.1398e+01],\n",
       "         [ 1.1929e+00,  4.0763e-01, -8.0182e+00],\n",
       "         [ 1.1953e+00,  3.8904e-01, -7.9553e+00],\n",
       "         [ 1.1929e+00,  3.8783e-01, -7.9390e+00],\n",
       "         [ 1.1929e+00,  4.0762e-01, -8.0182e+00],\n",
       "         [ 1.1929e+00,  4.0762e-01, -8.0182e+00],\n",
       "         [ 2.4831e+00, -1.4931e+00, -7.9560e+00],\n",
       "         [ 2.4759e+00, -1.5346e+00, -7.7782e+00],\n",
       "         [ 2.4781e+00, -1.5199e+00, -7.8696e+00],\n",
       "         [ 2.0419e+00,  5.4056e-01, -1.1711e+01],\n",
       "         [ 1.4089e+00, -1.5443e-01, -6.4774e+00],\n",
       "         [ 1.4155e+00, -1.4276e-01, -6.5455e+00],\n",
       "         [ 1.4194e+00, -1.6144e-01, -6.4875e+00],\n",
       "         [ 1.9821e+00, -4.1520e-01, -7.3099e+00],\n",
       "         [ 1.9801e+00, -4.0800e-01, -7.3325e+00],\n",
       "         [ 1.9776e+00, -4.0696e-01, -7.3233e+00],\n",
       "         [-5.7031e+00,  3.3416e+00, -8.8505e+00],\n",
       "         [ 1.9762e+00, -4.0768e-01, -7.3174e+00],\n",
       "         [ 1.8855e+00, -2.0976e-01, -7.6544e+00]]),\n",
       " tensor([[  1.9743,  -0.4068,  -7.3097],\n",
       "         [ -5.3793,   2.5210, -10.1623],\n",
       "         [ -5.0858,   2.2496,  -9.8137],\n",
       "         [  1.9734,  -0.4051,  -7.3184],\n",
       "         [  1.9837,  -0.4114,  -7.3307],\n",
       "         [  3.7893,  -0.7031, -10.3154],\n",
       "         [  3.7782,  -0.6968, -10.2913],\n",
       "         [ -1.7374,   1.9790,  -6.4586],\n",
       "         [  1.5719,  -0.1165,  -6.5456],\n",
       "         [ -1.2162,   1.7462,  -6.7073],\n",
       "         [  1.9574,  -2.0776,  -4.3153],\n",
       "         [ -0.7087,   1.3254,  -5.4134],\n",
       "         [ -3.6321,   2.3318,  -8.9150],\n",
       "         [  1.9571,  -2.4833,  -3.6571],\n",
       "         [  1.8898,  -0.8580,  -6.0988],\n",
       "         [  1.9525,  -2.5073,  -3.6053],\n",
       "         [  1.5719,  -0.1165,  -6.5456],\n",
       "         [  1.9476,  -2.4860,  -3.6208],\n",
       "         [  1.9511,  -2.4669,  -3.6634],\n",
       "         [  1.8009,  -0.9488,  -5.6433],\n",
       "         [ -0.8628,   1.5649,  -6.3362],\n",
       "         [ -1.0070,   1.6099,  -6.4497],\n",
       "         [  1.9763,  -2.2509,  -4.1103],\n",
       "         [  1.9196,  -1.1474,  -5.7055],\n",
       "         [  1.9511,  -2.5074,  -3.6034],\n",
       "         [ -2.1186,   2.1648,  -6.6464],\n",
       "         [ -6.3868,   0.8561, -10.0769],\n",
       "         [  1.4846,  -1.2949,  -4.9372],\n",
       "         [  1.4783,  -1.3428,  -4.8642],\n",
       "         [ -4.4436,   2.9297, -13.4543],\n",
       "         [  1.1333,   0.5966,  -8.2150],\n",
       "         [  2.3130,  -0.7242,  -7.8690],\n",
       "         [  2.2968,  -0.7215,  -7.8182],\n",
       "         [  2.0692,  -0.5252,  -6.9018],\n",
       "         [  2.0637,  -0.5269,  -6.8713],\n",
       "         [  2.0650,  -0.5236,  -6.8931],\n",
       "         [  2.0612,  -0.4636,  -7.0703],\n",
       "         [  2.0653,  -0.5365,  -6.8420],\n",
       "         [  2.4975,  -0.3758,  -8.0971],\n",
       "         [ -2.3737,   3.5227, -11.1524],\n",
       "         [ -2.3473,   3.6586, -12.9638],\n",
       "         [ -2.6572,   4.2045, -12.7488],\n",
       "         [ -0.3333,   2.3210,  -8.3198],\n",
       "         [ -2.9521,   3.7368, -12.9666],\n",
       "         [ -2.4167,   3.4549,  -9.6121],\n",
       "         [ -2.1291,   3.2437,  -9.4017],\n",
       "         [ -2.9257,   2.1346,  -6.3046],\n",
       "         [ -2.9498,   3.8788, -13.5965],\n",
       "         [  2.5086,  -0.4258,  -7.9742],\n",
       "         [ -2.0976,   3.3123,  -9.6053],\n",
       "         [ -2.3929,   3.4918,  -9.7603],\n",
       "         [ -2.6800,   3.4716, -12.4747],\n",
       "         [ -2.4329,   3.4088, -10.1677],\n",
       "         [  1.9191,  -1.6920,  -4.4845],\n",
       "         [ -1.9702,   2.0118, -15.1032],\n",
       "         [  1.4436,  -0.1731,  -5.3768],\n",
       "         [ -4.9163,   4.3990,  -7.5171],\n",
       "         [ -4.4126,   4.0711, -15.4835],\n",
       "         [ -2.7037,   2.6798,  -5.4627],\n",
       "         [ -2.1576,   2.4903,  -6.2628],\n",
       "         [  2.5013,  -0.4200,  -7.6982],\n",
       "         [ -1.2008,   2.0309,  -6.0887],\n",
       "         [ -2.1036,   2.5784,  -6.8723],\n",
       "         [ -1.2285,   2.0867,  -6.3306]]),\n",
       " tensor([[-7.5278e-01,  2.4036e+00, -1.0818e+01],\n",
       "         [-7.5937e-01,  2.4359e+00, -1.0975e+01],\n",
       "         [ 2.1203e+00, -1.7277e+00, -4.9145e+00],\n",
       "         [-7.4477e-02,  1.9674e+00, -1.0005e+01],\n",
       "         [ 1.9053e+00, -4.0780e-01, -6.5506e+00],\n",
       "         [ 2.1179e+00, -1.7266e+00, -4.9053e+00],\n",
       "         [-4.7270e-01,  2.4626e+00, -1.1464e+01],\n",
       "         [-9.8706e-01,  2.8974e+00, -1.2454e+01],\n",
       "         [-2.3576e+00,  2.8441e+00, -1.4302e+01],\n",
       "         [-1.9385e+00,  3.1145e+00, -1.3784e+01],\n",
       "         [-7.4477e-02,  1.9674e+00, -1.0005e+01],\n",
       "         [ 2.1258e+00, -1.7262e+00, -4.9380e+00],\n",
       "         [-1.5467e-01,  2.0161e+00, -1.0078e+01],\n",
       "         [ 2.1225e+00, -1.7081e+00, -4.9486e+00],\n",
       "         [ 1.5531e+00, -2.9991e-02, -6.9315e+00],\n",
       "         [ 2.1270e+00, -1.7267e+00, -4.9370e+00],\n",
       "         [-2.7805e+00,  8.4574e-01, -1.8604e+01],\n",
       "         [-2.3325e+00,  1.9497e-02, -1.8901e+01],\n",
       "         [ 2.1092e+00, -1.4835e+00, -5.1845e+00],\n",
       "         [ 1.8372e-01,  1.3855e+00, -8.2618e+00],\n",
       "         [ 2.1174e+00, -1.5002e+00, -5.1850e+00],\n",
       "         [ 2.1273e+00, -1.7025e+00, -4.9706e+00],\n",
       "         [ 2.1242e+00, -1.7355e+00, -4.9212e+00],\n",
       "         [ 2.1210e+00, -1.7075e+00, -4.9433e+00],\n",
       "         [ 2.1163e+00, -1.6818e+00, -4.9651e+00],\n",
       "         [ 2.1264e+00, -1.7451e+00, -4.9067e+00],\n",
       "         [ 2.1212e+00, -1.7147e+00, -4.9362e+00],\n",
       "         [ 2.1204e+00, -1.6919e+00, -4.9641e+00],\n",
       "         [ 2.1255e+00, -1.7196e+00, -4.9424e+00],\n",
       "         [ 2.1243e+00, -1.7202e+00, -4.9422e+00],\n",
       "         [-6.3018e-01,  2.3129e+00, -1.0422e+01],\n",
       "         [ 1.7752e-01,  1.7121e+00, -9.6837e+00],\n",
       "         [ 2.0344e+00, -1.5823e+00, -4.8113e+00],\n",
       "         [ 2.0944e+00, -1.3381e+00, -5.3344e+00],\n",
       "         [ 2.1092e+00, -1.6986e+00, -4.9220e+00],\n",
       "         [ 2.0689e+00, -9.7562e-01, -5.7432e+00],\n",
       "         [ 2.0923e+00, -1.2662e+00, -5.4311e+00],\n",
       "         [ 2.0637e+00, -8.8918e-01, -5.8445e+00],\n",
       "         [ 2.1170e+00, -1.7166e+00, -4.9189e+00],\n",
       "         [ 2.1218e+00, -1.7133e+00, -4.9376e+00],\n",
       "         [-4.3390e-01,  1.9704e+00, -9.2699e+00],\n",
       "         [ 2.0662e+00, -9.7363e-01, -5.7484e+00],\n",
       "         [ 1.9974e+00, -5.9939e-01, -6.2117e+00],\n",
       "         [ 2.0147e+00, -6.5441e-01, -6.1109e+00],\n",
       "         [ 1.9163e+00, -4.3154e-01, -6.5076e+00],\n",
       "         [ 1.8600e+00, -3.3661e-01, -6.6977e+00],\n",
       "         [-5.0372e-01,  2.2067e+00, -1.0096e+01],\n",
       "         [ 1.9679e+00, -1.3322e+00, -4.8934e+00],\n",
       "         [-2.2883e+00,  3.0737e+00, -1.3813e+01],\n",
       "         [ 2.0537e+00, -8.2004e-01, -5.9113e+00],\n",
       "         [ 3.4735e+00, -3.1662e-01, -1.0622e+01],\n",
       "         [-4.7573e-01,  3.1414e+00, -9.7982e+00],\n",
       "         [-3.8037e+00,  5.7541e+00, -1.4054e+01],\n",
       "         [ 4.0577e+00, -1.2075e+00, -1.0581e+01],\n",
       "         [-2.6488e+00,  3.3805e+00, -1.6601e+01],\n",
       "         [-2.6696e+00,  3.2552e+00, -1.5176e+01],\n",
       "         [-2.8542e+00,  3.2247e+00, -1.6185e+01],\n",
       "         [-3.1486e+00,  6.6089e-01, -1.9066e+01],\n",
       "         [-3.4786e+00,  1.1272e+00, -1.9729e+01],\n",
       "         [ 1.7289e+00,  3.4249e-01, -9.7983e+00],\n",
       "         [ 1.4310e+00,  7.3914e-01, -9.6658e+00],\n",
       "         [-2.9682e+00,  5.0255e-01, -1.8457e+01],\n",
       "         [ 1.7243e+00,  3.4514e-01, -9.7861e+00],\n",
       "         [ 1.7601e+00,  1.9179e-01, -9.2369e+00]]),\n",
       " tensor([[-3.7462e+00,  1.3168e+00, -2.0801e+01],\n",
       "         [-4.3593e+00,  2.3603e+00, -8.0748e+00],\n",
       "         [-3.5873e+00,  1.0409e+00, -9.9838e+00],\n",
       "         [-3.9157e+00,  1.1362e+00, -8.6204e+00],\n",
       "         [-3.3111e+00,  7.5844e-01, -9.5842e+00],\n",
       "         [-3.8807e+00,  3.2950e+00, -1.2573e+01],\n",
       "         [-5.5118e+00,  3.7697e+00, -2.2714e+01],\n",
       "         [ 2.5857e+00, -1.9868e+00, -6.9080e+00],\n",
       "         [-8.7927e-01,  3.0483e+00, -1.4238e+01],\n",
       "         [ 1.1121e+00,  1.2377e+00, -1.0465e+01],\n",
       "         [ 2.5709e+00, -7.2227e-01, -9.2121e+00],\n",
       "         [ 2.5722e+00, -1.8108e+00, -7.2015e+00],\n",
       "         [ 1.3315e+00, -5.6303e-02, -5.7914e+00],\n",
       "         [ 3.6493e+00, -5.0166e-01, -1.0758e+01],\n",
       "         [ 3.6990e+00, -4.7988e-01, -1.0904e+01],\n",
       "         [ 3.5998e+00, -5.8226e-01, -1.0400e+01],\n",
       "         [ 3.6340e+00, -5.7888e-01, -1.0499e+01],\n",
       "         [-1.1022e+00,  3.3943e+00, -7.2226e+00],\n",
       "         [ 3.6401e+00, -5.2653e-01, -1.0670e+01],\n",
       "         [-1.0854e+00,  2.6956e+00, -7.8009e+00],\n",
       "         [ 2.4880e+00, -5.7472e-01, -6.6924e+00],\n",
       "         [ 2.4321e+00, -5.3704e-01, -6.5832e+00],\n",
       "         [ 2.3973e+00, -5.3982e-01, -6.3108e+00],\n",
       "         [ 2.4265e+00, -4.6651e-01, -6.6182e+00],\n",
       "         [ 2.5046e+00, -5.7383e-01, -6.7208e+00],\n",
       "         [ 2.4661e+00, -5.8029e-01, -6.6247e+00],\n",
       "         [ 6.7065e-01,  7.9753e-01, -8.6515e+00],\n",
       "         [ 1.0158e+00, -1.9512e-02, -4.4887e+00],\n",
       "         [ 1.0709e+00, -6.8618e-02, -4.5798e+00],\n",
       "         [ 1.0673e+00, -6.8426e-02, -4.5753e+00],\n",
       "         [-2.4347e+00,  7.3321e-01, -9.8681e+00],\n",
       "         [ 1.0850e+00, -8.7099e-02, -4.5545e+00],\n",
       "         [ 1.4588e+00, -1.3761e-01, -6.4890e+00],\n",
       "         [ 1.6285e+00, -1.3732e-01, -7.1671e+00],\n",
       "         [-4.0998e+00,  3.4394e+00, -8.9628e+00],\n",
       "         [ 1.7295e+00, -4.7082e-01, -6.2582e+00],\n",
       "         [ 1.6742e+00, -1.8751e-01, -6.8125e+00],\n",
       "         [-1.9834e+00,  3.1680e+00, -1.6550e+01],\n",
       "         [ 2.3162e-01,  2.5400e+00, -1.4518e+01],\n",
       "         [ 1.0890e+00,  3.1937e-01, -5.9853e+00],\n",
       "         [ 1.4822e+00,  2.3644e+00, -1.4052e+01],\n",
       "         [ 2.4079e-01,  2.6218e+00, -1.1691e+01],\n",
       "         [ 3.4400e+00, -4.9843e-02, -1.0842e+01],\n",
       "         [ 1.0493e+00, -1.4254e+00, -1.5892e+01],\n",
       "         [-4.2347e-01,  1.4444e+00, -7.1012e+00],\n",
       "         [-4.8683e-01,  1.4941e+00, -7.5620e+00],\n",
       "         [ 1.5239e+00, -4.7180e-01, -4.2465e+00],\n",
       "         [ 1.8192e+00, -1.2919e+00, -5.5981e+00],\n",
       "         [ 1.8210e+00, -1.2870e+00, -5.6030e+00],\n",
       "         [ 1.9462e+00, -2.4124e+00, -2.8496e+00],\n",
       "         [ 1.2234e+00, -5.6468e-01, -4.1759e+00],\n",
       "         [ 1.0650e+00, -2.1399e-01, -4.6834e+00],\n",
       "         [ 1.9814e+00, -2.4023e+00, -2.8667e+00],\n",
       "         [ 1.9459e+00, -2.3935e+00, -2.8486e+00],\n",
       "         [-4.9403e+00,  2.8802e+00, -1.3372e+01],\n",
       "         [ 1.3840e+00, -1.9658e-01, -6.2352e+00],\n",
       "         [ 8.7518e-01,  4.7361e-01, -6.0199e+00],\n",
       "         [-4.8501e+00,  1.7356e+00, -1.0420e+01],\n",
       "         [-5.2325e+00,  3.2350e+00, -1.5995e+01],\n",
       "         [ 1.7129e+00, -2.8384e-03, -8.4583e+00],\n",
       "         [ 1.7263e+00,  4.0816e-03, -8.5201e+00],\n",
       "         [ 1.7105e+00,  1.2741e-02, -8.4745e+00],\n",
       "         [ 1.7214e+00,  1.2118e-02, -8.5204e+00],\n",
       "         [ 1.7180e+00,  2.2852e-02, -8.5720e+00]]),\n",
       " tensor([[ -0.2355,   1.8728,  -3.7001],\n",
       "         [  1.5491,  -0.0906,  -3.9027],\n",
       "         [  0.6743,   1.1167,  -4.3655],\n",
       "         [  2.1572,  -0.5939,  -5.4284],\n",
       "         [  2.1262,   0.6796,  -6.1539],\n",
       "         [  0.7813,  -2.1873, -13.2471],\n",
       "         [ -0.3697,  -2.7874,   0.7837],\n",
       "         [  1.7465,  -1.9364,  -3.8553],\n",
       "         [ -4.4970,   1.6621,  -4.3501],\n",
       "         [ -6.5656,   4.4029, -14.2298],\n",
       "         [ -7.3828,   4.5549, -11.3741],\n",
       "         [ -6.0176,   3.4080,  -6.9607],\n",
       "         [  1.9639,  -2.4532,  -4.0767],\n",
       "         [ -2.6989,   0.2910,  -2.2062],\n",
       "         [ -2.7576,   0.2544,  -2.1463],\n",
       "         [  1.8818,  -2.0603,  -4.0604],\n",
       "         [  0.4320,  -1.6038,  -1.6780],\n",
       "         [  1.9563,  -2.2727,  -4.1760],\n",
       "         [  1.9611,  -2.2741,  -4.3175],\n",
       "         [  0.6774,  -1.8019,  -1.7073],\n",
       "         [ -2.9617,   0.7507,  -3.0326],\n",
       "         [ -2.8392,   0.5318,  -2.6421],\n",
       "         [  1.9629,  -2.4460,  -4.0859],\n",
       "         [  1.9608,  -2.4512,  -4.0779],\n",
       "         [  0.9219,  -1.8855,  -1.9655],\n",
       "         [ -6.9494,   4.2206, -11.0986],\n",
       "         [  1.9537,  -2.2792,  -4.1909],\n",
       "         [  1.9650,  -2.2948,  -4.3266],\n",
       "         [  0.3965,  -1.5947,  -1.6314],\n",
       "         [  0.6323,  -1.7607,  -1.7136],\n",
       "         [  1.9521,  -2.2670,  -4.2919],\n",
       "         [ -2.9529,   0.7131,  -2.9239],\n",
       "         [ -7.3678,   4.8666, -12.1539],\n",
       "         [ -3.7980,   0.8149,  -2.7925],\n",
       "         [ -4.1016,   1.0928,  -3.3086],\n",
       "         [ -4.2674,   1.2814,  -3.5214],\n",
       "         [ -4.0340,   0.9925,  -3.2024],\n",
       "         [ -5.8821,   3.3321, -14.2418],\n",
       "         [ -3.9945,   1.1423,  -3.4394],\n",
       "         [ -3.2195,   0.3211,  -2.1386],\n",
       "         [ -2.7987,   0.2669,  -2.1277],\n",
       "         [ -2.7884,   0.3371,  -2.2788],\n",
       "         [ -2.6944,   0.4528,  -2.5832],\n",
       "         [ -2.8711,   0.6295,  -2.8536],\n",
       "         [  1.0042,  -1.9055,  -2.0686],\n",
       "         [ -2.9967,   0.8017,  -3.1021],\n",
       "         [ -0.9557,  -0.1741,  -2.6725],\n",
       "         [ -5.9079,   3.2768,  -6.6386],\n",
       "         [ -0.9387,  -0.1910,  -2.7630],\n",
       "         [ -0.8185,  -0.3353,  -2.5303],\n",
       "         [  1.9290,  -2.1772,  -4.0314],\n",
       "         [  1.9311,  -2.1954,  -4.0016],\n",
       "         [ -6.3615,   4.0384, -13.4275],\n",
       "         [  0.9615,  -1.7169,  -2.3476],\n",
       "         [  1.1991,  -1.9566,  -2.3780],\n",
       "         [  0.7969,  -1.8924,  -1.7481],\n",
       "         [  1.9678,  -2.4608,  -4.0134],\n",
       "         [  1.9940,  -2.3693,  -4.1367],\n",
       "         [  0.6590,  -1.7516,  -1.7448],\n",
       "         [  0.3788,  -1.5671,  -1.6493],\n",
       "         [  0.4168,  -1.5938,  -1.7744],\n",
       "         [  0.4168,  -1.5938,  -1.7744],\n",
       "         [  1.3865,  -1.8348,  -3.0811],\n",
       "         [  1.7514,  -1.9407,  -3.8582]]),\n",
       " tensor([[  1.8772,  -2.0757,  -4.0192],\n",
       "         [  1.8938,  -2.1046,  -4.0281],\n",
       "         [  1.9398,  -2.2504,  -3.9783],\n",
       "         [  1.9426,  -2.3092,  -4.0300],\n",
       "         [  1.9652,  -2.2521,  -4.2835],\n",
       "         [ -7.0529,   4.2916, -11.2578],\n",
       "         [  1.9614,  -2.2792,  -4.3109],\n",
       "         [ -6.9273,   4.1610, -10.6320],\n",
       "         [ -6.9762,   4.1025, -10.6795],\n",
       "         [ -7.1762,   4.1481, -12.0214],\n",
       "         [ -6.8257,   4.1071, -10.0739],\n",
       "         [ -5.7828,   2.9613,  -5.8583],\n",
       "         [ -5.7828,   2.9613,  -5.8583],\n",
       "         [ -5.9322,   2.9015,  -5.7538],\n",
       "         [ -5.7321,   3.1702,  -6.5920],\n",
       "         [ -3.9764,   1.1958,  -3.6073],\n",
       "         [ -5.8882,   3.1959,  -6.5764],\n",
       "         [ -3.9171,   0.9168,  -2.9807],\n",
       "         [ -5.9322,   2.9015,  -5.7538],\n",
       "         [ -2.7987,   0.2669,  -2.1277],\n",
       "         [ -3.0785,   0.7599,  -2.9255],\n",
       "         [ -0.8185,  -0.3353,  -2.5303],\n",
       "         [ -0.7782,  -0.4381,  -2.3341],\n",
       "         [ -3.0965,   0.8808,  -3.1765],\n",
       "         [ -5.8914,   3.2270,  -6.6051],\n",
       "         [  1.9398,  -2.2880,  -4.2199],\n",
       "         [  1.9521,  -2.2670,  -4.2919],\n",
       "         [  1.8744,  -2.0991,  -3.9728],\n",
       "         [ -4.7766,   1.1643,  -9.6045],\n",
       "         [ -3.0352,   0.8339,  -3.1644],\n",
       "         [  1.9597,  -2.2492,  -4.1706],\n",
       "         [ -2.7254,   0.4972,  -2.6632],\n",
       "         [  1.9615,  -2.2432,  -4.3037],\n",
       "         [ -2.9024,   0.7451,  -3.0837],\n",
       "         [ -3.2170,   0.3477,  -2.1963],\n",
       "         [  1.9588,  -2.4689,  -4.0396],\n",
       "         [ -0.3236,  -0.6933,  -2.3864],\n",
       "         [ -6.8587,   4.7371, -12.6751],\n",
       "         [ -0.5806,  -0.5343,  -2.3371],\n",
       "         [ -6.7544,   4.0920, -11.1834],\n",
       "         [ -4.0463,   1.1173,  -3.3176],\n",
       "         [ -3.9796,   1.0568,  -3.2404],\n",
       "         [ -2.9888,   0.8372,  -3.2180],\n",
       "         [ -3.2195,   0.3211,  -2.1386],\n",
       "         [ -5.7721,   3.1551,  -6.5760],\n",
       "         [ -0.3616,  -0.5908,  -2.4956],\n",
       "         [  0.0957,  -1.0356,  -2.1712],\n",
       "         [  1.8717,  -2.0199,  -4.1041],\n",
       "         [  1.9203,  -2.1563,  -4.0323],\n",
       "         [  1.9398,  -2.2504,  -3.9783],\n",
       "         [ -5.4961,   2.9342,  -6.3272],\n",
       "         [ -2.8835,   0.6220,  -2.8542],\n",
       "         [ -3.9292,   1.0132,  -3.2069],\n",
       "         [ -7.5411,   5.2629, -11.6903],\n",
       "         [  0.9609,  -1.8346,  -2.1466],\n",
       "         [ -2.9978,   0.7652,  -3.0516],\n",
       "         [  1.9606,  -2.4488,  -4.0527],\n",
       "         [ -6.8644,   4.1045, -10.1762],\n",
       "         [  1.9615,  -2.2432,  -4.3037],\n",
       "         [ -2.8212,   0.3003,  -2.1741],\n",
       "         [ -2.7821,   0.0894,  -1.8972],\n",
       "         [  0.9938,  -1.8355,  -2.2031],\n",
       "         [  0.7832,  -1.6218,  -2.2365],\n",
       "         [  0.9782,  -1.7203,  -2.3756]]),\n",
       " tensor([[  1.0134,  -1.7103,  -2.4514],\n",
       "         [ -6.8644,   4.1045, -10.1762],\n",
       "         [  0.9602,  -1.9049,  -1.9903],\n",
       "         [  0.8530,  -1.8706,  -1.8668],\n",
       "         [  1.9609,  -2.4634,  -4.0406],\n",
       "         [  1.9645,  -2.4266,  -4.0857],\n",
       "         [  1.6899,  -1.9307,  -3.7016],\n",
       "         [  1.9621,  -2.4327,  -4.0995],\n",
       "         [  1.8682,  -2.0257,  -4.0848],\n",
       "         [  1.8744,  -2.0991,  -3.9728],\n",
       "         [  1.9543,  -2.2616,  -4.0492],\n",
       "         [  1.9539,  -2.2827,  -4.2964],\n",
       "         [ -6.4637,   4.1670,  -9.3705],\n",
       "         [ -5.8495,   3.4746,  -7.1233],\n",
       "         [ -5.8875,   3.0584,  -6.2625],\n",
       "         [ -6.5473,   4.3993, -13.3285],\n",
       "         [ -3.9407,   0.8496,  -2.9008],\n",
       "         [ -3.2868,   0.2746,  -2.0691],\n",
       "         [ -5.8354,   2.9433,  -5.8815],\n",
       "         [ -3.2192,   0.2967,  -2.0606],\n",
       "         [ -2.8921,   0.1833,  -2.0802],\n",
       "         [ -2.7330,   0.2913,  -2.2395],\n",
       "         [ -6.8043,   4.0786,  -9.7725],\n",
       "         [  1.0222,  -1.8942,  -2.1265],\n",
       "         [ -3.0279,   0.8100,  -3.0833],\n",
       "         [  0.6522,  -1.7589,  -1.7376],\n",
       "         [  0.0441,  -1.0181,  -2.1497],\n",
       "         [  0.7554,  -1.5512,  -2.2921],\n",
       "         [ -6.9434,   4.2316, -11.2057],\n",
       "         [ -3.2195,   0.3211,  -2.1386],\n",
       "         [ -0.9557,  -0.1741,  -2.6725],\n",
       "         [ -4.0670,   1.0403,  -3.2078],\n",
       "         [ -7.1692,   4.8203, -12.1320],\n",
       "         [  1.9641,  -2.4607,  -4.0633],\n",
       "         [ -6.9112,   4.1967, -11.4791],\n",
       "         [  0.8180,  -1.8747,  -1.7938],\n",
       "         [ -7.4803,   4.6212, -11.6610],\n",
       "         [ -2.9403,   0.6873,  -2.8933],\n",
       "         [  1.9513,  -2.2471,  -4.0281],\n",
       "         [ -2.7470,   0.1038,  -1.9132],\n",
       "         [  1.8798,  -2.0920,  -4.0020],\n",
       "         [ -2.6858,   0.0915,  -1.8949],\n",
       "         [  0.8064,  -1.8881,  -1.7494],\n",
       "         [  1.9558,  -2.4517,  -4.0455],\n",
       "         [  0.5778,  -1.7076,  -1.7048],\n",
       "         [  0.1451,  -1.1270,  -2.0415],\n",
       "         [  1.9500,  -2.4821,  -3.9879],\n",
       "         [  0.9609,  -1.8346,  -2.1466],\n",
       "         [ -0.8967,  -0.2428,  -2.6727],\n",
       "         [ -2.9615,   0.6992,  -2.9183],\n",
       "         [  1.9531,  -2.2763,  -4.2939],\n",
       "         [  1.9543,  -2.2616,  -4.0492],\n",
       "         [  0.5710,  -1.6703,  -1.7560],\n",
       "         [ -5.5740,   2.9461,  -6.2555],\n",
       "         [  0.6522,  -1.7589,  -1.7376],\n",
       "         [  0.8031,  -1.5580,  -2.3646],\n",
       "         [  1.9621,  -2.4502,  -4.0738],\n",
       "         [  1.9622,  -2.4666,  -4.0458],\n",
       "         [ -0.9273,  -0.1492,  -2.8184],\n",
       "         [ -3.1165,   0.6940,  -2.9535],\n",
       "         [  0.6323,  -1.7607,  -1.7136],\n",
       "         [ -6.9533,   4.2409, -11.4908],\n",
       "         [ -5.8036,   3.0002,  -6.0181],\n",
       "         [ -0.1656,  -0.7612,  -2.4124]]),\n",
       " tensor([[-5.8574e+00,  3.0908e+00, -6.3139e+00],\n",
       "         [ 4.3000e-01,  2.6203e+00, -1.1026e+01],\n",
       "         [ 1.4335e+00,  1.8016e+00, -1.1206e+01],\n",
       "         [ 2.3217e+00,  9.4737e-01, -1.1575e+01],\n",
       "         [-4.0422e+00,  3.1633e+00, -1.6197e+01],\n",
       "         [ 3.4598e-01,  2.4911e+00, -1.0615e+01],\n",
       "         [ 1.0849e+00,  2.1941e+00, -1.1472e+01],\n",
       "         [ 3.1054e-01,  2.6669e+00, -1.1787e+01],\n",
       "         [ 9.2808e-01,  2.1818e+00, -1.1160e+01],\n",
       "         [ 2.4268e+00,  9.0781e-01, -1.1938e+01],\n",
       "         [ 8.5400e-01,  2.3754e+00, -1.1344e+01],\n",
       "         [ 4.8929e-01,  2.5019e+00, -1.0862e+01],\n",
       "         [ 3.1559e+00, -2.5099e-01, -1.0279e+01],\n",
       "         [-3.4345e+00,  2.2072e+00, -6.8753e+00],\n",
       "         [-3.5957e+00,  2.4219e+00, -7.9781e+00],\n",
       "         [ 1.9698e+00,  4.5588e-02, -7.1688e+00],\n",
       "         [ 4.0534e+00,  1.9144e+00, -1.5266e+01],\n",
       "         [ 7.0652e+00, -7.4547e-01, -1.9481e+01],\n",
       "         [ 7.0942e+00, -4.6966e-01, -1.9646e+01],\n",
       "         [ 7.0644e+00, -7.0036e-01, -1.9419e+01],\n",
       "         [-1.7615e-01,  2.4472e+00, -7.7347e+00],\n",
       "         [-3.0174e-01,  2.1017e+00, -9.9168e+00],\n",
       "         [-2.1246e+00,  4.0802e+00, -1.5498e+01],\n",
       "         [-1.9608e+00,  4.0164e+00, -1.5543e+01],\n",
       "         [-2.5801e+00,  4.0766e+00, -1.4385e+01],\n",
       "         [ 1.6891e+00,  6.9970e-01, -1.0959e+01],\n",
       "         [-6.2030e+00,  2.5838e+00, -1.6388e+01],\n",
       "         [-2.4990e-01,  1.3188e+00, -7.1792e+00],\n",
       "         [ 1.3496e+00, -2.8407e-01, -6.1788e+00],\n",
       "         [ 3.3343e+00, -7.3308e-01, -9.4606e+00],\n",
       "         [ 2.1136e+00,  3.2408e-01, -9.8766e+00],\n",
       "         [-2.4222e+00,  2.3471e+00, -2.2325e+01],\n",
       "         [ 2.4052e+00, -1.4163e-01, -9.7935e+00],\n",
       "         [ 2.1514e+00,  3.4447e-01, -1.0261e+01],\n",
       "         [ 1.9631e+00,  1.1022e+00, -1.2717e+01],\n",
       "         [ 2.1464e+00,  3.3272e-01, -1.0138e+01],\n",
       "         [ 8.4598e-02,  2.7044e+00, -1.3624e+01],\n",
       "         [-1.9493e+00,  4.9354e+00, -2.1597e+01],\n",
       "         [ 1.6258e-01,  2.9587e+00, -1.5015e+01],\n",
       "         [ 3.2024e-01,  3.1565e+00, -1.6310e+01],\n",
       "         [ 2.3362e+00, -3.1939e-02, -9.9458e+00],\n",
       "         [ 2.4071e+00, -1.4545e-01, -9.7715e+00],\n",
       "         [ 2.2501e+00,  1.9695e-01, -1.0294e+01],\n",
       "         [ 2.1129e+00,  3.2386e-01, -9.8157e+00],\n",
       "         [-4.8056e+00,  4.5673e+00, -2.3148e+01],\n",
       "         [ 1.3970e-01,  3.2402e+00, -1.6372e+01],\n",
       "         [ 2.2713e+00,  1.9588e-01, -1.0445e+01],\n",
       "         [ 2.3090e+00,  2.2398e-02, -9.9728e+00],\n",
       "         [ 2.0966e+00,  5.2234e-01, -1.0783e+01],\n",
       "         [-1.9236e+00,  4.8698e+00, -2.1511e+01],\n",
       "         [-1.3793e+00,  4.1710e+00, -1.7752e+01],\n",
       "         [ 1.9029e+00, -8.5324e-01, -5.5802e+00],\n",
       "         [ 1.5476e+00, -8.2929e-01, -4.9238e+00],\n",
       "         [ 2.4334e+00, -2.9361e+00, -3.7450e+00],\n",
       "         [ 2.4334e+00, -2.9361e+00, -3.7450e+00],\n",
       "         [ 2.4322e+00, -2.9800e+00, -3.6714e+00],\n",
       "         [-4.3588e+00,  1.5947e+00, -5.8472e+00],\n",
       "         [ 2.9910e+00, -6.4070e-01, -8.5361e+00],\n",
       "         [ 3.0996e+00, -6.2099e-01, -8.9286e+00],\n",
       "         [ 5.6332e+00, -3.8972e-01, -1.5650e+01],\n",
       "         [ 5.5815e+00, -3.7121e-01, -1.5468e+01],\n",
       "         [-1.8188e+00,  4.5039e+00, -1.0660e+01],\n",
       "         [-3.2911e+00,  3.4960e+00, -9.6248e+00],\n",
       "         [-3.3118e+00,  4.7658e+00, -1.1223e+01]]),\n",
       " tensor([[ 5.3263e+00, -2.3569e-01, -1.5240e+01],\n",
       "         [ 2.3600e+00,  2.6261e+00, -1.6160e+01],\n",
       "         [-2.7142e+00,  5.1726e+00, -1.1457e+01],\n",
       "         [-1.9477e+00,  4.4453e+00, -9.2315e+00],\n",
       "         [ 2.7854e+00,  2.0052e+00, -1.5592e+01],\n",
       "         [ 5.3879e+00, -3.8113e-01, -1.4999e+01],\n",
       "         [ 5.6478e+00, -3.7814e-01, -1.5815e+01],\n",
       "         [ 5.4250e+00, -2.8641e-01, -1.5135e+01],\n",
       "         [-1.6812e+00,  4.4231e+00, -9.6848e+00],\n",
       "         [ 5.2927e+00, -2.8540e-01, -1.4860e+01],\n",
       "         [ 2.3557e+00, -2.4812e-02, -6.9986e+00],\n",
       "         [ 2.1563e+00,  1.7553e-01, -6.7163e+00],\n",
       "         [ 3.0991e-01,  1.0886e+00, -3.1936e+00],\n",
       "         [ 8.0359e-02,  1.4927e+00, -4.4318e+00],\n",
       "         [-1.8751e+00,  2.6266e+00, -1.3331e+01],\n",
       "         [ 1.7648e+00, -3.3903e-01, -6.9281e+00],\n",
       "         [ 1.7526e+00, -3.4228e-01, -6.8901e+00],\n",
       "         [ 1.7511e+00, -3.3908e-01, -6.8901e+00],\n",
       "         [ 1.7608e+00, -3.4111e-01, -6.9006e+00],\n",
       "         [-1.4049e+00,  2.0219e+00, -7.3490e+00],\n",
       "         [ 1.3292e-01, -4.9269e-01, -2.1678e+01],\n",
       "         [ 1.7628e+00, -2.6457e-01, -7.0538e+00],\n",
       "         [ 4.7869e-02, -5.0894e-01, -2.1120e+01],\n",
       "         [ 1.7611e+00, -3.4375e-01, -6.9057e+00],\n",
       "         [ 2.9850e+00,  2.5390e-01, -1.1173e+01],\n",
       "         [ 3.0176e+00,  2.5169e-02, -1.0605e+01],\n",
       "         [ 3.0399e+00,  6.3650e-03, -1.0638e+01],\n",
       "         [-5.2623e-01,  3.1312e+00, -1.0775e+01],\n",
       "         [-2.1675e+00,  3.4947e+00, -1.7076e+01],\n",
       "         [-1.9155e+00,  1.3274e+00, -3.9653e+00],\n",
       "         [ 8.6998e-01, -3.6793e-03, -5.7901e+00],\n",
       "         [-4.4884e+00,  5.2665e+00, -1.4014e+01],\n",
       "         [ 3.1467e+00, -5.7909e-01, -8.3674e+00],\n",
       "         [ 3.1793e+00, -4.6424e-01, -8.9306e+00],\n",
       "         [ 3.2693e+00, -3.0235e-01, -9.7956e+00],\n",
       "         [ 2.8480e+00,  3.7629e-01, -9.9566e+00],\n",
       "         [ 3.1368e+00, -5.9385e-01, -8.3006e+00],\n",
       "         [ 3.1453e+00, -5.8760e-01, -8.3426e+00],\n",
       "         [-3.2031e+00,  4.7037e+00, -1.2510e+01],\n",
       "         [-9.3285e-01,  3.1968e+00, -9.1023e+00],\n",
       "         [-2.7823e+00,  4.3616e+00, -1.1458e+01],\n",
       "         [ 3.2510e+00, -3.1947e-01, -9.6825e+00],\n",
       "         [ 3.1738e+00, -4.5932e-01, -8.9387e+00],\n",
       "         [ 1.1473e+00, -5.8334e-01, -4.0343e+00],\n",
       "         [ 2.4152e-01,  1.2203e+00, -6.9968e+00],\n",
       "         [ 9.3241e-01, -5.0062e-01, -3.6415e+00],\n",
       "         [-8.4418e-01,  2.0501e+00, -1.0187e+01],\n",
       "         [ 2.1709e+00, -1.9198e+00, -4.4857e+00],\n",
       "         [-1.0327e+00,  1.5097e+00, -1.0611e+01],\n",
       "         [ 1.9109e+00, -7.1006e-01, -6.2205e+00],\n",
       "         [-4.6117e+00,  3.3535e+00, -9.9429e+00],\n",
       "         [ 1.3424e+00, -1.0854e+00, -4.4065e+00],\n",
       "         [ 1.3471e+00, -1.1118e+00, -4.3942e+00],\n",
       "         [-4.2179e+00,  4.3488e+00, -1.1267e+01],\n",
       "         [-5.9398e+00,  4.5101e+00, -2.0960e+01],\n",
       "         [ 3.0368e+00, -3.9082e+00, -3.4789e+00],\n",
       "         [-5.2987e+00,  1.4643e+00, -7.8120e+00],\n",
       "         [ 1.0030e+00, -2.0824e-02, -5.6109e+00],\n",
       "         [ 1.0093e+00, -3.6449e-02, -5.5848e+00],\n",
       "         [ 8.3345e-01,  3.9487e-01, -7.0321e+00],\n",
       "         [ 9.6365e-01,  9.7358e-02, -5.9540e+00],\n",
       "         [ 2.3331e+00, -1.0588e+00, -6.7134e+00],\n",
       "         [-6.0475e-01,  2.4062e+00, -1.0303e+01],\n",
       "         [ 3.2419e+00, -1.2269e+00, -7.9661e+00]]),\n",
       " tensor([[ 1.7275e+00, -8.9216e-01, -5.7712e+00],\n",
       "         [-1.5444e+00,  2.9682e+00, -1.0903e+01],\n",
       "         [-1.1532e+00,  1.1195e+00, -3.7988e+00],\n",
       "         [ 1.8651e+00, -1.8121e+00, -4.2258e+00],\n",
       "         [ 1.8660e+00, -1.8063e+00, -4.2489e+00],\n",
       "         [ 8.5825e-01,  1.1656e+00, -7.9561e+00],\n",
       "         [ 1.7304e+00, -8.9098e-01, -5.7914e+00],\n",
       "         [ 2.1063e+00, -1.3661e+00, -5.3158e+00],\n",
       "         [ 2.1086e+00, -1.3448e+00, -5.3852e+00],\n",
       "         [ 2.1077e+00, -1.3521e+00, -5.3758e+00],\n",
       "         [-7.1164e-01,  1.1430e+00, -7.4897e+00],\n",
       "         [ 1.0118e+00, -1.7306e-01, -4.4002e+00],\n",
       "         [ 9.3513e-01,  4.3801e-02, -4.5857e+00],\n",
       "         [-7.3198e-01,  1.1178e+00, -7.3770e+00],\n",
       "         [ 3.6147e-01,  2.2950e+00, -1.2712e+01],\n",
       "         [ 2.5321e+00, -3.5800e-01, -8.5005e+00],\n",
       "         [ 2.5142e+00, -3.6124e-01, -8.4946e+00],\n",
       "         [ 2.7499e-01,  2.3964e+00, -1.2820e+01],\n",
       "         [ 1.0332e+00,  1.4215e+00, -1.1125e+01],\n",
       "         [ 3.5583e-01,  2.2288e+00, -1.2069e+01],\n",
       "         [ 2.0998e-03,  3.4550e+00, -1.6612e+01],\n",
       "         [-2.7979e+00,  5.5102e+00, -2.0562e+01],\n",
       "         [ 2.5137e+00, -1.0341e+00, -6.1241e+00],\n",
       "         [ 2.5979e+00, -1.0939e+00, -6.3041e+00],\n",
       "         [ 2.6015e+00, -1.0970e+00, -6.3110e+00],\n",
       "         [ 9.9309e-02,  2.3316e+00, -1.6100e+01],\n",
       "         [ 2.6866e+00, -7.1961e-01, -8.1748e+00],\n",
       "         [ 1.6544e-01,  1.7823e+00, -1.1968e+01],\n",
       "         [ 1.2113e+00, -1.0896e+00, -4.5032e+00],\n",
       "         [ 1.4830e+00, -6.9237e-02, -6.1926e+00],\n",
       "         [ 1.4774e+00, -7.1530e-02, -6.0945e+00],\n",
       "         [ 1.5316e+00, -1.3123e-01, -6.1783e+00],\n",
       "         [-1.3246e+00,  1.9938e+00, -5.3087e+00],\n",
       "         [ 3.1613e+00, -5.3172e-01, -1.0194e+01],\n",
       "         [ 3.1503e+00, -4.7974e-01, -1.0306e+01],\n",
       "         [ 1.4612e+00, -2.1719e+00, -3.1171e+00],\n",
       "         [ 1.4627e+00, -2.1541e+00, -3.1500e+00],\n",
       "         [ 1.4619e+00, -2.1745e+00, -3.1170e+00],\n",
       "         [ 4.1664e+00, -1.1632e+00, -1.0832e+01],\n",
       "         [ 4.1387e+00, -1.1284e+00, -1.0827e+01],\n",
       "         [ 4.0791e+00, -5.5770e-01, -1.2679e+01],\n",
       "         [ 1.3286e+00, -2.3542e+00, -2.6097e+00],\n",
       "         [ 1.7313e+00, -1.2260e+00, -5.9835e+00],\n",
       "         [ 8.0911e-01,  8.0579e-01, -8.3052e+00],\n",
       "         [ 2.0590e+00,  3.9335e-02, -9.0379e+00],\n",
       "         [ 3.9954e+00, -6.6492e-01, -1.3011e+01],\n",
       "         [-2.6155e+00,  4.4651e+00, -1.1697e+01],\n",
       "         [-3.4023e+00,  5.4099e+00, -1.6973e+01],\n",
       "         [-2.5998e+00,  4.5276e+00, -1.1682e+01],\n",
       "         [-3.8600e-01, -2.6913e-01, -2.5834e+00],\n",
       "         [-2.9852e+00,  4.1400e+00, -1.3825e+01],\n",
       "         [ 1.7032e+00, -8.6560e-02, -7.0803e+00],\n",
       "         [ 1.7120e+00, -8.5459e-02, -7.1228e+00],\n",
       "         [ 1.6141e+00, -8.0151e-02, -6.7436e+00],\n",
       "         [-2.2464e-02,  2.4854e+00, -1.1757e+01],\n",
       "         [-2.8660e+00,  2.4755e+00, -1.1071e+01],\n",
       "         [-2.6442e+00,  3.3540e+00, -8.0571e+00],\n",
       "         [-1.0356e+00,  2.1528e+00, -6.9568e+00],\n",
       "         [ 1.8991e+00,  1.6675e-01, -7.9008e+00],\n",
       "         [-2.7783e+00,  3.5484e+00, -8.9056e+00],\n",
       "         [-2.4344e+00,  1.9437e-01, -5.6201e+00],\n",
       "         [-3.1865e-01,  9.9312e-01, -5.6857e+00],\n",
       "         [ 1.6841e+00, -2.2201e+00, -2.9482e+00],\n",
       "         [ 1.6805e+00, -2.2237e+00, -2.9373e+00]]),\n",
       " tensor([[  1.6853,  -2.2232,  -2.9509],\n",
       "         [  1.6857,  -2.2338,  -2.9387],\n",
       "         [  3.9239,  -1.3693,  -8.6210],\n",
       "         [  0.1312,   0.4930,  -4.2393],\n",
       "         [  1.3308,  -1.2246,  -3.7821],\n",
       "         [  1.3158,  -1.2297,  -3.7212],\n",
       "         [  1.2523,  -1.2616,  -3.6568],\n",
       "         [  1.3227,  -1.1854,  -3.8226],\n",
       "         [  1.3194,  -1.1728,  -3.8398],\n",
       "         [  1.3144,  -1.1889,  -3.7893],\n",
       "         [ -1.1423,   1.8092,  -9.9980],\n",
       "         [  2.2065,  -0.3969,  -7.3704],\n",
       "         [  2.2007,  -0.4307,  -7.1988],\n",
       "         [ -0.7681,   1.8139,  -7.9569],\n",
       "         [  1.5688,  -1.2623,  -5.2904],\n",
       "         [  1.3623,  -0.5695,  -4.8628],\n",
       "         [ -1.4000,   0.1069,  -1.6602],\n",
       "         [ -2.1086,   0.9671,  -2.9270],\n",
       "         [  1.3600,  -0.4109,  -5.0649],\n",
       "         [ -5.3937,   5.3995, -16.4637],\n",
       "         [ -5.3875,   5.4102, -16.4080],\n",
       "         [  1.3596,   1.4722, -11.0101],\n",
       "         [ -1.3689,   1.6870, -13.2038],\n",
       "         [  1.9667,  -0.1266,  -7.0681],\n",
       "         [  0.1984,   1.0382,  -5.8996],\n",
       "         [ -0.1629,   1.3876,  -7.2716],\n",
       "         [ -0.6735,   1.1390,  -7.5268],\n",
       "         [  1.0388,   0.7027,  -6.5822],\n",
       "         [  1.3835,   0.2189,  -4.5018],\n",
       "         [  2.8645,  -1.3240,  -5.9368],\n",
       "         [ -1.4089,   1.3338,  -3.2013],\n",
       "         [  2.8512,  -1.3232,  -5.8921],\n",
       "         [  2.8584,  -1.3258,  -5.9145],\n",
       "         [  1.4303,  -1.2623,  -4.1638],\n",
       "         [  1.4287,  -1.2793,  -4.1365],\n",
       "         [ -2.6876,   2.9368,  -9.9584],\n",
       "         [  1.2030,  -0.5788,  -4.7458],\n",
       "         [  1.4277,  -1.2775,  -4.1574],\n",
       "         [  1.4181,  -1.2231,  -4.1454],\n",
       "         [  1.9772,  -0.4180,  -7.2755],\n",
       "         [  1.9847,  -0.4300,  -7.2613],\n",
       "         [  3.5259,  -1.8065,  -6.6961],\n",
       "         [  0.3567,   0.9302,  -6.2843],\n",
       "         [  0.4367,   0.8559,  -6.2891],\n",
       "         [  1.8830,  -0.6118,  -5.5015],\n",
       "         [  1.8674,  -2.1360,  -2.5582],\n",
       "         [  1.9756,  -0.3516,  -6.4325],\n",
       "         [  1.9871,  -0.4080,  -6.3373],\n",
       "         [  1.9634,  -0.3699,  -6.3374],\n",
       "         [  1.0478,  -2.4702,  -1.3066],\n",
       "         [  1.0614,  -2.4717,  -1.3352],\n",
       "         [  1.4542,  -2.3807,  -1.1093],\n",
       "         [  1.4758,  -2.7998,  -1.2116],\n",
       "         [  2.0252,  -2.0820,  -4.6079],\n",
       "         [  2.6745,  -0.6665,  -8.3397],\n",
       "         [  1.5816,  -0.6273,  -6.3749],\n",
       "         [  1.6231,  -2.4642,  -2.2450],\n",
       "         [  2.1874,  -1.9456,  -4.6899],\n",
       "         [  1.9009,  -0.3455,  -6.4588],\n",
       "         [  2.3153,  -0.0329,  -9.2113],\n",
       "         [  2.0630,  -1.5397,  -5.9336],\n",
       "         [  2.0644,  -1.5493,  -5.9251],\n",
       "         [  2.0632,  -1.5533,  -5.9142],\n",
       "         [  4.5539,  -0.9176, -12.9652]]),\n",
       " tensor([[ 3.8882e+00, -1.0430e+00, -8.2138e+00],\n",
       "         [ 1.1552e+00, -2.8668e+00, -1.0193e+00],\n",
       "         [ 1.6249e+00,  1.7499e-03, -6.6432e+00],\n",
       "         [ 2.0666e+00, -1.5318e+00, -5.9576e+00],\n",
       "         [ 2.0695e+00, -1.5420e+00, -5.9598e+00],\n",
       "         [ 2.3165e+00, -2.2791e-02, -9.2562e+00],\n",
       "         [ 2.3088e+00, -1.5501e-02, -9.2744e+00],\n",
       "         [ 2.3168e+00, -2.3031e-02, -9.2640e+00],\n",
       "         [ 2.7396e+00,  4.4579e-01, -1.1422e+01],\n",
       "         [ 2.1336e+00, -1.5884e+00, -5.6385e+00],\n",
       "         [ 2.0613e+00, -1.5009e+00, -5.9640e+00],\n",
       "         [ 2.0672e+00, -1.5262e+00, -5.9692e+00],\n",
       "         [ 2.0638e+00, -1.5375e+00, -5.9366e+00],\n",
       "         [ 2.0735e+00, -1.5014e+00, -6.0254e+00],\n",
       "         [ 2.0677e+00, -1.5535e+00, -5.9357e+00],\n",
       "         [ 2.0664e+00, -1.5487e+00, -5.9317e+00],\n",
       "         [ 2.0630e+00, -1.5794e+00, -5.8770e+00],\n",
       "         [ 2.0652e+00, -1.5189e+00, -5.9707e+00],\n",
       "         [ 1.4937e+00, -2.8352e+00, -1.1932e+00],\n",
       "         [ 4.1082e+00, -1.3009e+00, -9.8754e+00],\n",
       "         [ 4.0963e+00, -1.2875e+00, -9.8765e+00],\n",
       "         [ 4.0845e+00, -1.2634e+00, -9.9104e+00],\n",
       "         [ 4.1176e+00, -1.2979e+00, -9.9162e+00],\n",
       "         [ 1.1417e+00, -2.8149e+00, -1.1154e+00],\n",
       "         [ 1.1561e+00, -2.8691e+00, -1.0486e+00],\n",
       "         [ 2.9791e+00, -7.7965e-01, -8.0938e+00],\n",
       "         [ 2.9626e+00, -7.7573e-01, -8.0609e+00],\n",
       "         [ 2.9728e+00, -7.5907e-01, -8.1305e+00],\n",
       "         [ 2.9633e+00, -7.7085e-01, -8.0757e+00],\n",
       "         [ 3.0339e+00, -2.0758e+00, -7.1358e+00],\n",
       "         [ 2.0130e+00, -4.2942e-01, -6.3528e+00],\n",
       "         [ 1.7667e+00, -1.0602e-01, -7.8457e+00],\n",
       "         [ 2.2123e+00, -2.3437e+00, -5.6899e+00],\n",
       "         [ 3.6643e-01,  8.9819e-01, -6.1763e+00],\n",
       "         [ 2.4995e+00, -4.1972e-01, -7.6914e+00],\n",
       "         [ 3.2315e+00, -5.6996e-01, -1.0136e+01],\n",
       "         [ 2.3177e+00, -1.8000e+00, -5.3514e+00],\n",
       "         [ 1.4857e+00, -2.8227e+00, -1.2130e+00],\n",
       "         [ 2.0616e+00, -1.5444e+00, -5.9158e+00],\n",
       "         [ 2.0689e+00, -1.5514e+00, -5.9435e+00],\n",
       "         [ 2.0710e+00, -1.5082e+00, -6.0098e+00],\n",
       "         [ 2.0648e+00, -1.5221e+00, -5.9530e+00],\n",
       "         [ 2.0603e+00, -1.5490e+00, -5.9092e+00],\n",
       "         [ 2.0681e+00, -1.5289e+00, -5.9753e+00],\n",
       "         [ 2.0696e+00, -1.5225e+00, -5.9867e+00],\n",
       "         [ 2.0665e+00, -1.5369e+00, -5.9562e+00],\n",
       "         [ 2.0702e+00, -1.5330e+00, -5.9766e+00],\n",
       "         [ 2.0642e+00, -1.5474e+00, -5.9256e+00],\n",
       "         [ 2.0662e+00, -1.4948e+00, -5.9990e+00],\n",
       "         [ 2.0651e+00, -1.5294e+00, -5.9539e+00],\n",
       "         [ 2.0648e+00, -1.5335e+00, -5.9497e+00],\n",
       "         [ 2.0634e+00, -1.5623e+00, -5.9005e+00],\n",
       "         [ 1.9552e+00, -2.4898e+00, -3.6402e+00],\n",
       "         [ 1.4735e+00, -1.7841e+00, -3.2193e+00],\n",
       "         [ 1.4774e+00, -1.7940e+00, -3.2302e+00],\n",
       "         [ 2.1185e+00, -1.6890e+00, -4.9642e+00],\n",
       "         [ 2.1255e+00, -1.7100e+00, -4.9582e+00],\n",
       "         [ 2.1161e+00, -1.6996e+00, -4.9426e+00],\n",
       "         [ 2.8847e+00, -9.7797e-01, -7.0966e+00],\n",
       "         [ 2.7690e+00, -1.2907e+00, -6.0277e+00],\n",
       "         [ 4.1431e+00, -1.0377e+00, -9.9014e+00],\n",
       "         [ 4.1422e+00, -1.0420e+00, -9.8847e+00],\n",
       "         [ 4.1588e+00, -1.0426e+00, -9.9330e+00],\n",
       "         [ 4.1436e+00, -1.0417e+00, -9.8880e+00]]),\n",
       " tensor([[ 4.1575e+00, -1.0482e+00, -9.9086e+00],\n",
       "         [ 4.1590e+00, -1.0484e+00, -9.9154e+00],\n",
       "         [ 2.1327e+00, -5.8312e-01, -6.4766e+00],\n",
       "         [ 2.3574e+00, -3.3293e-01, -8.4470e+00],\n",
       "         [ 2.0634e+00, -1.5573e+00, -5.9138e+00],\n",
       "         [ 1.4907e+00, -2.7948e+00, -1.2525e+00],\n",
       "         [ 1.4897e+00, -2.8607e+00, -1.1356e+00],\n",
       "         [ 1.4900e+00, -2.8216e+00, -1.1852e+00],\n",
       "         [ 1.5014e+00, -2.8250e+00, -1.1888e+00],\n",
       "         [ 1.4887e+00, -2.8342e+00, -1.1599e+00],\n",
       "         [ 1.4892e+00, -2.8510e+00, -1.1452e+00],\n",
       "         [ 1.4709e+00, -2.8200e+00, -1.0949e+00],\n",
       "         [ 9.3540e-01, -2.2274e+00, -9.8785e-01],\n",
       "         [ 2.8858e+00, -9.9431e-01, -7.0393e+00],\n",
       "         [ 1.3629e+00, -2.8226e-01, -6.2941e+00],\n",
       "         [ 2.0516e+00, -3.3109e-02, -8.2320e+00],\n",
       "         [ 1.5901e+00, -1.2365e+00, -4.9741e+00],\n",
       "         [ 1.8924e+00, -2.8745e-03, -7.3203e+00],\n",
       "         [ 2.1760e+00, -1.9490e+00, -4.4225e+00],\n",
       "         [ 2.1734e+00, -1.8991e+00, -4.5071e+00],\n",
       "         [ 1.9744e+00, -3.0615e+00, -2.1786e+00],\n",
       "         [ 2.4565e+00, -6.1102e-01, -8.5473e+00],\n",
       "         [ 2.3074e+00, -2.2006e-02, -9.2331e+00],\n",
       "         [ 1.8455e+00, -1.6323e+00, -4.9059e+00],\n",
       "         [ 1.8261e+00, -1.6616e+00, -4.8299e+00],\n",
       "         [ 1.8427e+00, -1.6278e+00, -4.9102e+00],\n",
       "         [ 2.1902e+00, -5.5427e-01, -7.0525e+00],\n",
       "         [ 2.1853e+00, -5.5828e-01, -7.0202e+00],\n",
       "         [ 1.3748e+00, -3.3718e-01, -4.8686e+00],\n",
       "         [ 2.1607e+00, -1.0806e+00, -5.4425e+00],\n",
       "         [ 2.1634e+00, -1.0883e+00, -5.4140e+00],\n",
       "         [ 2.1311e+00, -1.0639e+00, -5.4127e+00],\n",
       "         [ 2.1469e+00, -1.0758e+00, -5.4245e+00],\n",
       "         [ 1.4057e+00, -4.2571e-01, -5.9883e+00],\n",
       "         [ 1.4064e+00, -4.3053e-01, -5.9968e+00],\n",
       "         [ 3.3125e+00, -7.0922e-01, -9.4729e+00],\n",
       "         [ 3.2855e+00, -5.5747e-01, -9.8524e+00],\n",
       "         [ 3.3185e+00, -7.1936e-01, -9.4651e+00],\n",
       "         [ 3.3306e+00, -7.2476e-01, -9.4812e+00],\n",
       "         [ 1.4931e+00, -2.8540e+00, -1.1671e+00],\n",
       "         [ 2.0515e+00, -2.6806e+00, -2.8444e+00],\n",
       "         [ 1.4862e+00,  4.7937e-01, -8.5320e+00],\n",
       "         [ 1.5699e+00,  2.9005e-01, -8.2559e+00],\n",
       "         [ 1.5682e+00,  2.8272e-01, -8.2194e+00],\n",
       "         [ 1.5775e+00,  2.8318e-01, -8.2901e+00],\n",
       "         [ 1.9558e+00,  5.4083e-01, -1.0603e+01],\n",
       "         [ 1.9479e+00,  5.3760e-01, -1.0568e+01],\n",
       "         [ 2.3151e+00, -2.5582e-02, -9.2483e+00],\n",
       "         [ 2.3096e+00, -3.8418e-02, -9.1826e+00],\n",
       "         [ 1.7198e+00, -7.4905e-01, -8.2056e+00],\n",
       "         [ 1.7418e+00, -7.6924e-01, -8.2111e+00],\n",
       "         [ 1.4815e+00, -2.7916e+00, -1.2333e+00],\n",
       "         [ 2.3097e+00, -3.9696e-02, -9.1286e+00],\n",
       "         [ 1.4876e+00, -2.8400e+00, -1.1807e+00],\n",
       "         [ 2.3666e+00, -3.0038e+00, -4.2543e+00],\n",
       "         [ 1.5725e+00, -1.4701e+00, -4.6183e+00],\n",
       "         [ 7.6901e-01, -9.1414e-01, -4.1934e+00],\n",
       "         [ 8.1898e-01, -9.7100e-01, -4.1926e+00],\n",
       "         [ 1.7739e+00, -7.1789e-01, -6.6616e+00],\n",
       "         [ 1.7705e+00, -7.0848e-01, -6.6563e+00],\n",
       "         [ 1.7756e+00, -7.3960e-01, -6.6426e+00],\n",
       "         [ 1.7831e+00, -7.0917e-01, -6.7145e+00],\n",
       "         [ 2.1360e+00, -1.0714e+00, -5.4119e+00],\n",
       "         [ 2.7699e+00, -4.7441e-01, -8.9580e+00]]),\n",
       " tensor([[  1.7382,  -0.8138,  -6.8377],\n",
       "         [  1.7405,  -0.8094,  -6.8540],\n",
       "         [  2.7397,   0.4458, -11.4007],\n",
       "         [  2.0687,  -2.2939,  -3.2470],\n",
       "         [  1.4887,  -2.8353,  -1.1854],\n",
       "         [  1.4866,  -2.8485,  -1.1506],\n",
       "         [  1.4956,  -2.8507,  -1.1759],\n",
       "         [  1.4893,  -2.8578,  -1.1560],\n",
       "         [  1.4910,  -2.8392,  -1.1802],\n",
       "         [  1.4893,  -2.8304,  -1.1987],\n",
       "         [  1.4885,  -2.8265,  -1.1873],\n",
       "         [  1.4963,  -2.8009,  -1.2463],\n",
       "         [  1.4925,  -2.8223,  -1.1789],\n",
       "         [  1.4982,  -2.8238,  -1.2158],\n",
       "         [  1.4904,  -2.8262,  -1.2069],\n",
       "         [  1.4901,  -2.8676,  -1.1264],\n",
       "         [  1.4858,  -2.8165,  -1.2072],\n",
       "         [  1.4872,  -2.8285,  -1.1747],\n",
       "         [  2.0649,  -1.5413,  -5.9245],\n",
       "         [  1.4888,  -2.8521,  -1.1494],\n",
       "         [  1.4895,  -2.8755,  -1.1324],\n",
       "         [  1.4913,  -2.8307,  -1.1702],\n",
       "         [  1.4941,  -2.8082,  -1.2469],\n",
       "         [  2.1026,  -0.1122,  -9.2572],\n",
       "         [  2.9786,  -0.7689,  -8.1235],\n",
       "         [  2.9518,  -0.7759,  -8.0243],\n",
       "         [  3.6197,  -0.8180,  -9.7524],\n",
       "         [  3.6360,  -0.8348,  -9.7440],\n",
       "         [  4.1181,  -1.2858,  -9.9557],\n",
       "         [  3.6335,  -0.8279,  -9.7565],\n",
       "         [  3.6394,  -0.8347,  -9.7573],\n",
       "         [  1.0170,   0.1405,  -6.3638],\n",
       "         [  1.9596,  -0.6961,  -5.3968],\n",
       "         [  2.7408,   0.4189, -11.3081],\n",
       "         [  1.4877,  -2.8071,  -1.2255],\n",
       "         [  1.4844,  -2.7813,  -1.2472],\n",
       "         [  1.4949,  -2.8311,  -1.1978],\n",
       "         [  1.5013,  -2.8290,  -1.2102],\n",
       "         [  1.4958,  -2.8827,  -1.1272],\n",
       "         [  1.4927,  -2.8136,  -1.2163],\n",
       "         [  1.4883,  -2.8367,  -1.1789],\n",
       "         [  1.4913,  -2.8386,  -1.1768],\n",
       "         [  1.4949,  -2.8323,  -1.1941],\n",
       "         [  1.4871,  -2.8394,  -1.1734],\n",
       "         [  1.4931,  -2.8214,  -1.2156],\n",
       "         [  1.1848,  -0.8649,  -4.4495],\n",
       "         [  1.4762,  -1.7730,  -3.2384],\n",
       "         [  2.3637,  -3.0049,  -4.2358],\n",
       "         [  2.3689,  -2.9830,  -4.2896],\n",
       "         [  2.3670,  -3.0020,  -4.2411],\n",
       "         [  2.3682,  -2.9936,  -4.2757],\n",
       "         [  1.5222,  -2.0207,  -2.6336],\n",
       "         [  1.6871,  -1.5805,  -5.0100],\n",
       "         [  1.6881,  -1.5870,  -5.0050],\n",
       "         [  2.1287,  -1.7298,  -4.9414],\n",
       "         [  1.9522,  -2.4394,  -2.8161],\n",
       "         [  1.1146,   0.6601,  -9.3609],\n",
       "         [  2.1215,  -1.7382,  -4.9061],\n",
       "         [  2.0459,  -5.4595,   0.6852],\n",
       "         [  1.3090,  -0.2893,  -5.2649],\n",
       "         [  1.3233,  -0.2780,  -5.3109],\n",
       "         [  1.7866,  -1.8671,  -3.0991],\n",
       "         [  1.7838,  -1.8539,  -3.1112],\n",
       "         [  1.7897,  -1.8601,  -3.1101]]),\n",
       " tensor([[  0.9114,  -2.4511,   0.0824],\n",
       "         [  1.8069,   0.2185,  -8.5092],\n",
       "         [  2.2892,  -0.5147,  -6.6818],\n",
       "         [  3.5518,  -1.8565,  -6.6862],\n",
       "         [  2.0122,  -0.7069,  -7.0099],\n",
       "         [  3.3292,  -0.7255,  -9.4734],\n",
       "         [  1.7445,  -1.3969,  -4.4888],\n",
       "         [  1.7534,  -1.4089,  -4.4944],\n",
       "         [  1.7478,  -1.4290,  -4.4463],\n",
       "         [  1.5649,  -1.1007,  -5.1595],\n",
       "         [  1.7748,  -0.7259,  -6.6598],\n",
       "         [  1.7734,  -0.7253,  -6.6502],\n",
       "         [  1.7755,  -0.7231,  -6.6733],\n",
       "         [  1.7703,  -0.7123,  -6.6571],\n",
       "         [  1.7758,  -0.7177,  -6.6797],\n",
       "         [  1.7730,  -0.7441,  -6.6276],\n",
       "         [  2.0975,  -1.4434,  -4.7480],\n",
       "         [  1.4908,  -2.8354,  -1.1835],\n",
       "         [  1.4916,  -2.8231,  -1.1765],\n",
       "         [  1.4941,  -2.8585,  -1.1326],\n",
       "         [  1.4823,  -2.8577,  -1.1357],\n",
       "         [  1.5585,  -1.0260,  -5.3159],\n",
       "         [  2.0981,  -0.1148,  -9.2266],\n",
       "         [  2.1408,  -1.0703,  -5.4206],\n",
       "         [  1.5586,  -1.2227,  -5.2021],\n",
       "         [  4.5709,  -0.9003, -13.0703],\n",
       "         [  4.5634,  -0.9125, -13.0064],\n",
       "         [  4.5447,  -0.9012, -12.9881],\n",
       "         [  4.5688,  -0.9250, -12.9878],\n",
       "         [  4.5577,  -0.9108, -12.9925],\n",
       "         [  4.5587,  -0.9012, -13.0308],\n",
       "         [  4.5372,  -0.8978, -12.9761],\n",
       "         [  4.5567,  -0.9059, -13.0152],\n",
       "         [  4.5812,  -0.8903, -13.1272],\n",
       "         [  4.5671,  -0.9014, -13.0425],\n",
       "         [  4.5163,  -0.9052, -12.8924],\n",
       "         [  4.5476,  -0.9034, -12.9912],\n",
       "         [  4.5563,  -0.9069, -13.0137],\n",
       "         [  1.4736,  -1.7730,  -3.2278],\n",
       "         [  1.4374,  -2.1720,  -3.0244],\n",
       "         [  1.4639,  -2.1760,  -3.1227],\n",
       "         [  1.1231,   0.6770,  -9.4651],\n",
       "         [  1.1201,   0.6625,  -9.4037],\n",
       "         [  1.1213,   0.6892,  -9.5277],\n",
       "         [  1.1253,   0.6718,  -9.4604],\n",
       "         [  1.1269,   0.6568,  -9.4040],\n",
       "         [  1.1237,   0.6696,  -9.4480],\n",
       "         [  3.6435,  -0.8342,  -9.7709],\n",
       "         [  4.0753,  -1.2971,  -9.7896],\n",
       "         [  0.9375,  -2.2271,  -0.9965],\n",
       "         [  1.9859,  -0.7135,  -5.4184],\n",
       "         [  1.9689,  -0.7056,  -5.4063],\n",
       "         [  1.9270,  -0.3701,  -7.4800],\n",
       "         [  1.4829,  -2.8110,  -1.2130],\n",
       "         [  1.4828,  -2.8235,  -1.1863],\n",
       "         [  1.4921,  -2.8151,  -1.2102],\n",
       "         [  1.4911,  -2.8669,  -1.1352],\n",
       "         [  1.4902,  -2.8472,  -1.1494],\n",
       "         [  1.4771,  -2.7630,  -1.2600],\n",
       "         [  1.4886,  -2.8382,  -1.1775],\n",
       "         [  1.4915,  -2.8196,  -1.2149],\n",
       "         [  1.4917,  -2.8643,  -1.1440],\n",
       "         [  1.5053,  -2.8413,  -1.2037],\n",
       "         [  1.4848,  -2.8515,  -1.1620]]),\n",
       " tensor([[  1.4903,  -2.8330,  -1.1985],\n",
       "         [  2.0165,  -5.4490,   0.7284],\n",
       "         [  1.8603,  -0.8227,  -5.4598],\n",
       "         [  2.0702,  -1.5342,  -5.9708],\n",
       "         [  1.7312,  -3.6401,  -0.6291],\n",
       "         [  1.4758,  -2.7998,  -1.2116],\n",
       "         [  1.4879,  -2.8216,  -1.2104],\n",
       "         [  1.9861,  -1.3723,  -4.7274],\n",
       "         [  1.6125,  -2.2609,  -1.8116],\n",
       "         [  1.4903,  -2.8641,  -1.1389],\n",
       "         [  1.5156,  -2.0448,  -3.0114],\n",
       "         [  1.5133,  -2.0546,  -2.9946],\n",
       "         [  2.1549,  -0.1081,  -6.9690],\n",
       "         [  1.8479,  -2.0827,  -2.5720],\n",
       "         [  1.8928,  -2.6028,  -3.2499],\n",
       "         [  1.5547,  -0.9740,  -5.3801],\n",
       "         [  1.9512,  -2.4428,  -2.8143],\n",
       "         [  1.9590,  -2.4446,  -2.8283],\n",
       "         [  1.8470,  -0.5617,  -5.5285],\n",
       "         [  0.9360,  -2.2264,  -0.9655],\n",
       "         [  1.6457,  -1.4643,  -3.7452],\n",
       "         [  1.9871,  -0.4209,  -7.3046],\n",
       "         [  1.9783,  -0.4059,  -7.3338],\n",
       "         [  2.2441,  -2.0123,  -4.8114],\n",
       "         [  4.1181,  -1.0485,  -9.7998],\n",
       "         [  4.1515,  -1.0406,  -9.9065],\n",
       "         [  2.3403,  -1.1082,  -5.4261],\n",
       "         [  2.3458,  -1.1057,  -5.4413],\n",
       "         [  2.3324,  -1.0873,  -5.4337],\n",
       "         [  2.1234,  -0.6093,  -6.4556],\n",
       "         [  1.0333,  -1.8017,  -1.9567],\n",
       "         [  1.9854,  -1.9411,  -4.2749],\n",
       "         [  5.2133,  -1.2615, -13.8127],\n",
       "         [  5.2148,  -1.2807, -13.7435],\n",
       "         [  1.4669,  -3.4638,  -2.5022],\n",
       "         [  1.2959,   0.9291,  -8.6156],\n",
       "         [  1.7486,  -1.3827,  -4.5262],\n",
       "         [  2.4417,  -0.5557,  -8.6241],\n",
       "         [  1.1893,  -0.8626,  -4.4737],\n",
       "         [  3.2067,  -0.2842,  -7.1362],\n",
       "         [  3.0946,  -1.0312,  -7.5035],\n",
       "         [  1.9892,  -0.4078,  -7.3672],\n",
       "         [  1.9077,  -1.5551,  -4.0445],\n",
       "         [  1.1843,  -0.8626,  -4.4436],\n",
       "         [  1.6221,  -2.2454,  -1.8468],\n",
       "         [  1.5106,  -2.0508,  -2.9874],\n",
       "         [  1.5142,  -2.0442,  -3.0052],\n",
       "         [  1.5133,  -2.0456,  -2.9969],\n",
       "         [  1.5165,  -2.0583,  -2.9984],\n",
       "         [  1.5172,  -2.0441,  -3.0079],\n",
       "         [  1.5130,  -2.0647,  -2.9719],\n",
       "         [  2.7477,  -2.0709,  -4.1890],\n",
       "         [  2.7452,  -2.0716,  -4.1431],\n",
       "         [  2.1874,  -0.1305,  -6.7672],\n",
       "         [  1.1457,   0.1176,  -6.5146],\n",
       "         [  2.0938,  -1.4584,  -4.7137],\n",
       "         [  2.4932,  -0.3552,  -8.1416],\n",
       "         [  0.6548,   0.9863,  -7.5969],\n",
       "         [  2.5096,  -0.4056,  -8.0343],\n",
       "         [  1.8725,  -0.1539,  -7.4356],\n",
       "         [  2.0085,  -2.8146,  -2.9533],\n",
       "         [  1.8904,  -2.6234,  -3.2057],\n",
       "         [  1.8064,  -0.3846,  -7.0805],\n",
       "         [  0.8934,   0.6058,  -7.6043]]),\n",
       " tensor([[ 1.9525,  0.4327, -6.5221],\n",
       "         [ 1.8664, -4.0821, -0.5567],\n",
       "         [ 1.8652, -4.1501, -0.4300],\n",
       "         [ 1.9520, -2.4186, -2.8603],\n",
       "         [ 2.9718, -0.7665, -8.1194],\n",
       "         [ 1.7373, -2.3823, -2.5497],\n",
       "         [ 3.8397, -0.8513, -9.5566],\n",
       "         [ 3.0352, -2.1886, -6.8984],\n",
       "         [ 0.4822,  0.4798, -6.1437],\n",
       "         [ 1.9567, -2.4321, -2.8417],\n",
       "         [ 1.9528, -2.4292, -2.8337],\n",
       "         [ 3.5449, -1.8395, -6.6988],\n",
       "         [ 3.0214, -2.1199, -7.0386],\n",
       "         [ 2.0032, -3.1929, -2.3901],\n",
       "         [ 1.6421, -0.4820, -5.5098],\n",
       "         [ 1.6333, -0.4803, -5.4722],\n",
       "         [ 1.4001, -0.5149, -5.4020],\n",
       "         [ 1.6880, -1.5859, -5.0175],\n",
       "         [ 2.0940, -2.0252, -4.5541],\n",
       "         [ 2.3170, -0.0332, -9.2147],\n",
       "         [ 1.4085, -0.4260, -5.9922],\n",
       "         [ 1.4089, -0.4237, -6.0089],\n",
       "         [ 2.0530, -3.2521, -2.1042],\n",
       "         [ 2.1225, -1.6859, -4.9822],\n",
       "         [ 3.5584, -1.8059, -6.7748],\n",
       "         [ 2.1363, -0.6079, -6.4694],\n",
       "         [ 2.1167, -0.5899, -6.4852],\n",
       "         [ 1.9760, -1.9008, -4.3657],\n",
       "         [ 1.8399, -0.6901, -6.0672],\n",
       "         [ 1.8421, -0.6936, -6.0572],\n",
       "         [ 2.0876, -2.0302, -4.5245],\n",
       "         [ 1.2781, -0.2604, -5.2383],\n",
       "         [ 1.4616, -2.3995, -1.1047],\n",
       "         [ 2.2081, -1.9871, -4.7163],\n",
       "         [ 1.9561, -1.8704, -4.3110],\n",
       "         [ 1.1695, -1.3846, -4.0061],\n",
       "         [ 2.1173, -0.6628, -6.3765],\n",
       "         [ 1.4710, -2.3654, -1.1788],\n",
       "         [ 1.8428, -0.6945, -6.0588],\n",
       "         [ 0.2790,  0.9677, -6.0939],\n",
       "         [ 1.8500, -0.6964, -6.0750],\n",
       "         [ 1.3094, -0.0272, -6.2556],\n",
       "         [ 1.9293, -0.4979, -6.5000],\n",
       "         [ 1.7641, -0.3447, -6.9295],\n",
       "         [ 1.7510, -0.3284, -6.9156],\n",
       "         [ 3.2257, -0.8602, -8.6213],\n",
       "         [ 1.7313, -1.2656, -5.8979],\n",
       "         [ 2.1114, -0.0545, -8.8623],\n",
       "         [ 2.7439, -0.4710, -8.9019],\n",
       "         [ 1.7809, -0.7451, -6.6343],\n",
       "         [ 1.7791, -0.6782, -6.7296],\n",
       "         [ 1.5191, -1.8295, -3.6341],\n",
       "         [ 0.1739, -3.6504,  0.5325],\n",
       "         [ 1.5210, -1.8207, -3.6642],\n",
       "         [ 1.5191, -1.8223, -3.6575],\n",
       "         [ 1.2199,  0.2017, -6.0327],\n",
       "         [ 1.8364, -0.1850, -7.9276],\n",
       "         [ 1.8336, -0.1866, -7.9134],\n",
       "         [ 1.8290, -0.2119, -7.7843],\n",
       "         [ 1.6877, -1.5731, -5.0330],\n",
       "         [ 2.3122, -0.4989, -8.0254],\n",
       "         [ 1.3670, -0.3085, -6.2782],\n",
       "         [ 2.2998, -0.5010, -7.9669],\n",
       "         [ 1.7878, -1.8605, -3.1121]]),\n",
       " tensor([[ 2.2496e+00, -1.7110e+00, -4.4381e+00],\n",
       "         [ 1.8003e+00, -8.9632e-02, -8.3257e+00],\n",
       "         [ 1.7995e+00, -8.7613e-02, -8.3281e+00],\n",
       "         [ 1.5559e+00, -1.2084e+00, -5.2333e+00],\n",
       "         [ 2.1110e+00, -4.1936e-01, -7.8261e+00],\n",
       "         [ 1.5606e+00, -1.2116e+00, -5.2299e+00],\n",
       "         [ 1.9790e+00, -4.4387e-01, -6.6898e+00],\n",
       "         [ 2.3177e+00, -2.3747e-02, -9.2494e+00],\n",
       "         [ 4.7154e+00, -1.0111e+00, -1.2540e+01],\n",
       "         [ 1.9526e+00,  5.2794e-01, -1.0558e+01],\n",
       "         [ 2.1338e+00, -1.6230e+00, -5.5740e+00],\n",
       "         [ 1.5672e+00, -9.8922e-01, -5.3899e+00],\n",
       "         [ 1.5524e+00, -1.0274e+00, -5.2949e+00],\n",
       "         [ 2.2142e+00, -2.3277e+00, -5.7008e+00],\n",
       "         [ 1.7340e+00, -3.6440e+00, -6.1803e-01],\n",
       "         [ 1.7966e+00, -6.5959e-01, -4.9066e+00],\n",
       "         [ 1.7357e+00, -3.6395e+00, -6.2989e-01],\n",
       "         [ 1.2034e+00,  8.1894e-01, -1.2610e+01],\n",
       "         [ 2.1587e+00, -1.9334e+00, -4.5996e+00],\n",
       "         [ 1.4901e+00, -2.8463e+00, -1.1575e+00],\n",
       "         [ 1.4920e+00, -2.8160e+00, -1.2161e+00],\n",
       "         [ 1.4909e+00, -2.8085e+00, -1.2299e+00],\n",
       "         [ 1.4837e+00, -2.8233e+00, -1.1917e+00],\n",
       "         [ 1.6502e+00, -3.6633e-02, -8.4940e+00],\n",
       "         [ 1.4906e+00, -2.8481e+00, -1.1567e+00],\n",
       "         [ 1.4955e+00, -2.8608e+00, -1.1522e+00],\n",
       "         [ 1.4934e+00, -2.8285e+00, -1.1962e+00],\n",
       "         [ 2.3241e+00, -3.6666e-02, -9.2105e+00],\n",
       "         [ 2.4613e+00, -5.8137e-01, -8.6082e+00],\n",
       "         [ 1.7465e+00,  3.4563e-01, -9.8510e+00],\n",
       "         [ 1.7020e+00, -7.4070e-01, -8.1443e+00],\n",
       "         [ 1.6278e+00,  7.3730e-02, -7.9537e+00],\n",
       "         [ 1.8905e+00,  1.7186e-02, -7.4083e+00],\n",
       "         [ 3.3402e+00, -7.3064e-01, -9.4917e+00],\n",
       "         [ 4.5565e+00, -8.9136e-01, -1.3044e+01],\n",
       "         [ 4.5544e+00, -9.0130e-01, -1.3015e+01],\n",
       "         [ 4.5392e+00, -9.0244e-01, -1.2965e+01],\n",
       "         [ 4.5696e+00, -9.2280e-01, -1.2997e+01],\n",
       "         [ 4.5406e+00, -8.8913e-01, -1.3018e+01],\n",
       "         [ 4.5613e+00, -9.0591e-01, -1.3024e+01],\n",
       "         [ 4.5696e+00, -9.0346e-01, -1.3056e+01],\n",
       "         [ 4.5777e+00, -9.0260e-01, -1.3069e+01],\n",
       "         [ 4.5488e+00, -8.9830e-01, -1.3005e+01],\n",
       "         [ 4.5579e+00, -8.9153e-01, -1.3051e+01],\n",
       "         [ 1.9217e+00, -2.7109e+00, -2.6100e+00],\n",
       "         [ 1.3807e+00, -3.5771e-01, -4.8240e+00],\n",
       "         [ 2.4051e+00, -1.3951e-01, -9.7895e+00],\n",
       "         [ 2.4157e+00, -1.5243e-01, -9.7786e+00],\n",
       "         [ 2.3186e+00, -2.3589e-02, -9.2709e+00],\n",
       "         [ 2.3304e+00, -2.1619e-02, -9.2960e+00],\n",
       "         [ 3.0082e+00, -9.6661e-01, -7.4237e+00],\n",
       "         [ 2.7423e+00,  4.4499e-01, -1.1399e+01],\n",
       "         [ 2.7403e+00,  4.5069e-01, -1.1425e+01],\n",
       "         [ 1.8252e+00, -3.8776e+00, -7.5592e-01],\n",
       "         [ 1.8991e+00, -1.6140e-02, -7.2750e+00],\n",
       "         [ 1.7923e+00, -1.8692e+00, -3.1103e+00],\n",
       "         [ 1.5235e+00, -3.2545e+00, -5.4276e-01],\n",
       "         [ 2.1029e+00, -1.0837e-01, -9.2738e+00],\n",
       "         [-3.5850e+00,  2.0729e+00, -1.7516e+01],\n",
       "         [ 1.6288e+00, -7.0991e-02, -6.9177e+00],\n",
       "         [ 1.7680e+00, -3.5835e-01, -6.9026e+00],\n",
       "         [ 6.0177e-01, -3.3991e+00,  1.3042e+00],\n",
       "         [ 2.1019e+00, -1.4298e+00, -4.7722e+00],\n",
       "         [ 3.5128e+00, -3.0011e-01, -1.1152e+01]]),\n",
       " tensor([[  3.5193,  -0.3138, -11.1328],\n",
       "         [  3.5246,  -0.3150, -11.1525],\n",
       "         [  3.5247,  -0.3217, -11.1298],\n",
       "         [  2.3413,  -1.0521,  -6.7693],\n",
       "         [  2.3420,  -1.0631,  -6.7399],\n",
       "         [  2.3007,  -1.1192,  -6.6398],\n",
       "         [  1.5781,   0.1926,  -7.8898],\n",
       "         [  2.3481,  -1.0597,  -6.7357],\n",
       "         [  1.7068,   0.3839,  -9.8833],\n",
       "         [  1.7307,   0.3910,  -9.9825],\n",
       "         [  1.6721,  -1.0214,  -4.4789],\n",
       "         [  3.1801,  -0.2906, -10.2290],\n",
       "         [  1.8970,   0.4859,  -6.4587],\n",
       "         [  1.8066,  -0.3877,  -7.0673],\n",
       "         [  1.9058,  -0.8890,  -4.2757],\n",
       "         [  3.8364,  -1.2107,  -9.3939],\n",
       "         [  2.4107,  -0.1475,  -9.7855],\n",
       "         [  2.4119,  -0.1487,  -9.7766],\n",
       "         [  2.4074,  -0.1486,  -9.7729],\n",
       "         [  2.4119,  -0.1453,  -9.7964],\n",
       "         [  2.4060,  -0.1438,  -9.7842],\n",
       "         [  2.3997,  -0.1334,  -9.8010],\n",
       "         [  1.0530,  -2.4613,  -1.3270],\n",
       "         [  0.1775,  -3.6799,   0.5679],\n",
       "         [  0.1889,  -3.6974,   0.5777],\n",
       "         [  0.1829,  -3.6998,   0.5979],\n",
       "         [  0.1941,  -3.6564,   0.5025],\n",
       "         [  1.3185,  -0.3036,  -5.2728],\n",
       "         [  1.7994,  -0.6608,  -4.9156],\n",
       "         [  1.9970,  -3.1572,  -2.4188],\n",
       "         [  1.6876,  -1.5907,  -5.0049],\n",
       "         [  1.7339,  -3.6387,  -0.6289],\n",
       "         [  1.7319,  -3.6439,  -0.6161],\n",
       "         [  1.1469,   0.8782, -12.3729],\n",
       "         [  4.7248,  -1.0153, -12.5532],\n",
       "         [  1.6952,  -0.5433,  -5.1489],\n",
       "         [  1.7109,  -0.5586,  -5.1415],\n",
       "         [  1.7032,  -0.5492,  -5.1478],\n",
       "         [  2.9219,  -0.9110,  -7.8612],\n",
       "         [  4.6912,  -1.0123, -12.4734],\n",
       "         [  1.8411,  -0.6880,  -6.0719],\n",
       "         [  2.0308,  -0.6413,  -5.9474],\n",
       "         [  2.0682,  -2.3091,  -3.2240],\n",
       "         [  2.0596,  -2.2713,  -3.2568],\n",
       "         [  2.0674,  -1.5464,  -5.9461],\n",
       "         [  1.4885,  -2.8629,  -1.1420],\n",
       "         [  1.4887,  -2.8353,  -1.1854],\n",
       "         [  2.0161,  -4.3553,  -0.6163],\n",
       "         [  1.7035,  -1.3089,  -6.4147],\n",
       "         [  3.5283,  -0.3263, -11.1057],\n",
       "         [  1.2001,  -0.7608,  -4.0125],\n",
       "         [  2.9679,  -0.7588,  -8.1210],\n",
       "         [  4.1281,  -1.3151,  -9.8883],\n",
       "         [  1.9231,  -0.3676,  -7.4682],\n",
       "         [  3.1708,  -0.7650,  -8.7773],\n",
       "         [  1.5197,  -1.8252,  -3.6475],\n",
       "         [  1.9607,  -2.3421,  -3.4462],\n",
       "         [  1.5563,  -1.0248,  -5.3043],\n",
       "         [  1.8964,  -0.9099,  -4.2417],\n",
       "         [  1.8894,  -0.9023,  -4.2343],\n",
       "         [  0.1914,  -3.6731,   0.5373],\n",
       "         [  1.7762,  -0.7986,  -4.7990],\n",
       "         [  2.2500,  -1.7154,  -4.4304],\n",
       "         [  2.3032,  -0.4947,  -8.0078]]),\n",
       " tensor([[  0.9410,   0.4652,  -5.4131],\n",
       "         [  1.7986,  -1.8713,  -3.1276],\n",
       "         [  0.4257,   0.8689,  -6.2786],\n",
       "         [  0.6162,   1.0371,  -7.6821],\n",
       "         [  1.1863,  -0.8611,  -4.4664],\n",
       "         [  1.6291,  -0.0686,  -6.9066],\n",
       "         [  1.5164,  -2.0672,  -2.9674],\n",
       "         [  1.5077,  -2.0463,  -2.9795],\n",
       "         [  0.5946,  -3.3900,   1.3081],\n",
       "         [  0.8036,   0.5991,  -8.1035],\n",
       "         [  1.8525,  -0.8309,  -5.4066],\n",
       "         [  2.2521,   0.5338, -11.1473],\n",
       "         [  3.5301,  -0.3170, -11.1568],\n",
       "         [  1.9022,   0.4707,  -6.4549],\n",
       "         [  1.8088,  -3.8493,  -0.7573],\n",
       "         [  1.7465,   0.3417,  -9.8354],\n",
       "         [  1.8119,  -3.8871,  -0.7228],\n",
       "         [  2.4154,  -0.1488,  -9.7960],\n",
       "         [  2.4081,  -0.1448,  -9.7715],\n",
       "         [  2.2849,  -2.3228,  -4.6967],\n",
       "         [  2.2764,  -2.3572,  -4.6097],\n",
       "         [  3.2011,  -0.2884,  -7.1409],\n",
       "         [  2.0244,  -0.2498,  -8.0003],\n",
       "         [  2.0033,  -3.1425,  -2.4662],\n",
       "         [  1.6319,  -1.4206,  -3.7700],\n",
       "         [  1.2863,  -0.2742,  -5.2224],\n",
       "         [  1.7294,  -3.6236,  -0.6312],\n",
       "         [  1.7345,  -3.6372,  -0.6309],\n",
       "         [  1.1387,   0.8296, -12.2495],\n",
       "         [  2.1178,  -1.7643,  -4.8587],\n",
       "         [  1.3939,   0.1937,  -5.9451],\n",
       "         [  4.1507,  -1.0375,  -9.9201],\n",
       "         [  4.7189,  -1.0155, -12.5377],\n",
       "         [  4.7186,  -1.0114, -12.5452],\n",
       "         [  4.7196,  -1.0062, -12.5669],\n",
       "         [  1.8409,  -0.6852,  -6.0836],\n",
       "         [  1.8452,  -0.6934,  -6.0559],\n",
       "         [  1.3117,   0.9373,  -8.7059],\n",
       "         [  1.2906,   0.9458,  -8.6354],\n",
       "         [  1.8397,  -0.6882,  -6.0828],\n",
       "         [  1.3418,   0.9128,  -8.7790],\n",
       "         [  2.3257,  -1.9614,  -4.1621],\n",
       "         [  2.4556,  -0.5842,  -8.6056],\n",
       "         [  2.7642,  -0.4848,  -8.9136],\n",
       "         [  1.4712,  -1.7860,  -3.2105],\n",
       "         [  1.7968,  -3.8487,  -0.7313],\n",
       "         [  1.8172,  -3.8371,  -0.7965],\n",
       "         [  1.7072,   0.3736,  -9.8477],\n",
       "         [  1.8092,  -0.3863,  -7.0533],\n",
       "         [  0.1960,  -3.7022,   0.5898],\n",
       "         [  1.7213,   0.3551,  -9.8173],\n",
       "         [  2.2690,  -2.3144,  -4.6556],\n",
       "         [  0.1834,  -3.6636,   0.5293],\n",
       "         [  2.2822,  -2.3162,  -4.6957],\n",
       "         [  1.7327,   0.3586,  -9.8660],\n",
       "         [  2.2781,  -2.2998,  -4.7137],\n",
       "         [  2.2842,  -2.3150,  -4.7071],\n",
       "         [  1.2779,  -0.2586,  -5.2480],\n",
       "         [  1.7346,  -3.6374,  -0.6295],\n",
       "         [  4.7193,  -1.0125, -12.5439],\n",
       "         [  1.2803,  -0.2485,  -5.3020],\n",
       "         [  1.5584,  -1.2373,  -5.1708],\n",
       "         [  1.3332,  -2.1938,  -3.3040],\n",
       "         [  1.1190,   0.8379, -12.8325]]),\n",
       " tensor([[  2.4069,  -0.1371,  -9.8107],\n",
       "         [  2.0406,  -5.4563,   0.6889],\n",
       "         [  3.0081,  -1.9991,  -6.6077],\n",
       "         [  3.0217,  -1.9678,  -6.6933],\n",
       "         [  3.0281,  -1.9687,  -6.7082],\n",
       "         [  3.0349,  -1.9507,  -6.7504],\n",
       "         [  3.0106,  -1.9953,  -6.6187],\n",
       "         [  0.1883,  -3.6953,   0.5700],\n",
       "         [  1.2163,  -0.3186,  -4.2019],\n",
       "         [  1.1851,   0.8186, -12.4926],\n",
       "         [  1.6117,  -0.1405,  -5.6331],\n",
       "         [  1.9558,  -2.4719,  -3.6742],\n",
       "         [  1.2090,  -0.8080,  -4.0305],\n",
       "         [  2.0679,  -1.5348,  -5.9569],\n",
       "         [  2.0674,  -1.5591,  -5.9295],\n",
       "         [  1.7340,   0.3654,  -9.8944],\n",
       "         [  1.3064,  -1.5186,  -2.5262],\n",
       "         [  4.5780,  -0.9037, -13.0700],\n",
       "         [  4.5555,  -0.8828, -13.0593],\n",
       "         [  4.5584,  -0.9205, -12.9688],\n",
       "         [  4.5452,  -0.9001, -12.9932],\n",
       "         [  4.5879,  -0.8659, -13.2101],\n",
       "         [  4.5516,  -0.9051, -12.9922],\n",
       "         [  2.0253,  -5.4344,   0.6841],\n",
       "         [  1.9437,   0.5426, -10.5657],\n",
       "         [  1.7625,   0.3293,  -9.8147],\n",
       "         [  1.7227,   0.3652,  -9.8627],\n",
       "         [  1.8500,  -4.0803,  -0.5164],\n",
       "         [  0.8927,  -1.7657,  -1.5815],\n",
       "         [  2.2846,  -2.3061,  -4.7253],\n",
       "         [  2.2905,  -2.3076,  -4.7357],\n",
       "         [  2.0044,  -3.1763,  -2.4210],\n",
       "         [  1.2726,  -0.2486,  -5.2667],\n",
       "         [  1.7339,  -3.6436,  -0.6209],\n",
       "         [  1.7339,  -3.6461,  -0.6246],\n",
       "         [  1.1023,   0.8380, -12.1823],\n",
       "         [  2.1740,  -1.9260,  -4.6552],\n",
       "         [  1.6479,  -0.0181,  -8.5597],\n",
       "         [  1.6463,  -0.0260,  -8.5173],\n",
       "         [  1.6448,  -0.0382,  -8.4545],\n",
       "         [  1.6485,  -0.0632,  -8.3581],\n",
       "         [  1.7176,  -0.5517,  -5.1679],\n",
       "         [  1.5562,  -1.2460,  -5.1405],\n",
       "         [  2.1057,  -0.4552,  -7.7736],\n",
       "         [  1.7449,  -0.8032,  -6.8873],\n",
       "         [  2.2392,   0.5538, -11.2358],\n",
       "         [  4.0998,  -1.2945,  -9.8674],\n",
       "         [  2.9711,  -0.7729,  -8.0935],\n",
       "         [  1.9511,  -0.6955,  -5.3822],\n",
       "         [  1.9269,  -0.3797,  -7.4361],\n",
       "         [  3.0268,  -0.0844,  -9.8122],\n",
       "         [  1.0298,  -2.9486,   0.0993],\n",
       "         [  2.8738,  -0.3418,  -9.0747],\n",
       "         [  4.5399,  -0.8979, -12.9853],\n",
       "         [  4.5504,  -0.8953, -13.0252],\n",
       "         [  1.2899,  -3.0723,  -0.5605],\n",
       "         [  1.2873,  -3.0677,  -0.5611],\n",
       "         [  1.5571,  -1.0261,  -5.3120],\n",
       "         [  2.0972,  -0.1139,  -9.2193],\n",
       "         [  1.1847,  -0.8517,  -4.4610],\n",
       "         [  1.4736,  -1.7658,  -3.2306],\n",
       "         [  1.6758,  -1.0411,  -4.4562],\n",
       "         [  3.6262,  -0.8274,  -9.7370],\n",
       "         [  1.4063,  -0.4219,  -5.9961]]),\n",
       " tensor([[  1.7972,  -0.6555,  -4.9240],\n",
       "         [  2.2344,  -1.9746,  -4.8323],\n",
       "         [  2.2189,  -1.9955,  -4.7321],\n",
       "         [  1.6485,  -0.0224,  -8.5422],\n",
       "         [  2.9893,  -0.9562,  -7.4000],\n",
       "         [  1.2389,  -0.9066,  -4.4271],\n",
       "         [  1.4777,  -2.3839,  -1.1685],\n",
       "         [  1.4809,  -2.3874,  -1.1776],\n",
       "         [  1.5161,  -2.0547,  -2.9986],\n",
       "         [  2.0862,  -1.4283,  -4.7469],\n",
       "         [  2.0988,  -1.4208,  -4.7727],\n",
       "         [  2.0948,  -1.3880,  -4.8281],\n",
       "         [  1.8678,  -2.1442,  -2.5591],\n",
       "         [  1.8605,  -0.8184,  -5.4548],\n",
       "         [  2.5358,  -0.0730,  -8.7616],\n",
       "         [  1.6490,  -0.0223,  -8.5484],\n",
       "         [  1.6529,  -0.0365,  -8.5096],\n",
       "         [  1.6454,  -0.0209,  -8.5435],\n",
       "         [  3.0339,  -0.9773,  -7.4606],\n",
       "         [  1.4854,  -2.3698,  -1.2260],\n",
       "         [  1.5153,  -2.0523,  -2.9995],\n",
       "         [  1.7634,  -0.3545,  -6.8948],\n",
       "         [  1.1105,  -0.9554,  -5.5245],\n",
       "         [  1.5558,  -1.0317,  -5.2965],\n",
       "         [  1.6333,   0.0891,  -8.0443],\n",
       "         [  2.2600,  -1.6991,  -4.4555],\n",
       "         [  2.7616,  -1.2849,  -6.0269],\n",
       "         [  1.6532,  -0.0446,  -8.4819],\n",
       "         [  2.3056,  -0.5027,  -7.9866],\n",
       "         [  2.3112,  -0.4903,  -8.0479],\n",
       "         [  2.2974,  -0.4933,  -7.9884],\n",
       "         [  2.3078,  -0.4937,  -8.0265],\n",
       "         [  4.1668,  -1.0412,  -9.9537],\n",
       "         [  1.2634,  -0.1743,  -5.9008],\n",
       "         [  0.5844,   0.9501,  -7.5293],\n",
       "         [  1.6263,  -0.0759,  -6.8865],\n",
       "         [  2.6668,  -0.6589,  -8.3704],\n",
       "         [  3.2317,  -1.2217,  -7.9545],\n",
       "         [  0.8900,  -0.0653,  -5.6983],\n",
       "         [  1.6832,  -2.2383,  -2.9379],\n",
       "         [  1.6082,  -0.2797,  -6.5315],\n",
       "         [  2.2789,  -0.7849,  -8.4056],\n",
       "         [  2.4979,  -0.4317,  -7.6520],\n",
       "         [  1.8170,   0.1015,  -6.1311],\n",
       "         [  1.8345,   0.0760,  -6.1325],\n",
       "         [  1.7036,  -2.3946,  -2.3994],\n",
       "         [  1.0201,  -4.2470,   1.2738],\n",
       "         [  1.8918,  -2.5536,  -3.3197],\n",
       "         [  3.8961,  -0.5294, -10.2242],\n",
       "         [  1.5790,   0.1670,  -7.7511],\n",
       "         [  1.7671,  -0.1106,  -7.8325],\n",
       "         [  1.8032,  -0.3985,  -6.9912],\n",
       "         [  1.5673,  -1.4290,  -4.6479],\n",
       "         [  1.5632,  -1.4218,  -4.6490],\n",
       "         [  1.2902,  -3.0819,  -0.5516],\n",
       "         [  1.2907,  -3.0737,  -0.5604],\n",
       "         [  1.2873,  -3.0698,  -0.5474],\n",
       "         [  1.2785,  -3.0582,  -0.5528],\n",
       "         [  2.1354,  -1.6172,  -5.5627],\n",
       "         [  2.2406,   0.7580, -11.8952],\n",
       "         [  2.2387,   0.7866, -11.9997],\n",
       "         [  6.9358,  -0.7016, -19.0898],\n",
       "         [  2.3231,  -1.8121,  -5.3278],\n",
       "         [  1.9344,  -0.3603,  -6.6176]]),\n",
       " tensor([[  2.5195,  -1.0618,  -5.8555],\n",
       "         [  1.2307,  -1.3090,  -3.9285],\n",
       "         [  1.2330,  -1.2994,  -3.9415],\n",
       "         [  1.9493,  -0.1458,  -6.5945],\n",
       "         [  2.5128,  -0.6379,  -6.7652],\n",
       "         [  1.9856,  -2.4408,  -2.8753],\n",
       "         [  1.6639,  -0.3180,  -6.4672],\n",
       "         [  1.2566,  -0.8776,  -4.0476],\n",
       "         [  2.6908,  -0.1721, -11.2913],\n",
       "         [  2.8580,  -0.3038, -11.3044],\n",
       "         [  0.9061,  -2.4400,   0.0675],\n",
       "         [  2.5597,  -0.1193, -10.0367],\n",
       "         [  2.1675,  -1.9519,  -4.4292],\n",
       "         [  2.4679,  -2.8388,  -3.7150],\n",
       "         [  2.4243,   0.3805,  -8.7730],\n",
       "         [  2.7312,   0.1516,  -9.2256],\n",
       "         [  2.9716,  -1.6772,  -4.7492],\n",
       "         [  2.0317,  -0.5259,  -7.7272],\n",
       "         [  2.0373,  -0.5310,  -7.7388],\n",
       "         [  3.5588,  -1.7765,  -6.8191],\n",
       "         [  2.5733,  -0.2647,  -9.8573],\n",
       "         [  1.8885,   0.1823,  -9.1709],\n",
       "         [  1.8831,   0.1969,  -9.2236],\n",
       "         [  1.8930,   0.1813,  -9.1754],\n",
       "         [  1.9039,   0.1609,  -9.1350],\n",
       "         [  2.1178,  -2.7661,  -3.3262],\n",
       "         [  2.5845,  -3.1334,  -3.5152],\n",
       "         [  2.8618,  -0.3315,  -9.0875],\n",
       "         [  2.0096,  -0.9906,  -5.7678],\n",
       "         [  1.8690,   0.0952,  -7.4762],\n",
       "         [  1.8841,   0.0821,  -7.4944],\n",
       "         [  1.9245,  -1.5733,  -4.0474],\n",
       "         [  2.6796,  -0.4240,  -8.5567],\n",
       "         [  2.6713,  -0.4192,  -8.5454],\n",
       "         [  1.9882,  -2.1018,  -4.8500],\n",
       "         [  2.3410,  -0.8315,  -6.9973],\n",
       "         [  2.3434,  -0.8437,  -6.9709],\n",
       "         [  2.3390,  -1.1231,  -5.4050],\n",
       "         [  2.3499,  -1.1324,  -5.4152],\n",
       "         [  2.3398,  -1.1048,  -5.4250],\n",
       "         [  1.8916,  -0.8632,  -5.5863],\n",
       "         [  1.8998,  -0.8510,  -5.6104],\n",
       "         [  3.1025,  -0.2774, -10.3019],\n",
       "         [  1.9357,  -0.4980,  -6.5998],\n",
       "         [  2.1291,  -1.3328,  -6.6449],\n",
       "         [  1.8367,  -0.7349,  -5.9135],\n",
       "         [  1.6989,  -1.2686,  -6.4182],\n",
       "         [  1.5581,  -1.2472,  -5.1469],\n",
       "         [  3.0266,  -0.0649,  -9.8769],\n",
       "         [  1.4392,  -1.5525,  -2.9907],\n",
       "         [  1.1863,  -0.8645,  -4.4573],\n",
       "         [  2.1866,  -2.2584,  -4.0450],\n",
       "         [  2.1883,  -2.2806,  -4.0317],\n",
       "         [  2.6940,  -0.9187,  -6.4822],\n",
       "         [  2.6929,  -0.9244,  -6.4595],\n",
       "         [  2.6958,  -0.9251,  -6.4633],\n",
       "         [  2.6853,  -0.9284,  -6.4222],\n",
       "         [  2.7276,  -1.5194,  -5.5111],\n",
       "         [  3.2365,  -1.2237,  -7.9633],\n",
       "         [  2.0125,  -0.6886,  -5.6924],\n",
       "         [  1.6090,  -0.2772,  -6.5464],\n",
       "         [  0.5934,  -3.3954,   1.3125],\n",
       "         [  0.6149,   1.0391,  -7.6850],\n",
       "         [  1.7074,  -2.4013,  -2.4015]]),\n",
       " tensor([[ 1.7028e+00, -2.4130e+00, -2.3758e+00],\n",
       "         [ 2.2026e+00, -5.5567e-01, -7.0824e+00],\n",
       "         [ 2.9840e+00, -6.3962e-01, -8.4236e+00],\n",
       "         [ 1.0783e+00, -8.4178e-01, -5.6437e+00],\n",
       "         [ 2.1814e+00, -2.6921e+00, -3.8189e+00],\n",
       "         [ 3.2194e+00, -5.6520e-01, -1.0114e+01],\n",
       "         [ 2.8750e+00, -5.6141e-01, -8.1661e+00],\n",
       "         [ 3.1050e+00, -3.8082e-02, -1.0721e+01],\n",
       "         [ 9.4635e-01,  5.3899e-01, -7.7931e+00],\n",
       "         [ 9.7209e-01,  4.8539e-01, -7.6451e+00],\n",
       "         [ 1.6287e+00, -2.4877e+00, -3.0406e+00],\n",
       "         [ 3.9116e+00, -5.7016e-01, -1.0146e+01],\n",
       "         [ 1.1571e+00, -2.7110e+00, -7.0637e-01],\n",
       "         [ 1.5738e+00,  1.8499e-01, -7.8107e+00],\n",
       "         [ 1.8389e+00,  5.1987e-01, -6.3482e+00],\n",
       "         [ 3.8060e+00, -4.1961e-01, -1.1735e+01],\n",
       "         [ 3.1621e+00, -2.8969e-01, -1.0180e+01],\n",
       "         [ 1.3857e+00, -2.4785e-01, -5.8400e+00],\n",
       "         [ 1.2878e+00, -3.0834e+00, -5.4764e-01],\n",
       "         [ 1.3963e+00, -2.3682e-01, -5.5972e+00],\n",
       "         [ 4.4577e+00, -9.4422e-01, -9.7822e+00],\n",
       "         [ 6.9252e+00, -7.0234e-01, -1.9062e+01],\n",
       "         [ 2.4644e+00, -2.8766e+00, -3.6598e+00],\n",
       "         [ 2.8707e+00,  4.6730e-03, -9.3038e+00],\n",
       "         [ 1.9066e+00, -4.8653e-01, -5.6382e+00],\n",
       "         [ 2.0824e+00,  2.6521e-02, -9.7411e+00],\n",
       "         [ 2.0880e+00,  2.6336e-02, -9.7611e+00],\n",
       "         [ 1.4799e+00, -2.3485e+00, -3.6224e+00],\n",
       "         [ 2.9761e+00, -1.6877e+00, -4.7521e+00],\n",
       "         [ 2.9444e+00, -1.6983e+00, -4.6758e+00],\n",
       "         [ 2.0343e+00, -2.6166e-01, -7.9630e+00],\n",
       "         [ 1.6241e+00,  8.5859e-02, -7.9958e+00],\n",
       "         [ 2.2986e+00, -1.4561e+00, -4.8580e+00],\n",
       "         [ 1.4136e+00, -2.8289e+00, -1.1183e+00],\n",
       "         [ 3.7611e+00, -6.9788e-01, -1.0254e+01],\n",
       "         [ 3.7883e+00, -7.0799e-01, -1.0292e+01],\n",
       "         [ 1.5812e+00, -7.3635e-01, -6.1884e+00],\n",
       "         [ 1.8956e+00,  1.7633e-01, -9.1715e+00],\n",
       "         [ 3.8684e+00, -1.0424e+00, -8.1410e+00],\n",
       "         [ 1.7638e+00, -1.6583e+00, -5.9058e+00],\n",
       "         [ 1.7628e+00, -1.6538e+00, -5.9203e+00],\n",
       "         [ 1.7706e+00, -1.6844e+00, -5.8883e+00],\n",
       "         [ 1.7678e+00, -1.6550e+00, -5.9221e+00],\n",
       "         [ 1.7626e+00, -1.6426e+00, -5.9482e+00],\n",
       "         [ 1.3512e+00, -2.3021e-02, -5.9723e+00],\n",
       "         [ 2.0973e+00, -2.0247e+00, -4.5576e+00],\n",
       "         [ 2.8533e+00, -3.4371e-01, -8.9918e+00],\n",
       "         [ 1.2171e+00, -8.2226e-01, -4.0006e+00],\n",
       "         [ 1.9823e+00, -4.2207e-01, -7.2871e+00],\n",
       "         [ 1.7059e+00, -4.5132e-01, -5.7249e+00],\n",
       "         [ 1.7975e+00, -4.4196e+00,  3.8911e-02],\n",
       "         [ 4.1650e+00, -1.1591e+00, -1.0832e+01],\n",
       "         [ 4.1697e+00, -1.1809e+00, -1.0779e+01],\n",
       "         [ 1.5905e+00, -1.4557e+00, -4.6313e+00],\n",
       "         [ 1.9662e+00,  4.7637e-01, -1.0037e+01],\n",
       "         [ 2.6765e+00, -4.1461e-01, -8.6009e+00],\n",
       "         [ 1.9880e+00, -2.0742e+00, -4.8491e+00],\n",
       "         [ 2.3509e+00, -3.9197e-01, -8.1313e+00],\n",
       "         [ 2.3415e+00, -3.9108e-01, -8.1034e+00],\n",
       "         [ 1.7039e+00, -5.4403e-01, -5.1670e+00],\n",
       "         [ 2.1412e+00, -6.0057e-01, -6.5114e+00],\n",
       "         [ 3.1029e+00, -2.8083e-01, -1.0284e+01],\n",
       "         [ 3.1036e+00, -2.7612e-01, -1.0294e+01],\n",
       "         [ 2.3086e+00, -5.0020e-01, -8.0083e+00]]),\n",
       " tensor([[ 2.0559e+00, -2.0557e+00, -4.7675e+00],\n",
       "         [ 1.4998e+00, -2.7445e-01, -6.2820e+00],\n",
       "         [ 1.9368e+00, -5.0938e-01, -6.4824e+00],\n",
       "         [ 5.2022e+00, -1.2749e+00, -1.3721e+01],\n",
       "         [ 5.2043e+00, -1.2610e+00, -1.3766e+01],\n",
       "         [ 2.1232e+00, -1.3337e+00, -6.6207e+00],\n",
       "         [ 1.8400e+00, -7.5821e-01, -5.8902e+00],\n",
       "         [ 1.8426e+00, -7.2995e-01, -5.9500e+00],\n",
       "         [ 4.7154e+00, -1.0109e+00, -1.2537e+01],\n",
       "         [ 4.7110e+00, -1.0151e+00, -1.2516e+01],\n",
       "         [ 4.7167e+00, -1.0116e+00, -1.2541e+01],\n",
       "         [ 1.5788e+00, -2.1619e-01, -6.0422e+00],\n",
       "         [ 1.8394e+00, -6.9463e-01, -6.0477e+00],\n",
       "         [ 1.8398e+00, -6.9100e-01, -6.0562e+00],\n",
       "         [ 1.8381e+00, -6.9058e-01, -6.0602e+00],\n",
       "         [ 2.9054e+00, -8.9925e-01, -7.8615e+00],\n",
       "         [ 1.0859e+00, -7.5031e-01, -4.0531e+00],\n",
       "         [ 1.9071e+00, -4.4906e+00,  7.3985e-02],\n",
       "         [ 1.5564e+00, -1.2399e+00, -5.1586e+00],\n",
       "         [ 2.0981e+00, -6.1456e-01, -7.3958e+00],\n",
       "         [ 1.4848e+00, -2.1206e-01, -6.4184e+00],\n",
       "         [ 8.5311e-01,  7.0110e-01, -7.5865e+00],\n",
       "         [ 1.6313e+00, -8.3511e-02, -6.8701e+00],\n",
       "         [ 2.6911e+00, -9.3357e-01, -6.4285e+00],\n",
       "         [ 1.6139e+00, -2.2352e+00, -2.6185e+00],\n",
       "         [ 3.2393e+00, -1.2295e+00, -7.9503e+00],\n",
       "         [ 3.2374e+00, -1.2291e+00, -7.9496e+00],\n",
       "         [ 8.7265e-01,  3.8000e-03, -5.7502e+00],\n",
       "         [ 1.6108e+00, -2.7605e-01, -6.5584e+00],\n",
       "         [ 1.6321e+00, -1.7918e-01, -7.4061e+00],\n",
       "         [ 1.6346e+00, -1.6036e-01, -7.4852e+00],\n",
       "         [ 1.5150e+00, -6.7757e-01, -5.1232e+00],\n",
       "         [ 2.4672e+00, -2.9661e-01, -8.1842e+00],\n",
       "         [ 2.0203e+00, -8.6538e-02, -8.0856e+00],\n",
       "         [ 2.1878e+00, -5.5900e-01, -7.0243e+00],\n",
       "         [ 1.7091e+00, -9.6988e-02, -7.0757e+00],\n",
       "         [ 1.7068e+00, -9.9289e-02, -7.0539e+00],\n",
       "         [ 1.4385e+00, -2.6114e-01, -6.2954e+00],\n",
       "         [ 2.8929e+00, -5.7181e-01, -8.1979e+00],\n",
       "         [ 2.8895e+00, -5.5371e-01, -8.2129e+00],\n",
       "         [ 2.8983e+00, -5.7777e-01, -8.1900e+00],\n",
       "         [ 3.0752e+00, -1.8794e-02, -1.0681e+01],\n",
       "         [ 3.5251e+00, -3.1782e-01, -1.1146e+01],\n",
       "         [ 2.3011e+00, -1.1106e+00, -6.6421e+00],\n",
       "         [ 1.7691e+00, -1.1042e-01, -7.8507e+00],\n",
       "         [ 1.7685e+00, -1.1281e-01, -7.8381e+00],\n",
       "         [ 1.1100e+00,  6.7089e-01, -9.3957e+00],\n",
       "         [ 1.6427e+00, -1.0686e-01, -6.4839e+00],\n",
       "         [ 1.6497e+00, -9.8222e-02, -6.5500e+00],\n",
       "         [ 2.7365e+00, -2.6553e+00, -4.7181e+00],\n",
       "         [ 1.5675e+00, -1.4178e+00, -4.6727e+00],\n",
       "         [ 3.8195e+00, -4.0080e-01, -1.1827e+01],\n",
       "         [ 1.4517e+00, -1.5183e-01, -5.5693e+00],\n",
       "         [ 3.1523e+00, -2.9890e-01, -1.0131e+01],\n",
       "         [ 1.2899e+00, -3.0740e+00, -5.5400e-01],\n",
       "         [ 2.2446e+00,  7.4308e-01, -1.1870e+01],\n",
       "         [ 2.2186e+00,  7.3202e-01, -1.1702e+01],\n",
       "         [ 1.3936e+00, -2.3557e-01, -5.5912e+00],\n",
       "         [ 1.9595e+00, -3.4356e-01, -6.3800e+00],\n",
       "         [ 2.3081e+00, -1.8126e+00, -5.3100e+00],\n",
       "         [ 2.3114e+00, -1.7744e+00, -5.3563e+00],\n",
       "         [ 1.9324e+00, -3.6241e-01, -6.6008e+00],\n",
       "         [ 1.2336e+00, -1.2817e+00, -3.9697e+00],\n",
       "         [ 1.9592e+00, -4.2825e-01, -6.6206e+00]]),\n",
       " tensor([[  1.9449,  -2.4371,  -2.8161],\n",
       "         [  2.7606,  -0.2293, -11.4488],\n",
       "         [  2.1790,  -1.9171,  -4.4992],\n",
       "         [  2.1755,  -1.9227,  -4.4786],\n",
       "         [  2.4624,  -2.8849,  -3.6442],\n",
       "         [  2.0908,   0.0280,  -9.7663],\n",
       "         [  1.4753,  -2.3324,  -3.6271],\n",
       "         [  2.9653,  -1.7066,  -4.7119],\n",
       "         [  2.0129,  -0.2563,  -7.9526],\n",
       "         [  2.2816,  -2.0407,  -4.3763],\n",
       "         [  1.4136,  -2.8217,  -1.1260],\n",
       "         [  1.9831,  -1.3816,  -5.3547],\n",
       "         [  1.5747,  -0.6245,  -6.3590],\n",
       "         [  2.0028,  -3.1680,  -2.4237],\n",
       "         [  3.8771,  -1.0319,  -8.1767],\n",
       "         [  3.8873,  -1.0425,  -8.1745],\n",
       "         [  1.7669,  -1.6788,  -5.8685],\n",
       "         [  1.7723,  -1.6850,  -5.8959],\n",
       "         [  1.3539,  -0.2205,  -5.5707],\n",
       "         [  2.0993,  -2.0578,  -4.4922],\n",
       "         [  2.8729,  -0.3397,  -9.0820],\n",
       "         [  2.8664,  -0.3351,  -9.0850],\n",
       "         [  2.8565,  -0.3488,  -9.0023],\n",
       "         [  2.0081,  -0.9948,  -5.7487],\n",
       "         [  1.9777,  -0.4145,  -7.2994],\n",
       "         [  1.6968,  -0.4151,  -5.7114],\n",
       "         [  1.4034,  -0.9197,  -4.5887],\n",
       "         [  4.1644,  -1.1692, -10.8046],\n",
       "         [  3.4250,  -0.4804,  -9.6031],\n",
       "         [  1.9667,   0.4677, -10.0113],\n",
       "         [  2.3693,  -0.3930,  -8.1951],\n",
       "         [  2.3560,  -0.4035,  -8.1067],\n",
       "         [  3.0931,  -1.0343,  -7.4913],\n",
       "         [  1.8961,  -0.8624,  -5.5911],\n",
       "         [  1.6188,  -0.7954,  -5.5705],\n",
       "         [  3.0948,  -0.2816, -10.2490],\n",
       "         [  1.9477,  -0.5078,  -6.5757],\n",
       "         [  5.1631,  -1.2735, -13.5582],\n",
       "         [  4.7000,  -1.0123, -12.4949],\n",
       "         [  1.4510,  -3.4679,  -2.4765],\n",
       "         [  1.3891,  -0.4800,  -5.7951],\n",
       "         [  1.8937,  -0.1427,  -8.1308],\n",
       "         [  2.5909,  -1.0913,  -6.2816],\n",
       "         [  1.6672,  -0.0532,  -6.5037],\n",
       "         [  1.5951,  -0.2353,  -6.0168],\n",
       "         [  1.9257,  -0.3679,  -7.4778],\n",
       "         [  1.8324,  -0.6862,  -6.0550],\n",
       "         [  2.3829,   0.0287, -10.7190],\n",
       "         [  1.5354,  -0.6552,  -4.3223],\n",
       "         [  4.1590,  -1.4797, -10.9225],\n",
       "         [  2.8062,  -0.0659, -11.5806],\n",
       "         [  2.8096,  -0.0781, -11.5470],\n",
       "         [  3.0192,  -0.0749,  -9.8269],\n",
       "         [  1.4182,  -1.5386,  -2.9288],\n",
       "         [  1.4364,  -1.5550,  -2.9833],\n",
       "         [  1.4450,  -1.5706,  -2.9934],\n",
       "         [  1.4863,  -0.2215,  -6.3859],\n",
       "         [  2.0901,  -0.5586,  -6.3820],\n",
       "         [  2.2015,  -2.2716,  -4.0751],\n",
       "         [  2.7305,  -1.4981,  -5.5516],\n",
       "         [  1.6151,  -2.2300,  -2.6285],\n",
       "         [  2.1552,  -0.1225,  -6.7344],\n",
       "         [  3.2501,  -1.2330,  -7.9685],\n",
       "         [  0.8847,  -0.0423,  -5.7021]]),\n",
       " tensor([[  2.0211,  -0.7278,  -5.6038],\n",
       "         [  1.6865,  -2.3965,  -2.3628],\n",
       "         [  1.0306,   0.2894,  -7.1293],\n",
       "         [  2.9847,  -0.6390,  -8.4275],\n",
       "         [  1.7105,  -0.1030,  -7.0640],\n",
       "         [  2.1714,  -2.7068,  -3.7674],\n",
       "         [  1.8555,  -2.4643,  -3.7430],\n",
       "         [  1.8583,  -2.4263,  -3.8198],\n",
       "         [  3.2305,  -0.5731, -10.1189],\n",
       "         [  2.8957,  -0.5729,  -8.1990],\n",
       "         [  2.8834,  -0.5589,  -8.2031],\n",
       "         [  2.9046,  -0.5903,  -8.1770],\n",
       "         [  1.9048,  -2.6553,  -3.1844],\n",
       "         [  0.9731,   0.4919,  -7.6987],\n",
       "         [  1.6235,  -2.5047,  -2.9959],\n",
       "         [  1.6312,  -2.4888,  -3.0511],\n",
       "         [  1.6295,  -2.4888,  -3.0473],\n",
       "         [  1.8597,  -1.8287,  -4.1936],\n",
       "         [  1.7664,  -0.1116,  -7.8205],\n",
       "         [  1.7666,  -0.1108,  -7.8405],\n",
       "         [  2.5389,  -0.3688,  -8.5250],\n",
       "         [  1.7729,  -0.7246,  -6.6487],\n",
       "         [  1.4982,   0.2859,  -8.3951],\n",
       "         [  3.8114,  -0.4087, -11.7910],\n",
       "         [  3.1741,  -0.2949, -10.1929],\n",
       "         [  2.2368,   0.7197, -11.7998],\n",
       "         [  2.6481,  -2.4143,  -5.4095],\n",
       "         [  0.9381,   0.4545,  -7.0785],\n",
       "         [  2.3060,  -1.8415,  -5.2643],\n",
       "         [  2.3163,  -1.7873,  -5.3718],\n",
       "         [  1.9310,  -0.3535,  -6.6252],\n",
       "         [  1.2294,  -1.2889,  -3.9388],\n",
       "         [  1.2337,  -1.2961,  -3.9466],\n",
       "         [  1.9579,  -0.1811,  -6.5014],\n",
       "         [  1.2583,  -0.8942,  -4.0409],\n",
       "         [  3.0290,  -2.1436,  -6.9793],\n",
       "         [  1.6396,  -0.4840,  -5.4960],\n",
       "         [  2.1846,  -1.9145,  -4.5105],\n",
       "         [  1.4640,  -2.1688,  -3.1357],\n",
       "         [  1.7185,  -0.4766,  -5.8489],\n",
       "         [  2.0351,  -0.5273,  -7.7189],\n",
       "         [  2.2858,  -2.0474,  -4.3598],\n",
       "         [  2.2848,  -2.0416,  -4.3660],\n",
       "         [  2.2804,  -2.0433,  -4.3650],\n",
       "         [  2.2798,  -2.0580,  -4.3319],\n",
       "         [  1.2108,  -0.3152,  -4.1861],\n",
       "         [  1.8309,  -0.1881,  -7.8943],\n",
       "         [  1.8884,   0.1877,  -9.1879],\n",
       "         [  2.1157,  -2.7730,  -3.3069],\n",
       "         [  1.5651,  -0.0236,  -7.4736],\n",
       "         [  2.5733,  -3.1378,  -3.4807],\n",
       "         [  1.7699,  -1.6665,  -5.9161],\n",
       "         [  2.2041,   0.1995,  -9.4608],\n",
       "         [  1.2812,  -0.2694,  -5.2082],\n",
       "         [  2.0034,  -0.9860,  -5.7417],\n",
       "         [  1.9842,  -0.4016,  -7.3760],\n",
       "         [  1.3452,  -1.2138,  -4.1892],\n",
       "         [  1.6567,  -0.3715,  -5.7450],\n",
       "         [  1.7938,  -4.3967,   0.0185],\n",
       "         [  1.3626,  -2.0843,  -3.5748],\n",
       "         [  1.5902,  -1.4920,  -4.5763],\n",
       "         [  1.5865,  -1.4255,  -4.6648],\n",
       "         [  2.1172,  -0.9321,  -6.3481],\n",
       "         [  1.7445,   0.2266,  -8.9402]]),\n",
       " tensor([[  1.6479,  -0.0466,  -8.4517],\n",
       "         [  1.8918,  -0.8499,  -5.5924],\n",
       "         [  2.3411,  -0.7858,  -7.6395],\n",
       "         [  3.0921,  -0.2894, -10.1984],\n",
       "         [  3.0872,  -0.2826, -10.1885],\n",
       "         [  1.5918,   0.6185,  -8.7821],\n",
       "         [  1.9469,  -0.5117,  -6.5450],\n",
       "         [  1.9469,  -0.5127,  -6.5391],\n",
       "         [  1.7085,  -0.5129,  -6.2265],\n",
       "         [  3.0024,  -0.9592,  -7.4256],\n",
       "         [  1.8389,  -0.7295,  -5.9381],\n",
       "         [  4.6917,  -1.0143, -12.4611],\n",
       "         [  2.2278,   0.0552,  -9.6295],\n",
       "         [  1.9200,  -0.3734,  -7.4389],\n",
       "         [  1.8399,  -0.6889,  -6.0733],\n",
       "         [  2.3756,   0.0518, -10.7834],\n",
       "         [  4.1537,  -1.0408,  -9.9184],\n",
       "         [  4.1733,  -1.0530,  -9.9418],\n",
       "         [  4.1734,  -1.0495,  -9.9471],\n",
       "         [  1.6330,  -0.0885,  -6.8557],\n",
       "         [  3.2489,  -1.2336,  -7.9667],\n",
       "         [  2.0934,  -1.4310,  -4.7534],\n",
       "         [  1.4965,  -5.7568,   1.5639],\n",
       "         [  5.0149,  -0.6442, -13.6543],\n",
       "         [  4.9974,  -0.6358, -13.6303],\n",
       "         [  0.9601,   0.5059,  -7.7046],\n",
       "         [  0.9862,   0.4584,  -7.5871],\n",
       "         [  2.4625,  -0.5957,  -8.5901],\n",
       "         [  1.8640,  -1.8409,  -4.1910],\n",
       "         [  1.8646,  -1.8497,  -4.1807],\n",
       "         [  3.5103,  -0.2992, -11.1499],\n",
       "         [  3.5111,  -0.3033, -11.1243],\n",
       "         [  3.8566,  -0.8524,  -9.6052],\n",
       "         [  2.3211,  -1.7786,  -5.3824],\n",
       "         [  2.5299,  -1.1012,  -5.8108],\n",
       "         [  1.6361,  -0.4796,  -5.4909],\n",
       "         [  2.3897,  -1.8710,  -8.9834],\n",
       "         [  2.1669,  -1.9433,  -4.4198],\n",
       "         [  1.4767,  -2.3227,  -3.6214],\n",
       "         [  1.8289,  -0.1808,  -7.9194],\n",
       "         [  1.8314,  -0.1757,  -7.9521],\n",
       "         [  1.0538,  -0.8561,  -3.1964],\n",
       "         [  2.5828,  -3.1321,  -3.5153],\n",
       "         [  1.2796,  -0.2568,  -5.2459],\n",
       "         [  1.2772,  -0.2674,  -5.2402],\n",
       "         [  2.8629,  -0.3393,  -9.0539],\n",
       "         [  1.8003,  -4.4145,   0.0295],\n",
       "         [  2.2506,  -1.7198,  -4.4111],\n",
       "         [  2.3513,  -0.8566,  -6.9819],\n",
       "         [  2.3275,  -0.7794,  -7.6147],\n",
       "         [  3.0972,  -0.2753, -10.2830],\n",
       "         [  3.1029,  -0.2806, -10.2872],\n",
       "         [  1.9353,  -0.5012,  -6.5656],\n",
       "         [  1.9492,  -0.5156,  -6.5197],\n",
       "         [  5.2201,  -1.2787, -13.7745],\n",
       "         [  4.7229,  -1.0134, -12.5559],\n",
       "         [  2.6014,  -1.1142,  -6.2801],\n",
       "         [  2.5868,  -1.0346,  -6.3600],\n",
       "         [  1.9227,  -0.3717,  -7.4589],\n",
       "         [  2.8992,  -0.8660,  -7.8801],\n",
       "         [  1.3274,   0.9116,  -8.6520],\n",
       "         [  0.0201,   1.4315,  -3.2141],\n",
       "         [  1.6561,  -1.1455,  -4.7753],\n",
       "         [  1.6530,  -1.1541,  -4.7264]]),\n",
       " tensor([[  1.7120,  -1.3526,  -4.5422],\n",
       "         [  4.1432,  -1.0389,  -9.8934],\n",
       "         [  4.1484,  -1.0347,  -9.9225],\n",
       "         [  3.2232,  -0.5627, -10.1345],\n",
       "         [  2.8940,  -0.5602,  -8.2328],\n",
       "         [  3.8069,  -0.4379, -11.6763],\n",
       "         [  2.4043,  -0.1407,  -9.7936],\n",
       "         [  2.4037,  -0.1404,  -9.7948],\n",
       "         [  1.2358,  -1.2845,  -3.9755],\n",
       "         [  2.4424,  -1.7763,  -5.1744],\n",
       "         [  1.9186,  -1.5651,  -4.0425],\n",
       "         [  1.6504,  -0.0462,  -8.4551],\n",
       "         [  1.6494,  -0.0498,  -8.4341],\n",
       "         [  3.0937,  -1.0234,  -7.5136],\n",
       "         [  4.7227,  -1.0296, -12.5247],\n",
       "         [  2.5973,  -1.1109,  -6.2721],\n",
       "         [  1.7583,  -1.3456,  -4.7961],\n",
       "         [  1.4811,  -2.3893,  -1.1636],\n",
       "         [  2.1087,  -0.6111,  -7.4450],\n",
       "         [  4.1686,  -1.0483,  -9.9388],\n",
       "         [  4.1471,  -1.0461,  -9.8875],\n",
       "         [  4.1369,  -1.0574,  -9.8307],\n",
       "         [  4.1598,  -1.0511,  -9.9088],\n",
       "         [  1.1895,  -0.8607,  -4.4833],\n",
       "         [  1.8723,  -2.1178,  -2.5798],\n",
       "         [  2.1786,  -2.0457,  -4.1226],\n",
       "         [  1.8614,  -1.8508,  -4.1650],\n",
       "         [  1.7676,  -0.1127,  -7.8352],\n",
       "         [  2.5316,  -0.3613,  -8.5426],\n",
       "         [  1.9658,  -2.5260,  -3.6259],\n",
       "         [  0.3966,  -0.5108,  -1.2612],\n",
       "         [  2.3634,  -3.0053,  -4.2378],\n",
       "         [  2.2790,  -2.0554,  -4.3225],\n",
       "         [  1.7744,  -1.6821,  -5.9008],\n",
       "         [  1.7002,  -0.4513,  -5.6764],\n",
       "         [  2.5579,  -0.0887,  -8.7675],\n",
       "         [  1.6513,  -0.0300,  -8.5256],\n",
       "         [  1.6499,  -0.0294,  -8.5292],\n",
       "         [  2.2956,  -0.4903,  -7.9856],\n",
       "         [  4.6984,  -1.0130, -12.4944],\n",
       "         [  3.2520,  -0.9722,  -7.5420],\n",
       "         [  1.6927,  -0.0863,  -7.2778],\n",
       "         [  1.8654,  -1.8322,  -4.2145],\n",
       "         [  2.3095,  -1.7638,  -5.3824],\n",
       "         [  1.3024,  -0.0218,  -6.2199],\n",
       "         [  2.8287, -10.0501,   3.1986],\n",
       "         [  1.3246,  -2.5648,  -1.4369],\n",
       "         [  2.9791,  -0.7797,  -8.0938],\n",
       "         [  2.3193,  -0.0348,  -9.2068],\n",
       "         [  2.0924,  -2.0538,  -4.4974],\n",
       "         [  3.1074,  -0.2682, -10.3422],\n",
       "         [  2.0690,  -1.5311,  -5.9756],\n",
       "         [  1.4977,  -2.8922,  -1.0880],\n",
       "         [  1.5028,  -2.8248,  -1.2245],\n",
       "         [  1.4901,  -2.8565,  -1.1484],\n",
       "         [  1.4871,  -2.8532,  -1.1547],\n",
       "         [  1.8193,  -3.8913,  -0.7293],\n",
       "         [  1.8310,  -3.9918,  -0.6906],\n",
       "         [  1.8412,  -3.9806,  -0.7329],\n",
       "         [  0.1610,  -3.7451,   0.6893],\n",
       "         [ -1.6263,   0.1588,  -6.0920],\n",
       "         [  0.1937,  -3.6437,   0.4844],\n",
       "         [  0.1749,  -3.6717,   0.5476],\n",
       "         [  2.2840,  -2.3289,  -4.6813]]),\n",
       " tensor([[ 2.2850e+00, -2.3208e+00, -4.7010e+00],\n",
       "         [ 2.2815e+00, -2.3100e+00, -4.7048e+00],\n",
       "         [ 2.2792e+00, -2.3263e+00, -4.6707e+00],\n",
       "         [ 2.2670e+00, -2.2906e+00, -4.6922e+00],\n",
       "         [ 2.2854e+00, -2.3250e+00, -4.6903e+00],\n",
       "         [ 2.2846e+00, -2.3084e+00, -4.7160e+00],\n",
       "         [ 2.2833e+00, -2.3113e+00, -4.7064e+00],\n",
       "         [ 2.2814e+00, -2.3053e+00, -4.7109e+00],\n",
       "         [ 2.2851e+00, -2.3119e+00, -4.7122e+00],\n",
       "         [ 1.6519e+00, -2.9870e-02, -8.5344e+00],\n",
       "         [ 1.6458e+00, -2.8095e-02, -8.5125e+00],\n",
       "         [ 1.6521e+00, -2.6046e-02, -8.5512e+00],\n",
       "         [ 1.6488e+00, -2.7469e-02, -8.5281e+00],\n",
       "         [ 1.6492e+00, -2.4381e-02, -8.5481e+00],\n",
       "         [ 1.6459e+00, -9.8170e-03, -8.5975e+00],\n",
       "         [ 1.6528e+00, -3.3078e-02, -8.5279e+00],\n",
       "         [ 1.6509e+00, -2.7136e-02, -8.5421e+00],\n",
       "         [ 1.6493e+00, -1.9632e-02, -8.5689e+00],\n",
       "         [ 1.6467e+00, -2.2502e-02, -8.5415e+00],\n",
       "         [ 1.9533e+00,  5.2966e-01, -1.0559e+01],\n",
       "         [ 1.7422e+00,  3.9046e-01, -1.0047e+01],\n",
       "         [ 1.7591e+00,  3.2928e-01, -9.8088e+00],\n",
       "         [ 1.7599e+00,  3.2225e-01, -9.7869e+00],\n",
       "         [ 1.7388e+00,  3.7003e-01, -9.9271e+00],\n",
       "         [ 1.7381e+00,  3.5271e-01, -9.8609e+00],\n",
       "         [ 1.7428e+00,  3.6998e-01, -9.9500e+00],\n",
       "         [ 1.8638e+00, -4.0774e+00, -5.5656e-01],\n",
       "         [ 1.8188e+00, -3.8653e+00, -7.6532e-01],\n",
       "         [ 1.8230e+00, -3.8420e+00, -7.9491e-01],\n",
       "         [ 1.8130e+00, -3.8451e+00, -7.7087e-01],\n",
       "         [ 1.9261e+00, -3.5665e-01, -6.5898e+00],\n",
       "         [ 1.9240e+00, -3.6718e-01, -6.5520e+00],\n",
       "         [ 1.6858e-01, -3.6819e+00,  5.8411e-01],\n",
       "         [ 1.6929e-01, -3.6711e+00,  5.6265e-01],\n",
       "         [ 2.2847e+00, -2.3142e+00, -4.7077e+00],\n",
       "         [ 2.2846e+00, -2.3122e+00, -4.7142e+00],\n",
       "         [ 2.2862e+00, -2.3185e+00, -4.7058e+00],\n",
       "         [ 2.3189e+00, -2.3414e-02, -9.2586e+00],\n",
       "         [ 1.6529e+00, -3.2749e+00, -9.2471e-01],\n",
       "         [ 1.7323e+00, -3.6432e+00, -6.2145e-01],\n",
       "         [ 1.7325e+00, -3.6440e+00, -6.1193e-01],\n",
       "         [ 1.7327e+00, -3.6336e+00, -6.3660e-01],\n",
       "         [ 2.2215e+00, -1.9820e+00, -4.7683e+00],\n",
       "         [ 1.6489e+00, -3.3408e-02, -8.5032e+00],\n",
       "         [ 1.6413e+00, -3.4584e-02, -8.4617e+00],\n",
       "         [ 1.6463e+00, -2.4670e-02, -8.5197e+00],\n",
       "         [ 1.6395e+00, -1.3837e-02, -8.5430e+00],\n",
       "         [ 1.6519e+00, -3.3593e-02, -8.5189e+00],\n",
       "         [ 1.6473e+00, -3.4262e-02, -8.4882e+00],\n",
       "         [ 1.6527e+00, -4.9992e-02, -8.4457e+00],\n",
       "         [ 2.1101e+00, -5.1799e-01, -7.6247e+00],\n",
       "         [ 2.1105e+00, -5.6001e-01, -7.5649e+00],\n",
       "         [ 1.5146e+00, -2.0481e+00, -3.0039e+00],\n",
       "         [ 1.5120e+00, -2.0598e+00, -2.9759e+00],\n",
       "         [ 1.5173e+00, -2.0521e+00, -3.0031e+00],\n",
       "         [ 1.4085e+00, -4.2510e-01, -6.0011e+00],\n",
       "         [ 2.0069e+00, -3.1598e+00, -2.4592e+00],\n",
       "         [ 1.5717e+00, -3.0003e-02, -7.4747e+00],\n",
       "         [ 2.1401e+00, -5.9979e-01, -6.4764e+00],\n",
       "         [ 3.0152e+00, -9.6308e-01, -7.4525e+00],\n",
       "         [ 1.8393e+00, -6.9050e-01, -6.0687e+00],\n",
       "         [ 2.0083e+00, -2.8309e+00, -2.9387e+00],\n",
       "         [ 2.0002e+00, -2.8350e+00, -2.9063e+00],\n",
       "         [ 1.7448e+00, -2.3804e+00, -2.5732e+00]]),\n",
       " tensor([[  3.0220,  -2.1319,  -6.9831],\n",
       "         [  1.0482,  -2.4675,  -1.3142],\n",
       "         [  2.0695,  -1.5420,  -5.9598],\n",
       "         [  4.0678,  -1.2909,  -9.7651],\n",
       "         [  1.8688,  -2.9507,  -2.4145],\n",
       "         [  3.0322,  -0.0699,  -9.8853],\n",
       "         [  1.8145,  -3.8528,  -0.7680],\n",
       "         [  1.8176,  -3.8847,  -0.7129],\n",
       "         [  1.8199,  -3.8282,  -0.8007],\n",
       "         [  2.2845,  -2.3120,  -4.7102],\n",
       "         [  1.2838,  -0.2560,  -5.2677],\n",
       "         [  1.7333,  -3.6323,  -0.6351],\n",
       "         [  1.7323,  -3.6496,  -0.6104],\n",
       "         [  1.7326,  -3.6386,  -0.6240],\n",
       "         [  1.6514,  -0.0410,  -8.4770],\n",
       "         [  1.6466,  -0.0337,  -8.4913],\n",
       "         [  1.7378,   0.3281,  -9.7544],\n",
       "         [  1.8570,  -4.0680,  -0.5498],\n",
       "         [  2.2762,  -2.3629,  -4.6014],\n",
       "         [  2.2799,  -2.3132,  -4.6953],\n",
       "         [  1.7332,  -3.6521,  -0.6156],\n",
       "         [  1.7304,  -3.6394,  -0.6160],\n",
       "         [  1.6396,  -0.0209,  -8.5204],\n",
       "         [  2.1064,  -0.5976,  -7.4493],\n",
       "         [  2.1021,  -0.5874,  -7.4720],\n",
       "         [  1.8086,  -3.8459,  -0.7663],\n",
       "         [  0.1889,  -3.6654,   0.5264],\n",
       "         [  2.2700,  -2.2906,  -4.7063],\n",
       "         [  2.2835,  -2.2557,  -4.7986],\n",
       "         [  1.7344,  -3.6380,  -0.6351],\n",
       "         [  1.7327,  -3.6307,  -0.6375],\n",
       "         [  1.7300,  -3.6351,  -0.6240],\n",
       "         [  1.7340,  -3.6349,  -0.6383],\n",
       "         [  1.6516,  -0.0225,  -8.5598],\n",
       "         [  2.1145,  -0.6807,  -7.3281],\n",
       "         [  1.8259,  -3.8883,  -0.7461],\n",
       "         [  1.8078,  -3.8376,  -0.7616],\n",
       "         [  1.3086,  -0.2905,  -5.2555],\n",
       "         [  1.1536,   0.8412, -12.5110],\n",
       "         [  1.6540,  -0.0416,  -8.4860],\n",
       "         [  1.6489,  -0.0261,  -8.5350],\n",
       "         [  3.0054,  -0.9599,  -7.4297],\n",
       "         [  0.2939,   0.9821,  -6.2392],\n",
       "         [  1.7912,  -0.0792,  -8.3243],\n",
       "         [  2.3163,  -0.0267,  -9.2294],\n",
       "         [  2.3131,  -0.0273,  -9.2347],\n",
       "         [  3.0894,  -0.2680, -10.2751],\n",
       "         [  1.1857,  -0.8540,  -4.4646],\n",
       "         [  1.8468,  -2.1558,  -3.4709],\n",
       "         [  2.1249,  -0.0698,  -8.8876],\n",
       "         [  1.4767,   0.4979,  -8.5802],\n",
       "         [  1.4769,   0.5138,  -8.6563],\n",
       "         [  1.8711,  -2.9372,  -2.4439],\n",
       "         [  3.0004,  -1.9694,  -6.6389],\n",
       "         [  2.7395,   0.4183, -11.2550],\n",
       "         [  1.6297,   0.0739,  -7.9620],\n",
       "         [  1.8894,   0.0245,  -7.4242],\n",
       "         [  1.5529,  -1.2570,  -5.1067],\n",
       "         [  2.8088,  -0.0778, -11.5404],\n",
       "         [  1.9627,  -3.2863,  -2.2651],\n",
       "         [  1.7319,  -3.6413,  -0.6241],\n",
       "         [  1.6295,   0.0712,  -7.9430],\n",
       "         [  1.6796,  -5.7029,   1.7606],\n",
       "         [  2.7395,   0.4417, -11.3826]]),\n",
       " tensor([[ 2.7372e+00,  4.3424e-01, -1.1345e+01],\n",
       "         [ 3.8579e+00, -8.5828e-01, -9.5784e+00],\n",
       "         [ 2.4892e+00, -1.2128e+00, -6.8732e+00],\n",
       "         [ 1.8233e+00, -3.9421e+00, -7.6683e-01],\n",
       "         [ 2.4496e+00, -5.4296e-01, -8.6636e+00],\n",
       "         [ 1.8948e+00,  6.0936e-03, -7.3650e+00],\n",
       "         [ 2.2390e+00,  5.2400e-01, -1.1099e+01],\n",
       "         [ 1.9274e+00, -1.5673e+00, -4.0501e+00],\n",
       "         [ 1.4889e+00, -2.8383e+00, -1.1766e+00],\n",
       "         [ 2.0882e+00, -2.0303e+00, -4.5247e+00],\n",
       "         [ 2.1015e+00, -1.9729e+00, -4.6606e+00],\n",
       "         [ 1.2849e+00, -1.8781e+00, -2.8485e+00],\n",
       "         [ 1.2839e+00, -1.8745e+00, -2.8372e+00],\n",
       "         [ 3.1043e+00, -2.8437e-01, -1.0270e+01],\n",
       "         [ 1.4854e+00, -2.3807e+00, -1.1986e+00],\n",
       "         [ 2.3404e+00, -1.9384e+00, -4.2264e+00],\n",
       "         [ 2.0388e+00, -5.4593e+00,  7.0232e-01],\n",
       "         [ 2.3187e+00, -3.0618e-02, -9.2208e+00],\n",
       "         [ 1.8612e+00, -1.5836e+00, -5.0428e+00],\n",
       "         [ 1.7334e+00,  3.5786e-01, -9.8658e+00],\n",
       "         [ 1.7426e+00,  3.4320e-01, -9.8304e+00],\n",
       "         [ 1.8032e+00, -3.9846e-01, -6.9912e+00],\n",
       "         [ 1.9302e+00, -3.5841e-01, -6.6091e+00],\n",
       "         [ 1.0409e+00, -2.4198e+00, -1.3376e+00],\n",
       "         [ 2.2828e+00, -2.3217e+00, -4.6892e+00],\n",
       "         [ 2.2752e+00, -2.3412e+00, -4.6315e+00],\n",
       "         [ 1.7349e+00, -3.6506e+00, -6.2691e-01],\n",
       "         [ 1.7343e+00, -3.6468e+00, -6.2169e-01],\n",
       "         [ 1.6490e+00, -3.8505e-02, -8.4861e+00],\n",
       "         [ 1.2427e+00, -9.7776e-01, -4.2976e+00],\n",
       "         [ 1.2092e+00, -3.1313e+00, -1.4930e+00],\n",
       "         [ 2.0280e+00, -5.4786e+00,  7.4390e-01],\n",
       "         [ 1.4713e+00, -1.7747e+00, -3.2205e+00],\n",
       "         [ 1.7288e+00,  3.1693e-01, -9.6699e+00],\n",
       "         [ 1.8166e+00, -3.8469e+00, -7.7562e-01],\n",
       "         [ 1.9270e+00, -3.6846e-01, -6.5717e+00],\n",
       "         [ 1.0539e+00, -2.4655e+00, -1.3277e+00],\n",
       "         [ 1.7321e+00, -3.6471e+00, -6.1454e-01],\n",
       "         [ 1.6441e+00, -2.3030e-02, -8.5241e+00],\n",
       "         [ 1.6498e+00, -1.8683e-02, -8.5716e+00],\n",
       "         [ 3.1068e+00, -1.0333e+00, -7.5336e+00],\n",
       "         [ 1.8361e+00, -6.8774e-01, -6.0571e+00],\n",
       "         [ 1.7119e-01, -3.6603e+00,  5.4039e-01],\n",
       "         [ 1.7357e+00,  3.5634e-01, -9.8392e+00],\n",
       "         [ 1.8240e+00, -3.8922e+00, -7.3257e-01],\n",
       "         [ 1.8370e+00, -3.8931e+00, -7.4131e-01],\n",
       "         [ 1.8559e+00, -3.9862e+00, -7.7360e-01],\n",
       "         [ 1.5540e+00, -1.0137e+00, -5.3101e+00],\n",
       "         [ 1.3544e+00, -1.6609e+00, -2.8845e+00],\n",
       "         [ 1.6549e+00, -6.2030e-02, -8.3980e+00],\n",
       "         [ 3.1000e+00, -2.7512e-01, -1.0296e+01],\n",
       "         [ 1.2160e+00, -8.3149e-01, -4.5261e+00],\n",
       "         [ 2.1175e+00, -6.6293e-01, -7.3567e+00],\n",
       "         [ 1.6532e+00, -1.0810e+00, -4.8902e+00],\n",
       "         [ 1.4862e+00,  4.8453e-01, -8.5559e+00],\n",
       "         [ 5.0204e+00, -6.4331e-01, -1.3669e+01],\n",
       "         [ 1.6743e+00, -5.9495e-01, -4.8248e+00],\n",
       "         [ 2.0148e+00, -2.6154e+00, -2.8977e+00],\n",
       "         [ 2.1071e+00, -6.7281e-01, -7.3265e+00],\n",
       "         [ 1.8677e+00, -4.0023e+00, -7.7974e-01],\n",
       "         [ 1.8332e+00, -3.9886e+00, -7.0423e-01],\n",
       "         [ 1.6268e+00, -3.0499e-02, -8.3927e+00],\n",
       "         [ 1.6558e+00, -4.2196e-02, -8.4982e+00],\n",
       "         [ 1.6515e+00, -3.2367e-02, -8.5177e+00]]),\n",
       " tensor([[  2.3154,  -1.7858,  -5.3689],\n",
       "         [  3.2444,  -1.2263,  -7.9751],\n",
       "         [  3.2430,  -1.2306,  -7.9676],\n",
       "         [  1.7852,  -1.0213,  -5.1945],\n",
       "         [  1.6117,   0.1563,  -7.8207],\n",
       "         [  1.8179,  -3.8611,  -0.7639],\n",
       "         [  1.9267,  -0.3607,  -6.5801],\n",
       "         [  1.9298,  -0.3649,  -6.5850],\n",
       "         [  1.9334,  -0.3600,  -6.6136],\n",
       "         [  3.8383,  -1.2183,  -9.3740],\n",
       "         [  0.1876,  -3.6641,   0.5302],\n",
       "         [  0.8226,  -0.1732,  -3.0386],\n",
       "         [  2.2768,  -2.3155,  -4.6831],\n",
       "         [  1.4052,  -0.4325,  -5.9553],\n",
       "         [  2.0967,  -2.0054,  -4.5920],\n",
       "         [  1.3484,  -1.6659,  -4.0443],\n",
       "         [  1.5297,  -2.0706,  -2.5681],\n",
       "         [  1.6313,  -1.9561,  -2.6659],\n",
       "         [  1.6485,  -0.0317,  -8.5072],\n",
       "         [  1.8223,  -1.3000,  -5.5931],\n",
       "         [  1.8273,  -1.2955,  -5.6138],\n",
       "         [  4.7112,  -1.0159, -12.5186],\n",
       "         [  4.7083,  -1.0125, -12.5081],\n",
       "         [  1.2906,   0.9368,  -8.6066],\n",
       "         [  1.4690,  -2.3927,  -1.1194],\n",
       "         [  1.4796,  -2.3795,  -1.1831],\n",
       "         [  2.1064,  -0.6278,  -7.4077],\n",
       "         [  1.6524,  -1.1057,  -4.8284],\n",
       "         [  3.2426,  -1.2306,  -7.9639],\n",
       "         [  1.8886,  -0.1422,  -7.5331],\n",
       "         [  1.8613,  -0.8204,  -5.4692],\n",
       "         [  1.6756,  -0.9827,  -4.5411],\n",
       "         [  1.8613,  -4.0357,  -0.7168],\n",
       "         [  1.9285,  -0.3485,  -6.6354],\n",
       "         [  0.4911,   0.4938,  -6.1692],\n",
       "         [  0.1551,  -3.6511,   0.5612],\n",
       "         [  2.2838,  -2.3102,  -4.7128],\n",
       "         [  2.9119,   0.1734,  -7.6169],\n",
       "         [  1.6246,   0.0938,  -8.0288],\n",
       "         [ -4.2700,   1.4302, -14.3902],\n",
       "         [  1.5689,  -0.6784,  -6.2590],\n",
       "         [  1.5665,  -0.5591,  -6.4494],\n",
       "         [  1.6422,  -1.4994,  -3.6666],\n",
       "         [  2.2046,   0.2019,  -9.4873],\n",
       "         [  2.4766,  -3.5166,  -2.9521],\n",
       "         [  4.5572,  -0.9031, -13.0252],\n",
       "         [  1.6402,  -0.0450,  -8.4070],\n",
       "         [  4.7393,  -1.0131, -12.5949],\n",
       "         [  1.6758,  -0.0494,  -6.6088],\n",
       "         [  1.5309,  -3.2633,  -0.5489],\n",
       "         [  1.8948,  -0.3395,  -6.4472],\n",
       "         [  2.1576,  -0.0959,  -6.8354],\n",
       "         [  3.2441,  -1.2274,  -7.9703],\n",
       "         [  2.5042,  -0.4120,  -7.7379],\n",
       "         [  2.5039,  -0.4079,  -7.7380],\n",
       "         [  2.1847,  -0.5577,  -7.0133],\n",
       "         [  1.7369,  -0.8027,  -6.8507],\n",
       "         [  3.5194,  -0.3105, -11.1386],\n",
       "         [  1.8234,  -3.9235,  -0.6680],\n",
       "         [  2.1307,  -1.5853,  -5.5863],\n",
       "         [  1.9320,  -0.3609,  -6.6061],\n",
       "         [  1.9344,  -0.3586,  -6.6258],\n",
       "         [  1.9964,  -0.2128,  -6.5375],\n",
       "         [  1.6395,  -0.4901,  -5.4799]]),\n",
       " tensor([[  2.2812,  -2.3284,  -4.6761],\n",
       "         [  2.2835,  -2.3142,  -4.7074],\n",
       "         [  2.2823,  -2.3035,  -4.7183],\n",
       "         [  2.2706,  -2.2744,  -4.7266],\n",
       "         [  1.5758,  -0.7164,  -6.2028],\n",
       "         [  1.5611,  -0.6831,  -6.1680],\n",
       "         [  4.5500,  -0.9009, -13.0036],\n",
       "         [  1.9817,  -0.4147,  -7.3110],\n",
       "         [  1.9846,  -0.4182,  -7.3077],\n",
       "         [  1.6366,  -1.9552,  -2.6720],\n",
       "         [  1.6512,  -0.0288,  -8.5359],\n",
       "         [  1.6491,  -0.0451,  -8.4462],\n",
       "         [  3.0853,  -1.0259,  -7.4955],\n",
       "         [  1.8222,  -1.3007,  -5.5889],\n",
       "         [  1.4958,  -0.2541,  -6.3065],\n",
       "         [  1.4943,  -0.2571,  -6.2984],\n",
       "         [  1.8432,  -0.7311,  -5.9502],\n",
       "         [  2.2854,  -0.5071,  -6.7281],\n",
       "         [  2.9003,  -1.0032,  -7.0566],\n",
       "         [  1.8851,  -0.3366,  -6.4155],\n",
       "         [  1.8863,  -0.3283,  -6.4354],\n",
       "         [  1.5130,  -2.0649,  -2.9763],\n",
       "         [  1.5434,  -0.8713,  -4.9080],\n",
       "         [  2.1856,  -2.0179,  -4.1820],\n",
       "         [  1.6326,  -2.4876,  -3.0546],\n",
       "         [  1.8592,  -1.8411,  -4.1804],\n",
       "         [  2.3373,  -1.0185,  -6.7978],\n",
       "         [  1.9355,  -0.3621,  -6.6170],\n",
       "         [  1.9320,  -0.3609,  -6.6061],\n",
       "         [  1.9330,  -0.3608,  -6.6107],\n",
       "         [  1.9334,  -0.3584,  -6.6179],\n",
       "         [  0.8216,  -0.1684,  -3.0561],\n",
       "         [  2.2833,  -2.3152,  -4.6996],\n",
       "         [  2.2865,  -2.3097,  -4.7204],\n",
       "         [  2.3774,  -2.9887,  -4.3034],\n",
       "         [  2.3088,  -0.0155,  -9.2744],\n",
       "         [  4.5591,  -0.9041, -13.0137],\n",
       "         [  1.6328,  -1.9479,  -2.6748],\n",
       "         [  1.0438,   0.8767, -12.0996],\n",
       "         [  1.6507,  -0.0337,  -8.5159],\n",
       "         [  1.8229,  -1.2947,  -5.6024],\n",
       "         [  1.8217,  -1.3109,  -5.5695],\n",
       "         [  1.4982,  -0.2700,  -6.2920],\n",
       "         [  3.0329,  -0.9723,  -7.4535],\n",
       "         [  1.8362,  -0.7065,  -5.9487],\n",
       "         [  2.2865,  -0.5048,  -6.7319],\n",
       "         [  0.4129,   0.8759,  -6.2631],\n",
       "         [  1.3104,  -0.0133,  -6.3034],\n",
       "         [  1.2383,  -0.9228,  -4.4054],\n",
       "         [  1.4599,  -2.3753,  -1.1475],\n",
       "         [  1.4826,  -2.3803,  -1.1858],\n",
       "         [  1.4806,  -2.3814,  -1.1701],\n",
       "         [  0.8560,   0.7026,  -7.5859],\n",
       "         [  1.8204,   0.3515,  -8.5719],\n",
       "         [  1.8875,  -0.3308,  -6.4392],\n",
       "         [  3.2428,  -1.2258,  -7.9739],\n",
       "         [  3.2409,  -1.2254,  -7.9717],\n",
       "         [  2.5037,  -0.4140,  -7.7140],\n",
       "         [  2.4981,  -0.3938,  -7.7672],\n",
       "         [  2.4967,  -0.4062,  -7.7352],\n",
       "         [  2.5059,  -0.4022,  -7.7567],\n",
       "         [  2.5068,  -0.4081,  -7.7537],\n",
       "         [  2.1314,  -0.0705,  -8.9109],\n",
       "         [  1.5054,  -5.7570,   1.5526]]),\n",
       " tensor([[  1.5011,  -1.8720,  -3.4962],\n",
       "         [  1.4985,  -1.8973,  -3.4506],\n",
       "         [  1.4306,  -0.2660,  -6.2419],\n",
       "         [  1.4378,  -0.2656,  -6.2784],\n",
       "         [  2.1839,  -2.0626,  -4.1061],\n",
       "         [  1.6315,  -2.4718,  -3.0729],\n",
       "         [  1.8610,  -1.8402,  -4.1844],\n",
       "         [  1.8638,  -1.8640,  -4.1380],\n",
       "         [  1.8650,  -1.8454,  -4.1804],\n",
       "         [  3.5179,  -0.3045, -11.1501],\n",
       "         [  1.7679,  -0.1097,  -7.8433],\n",
       "         [  1.7677,  -0.1126,  -7.8419],\n",
       "         [  1.7692,  -0.1161,  -7.8365],\n",
       "         [  1.8394,  -3.9334,  -0.8063],\n",
       "         [  1.8275,  -3.9597,  -0.7345],\n",
       "         [  2.5320,  -0.3773,  -8.4966],\n",
       "         [  1.3972,  -0.2379,  -5.6024],\n",
       "         [  1.7533,  -0.1707,  -7.7173],\n",
       "         [  1.9307,  -0.3567,  -6.6162],\n",
       "         [  1.9278,  -0.3566,  -6.6088],\n",
       "         [  1.9320,  -0.3609,  -6.6061],\n",
       "         [  1.9360,  -0.3652,  -6.6084],\n",
       "         [  1.9342,  -0.3525,  -6.6416],\n",
       "         [  2.3992,  -0.1301,  -9.8227],\n",
       "         [  2.4157,  -0.1524,  -9.7786],\n",
       "         [  0.5239,   0.5920,  -6.4380],\n",
       "         [  1.9855,  -0.2252,  -6.4675],\n",
       "         [  0.2025,  -3.6540,   0.4982],\n",
       "         [  0.2128,  -3.6812,   0.5061],\n",
       "         [  0.1896,  -3.6580,   0.5025],\n",
       "         [  1.7513,  -0.7203,  -4.6903],\n",
       "         [  1.7628,  -0.7390,  -4.7009],\n",
       "         [  1.7553,  -0.7093,  -4.7343],\n",
       "         [ -2.0705,   1.2662, -13.7993],\n",
       "         [  1.7591,  -0.7119,  -4.7238],\n",
       "         [  2.2851,  -2.3077,  -4.7181],\n",
       "         [  2.2834,  -2.3305,  -4.6766],\n",
       "         [  2.2830,  -2.3054,  -4.7187],\n",
       "         [  1.2643,   0.6551,  -8.7687],\n",
       "         [  2.0129,  -0.2539,  -7.9536],\n",
       "         [  2.0109,  -0.2553,  -7.9533],\n",
       "         [  1.6272,   0.0877,  -8.0112],\n",
       "         [  1.9880,  -1.3717,  -5.3865],\n",
       "         [  1.9838,  -1.3439,  -5.4229],\n",
       "         [  0.9498,  -0.0208,  -4.5108],\n",
       "         [  1.5703,  -0.6532,  -6.2893],\n",
       "         [  1.5720,  -0.6195,  -6.3535],\n",
       "         [  1.5769,  -0.7004,  -6.2334],\n",
       "         [  1.5871,  -0.6448,  -6.3926],\n",
       "         [  1.5778,  -0.6861,  -6.2580],\n",
       "         [  1.7276,  -3.6269,  -0.6215],\n",
       "         [  1.7357,  -3.6462,  -0.6294],\n",
       "         [  1.6378,  -1.9462,  -2.6825],\n",
       "         [  1.6332,  -1.9657,  -2.6490],\n",
       "         [  1.6311,  -1.9606,  -2.6532],\n",
       "         [  1.6239,  -1.9707,  -2.6293],\n",
       "         [  1.1602,   0.8129, -12.5024],\n",
       "         [  2.0416,  -1.7431,  -4.3233],\n",
       "         [  2.3467,  -0.4074,  -8.0691],\n",
       "         [  4.1334,  -1.0449,  -9.8559],\n",
       "         [  2.6349,  -2.2935,  -5.0897],\n",
       "         [  2.6234,  -2.2294,  -5.1613],\n",
       "         [  2.6279,  -2.3221,  -5.0322],\n",
       "         [  1.6506,  -0.0359,  -8.4993]]),\n",
       " tensor([[ 1.6425e+00, -8.0208e-03, -8.5917e+00],\n",
       "         [ 1.6519e+00, -3.2944e-02, -8.5201e+00],\n",
       "         [ 2.2081e+00, -5.9488e-01, -6.3007e+00],\n",
       "         [ 2.2043e+00, -6.0332e-01, -6.2518e+00],\n",
       "         [ 3.1028e+00, -2.7398e-01, -1.0304e+01],\n",
       "         [ 3.1024e+00, -2.8279e-01, -1.0273e+01],\n",
       "         [ 1.8197e+00, -1.2996e+00, -5.5817e+00],\n",
       "         [ 1.8234e+00, -1.2857e+00, -5.6120e+00],\n",
       "         [ 1.8415e+00, -7.2153e-01, -5.9561e+00],\n",
       "         [ 1.8414e+00, -7.2452e-01, -5.9615e+00],\n",
       "         [ 1.8391e+00, -7.3031e-01, -5.9275e+00],\n",
       "         [ 1.8439e+00, -7.2216e-01, -5.9650e+00],\n",
       "         [ 1.8422e+00, -7.2309e-01, -5.9588e+00],\n",
       "         [ 2.2801e+00, -5.0409e-01, -6.7280e+00],\n",
       "         [ 2.2831e+00, -5.0631e-01, -6.7255e+00],\n",
       "         [ 2.2863e+00, -5.1013e-01, -6.7204e+00],\n",
       "         [ 2.2962e+00, -5.1542e-01, -6.7269e+00],\n",
       "         [ 2.2782e+00, -5.0513e-01, -6.7201e+00],\n",
       "         [ 2.2724e+00, -5.0347e-01, -6.7066e+00],\n",
       "         [ 3.0910e+00, -1.8760e-01, -1.0357e+01],\n",
       "         [ 3.0841e+00, -1.5963e-01, -1.0392e+01],\n",
       "         [ 3.0833e+00, -1.7559e-01, -1.0355e+01],\n",
       "         [ 1.6310e+00, -4.9816e-04, -6.6437e+00],\n",
       "         [ 1.7630e+00, -9.2236e-02, -8.1534e+00],\n",
       "         [ 1.2392e+00, -9.6903e-01, -4.3181e+00],\n",
       "         [ 1.2386e+00, -9.7438e-01, -4.2953e+00],\n",
       "         [ 1.2923e+00,  9.6136e-01, -8.7077e+00],\n",
       "         [ 1.2872e+00,  9.4349e-01, -8.6121e+00],\n",
       "         [ 1.2961e+00,  9.2941e-01, -8.6266e+00],\n",
       "         [ 1.4776e+00, -2.4204e+00, -1.0945e+00],\n",
       "         [ 2.1005e+00, -5.5087e-01, -7.5359e+00],\n",
       "         [ 1.4402e+00, -1.5619e+00, -2.9975e+00],\n",
       "         [ 1.4423e+00, -1.5141e+00, -3.0671e+00],\n",
       "         [ 1.2830e+00, -2.6206e-01, -5.2506e+00],\n",
       "         [ 1.4879e+00, -2.7352e+00, -1.3244e+00],\n",
       "         [ 2.5455e+00, -8.9785e-02, -8.7593e+00],\n",
       "         [ 2.3013e+00, -1.4377e+00, -4.8938e+00],\n",
       "         [ 2.3047e+00, -1.4275e+00, -4.9213e+00],\n",
       "         [ 1.9448e+00, -5.1057e-01, -6.5504e+00],\n",
       "         [ 1.2179e+00, -1.0876e+00, -4.5301e+00],\n",
       "         [ 2.1186e+00, -9.1926e-01, -6.3574e+00],\n",
       "         [ 3.0311e+00, -2.1556e+00, -6.9491e+00],\n",
       "         [ 1.2329e+00, -1.2412e+00, -4.0431e+00],\n",
       "         [ 2.2848e+00, -2.3464e+00, -4.6557e+00],\n",
       "         [ 4.1383e+00, -1.2915e+00, -1.0002e+01],\n",
       "         [ 1.5581e+00, -1.2683e+00, -5.1134e+00],\n",
       "         [ 1.7716e+00, -7.2136e-01, -6.6542e+00],\n",
       "         [ 4.1564e+00, -1.0439e+00, -9.9105e+00],\n",
       "         [ 2.8882e+00, -9.8401e-01, -7.0807e+00],\n",
       "         [ 1.2375e+00, -9.5284e-01, -4.3386e+00],\n",
       "         [ 3.0343e+00, -2.0022e+00, -6.6871e+00],\n",
       "         [ 2.4609e+00, -2.8729e+00, -3.6557e+00],\n",
       "         [ 2.0234e+00, -6.3674e-01, -5.9389e+00],\n",
       "         [ 4.1656e+00, -1.0509e+00, -9.9331e+00],\n",
       "         [ 1.7954e+00, -4.4155e+00,  3.0989e-02],\n",
       "         [ 1.2283e+00, -9.1491e-01, -4.3905e+00],\n",
       "         [ 1.9224e+00, -3.6603e-01, -7.4701e+00],\n",
       "         [ 2.3096e+00, -1.9866e+00, -4.1070e+00],\n",
       "         [ 1.8921e+00, -9.2914e-01, -4.1938e+00],\n",
       "         [ 2.4535e+00, -5.5022e-01, -8.6592e+00],\n",
       "         [ 4.0145e+00, -7.2595e-01, -1.1231e+01],\n",
       "         [ 1.5814e+00, -1.4411e+00, -3.9287e+00],\n",
       "         [ 2.2403e+00,  7.7661e-02, -9.7285e+00],\n",
       "         [ 5.2026e+00, -1.2633e+00, -1.3759e+01]]),\n",
       " tensor([[ 1.8551e+00, -6.3504e-01, -5.0947e+00],\n",
       "         [ 4.1751e+00, -1.1715e+00, -1.0841e+01],\n",
       "         [ 1.5152e+00, -1.8227e+00, -3.6529e+00],\n",
       "         [ 1.7683e+00, -1.6951e+00, -5.8489e+00],\n",
       "         [ 2.0955e+00, -5.6957e-01, -6.3639e+00],\n",
       "         [ 2.2340e+00,  4.5260e-02, -9.5951e+00],\n",
       "         [ 2.3940e+00, -8.6355e-01, -5.9664e+00],\n",
       "         [ 1.9070e+00, -4.7065e-01, -5.6883e+00],\n",
       "         [ 1.6411e+00, -1.5058e-02, -8.5476e+00],\n",
       "         [ 1.9603e+00, -4.2542e-01, -6.6751e+00],\n",
       "         [ 3.0101e+00, -8.5609e-02, -1.0108e+01],\n",
       "         [ 2.2181e+00,  6.9929e-02, -9.6534e+00],\n",
       "         [ 1.5694e+00, -1.4803e+00, -4.5918e+00],\n",
       "         [ 1.7310e+00, -2.6875e-01, -7.2373e+00],\n",
       "         [ 2.5283e+00, -6.9606e-02, -8.7565e+00],\n",
       "         [ 1.6911e+00, -2.3885e+00, -2.3738e+00],\n",
       "         [ 2.1205e+00, -3.6400e-01, -7.2216e+00],\n",
       "         [ 2.2839e+00, -2.3149e+00, -4.7038e+00],\n",
       "         [ 2.1209e+00, -9.6773e-01, -4.9742e+00],\n",
       "         [ 6.9265e+00, -7.0635e-01, -1.9061e+01],\n",
       "         [ 2.0083e+00, -1.0259e+00, -5.6956e+00],\n",
       "         [ 1.4719e+00, -1.7607e+00, -3.2412e+00],\n",
       "         [ 1.4823e+00, -2.0718e-01, -6.4150e+00],\n",
       "         [ 1.6569e+00, -1.6132e-01, -7.6012e+00],\n",
       "         [ 3.8066e+00, -4.1638e-01, -1.1739e+01],\n",
       "         [ 1.7001e+00, -2.3821e+00, -2.4161e+00],\n",
       "         [ 5.2000e+00, -1.2728e+00, -1.3696e+01],\n",
       "         [ 2.4958e+00, -6.0202e-01, -6.7379e+00],\n",
       "         [ 2.9979e+00, -6.1988e-02, -1.0152e+01],\n",
       "         [ 2.1289e+00, -9.6786e-01, -4.9933e+00],\n",
       "         [ 1.8969e+00, -8.5525e-01, -5.6004e+00],\n",
       "         [ 1.2208e+00, -1.0726e+00, -4.5644e+00],\n",
       "         [ 1.0467e+00, -1.7989e+00, -1.9818e+00],\n",
       "         [ 1.6902e+00, -2.3943e+00, -2.3641e+00],\n",
       "         [ 2.9675e+00, -2.9826e-02, -1.0172e+01],\n",
       "         [ 1.7633e+00, -4.3584e-01, -6.5558e+00],\n",
       "         [ 2.0100e+00, -1.0053e+00, -5.7315e+00],\n",
       "         [ 1.9695e+00, -2.0609e+00, -3.4779e+00],\n",
       "         [ 1.5692e+00,  1.6432e-01, -8.9434e+00],\n",
       "         [ 1.4600e+00, -1.7938e-01, -5.5157e+00],\n",
       "         [ 5.2057e+00, -1.2658e+00, -1.3764e+01],\n",
       "         [ 1.7240e+00, -2.8966e-01, -7.1323e+00],\n",
       "         [ 2.1065e+00,  5.0446e-02, -9.9003e+00],\n",
       "         [ 3.8444e+00, -1.1975e+00, -9.4525e+00],\n",
       "         [ 1.9012e+00, -8.5032e-01, -5.6199e+00],\n",
       "         [ 2.5757e+00, -3.1360e+00, -3.4926e+00],\n",
       "         [ 1.6315e+00, -2.4519e+00, -2.2835e+00],\n",
       "         [ 1.8761e+00, -6.5948e-01, -5.0689e+00],\n",
       "         [ 1.3117e+00,  6.1326e-01, -5.2981e+00],\n",
       "         [ 2.5298e+00, -3.7900e-01, -8.4920e+00],\n",
       "         [ 2.0643e+00, -2.2770e+00, -3.2650e+00],\n",
       "         [ 2.5703e+00, -1.8177e+00, -7.1823e+00],\n",
       "         [ 1.9632e+00, -2.0567e+00, -3.4446e+00],\n",
       "         [ 2.2710e+00, -2.0770e+00, -4.1379e+00],\n",
       "         [ 1.6610e+00, -3.2595e-01, -6.4204e+00],\n",
       "         [ 4.1386e+00, -1.0360e+00, -9.8772e+00],\n",
       "         [ 2.4650e+00, -2.8772e+00, -3.6614e+00],\n",
       "         [ 2.1985e+00, -2.2930e+00, -4.0387e+00],\n",
       "         [ 2.0871e+00, -1.2754e+00, -5.1151e+00],\n",
       "         [ 1.9079e+00, -8.4810e-01, -6.5559e+00],\n",
       "         [ 2.0885e+00, -1.2562e+00, -5.1203e+00],\n",
       "         [ 2.2868e+00, -5.1183e-01, -6.7173e+00],\n",
       "         [ 2.2469e+00,  5.1600e-01, -1.1063e+01],\n",
       "         [ 9.3254e-01,  5.4669e-01, -7.7712e+00]]),\n",
       " tensor([[  2.1072,  -0.6104,  -7.3810],\n",
       "         [  1.8844,  -0.8336,  -5.5816],\n",
       "         [  1.7757,  -1.6554,  -5.9371],\n",
       "         [  1.6223,  -2.1942,  -2.7213],\n",
       "         [  1.6498,  -0.0362,  -8.4916],\n",
       "         [  1.4836,  -0.1133,  -6.3372],\n",
       "         [  1.4004,   0.0478,  -6.6768],\n",
       "         [  3.1019,  -0.2841, -10.2645],\n",
       "         [  1.2957,  -1.8764,  -2.8507],\n",
       "         [  2.1866,  -2.2768,  -4.0277],\n",
       "         [  1.3323,  -2.3726,  -2.6035],\n",
       "         [  4.0510,  -0.7348, -11.3069],\n",
       "         [  5.2233,  -1.2759, -13.7744],\n",
       "         [  2.7637,  -2.0901,  -4.2002],\n",
       "         [  2.1279,  -2.1567,  -4.2749],\n",
       "         [  2.4598,  -0.5846,  -8.5915],\n",
       "         [  3.0076,  -0.0686, -10.1459],\n",
       "         [  2.1575,  -2.1301,  -4.4596],\n",
       "         [  2.3054,  -0.0274,  -9.1884],\n",
       "         [  1.6836,   0.4462,  -8.5720],\n",
       "         [  1.6557,  -0.0383,  -8.5157],\n",
       "         [  1.5053,   0.4195,  -8.3322],\n",
       "         [  1.8603,  -2.2056,  -3.4579],\n",
       "         [  1.6264,   0.1074,  -8.0976],\n",
       "         [  1.4830,   0.4763,  -8.5192],\n",
       "         [  1.5841,  -0.7490,  -6.1844],\n",
       "         [  2.1108,  -0.6143,  -7.3689],\n",
       "         [  1.4841,   0.4978,  -8.5959],\n",
       "         [  1.4867,   0.4908,  -8.5795],\n",
       "         [  1.4782,   0.4908,  -8.5746],\n",
       "         [  1.7351,  -3.6405,  -0.6284],\n",
       "         [  1.8096,  -3.8776,  -0.7251],\n",
       "         [  2.1250,  -1.6911,  -4.9844],\n",
       "         [  1.2284,  -0.9335,  -4.3561],\n",
       "         [  1.7325,  -3.6342,  -0.6332],\n",
       "         [  1.5733,  -0.7116,  -6.1855],\n",
       "         [  1.4409,  -3.4359,  -2.4960],\n",
       "         [  1.4858,   0.4869,  -8.5650],\n",
       "         [  1.7041,  -2.3789,  -2.4224],\n",
       "         [  1.7309,  -3.6479,  -0.6173],\n",
       "         [  1.8307,  -3.2750,  -1.8866],\n",
       "         [  3.0107,  -0.9668,  -7.4343],\n",
       "         [  0.1803,  -3.6929,   0.5952],\n",
       "         [  2.0146,  -4.3207,  -0.6673],\n",
       "         [  1.4798,   0.4897,  -8.5498],\n",
       "         [  1.8622,  -1.8553,  -4.1591],\n",
       "         [  1.4806,   0.4928,  -8.5641],\n",
       "         [  1.4731,   0.5186,  -8.6432],\n",
       "         [  1.8211,  -3.8782,  -0.7479],\n",
       "         [  2.0059,  -4.3558,  -0.5878],\n",
       "         [  1.6889,  -1.5892,  -5.0087],\n",
       "         [  1.1806,   0.8181, -12.4946],\n",
       "         [  2.3223,  -0.0403,  -9.1779],\n",
       "         [  1.7352,  -3.6427,  -0.6347],\n",
       "         [  2.0235,  -4.3814,  -0.5991],\n",
       "         [  1.5003,  -5.7538,   1.5570],\n",
       "         [  2.1126,  -0.6633,  -7.2739],\n",
       "         [  2.2743,  -2.3330,  -4.6443],\n",
       "         [  1.9979,  -4.3142,  -0.6522],\n",
       "         [  1.4811,   0.4835,  -8.5243],\n",
       "         [  1.5670,  -0.6437,  -6.2763],\n",
       "         [  1.8273,  -3.8934,  -0.7193],\n",
       "         [  2.2788,  -2.2845,  -4.7366],\n",
       "         [  0.5844,   0.9501,  -7.5293]]),\n",
       " tensor([[ 1.0505, -2.4486, -1.3322],\n",
       "         [ 1.4942,  0.4477, -8.4111],\n",
       "         [ 1.6934, -2.4136, -2.3439],\n",
       "         [ 1.5939, -0.6485, -4.8742],\n",
       "         [ 1.9876, -0.4196, -7.3163],\n",
       "         [ 1.4779,  0.4925, -8.5557],\n",
       "         [ 2.1035, -0.6243, -7.3292],\n",
       "         [ 2.0953, -2.0468, -4.5132],\n",
       "         [ 1.8504, -3.9861, -0.6187],\n",
       "         [ 1.6266, -2.4796, -3.0402],\n",
       "         [ 0.1798, -3.6695,  0.5632],\n",
       "         [ 2.0114, -4.3824, -0.5646],\n",
       "         [ 1.8165, -3.8801, -0.7346],\n",
       "         [ 0.1935, -3.7076,  0.5907],\n",
       "         [ 1.7335, -3.6425, -0.6272],\n",
       "         [ 2.2824, -2.3209, -4.6887],\n",
       "         [ 1.5069,  0.4288, -8.3748],\n",
       "         [ 1.7306, -3.6409, -0.6185],\n",
       "         [ 1.6259,  0.0876, -8.0066],\n",
       "         [ 1.8177, -3.8786, -0.7411],\n",
       "         [ 1.2644, -0.1710, -5.9260],\n",
       "         [ 1.5321, -4.0677, -0.3366],\n",
       "         [ 0.1787, -3.6350,  0.5030],\n",
       "         [ 1.6296, -2.4802, -3.0593],\n",
       "         [ 2.0109, -4.3452, -0.6200],\n",
       "         [ 3.0273, -2.1679, -6.9198],\n",
       "         [ 1.4847,  0.4776, -8.5298],\n",
       "         [ 1.2222, -0.8110, -4.5726],\n",
       "         [ 2.2854, -2.3241, -4.6931],\n",
       "         [ 2.1048, -0.6253, -7.3514],\n",
       "         [ 1.0517, -2.4615, -1.3255],\n",
       "         [ 1.4923,  0.4736, -8.5241],\n",
       "         [ 1.0512, -2.4676, -1.3162],\n",
       "         [ 1.9474, -2.4120, -2.8352],\n",
       "         [ 2.0361, -4.3851, -0.6172],\n",
       "         [ 1.4352, -0.2701, -6.2417],\n",
       "         [ 1.4802,  0.4951, -8.5862],\n",
       "         [ 2.2802, -2.3141, -4.6931],\n",
       "         [ 1.7044, -2.3818, -2.4189],\n",
       "         [ 1.7643, -1.7066, -4.0531],\n",
       "         [ 1.0457, -2.4292, -1.3334],\n",
       "         [ 1.5822, -0.7047, -6.2486],\n",
       "         [ 1.2833, -0.2522, -5.3000],\n",
       "         [ 0.1764, -3.6489,  0.5237],\n",
       "         [ 1.4858,  0.4893, -8.5675],\n",
       "         [ 2.0080, -4.3411, -0.6230],\n",
       "         [ 1.4767,  0.5057, -8.6164],\n",
       "         [ 2.2833, -2.2929, -4.7404],\n",
       "         [ 2.1272, -1.5662, -5.6129],\n",
       "         [ 2.1039, -0.6116, -7.3746],\n",
       "         [ 2.0036, -4.3199, -0.6362],\n",
       "         [ 1.9837, -4.3011, -0.6147],\n",
       "         [ 1.8235, -3.8874, -0.7322],\n",
       "         [ 2.1041, -0.6102, -7.3726],\n",
       "         [ 1.0481, -2.4502, -1.3281],\n",
       "         [ 1.4848,  0.5141, -8.6827],\n",
       "         [ 2.2820, -2.3051, -4.7123],\n",
       "         [ 1.9669, -0.4016, -7.3082],\n",
       "         [ 1.2675, -0.1736, -5.9323],\n",
       "         [ 2.0203, -4.3546, -0.6357],\n",
       "         [ 1.7321,  0.3842, -9.9677],\n",
       "         [ 1.5016, -5.7532,  1.5552],\n",
       "         [ 3.2240, -0.7299, -9.7124],\n",
       "         [ 1.2047,  0.3872, -7.9894]]),\n",
       " tensor([[  1.1094,   0.5172,  -8.4051],\n",
       "         [  1.2650,  -0.1730,  -5.9201],\n",
       "         [  2.7888,  -0.2245, -11.5050],\n",
       "         [  1.7410,  -0.8001,  -6.8542],\n",
       "         [  3.2372,  -1.2273,  -7.9561],\n",
       "         [  1.4817,   0.4904,  -8.5668],\n",
       "         [  1.2064,  -1.0521,  -4.5794],\n",
       "         [  2.1118,  -0.6092,  -7.3455],\n",
       "         [  1.4846,   0.5173,  -8.6966],\n",
       "         [  1.6997,  -2.3681,  -2.4228],\n",
       "         [  2.6352,  -2.3258,  -5.0342],\n",
       "         [  1.4793,   0.5070,  -8.6364],\n",
       "         [  1.4887,   0.4802,  -8.5453],\n",
       "         [  1.4836,   0.4907,  -8.5780],\n",
       "         [  0.8285,  -0.1726,  -3.0603],\n",
       "         [  1.6946,  -2.4209,  -2.3373],\n",
       "         [  2.1801,  -2.0346,  -4.1398],\n",
       "         [  1.6400,  -0.0647,  -8.3199],\n",
       "         [  1.7118,   0.3871,  -9.9108],\n",
       "         [  1.4052,  -0.4344,  -5.9516],\n",
       "         [  1.7322,  -3.6463,  -0.6115],\n",
       "         [  1.7316,  -3.6384,  -0.6271],\n",
       "         [  1.0495,  -2.4527,  -1.3284],\n",
       "         [  1.7406,   0.3605,  -9.9042],\n",
       "         [  0.8217,   0.7464,  -7.6493],\n",
       "         [  3.8606,  -0.8535,  -9.6024],\n",
       "         [  2.0086,  -4.3260,  -0.6515],\n",
       "         [  1.0955,   0.5383,  -8.4150],\n",
       "         [  0.5846,  -3.3883,   1.3196],\n",
       "         [  1.5802,  -1.4546,  -3.9009],\n",
       "         [  1.8943,  -0.8598,  -5.5680],\n",
       "         [  2.1112,  -0.6601,  -7.2873],\n",
       "         [  1.8983,  -0.8651,  -5.5884],\n",
       "         [  2.2845,  -0.5085,  -6.7308],\n",
       "         [  2.5252,  -0.3775,  -8.4599],\n",
       "         [  3.1719,  -0.2841, -10.2323],\n",
       "         [  2.2786,  -0.5022,  -6.7310],\n",
       "         [  1.5813,  -1.4740,  -3.8868],\n",
       "         [  1.9928,  -2.0727,  -4.9081],\n",
       "         [  1.5660,  -0.6936,  -6.1716],\n",
       "         [  2.6590,  -0.8388,  -7.1826],\n",
       "         [  2.0173,  -4.3365,  -0.6495],\n",
       "         [  1.7335,  -3.6509,  -0.6096],\n",
       "         [  0.9008,  -0.0578,  -5.6857],\n",
       "         [  1.8648,  -1.8424,  -4.1874],\n",
       "         [  1.2767,  -0.2655,  -5.2131],\n",
       "         [  1.6569,  -0.1613,  -7.6012],\n",
       "         [  2.0228,  -4.3506,  -0.6437],\n",
       "         [  1.8135,  -3.8827,  -0.7124],\n",
       "         [  1.8526,  -3.9769,  -0.7789],\n",
       "         [  1.5666,  -1.4140,  -3.9236],\n",
       "         [  1.6479,  -0.0153,  -8.5812],\n",
       "         [  2.6245,  -2.3028,  -5.0554],\n",
       "         [  1.6543,  -0.0419,  -8.4892],\n",
       "         [  1.6509,  -1.0902,  -4.8599],\n",
       "         [  1.1454,   0.8491, -12.4652],\n",
       "         [  2.2819,  -0.5155,  -6.6724],\n",
       "         [  1.9100,  -0.4766,  -5.6766],\n",
       "         [  1.2125,  -1.0878,  -4.5262],\n",
       "         [  1.7457,   0.3373,  -9.8123],\n",
       "         [  1.8411,  -3.9843,  -0.7397],\n",
       "         [  1.7358,  -3.6443,  -0.6303],\n",
       "         [  1.5738,  -0.7166,  -6.1827],\n",
       "         [  1.5718,  -0.5953,  -6.3843]]),\n",
       " tensor([[  1.5800,  -0.7133,  -6.2245],\n",
       "         [  1.5765,  -0.6759,  -6.2769],\n",
       "         [  1.6914,   0.4712,  -8.6224],\n",
       "         [  2.0217,  -4.3566,  -0.6338],\n",
       "         [  3.1955,  -0.7292,  -9.6288],\n",
       "         [  1.8256,  -3.8690,  -0.7716],\n",
       "         [  0.3637,   0.9307,  -6.3212],\n",
       "         [  0.9374,  -2.2282,  -0.9910],\n",
       "         [  1.8349,  -0.7034,  -5.9494],\n",
       "         [  1.8961,  -0.4543,  -5.7014],\n",
       "         [  1.8692,  -0.1386,  -7.5158],\n",
       "         [  1.6220,   0.0918,  -8.0049],\n",
       "         [  1.7327,  -3.6496,  -0.5979],\n",
       "         [  4.1282,  -1.4575, -10.8735],\n",
       "         [  1.4961,  -1.9138,  -3.4223],\n",
       "         [  2.0232,  -4.3619,  -0.6280],\n",
       "         [  2.0270,  -4.3592,  -0.6349],\n",
       "         [  1.4904,  -5.7557,   1.5703],\n",
       "         [  1.8424,  -0.7358,  -5.9436],\n",
       "         [  0.1547,  -3.6929,   0.6307],\n",
       "         [  1.9894,  -2.1163,  -4.8317],\n",
       "         [  2.1215,  -1.7128,  -4.9439],\n",
       "         [  2.2680,  -2.3378,  -4.6146],\n",
       "         [  2.4598,  -2.8534,  -3.6779],\n",
       "         [  4.1598,  -1.4796, -10.9191],\n",
       "         [  3.1031,  -0.2687, -10.3235],\n",
       "         [  4.1399,  -1.4668, -10.8921],\n",
       "         [  1.0523,  -0.8321,  -3.2451],\n",
       "         [  1.6421,  -0.0424,  -8.4155],\n",
       "         [  0.9763,  -0.0752,  -4.4995],\n",
       "         [  1.3186,   0.9313,  -8.7235],\n",
       "         [  2.7330,  -1.5205,  -5.5307],\n",
       "         [  1.4781,   0.4927,  -8.5593],\n",
       "         [  3.0989,  -0.3249, -10.0061],\n",
       "         [  2.0055,  -4.3414,  -0.6137],\n",
       "         [  1.4404,  -0.3889,  -6.1227],\n",
       "         [  1.4776,   0.5125,  -8.6533],\n",
       "         [  1.9807,  -0.4190,  -7.2928],\n",
       "         [  2.1336,  -1.5884,  -5.6385],\n",
       "         [  3.1991,  -0.7223,  -9.6678],\n",
       "         [  1.7358,  -3.6472,  -0.6241],\n",
       "         [  2.0272,  -0.2589,  -7.9657],\n",
       "         [  2.0068,  -4.3427,  -0.6145],\n",
       "         [  1.3925,  -0.2399,  -5.5791],\n",
       "         [  1.9934,  -4.3822,  -0.5417],\n",
       "         [  1.7068,   0.3839,  -9.8833],\n",
       "         [  1.3826,  -0.2450,  -5.8276],\n",
       "         [  1.7433,  -2.3876,  -2.5632],\n",
       "         [  3.0733,  -0.1566, -10.3736],\n",
       "         [  1.2577,  -0.1654,  -5.9008],\n",
       "         [  1.8976,  -0.8532,  -5.5953],\n",
       "         [  1.7347,  -3.6378,  -0.6395],\n",
       "         [  1.7324,  -3.6491,  -0.6182],\n",
       "         [  1.7308,  -3.6408,  -0.6213],\n",
       "         [  1.8975,  -0.3389,  -6.4704],\n",
       "         [  2.1126,  -0.6560,  -7.3118],\n",
       "         [  5.2090,  -1.2635, -13.8406],\n",
       "         [  2.6311,  -2.2949,  -5.0792],\n",
       "         [  1.4290,  -1.5198,  -3.0063],\n",
       "         [  0.9728,   1.1720, -10.8666],\n",
       "         [  3.1038,  -0.2760, -10.3008],\n",
       "         [  1.4260,  -0.3720,  -6.1369],\n",
       "         [  1.4794,   0.5062,  -8.6301],\n",
       "         [  2.1184,  -1.6501,  -5.4546]]),\n",
       " tensor([[  1.4805,   0.4951,  -8.5735],\n",
       "         [  1.9963,  -4.3620,  -0.5681],\n",
       "         [  1.6423,  -0.0357,  -8.4679],\n",
       "         [  2.1069,  -0.6304,  -7.3332],\n",
       "         [  0.8263,  -0.1702,  -3.0695],\n",
       "         [  3.0856,  -0.1786, -10.3367],\n",
       "         [  1.5890,  -0.6461,  -6.3918],\n",
       "         [  1.8231,  -1.2918,  -5.6018],\n",
       "         [  2.3573,  -1.1034,  -5.4673],\n",
       "         [  1.7282,  -1.2987,  -5.8366],\n",
       "         [  2.1012,  -2.0358,  -4.5412],\n",
       "         [  2.8692,  -0.3420,  -9.0594],\n",
       "         [  2.1083,  -0.6198,  -7.3517],\n",
       "         [  3.1462,  -0.5657, -10.0480],\n",
       "         [  1.8204,   0.3515,  -8.5719],\n",
       "         [  1.4805,   0.4930,  -8.5671],\n",
       "         [  2.4617,  -2.8801,  -3.6511],\n",
       "         [  1.5783,  -1.4092,  -3.9564],\n",
       "         [  1.8208,  -1.2986,  -5.5894],\n",
       "         [  1.6291,   0.0713,  -7.9518],\n",
       "         [  1.4819,   0.5362,  -8.7630],\n",
       "         [  1.8962,  -0.8524,  -5.5910],\n",
       "         [  2.2780,  -2.3194,  -4.6795],\n",
       "         [  1.5850,  -1.2241,  -5.0461],\n",
       "         [  2.4076,  -0.1468,  -9.7777],\n",
       "         [  1.8227,  -1.2784,  -5.6219],\n",
       "         [  3.1031,  -0.2741, -10.3038],\n",
       "         [  2.1185,  -1.6890,  -4.9642],\n",
       "         [  0.1842,  -3.6596,   0.5246],\n",
       "         [  1.0690,   0.5777,  -8.4821],\n",
       "         [  2.0243,  -4.3773,  -0.5995],\n",
       "         [  1.4845,   0.4777,  -8.5149],\n",
       "         [  2.1102,  -0.6503,  -7.3656],\n",
       "         [  1.8244,  -3.8950,  -0.7363],\n",
       "         [  2.3712,  -2.9965,  -4.2827],\n",
       "         [  1.6474,  -0.0208,  -8.5432],\n",
       "         [  1.9331,  -0.3540,  -6.6334],\n",
       "         [  1.4798,   0.4957,  -8.5794],\n",
       "         [  3.1030,  -0.1859, -10.4048],\n",
       "         [  2.0028,  -4.3569,  -0.5762],\n",
       "         [  1.8140,  -3.8393,  -0.7816],\n",
       "         [  1.8122,  -3.8272,  -0.8099],\n",
       "         [ -0.2405,  -1.4226,  -5.8127],\n",
       "         [  1.4729,   0.5084,  -8.6095],\n",
       "         [  1.4838,   0.4665,  -8.4587],\n",
       "         [  0.1855,  -3.6507,   0.5018],\n",
       "         [  2.5815,  -3.1303,  -3.5107],\n",
       "         [  2.2856,  -0.5087,  -6.7199],\n",
       "         [  2.0658,   0.0412,  -9.7435],\n",
       "         [  3.0706,  -0.1569, -10.3324],\n",
       "         [  1.4811,   0.4930,  -8.5763],\n",
       "         [  1.7335,  -3.6429,  -0.6282],\n",
       "         [  1.5356,  -0.8440,  -4.9698],\n",
       "         [  1.4392,  -1.5457,  -3.0188],\n",
       "         [  2.1052,  -0.6232,  -7.3672],\n",
       "         [  1.4458,  -0.5393,  -5.8289],\n",
       "         [  1.6372,  -0.4694,  -5.5295],\n",
       "         [  1.7388,  -3.6518,  -0.6185],\n",
       "         [  1.9849,  -0.4156,  -7.3201],\n",
       "         [  1.8390,  -0.7370,  -5.9275],\n",
       "         [  1.7158,   0.3785,  -9.8928],\n",
       "         [  1.1018,   0.5537,  -8.5522],\n",
       "         [  2.2853,  -0.5017,  -6.7533],\n",
       "         [  2.2699,  -2.3251,  -4.6461]]),\n",
       " tensor([[  1.5929,  -0.1935,  -6.0544],\n",
       "         [  2.1160,  -0.6505,  -7.3255],\n",
       "         [  2.1101,  -0.6472,  -7.3194],\n",
       "         [  1.4762,  -2.3780,  -1.1670],\n",
       "         [  1.6501,  -0.0358,  -8.4950],\n",
       "         [  2.1039,  -0.6001,  -7.3918],\n",
       "         [  2.0696,  -0.5217,  -6.9236],\n",
       "         [  2.1056,  -0.6063,  -7.3733],\n",
       "         [  1.2019,  -1.0242,  -4.5893],\n",
       "         [  1.6516,  -0.0418,  -8.4725],\n",
       "         [  1.4782,   0.4976,  -8.5823],\n",
       "         [  2.2845,  -2.3249,  -4.6921],\n",
       "         [  2.7244,  -1.5009,  -5.5333],\n",
       "         [  0.1894,  -3.6658,   0.5257],\n",
       "         [  0.1851,  -3.6330,   0.4822],\n",
       "         [  1.8242,  -1.2996,  -5.6025],\n",
       "         [  1.5616,  -1.2345,  -5.1947],\n",
       "         [  2.0110,  -4.3693,  -0.5879],\n",
       "         [  2.6371,  -2.3841,  -4.9668],\n",
       "         [  1.4353,  -0.4035,  -6.0626],\n",
       "         [  1.9815,  -2.0916,  -4.8492],\n",
       "         [  1.6517,  -0.0213,  -8.5694],\n",
       "         [  1.8307,  -3.8772,  -0.7662],\n",
       "         [  1.2643,  -0.1741,  -5.9106],\n",
       "         [  1.9873,  -0.4251,  -7.2916],\n",
       "         [  1.1043,   0.5188,  -8.3796],\n",
       "         [  2.1442,  -1.0769,  -5.4033],\n",
       "         [ -2.4784,   0.2332,  -2.8710],\n",
       "         [  1.4760,   0.5241,  -8.6886],\n",
       "         [  1.2575,   0.9667,  -8.5922],\n",
       "         [  1.8902,  -0.8412,  -5.5995],\n",
       "         [  2.3369,  -1.1206,  -5.4139],\n",
       "         [  2.5223,  -0.3587,  -8.5331],\n",
       "         [  2.1904,  -2.0592,  -4.1336],\n",
       "         [  1.7363,  -3.6401,  -0.6420],\n",
       "         [  2.1101,  -0.6302,  -7.4077],\n",
       "         [  1.4823,   0.4910,  -8.5673],\n",
       "         [  1.5150,  -2.0440,  -3.0101],\n",
       "         [  1.7090,  -2.3819,  -2.4340],\n",
       "         [  2.0853,   0.0298,  -9.7747],\n",
       "         [  2.2739,  -2.3002,  -4.6958],\n",
       "         [  1.5799,  -0.6831,  -6.2702],\n",
       "         [  1.7366,  -3.6524,  -0.6278],\n",
       "         [  0.1828,  -3.6461,   0.5048],\n",
       "         [  2.0292,  -4.3606,  -0.6393],\n",
       "         [  1.8445,  -0.7122,  -5.9794],\n",
       "         [  1.4975,  -1.8632,  -3.4924],\n",
       "         [  1.4926,  -0.2407,  -6.3198],\n",
       "         [  2.1861,  -2.0908,  -4.0700],\n",
       "         [  1.7323,  -3.6397,  -0.6180],\n",
       "         [  1.9839,  -2.0494,  -4.9323],\n",
       "         [  1.9857,  -0.4179,  -7.3155],\n",
       "         [  2.2852,  -0.5147,  -6.6868],\n",
       "         [  2.0993,  -0.5663,  -7.5062],\n",
       "         [  1.9850,  -4.3567,  -0.5431],\n",
       "         [  2.2267,  -0.6127,  -6.3090],\n",
       "         [  1.7279,  -3.6280,  -0.6261],\n",
       "         [  1.9861,  -2.0724,  -4.9089],\n",
       "         [  2.1124,  -0.6256,  -7.3343],\n",
       "         [  2.2573,   0.5144, -11.1232],\n",
       "         [  1.7285,  -0.8557,  -4.7946],\n",
       "         [  2.2888,  -0.5068,  -6.7268],\n",
       "         [  2.6590,  -0.8450,  -7.1636],\n",
       "         [  1.4801,   0.4928,  -8.5708]]),\n",
       " tensor([[ 2.2853e+00, -5.1157e-01, -6.7158e+00],\n",
       "         [ 2.0264e+00, -4.3642e+00, -6.3886e-01],\n",
       "         [ 1.4849e+00,  4.8957e-01, -8.5608e+00],\n",
       "         [ 1.3152e+00,  3.5967e-02, -7.3773e+00],\n",
       "         [ 1.7407e+00, -8.2545e-01, -6.8242e+00],\n",
       "         [ 2.0145e+00, -4.3329e+00, -6.4316e-01],\n",
       "         [ 1.6383e+00, -2.4627e-01, -6.0414e+00],\n",
       "         [ 1.8509e+00, -4.0379e+00, -6.5383e-01],\n",
       "         [ 1.6509e+00, -4.0273e-02, -8.4854e+00],\n",
       "         [ 1.6273e+00,  8.4182e-02, -8.0003e+00],\n",
       "         [ 2.0681e+00, -5.3036e-01, -6.8864e+00],\n",
       "         [ 2.2850e+00, -2.3293e+00, -4.6839e+00],\n",
       "         [ 1.9779e+00, -2.1216e-01, -6.4786e+00],\n",
       "         [ 2.2844e+00, -2.3106e+00, -4.7143e+00],\n",
       "         [ 8.1898e-01, -9.7100e-01, -4.1926e+00],\n",
       "         [ 2.4588e+00, -5.6163e-01, -8.6503e+00],\n",
       "         [ 1.7069e+00, -5.4510e-01, -5.1741e+00],\n",
       "         [-2.5493e-01, -1.4273e+00, -5.7393e+00],\n",
       "         [ 2.2126e+00,  1.9402e-01, -9.4703e+00],\n",
       "         [ 1.6742e+00, -1.0796e+00, -4.4050e+00],\n",
       "         [ 1.7022e+00, -2.4085e+00, -2.3680e+00],\n",
       "         [ 9.6110e-01,  5.0835e-01, -7.7147e+00],\n",
       "         [ 1.9836e+00, -4.2070e-01, -7.2970e+00],\n",
       "         [ 2.7320e+00, -1.5184e+00, -5.5268e+00],\n",
       "         [ 1.4759e+00,  4.9383e-01, -8.5429e+00],\n",
       "         [ 2.2846e+00, -2.3046e+00, -4.7245e+00],\n",
       "         [ 2.0155e+00, -4.3569e+00, -6.1476e-01],\n",
       "         [ 1.7685e+00, -1.1633e-01, -7.8314e+00],\n",
       "         [ 1.7373e+00, -8.2544e-01, -6.8227e+00],\n",
       "         [ 1.3808e+00, -3.5504e-01, -4.8375e+00],\n",
       "         [ 4.5660e+00, -9.0762e-01, -1.3033e+01],\n",
       "         [ 1.7316e+00, -3.6260e+00, -6.3747e-01],\n",
       "         [ 2.6529e+00, -8.5780e-01, -7.1099e+00],\n",
       "         [ 2.1087e+00, -6.5632e-01, -7.3041e+00],\n",
       "         [ 1.8966e+00, -8.5527e-01, -5.5992e+00],\n",
       "         [ 1.8131e+00, -3.8971e-01, -7.0726e+00],\n",
       "         [ 1.9893e+00, -4.1276e-01, -7.3475e+00],\n",
       "         [ 2.2848e+00, -2.3128e+00, -4.7149e+00],\n",
       "         [ 1.8404e+00, -7.3586e-01, -5.9321e+00],\n",
       "         [ 2.0991e+00,  4.9710e-03, -9.7105e+00],\n",
       "         [ 2.7088e+00, -2.1018e-01, -1.1373e+01],\n",
       "         [ 1.7264e+00, -3.6345e+00, -6.0750e-01],\n",
       "         [ 3.0980e+00, -2.7471e-01, -1.0281e+01],\n",
       "         [ 2.8715e+00, -3.4342e-01, -9.0599e+00],\n",
       "         [ 1.7543e+00, -1.6912e-01, -7.7331e+00],\n",
       "         [ 1.9205e+00, -3.6918e-01, -7.4544e+00],\n",
       "         [ 1.5376e+00, -1.3255e+00, -3.9582e+00],\n",
       "         [ 1.9851e+00, -4.2197e-01, -7.2987e+00],\n",
       "         [ 1.3952e+00, -2.4038e-01, -5.5816e+00],\n",
       "         [ 1.7317e+00, -3.6340e+00, -6.3706e-01],\n",
       "         [ 1.8772e-01, -3.6764e+00,  5.5704e-01],\n",
       "         [ 3.2120e+00, -7.3528e-01, -9.6626e+00],\n",
       "         [ 1.4082e+00, -1.5644e+00, -2.8580e+00],\n",
       "         [ 1.4847e+00,  5.1474e-01, -8.6866e+00],\n",
       "         [ 2.0009e+00, -4.3600e+00, -5.7231e-01],\n",
       "         [ 1.0428e+00, -2.4368e+00, -1.3320e+00],\n",
       "         [ 1.5809e+00, -2.1360e-01, -6.6937e+00],\n",
       "         [ 1.5503e+00, -8.6976e-01, -5.7994e+00],\n",
       "         [ 1.7943e+00,  3.6865e-01, -8.5550e+00],\n",
       "         [ 1.4979e+00, -2.4327e-01, -6.3372e+00],\n",
       "         [ 3.8574e+00, -8.6084e-01, -9.5652e+00],\n",
       "         [ 1.5304e+00, -1.9594e+00, -3.2818e+00],\n",
       "         [ 1.9829e+00, -4.0230e-01, -7.3661e+00],\n",
       "         [ 1.4864e+00,  4.8209e-01, -8.5417e+00]]),\n",
       " tensor([[  2.0039,  -4.3179,  -0.6300],\n",
       "         [  4.1470,  -1.4599, -10.9078],\n",
       "         [  1.5414,  -3.8911,  -0.5148],\n",
       "         [  1.9617,  -2.0537,  -3.4685],\n",
       "         [  1.8253,  -3.9539,  -0.6218],\n",
       "         [  1.6282,  -2.4786,  -3.0548],\n",
       "         [  1.4741,   0.5112,  -8.6298],\n",
       "         [  1.7273,  -3.6469,  -0.6041],\n",
       "         [  1.9865,  -0.4197,  -7.3060],\n",
       "         [  1.4927,   0.4958,  -8.6072],\n",
       "         [  2.6048,  -1.0933,  -6.3264],\n",
       "         [  1.7354,  -3.6510,  -0.6102],\n",
       "         [  1.7358,  -2.3801,  -2.5396],\n",
       "         [  1.1013,   0.5395,  -8.4616],\n",
       "         [  2.3056,  -1.4216,  -4.9389],\n",
       "         [  1.8645,  -2.4045,  -3.8774],\n",
       "         [  1.6766,  -2.0638,  -2.7354],\n",
       "         [  1.2596,  -0.8675,  -4.0822],\n",
       "         [  1.7543,  -0.7155,  -4.7154],\n",
       "         [  2.0945,  -1.4407,  -4.7500],\n",
       "         [  1.3286,  -0.1408,  -5.6509],\n",
       "         [  2.0522,  -3.2451,  -2.1089],\n",
       "         [  1.6005,   0.0789,  -7.8649],\n",
       "         [  2.3546,  -0.8580,  -7.0013],\n",
       "         [  1.2862,  -1.8803,  -2.8748],\n",
       "         [  1.2824,  -1.8579,  -2.8953],\n",
       "         [  1.2920,  -1.8995,  -2.7963],\n",
       "         [  1.2908,  -1.8381,  -2.8490],\n",
       "         [  1.2826,  -1.8347,  -2.8864],\n",
       "         [  1.2808,  -1.8680,  -2.8646],\n",
       "         [  2.0110,  -4.3758,  -0.5795],\n",
       "         [  2.3181,  -0.0309,  -9.2314],\n",
       "         [  1.8595,  -0.6595,  -5.0188],\n",
       "         [  1.8674,  -0.5371,  -5.6497],\n",
       "         [  2.1972,  -2.2798,  -4.0585],\n",
       "         [  3.0504,  -3.7205,  -3.9050],\n",
       "         [  1.5043,  -2.8657,  -2.2409],\n",
       "         [  1.8978,  -0.3991,  -7.1267],\n",
       "         [  2.7808,  -1.2889,  -6.0604],\n",
       "         [  3.0036,  -0.9686,  -7.3984],\n",
       "         [  1.9219,  -2.6971,  -2.6466],\n",
       "         [  2.4692,  -0.6451,  -6.6055],\n",
       "         [  2.0838,  -1.3415,  -4.9807],\n",
       "         [  2.3075,  -0.0149,  -9.2782],\n",
       "         [  1.6486,  -0.0355,  -8.4929],\n",
       "         [  1.8880,   0.0282,  -7.4371],\n",
       "         [  1.7352,  -0.1121,  -6.8256],\n",
       "         [  1.3789,  -0.3511,  -4.8445],\n",
       "         [  1.9704,  -2.3678,  -3.4411],\n",
       "         [  1.9573,  -2.3012,  -3.4906],\n",
       "         [  1.9693,  -2.3680,  -3.4379],\n",
       "         [  1.9708,  -2.3579,  -3.4580],\n",
       "         [  1.9638,  -2.3491,  -3.4467],\n",
       "         [  1.9818,  -2.3734,  -3.4814],\n",
       "         [  1.9729,  -2.3757,  -3.4431],\n",
       "         [  1.9600,  -2.3234,  -3.4650],\n",
       "         [  1.9638,  -2.3699,  -3.4169],\n",
       "         [  2.2711,  -2.3577,  -4.5947],\n",
       "         [  1.9991,  -4.3970,  -0.5189],\n",
       "         [  1.4817,   0.4949,  -8.5738],\n",
       "         [  1.6849,  -2.2116,  -2.9718],\n",
       "         [  2.2997,  -1.4220,  -4.9252],\n",
       "         [  1.8933,  -0.1571,  -8.0440],\n",
       "         [  2.7514,  -2.0889,  -4.2009]]),\n",
       " tensor([[  2.7601,  -2.0664,  -4.2591],\n",
       "         [  1.7438,   0.3541,  -9.8818],\n",
       "         [  2.4437,  -0.5240,  -8.7023],\n",
       "         [  1.7445,   0.3536,  -9.8838],\n",
       "         [  1.3351,   0.9239,  -8.7901],\n",
       "         [  1.4839,   0.4859,  -8.5449],\n",
       "         [  1.4810,   0.5001,  -8.6050],\n",
       "         [  1.4902,   0.4686,  -8.5013],\n",
       "         [  2.0166,  -4.3488,  -0.6366],\n",
       "         [  2.0140,  -4.3364,  -0.6486],\n",
       "         [  2.0203,  -4.3486,  -0.6414],\n",
       "         [  1.2862,  -3.0695,  -0.5546],\n",
       "         [  2.7341,  -2.6238,  -4.7629],\n",
       "         [  1.7400,   0.3729,  -9.9476],\n",
       "         [  2.3840,   0.0413, -10.7682],\n",
       "         [  1.4979,  -2.8281,  -1.2080],\n",
       "         [  3.8436,  -1.2198,  -9.3839],\n",
       "         [  1.4841,  -2.8295,  -1.1749],\n",
       "         [  3.8465,  -1.2110,  -9.4179],\n",
       "         [  1.4972,  -2.8125,  -1.2282],\n",
       "         [  1.4885,  -2.8629,  -1.1420],\n",
       "         [  1.9674,  -3.2846,  -2.2815],\n",
       "         [  2.5481,  -0.6146,  -6.8730],\n",
       "         [  2.5401,  -0.6208,  -6.8739],\n",
       "         [  2.3881,   0.0307, -10.7431],\n",
       "         [  2.3762,  -3.0070,  -4.2563],\n",
       "         [  2.0559,  -2.6841,  -2.8462],\n",
       "         [  1.4893,  -2.8268,  -1.1909],\n",
       "         [  1.4780,  -2.8015,  -1.2132],\n",
       "         [  1.4873,  -2.8160,  -1.2036],\n",
       "         [  1.4902,  -2.8475,  -1.1658],\n",
       "         [  1.4919,  -2.8359,  -1.1977],\n",
       "         [  1.4892,  -2.8055,  -1.2342],\n",
       "         [  1.4905,  -2.8218,  -1.2040],\n",
       "         [  1.4891,  -2.8112,  -1.2265],\n",
       "         [  2.7431,  -0.8817,  -7.1596],\n",
       "         [  3.8548,  -1.2219,  -9.4105],\n",
       "         [  2.3818,   0.0422, -10.7703],\n",
       "         [  2.3842,   0.0456, -10.7950],\n",
       "         [  1.2057,  -0.3101,  -4.2057],\n",
       "         [  2.4864,  -0.5900,  -6.7227],\n",
       "         [  2.3804,   0.0404, -10.7525],\n",
       "         [  2.3831,   0.0358, -10.7449],\n",
       "         [  1.4930,  -2.8581,  -1.1371],\n",
       "         [  1.4867,  -2.8373,  -1.1826],\n",
       "         [  1.4857,  -2.7846,  -1.2530],\n",
       "         [  2.7406,  -0.9262,  -7.0612],\n",
       "         [  2.3841,   0.0473, -10.7998],\n",
       "         [  2.4537,  -0.5703,  -8.6270],\n",
       "         [  1.4989,  -2.8178,  -1.2373],\n",
       "         [  2.3813,   0.0564, -10.8258],\n",
       "         [  3.8520,  -1.2145,  -9.4248],\n",
       "         [  2.3761,   0.0444, -10.7594],\n",
       "         [  2.7418,  -0.8837,  -7.1418],\n",
       "         [  3.8410,  -1.2198,  -9.3788],\n",
       "         [  1.5002,  -2.8418,  -1.2012],\n",
       "         [  1.4799,  -2.8240,  -1.1838],\n",
       "         [  2.0528,  -2.7040,  -2.8070],\n",
       "         [  1.4888,  -2.7837,  -1.2652],\n",
       "         [  3.8439,  -1.2191,  -9.3901],\n",
       "         [  2.0687,  -1.5272,  -5.9738],\n",
       "         [  1.4856,  -2.8654,  -1.1376],\n",
       "         [  1.4782,  -2.7777,  -1.2477],\n",
       "         [  3.8476,  -1.2047,  -9.4438]]),\n",
       " tensor([[  1.4878,  -2.8348,  -1.1799],\n",
       "         [  2.4355,  -2.9223,  -3.7716],\n",
       "         [  1.2976,  -1.8574,  -2.8932],\n",
       "         [  1.7292,   0.3459,  -9.7883],\n",
       "         [  1.4833,   0.5008,  -8.6096],\n",
       "         [  1.9833,  -0.4229,  -7.2835],\n",
       "         [  1.8892,  -0.8611,  -5.5796],\n",
       "         [  1.3261,   0.9030,  -8.6388],\n",
       "         [  1.5614,  -1.2108,  -5.2271],\n",
       "         [  1.4779,   0.5186,  -8.6697],\n",
       "         [  1.6282,   0.0669,  -7.9237],\n",
       "         [  2.0927,  -2.0296,  -4.5418],\n",
       "         [  2.6650,  -0.8442,  -7.1864],\n",
       "         [  2.0815,  -2.0154,  -4.5451],\n",
       "         [  1.7043,  -0.4567,  -5.4399],\n",
       "         [  0.7879,   0.7699,  -7.5864],\n",
       "         [  2.2198,   0.0804,  -9.6782],\n",
       "         [  1.6506,  -0.0318,  -8.5194],\n",
       "         [  0.7458,   0.8589,  -7.7876],\n",
       "         [  2.7378,  -2.0639,  -4.1913],\n",
       "         [  1.9131,  -1.0676,  -5.9119],\n",
       "         [  1.7342,  -3.6490,  -0.6138],\n",
       "         [  1.1208,   0.6639,  -9.4067],\n",
       "         [  1.5605,  -1.1939,  -5.2675],\n",
       "         [  0.1932,  -3.6943,   0.5811],\n",
       "         [  1.1142,   0.5062,  -8.3610],\n",
       "         [  2.1037,  -0.6181,  -7.4141],\n",
       "         [  2.1098,  -0.6394,  -7.3231],\n",
       "         [  1.7436,   0.3417,  -9.8295],\n",
       "         [  1.6506,  -1.1410,  -4.7900],\n",
       "         [  1.0436,  -0.8473,  -3.1968],\n",
       "         [  2.1123,  -0.6428,  -7.3084],\n",
       "         [  2.0146,  -4.3358,  -0.6501],\n",
       "         [  1.7722,  -0.7224,  -6.6492],\n",
       "         [  1.6515,  -0.0299,  -8.5327],\n",
       "         [  2.4549,  -0.5921,  -8.5794],\n",
       "         [  2.4496,  -0.5987,  -8.5624],\n",
       "         [  1.6549,  -1.1285,  -4.7971],\n",
       "         [  1.4363,  -3.8829,  -0.3158],\n",
       "         [  2.4612,  -0.5815,  -8.6249],\n",
       "         [  1.4737,   0.5132,  -8.6358],\n",
       "         [  2.2846,  -2.3122,  -4.7142],\n",
       "         [  1.6587,  -0.3190,  -6.4404],\n",
       "         [  2.8084,  -0.0742, -11.5492],\n",
       "         [  1.8900,  -1.5542,  -4.0354],\n",
       "         [  2.2016,  -0.0333,  -9.9868],\n",
       "         [  1.3561,   0.0124,  -7.4341],\n",
       "         [  1.7765,  -4.3959,   0.0799],\n",
       "         [  1.9485,   0.5399, -10.5838],\n",
       "         [  1.4736,   0.5090,  -8.6162],\n",
       "         [  1.4605,  -2.3825,  -1.1332],\n",
       "         [  1.8945,  -2.6528,  -3.1710],\n",
       "         [  0.9387,  -2.2285,  -0.9861],\n",
       "         [  3.0116,  -0.9549,  -7.4597],\n",
       "         [  2.2475,   0.0213,  -9.5137],\n",
       "         [  1.8930,  -0.8891,  -5.5397],\n",
       "         [  1.6274,   0.0705,  -7.9464],\n",
       "         [  1.0072,   0.4357,  -7.5761],\n",
       "         [  1.5114,   0.4139,  -8.3229],\n",
       "         [  1.2209,  -1.0590,  -4.5878],\n",
       "         [  2.0930,  -2.0686,  -4.4626],\n",
       "         [  1.1902,  -0.8367,  -4.5089],\n",
       "         [  2.4976,  -0.4175,  -7.6853],\n",
       "         [  3.0206,  -0.0699,  -9.8540]]),\n",
       " tensor([[  1.9482,  -0.5138,  -6.5426],\n",
       "         [  1.9209,  -0.3726,  -7.4444],\n",
       "         [  1.7004,  -2.3686,  -2.4335],\n",
       "         [  1.9810,  -0.4171,  -7.3011],\n",
       "         [  1.7484,   0.3720,  -9.9709],\n",
       "         [  1.3608,  -0.2990,  -4.9856],\n",
       "         [  2.2515,   0.5281, -11.1546],\n",
       "         [  2.7332,  -0.2023, -11.3890],\n",
       "         [  2.0249,  -4.3688,  -0.6209],\n",
       "         [  2.2808,  -2.3201,  -4.6875],\n",
       "         [  2.4566,  -0.5911,  -8.5806],\n",
       "         [  2.1389,  -1.5644,  -5.6866],\n",
       "         [  1.4944,   0.4657,  -8.4925],\n",
       "         [  1.8944,  -0.8562,  -5.5910],\n",
       "         [  3.0768,  -0.3069, -10.0122],\n",
       "         [  1.6071,  -0.2818,  -6.5242],\n",
       "         [  2.4370,  -0.5246,  -8.6894],\n",
       "         [  1.4792,   0.5077,  -8.6377],\n",
       "         [  2.4598,  -0.5945,  -8.5982],\n",
       "         [  1.4877,   0.4702,  -8.4975],\n",
       "         [  2.2925,  -2.3350,  -4.6912],\n",
       "         [  2.2766,  -0.5011,  -6.7242],\n",
       "         [  1.6610,  -0.3260,  -6.4204],\n",
       "         [  1.5589,  -1.3883,  -3.9363],\n",
       "         [  3.0979,  -0.3291,  -9.9922],\n",
       "         [  2.2262,   0.0517,  -9.5897],\n",
       "         [  2.8401,  -0.2461, -11.7364],\n",
       "         [  2.4602,  -0.6023,  -8.5629],\n",
       "         [  1.4797,   0.4931,  -8.5747],\n",
       "         [  1.7291,  -3.6312,  -0.6230],\n",
       "         [  1.5804,  -0.2150,  -6.6792],\n",
       "         [  2.1095,  -0.6244,  -7.3292],\n",
       "         [  3.0158,  -2.1553,  -6.9015],\n",
       "         [  3.0754,  -0.3098,  -9.9658],\n",
       "         [  1.8221,  -3.8554,  -0.7785],\n",
       "         [  2.4668,  -0.5757,  -8.6186],\n",
       "         [  3.1035,  -0.3549,  -9.9410],\n",
       "         [  2.0367,  -4.3692,  -0.6378],\n",
       "         [  2.4607,  -0.5619,  -8.6491],\n",
       "         [  1.8907,  -2.6124,  -3.2273],\n",
       "         [  2.2802,  -2.3287,  -4.6706],\n",
       "         [  1.7083,  -1.3182,  -4.5510],\n",
       "         [  2.0049,  -4.3222,  -0.6387],\n",
       "         [ -0.1903,  -1.5675,  -5.6900],\n",
       "         [  3.1009,  -0.2947, -10.1375],\n",
       "         [  2.0035,  -4.3314,  -0.6281],\n",
       "         [  1.4953,   0.4680,  -8.5108],\n",
       "         [  2.4603,  -0.5597,  -8.6587],\n",
       "         [  1.5727,  -1.4134,  -3.9341],\n",
       "         [  1.0821,   0.5594,  -8.4529],\n",
       "         [  1.5792,  -0.2146,  -6.6647],\n",
       "         [  1.4771,  -2.3502,  -1.2340],\n",
       "         [  2.1099,  -0.6206,  -7.3658],\n",
       "         [  2.2775,  -2.3058,  -4.6974],\n",
       "         [  1.9028,  -0.8581,  -5.6188],\n",
       "         [  3.0993,  -0.3019, -10.0715],\n",
       "         [  1.7616,  -1.3655,  -4.7557],\n",
       "         [  1.4811,   0.4721,  -8.4929],\n",
       "         [  1.7974,  -4.4076,   0.0262],\n",
       "         [  1.0390,  -2.3767,  -1.3756],\n",
       "         [  2.4508,  -0.5627,  -8.6115],\n",
       "         [  1.2781,  -0.2486,  -5.2934],\n",
       "         [  1.4824,   0.4807,  -8.5188],\n",
       "         [  2.2853,  -2.3074,  -4.7211]]),\n",
       " tensor([[ 1.8283e+00, -3.9430e+00, -6.3323e-01],\n",
       "         [ 1.4773e+00,  5.1290e-01, -8.6518e+00],\n",
       "         [ 2.3247e+00, -7.3234e-01, -6.3058e+00],\n",
       "         [ 1.9992e+00, -2.1292e-01, -6.5481e+00],\n",
       "         [ 2.0163e+00, -4.3616e+00, -6.1350e-01],\n",
       "         [ 1.7339e+00, -3.6429e+00, -6.2895e-01],\n",
       "         [ 1.8639e+00, -1.8441e+00, -4.1757e+00],\n",
       "         [ 1.1813e+00,  8.4500e-01, -1.2899e+01],\n",
       "         [ 1.4696e+00,  5.2980e-01, -8.6983e+00],\n",
       "         [ 2.2762e+00, -2.2702e+00, -4.7543e+00],\n",
       "         [ 2.4467e+00, -5.6374e-01, -8.6106e+00],\n",
       "         [ 2.1109e+00, -6.3901e-01, -7.3257e+00],\n",
       "         [ 2.0296e+00, -4.3570e+00, -6.4034e-01],\n",
       "         [ 3.0783e+00, -2.5641e-01, -1.0242e+01],\n",
       "         [ 1.6394e+00, -2.3653e-02, -8.5008e+00],\n",
       "         [ 2.8048e+00, -7.7296e-02, -1.1524e+01],\n",
       "         [ 2.0048e+00, -4.3589e+00, -5.8851e-01],\n",
       "         [ 2.7974e+00, -8.2416e-02, -1.1465e+01],\n",
       "         [ 1.6432e+00, -4.7064e-04, -8.6238e+00],\n",
       "         [ 1.2951e+00,  9.6177e-01, -8.6991e+00],\n",
       "         [ 1.7849e+00, -4.4082e+00,  5.0406e-02],\n",
       "         [ 1.4844e+00,  5.2778e-01, -8.7358e+00],\n",
       "         [ 2.4532e+00, -5.5595e-01, -8.6582e+00],\n",
       "         [ 3.1069e+00, -3.2069e-01, -1.0043e+01],\n",
       "         [ 1.4905e+00,  4.8967e-01, -8.5783e+00],\n",
       "         [ 2.4629e+00, -6.0258e-01, -8.5857e+00],\n",
       "         [ 1.7327e+00,  3.5856e-01, -9.8660e+00],\n",
       "         [ 1.8966e+00, -8.5528e-01, -5.5939e+00],\n",
       "         [ 2.6802e+00, -4.1513e-01, -8.6110e+00],\n",
       "         [ 1.9077e+00, -1.5616e+00, -4.0392e+00],\n",
       "         [ 4.0109e+00, -7.2667e-01, -1.1220e+01],\n",
       "         [ 1.2762e+00, -2.4931e-01, -5.2703e+00],\n",
       "         [ 2.2644e+00, -2.6800e-01, -6.5946e+00],\n",
       "         [ 1.2081e+00, -1.0525e+00, -4.5746e+00],\n",
       "         [ 2.1231e+00, -9.6119e-01, -6.3358e+00],\n",
       "         [ 4.0244e+00, -7.2612e-01, -1.1258e+01],\n",
       "         [ 2.6376e+00, -2.2785e+00, -5.1216e+00],\n",
       "         [ 1.3103e+00, -1.2759e-02, -6.2690e+00],\n",
       "         [ 1.1586e+00, -2.7245e+00, -7.0100e-01],\n",
       "         [ 1.4757e+00,  5.1383e-01, -8.6470e+00],\n",
       "         [ 2.4606e+00, -2.8679e+00, -3.6596e+00],\n",
       "         [ 2.2772e+00, -2.2977e+00, -4.7063e+00],\n",
       "         [ 4.0481e+00, -7.3738e-01, -1.1286e+01],\n",
       "         [ 2.4600e+00, -6.0026e-01, -8.5763e+00],\n",
       "         [ 2.2719e+00, -2.3049e+00, -4.6804e+00],\n",
       "         [ 1.2803e+00, -2.6210e-01, -5.2498e+00],\n",
       "         [ 1.8770e+00, -2.1433e+00, -2.5615e+00],\n",
       "         [ 1.7674e+00, -1.1043e-01, -7.8468e+00],\n",
       "         [ 2.2404e+00,  4.2518e-02, -9.5822e+00],\n",
       "         [ 2.5236e+00, -1.1188e+00, -5.7642e+00],\n",
       "         [ 1.4246e+00, -1.5030e+00, -3.3487e+00],\n",
       "         [ 1.6718e+00,  4.5557e-01, -8.5793e+00],\n",
       "         [ 2.2793e+00, -5.0511e-01, -6.7274e+00],\n",
       "         [ 3.0209e+00, -2.1526e+00, -6.9748e+00],\n",
       "         [ 1.5085e+00, -1.6762e+00, -3.6906e+00],\n",
       "         [ 1.6260e+00,  8.6860e-02, -8.0026e+00],\n",
       "         [ 1.4368e+00, -3.9360e+00, -2.0417e-01],\n",
       "         [ 3.0132e+00, -9.8280e-01, -7.3740e+00],\n",
       "         [ 2.0940e+00, -2.0231e+00, -4.5485e+00],\n",
       "         [ 1.2104e+00, -1.0685e+00, -4.5510e+00],\n",
       "         [ 2.6800e+00, -4.2081e-01, -8.5831e+00],\n",
       "         [ 2.9023e+00, -9.0268e-01, -7.8362e+00],\n",
       "         [ 1.7345e+00, -3.6394e+00, -6.3297e-01],\n",
       "         [ 1.3925e+00, -2.0885e-01, -6.2506e+00]]),\n",
       " tensor([[  1.7677,  -0.1105,  -7.8456],\n",
       "         [  1.2110,  -1.0643,  -4.5358],\n",
       "         [  1.1798,  -1.3772,  -4.0217],\n",
       "         [  3.2121,  -0.7304,  -9.6741],\n",
       "         [  2.0319,  -2.0450,  -4.6912],\n",
       "         [  2.4613,  -2.8727,  -3.6577],\n",
       "         [  2.3257,  -0.0462,  -9.1669],\n",
       "         [  2.3202,  -1.7677,  -5.4039],\n",
       "         [  1.2412,  -0.9677,  -4.3175],\n",
       "         [  2.3519,  -0.3671,  -8.2137],\n",
       "         [  1.8197,  -0.1847,  -7.8746],\n",
       "         [  1.0384,  -0.4779,  -3.7227],\n",
       "         [  2.0346,  -2.0668,  -4.6670],\n",
       "         [  2.8614,  -0.3345,  -9.0620],\n",
       "         [  3.1644,  -0.5458, -10.1612],\n",
       "         [  3.1599,  -0.5474, -10.1471],\n",
       "         [  3.2014,  -0.7344,  -9.6311],\n",
       "         [  1.5005,   0.4425,  -8.4087],\n",
       "         [  3.0842,  -0.3341,  -9.9098],\n",
       "         [  1.5765,  -0.6450,  -6.3241],\n",
       "         [  2.0617,  -2.0891,  -4.7457],\n",
       "         [  2.4682,  -0.6054,  -8.5560],\n",
       "         [  4.5590,  -0.9025, -13.0232],\n",
       "         [  1.4872,   0.4833,  -8.5418],\n",
       "         [  2.7830,  -0.9466,  -7.0481],\n",
       "         [  1.7944,  -4.4073,   0.0383],\n",
       "         [  2.0956,  -2.0287,  -4.5488],\n",
       "         [  2.3150,  -0.7423,  -6.2737],\n",
       "         [  2.3175,  -0.7153,  -6.3138],\n",
       "         [  1.4754,   0.5173,  -8.6439],\n",
       "         [  2.1214,  -0.9677,  -4.9761],\n",
       "         [  2.1724,  -2.7454,  -3.7072],\n",
       "         [  1.4721,   0.5243,  -8.6847],\n",
       "         [  1.7501,  -0.1752,  -7.6837],\n",
       "         [  1.4809,   0.4912,  -8.5667],\n",
       "         [  2.4647,  -2.8703,  -3.6685],\n",
       "         [  3.2173,  -0.7255,  -9.6987],\n",
       "         [  2.4573,  -0.5568,  -8.6405],\n",
       "         [  2.4626,  -0.5952,  -8.5942],\n",
       "         [  2.1110,  -0.9608,  -4.9637],\n",
       "         [  2.4621,  -2.8662,  -3.6688],\n",
       "         [  2.1729,  -2.6871,  -3.8086],\n",
       "         [  1.9825,  -2.0727,  -4.9010],\n",
       "         [  3.5525,  -1.8050,  -6.7584],\n",
       "         [  0.1865,  -3.6799,   0.5593],\n",
       "         [  2.2362,   0.0460,  -9.5682],\n",
       "         [  1.6512,  -0.0430,  -8.4723],\n",
       "         [  2.2864,  -2.2711,  -4.7870],\n",
       "         [  1.0492,  -2.4224,  -1.3529],\n",
       "         [  1.6220,   0.0675,  -7.9061],\n",
       "         [  2.2462,   0.5300, -11.1067],\n",
       "         [  2.2024,  -0.0375,  -9.9573],\n",
       "         [  1.7968,  -4.4055,   0.0220],\n",
       "         [  1.2889,   0.9460,  -8.6458],\n",
       "         [  2.0607,  -2.0896,  -4.7424],\n",
       "         [  2.2393,   0.0451,  -9.5698],\n",
       "         [  2.1812,  -2.0569,  -4.1091],\n",
       "         [  2.2538,   0.5259, -11.1302],\n",
       "         [  2.5916,  -3.1295,  -3.5457],\n",
       "         [  1.2973,  -3.0764,  -0.5683],\n",
       "         [  1.5567,  -1.2727,  -5.0989],\n",
       "         [  2.4536,  -0.5443,  -8.6788],\n",
       "         [  1.4339,  -3.8869,  -0.2901],\n",
       "         [  2.6725,  -0.4226,  -8.5509]]),\n",
       " tensor([[  1.8597,  -1.8287,  -4.1936],\n",
       "         [  1.7307,  -3.6367,  -0.6243],\n",
       "         [  2.2829,  -2.3094,  -4.7075],\n",
       "         [  1.8079,  -3.8683,  -0.7326],\n",
       "         [  1.8204,  -3.8603,  -0.7690],\n",
       "         [  2.2836,  -2.3041,  -4.7190],\n",
       "         [  2.8067,  -0.0676, -11.5708],\n",
       "         [  2.8104,  -0.0693, -11.5820],\n",
       "         [  1.2290,  -0.9413,  -4.3353],\n",
       "         [  3.0600,  -0.3127,  -9.9543],\n",
       "         [  2.2749,  -2.2958,  -4.7047],\n",
       "         [  1.7340,  -3.6414,  -0.6266],\n",
       "         [  1.9077,  -1.5551,  -4.0445],\n",
       "         [  2.2840,  -2.3233,  -4.6915],\n",
       "         [  2.4385,  -0.5258,  -8.6659],\n",
       "         [  1.6499,  -1.1120,  -4.8214],\n",
       "         [  1.5619,   0.5998,  -8.5548],\n",
       "         [  2.4564,  -0.5455,  -8.6846],\n",
       "         [  3.0723,  -0.3330,  -9.8838],\n",
       "         [  3.1784,  -1.6047,  -7.2935],\n",
       "         [  1.8874,  -0.3871,  -7.1258],\n",
       "         [  4.5438,  -0.9119, -12.9511],\n",
       "         [  3.0181,  -0.0431,  -9.9479],\n",
       "         [  1.6377,  -0.7631,  -4.7189],\n",
       "         [  2.3022,  -1.4451,  -4.8823],\n",
       "         [  6.2150,  -1.6740, -17.7801],\n",
       "         [  1.8959,   0.1752,  -9.1878],\n",
       "         [  1.7578,  -0.3413,  -6.9212],\n",
       "         [  1.7741,  -1.6826,  -5.9075],\n",
       "         [  1.6036,  -1.3867,  -4.0378],\n",
       "         [  4.7208,  -1.0221, -12.5379],\n",
       "         [  3.0791,  -0.0262, -10.6632],\n",
       "         [  2.7414,  -2.6224,  -4.7879],\n",
       "         [  2.6828,  -0.7238,  -8.1480],\n",
       "         [  1.4858,  -0.1995,  -6.4647],\n",
       "         [  1.8143,  -0.1511,  -7.9885],\n",
       "         [  3.0778,  -0.0213, -10.6838],\n",
       "         [  2.7978,  -0.0837, -11.4705],\n",
       "         [  1.4824,  -0.2097,  -6.4133],\n",
       "         [  1.8854,   0.1909,  -9.2046],\n",
       "         [  1.6279,  -2.4753,  -3.0595],\n",
       "         [  1.6434,  -0.7437,  -4.7498],\n",
       "         [  1.8399,  -0.6890,  -6.0716],\n",
       "         [  0.9515,   0.5231,  -7.7355],\n",
       "         [  2.2998,  -1.4497,  -4.8709],\n",
       "         [  2.5129,  -0.2879,  -8.1862],\n",
       "         [  4.6275,  -0.3421, -13.5214],\n",
       "         [  1.8850,  -1.5547,  -4.0155],\n",
       "         [  0.7608,   0.6499,  -8.2267],\n",
       "         [  1.7145,  -0.0818,  -7.1385],\n",
       "         [  2.2773,  -2.0518,  -4.3449],\n",
       "         [  2.8874,  -0.5547,  -8.2329],\n",
       "         [  1.8856,   0.1898,  -9.1978],\n",
       "         [  1.9560,  -2.4849,  -3.6589],\n",
       "         [  2.1682,  -2.7357,  -3.7185],\n",
       "         [  1.0545,  -1.8315,  -1.9383],\n",
       "         [  6.2268,  -1.6706, -17.8587],\n",
       "         [  2.3600,  -0.3866,  -8.1808],\n",
       "         [  1.2846,  -3.0799,  -0.5438],\n",
       "         [  1.7153,  -0.4610,  -6.3197],\n",
       "         [  2.8802,  -0.5699,  -8.1578],\n",
       "         [  2.0632,  -2.2803,  -3.2540],\n",
       "         [  1.7360,  -0.4928,  -5.8627],\n",
       "         [  1.8429,  -1.6179,  -4.9471]]),\n",
       " tensor([[ 1.4995e+00, -2.6917e-01, -6.2950e+00],\n",
       "         [ 1.3951e+00, -2.3278e-01, -5.6120e+00],\n",
       "         [ 2.8932e+00, -5.6450e-01, -8.2152e+00],\n",
       "         [ 8.6797e-01, -8.3183e-03, -5.7493e+00],\n",
       "         [ 4.7266e+00, -1.0219e+00, -1.2542e+01],\n",
       "         [ 1.7552e+00, -1.9676e+00, -6.0491e+00],\n",
       "         [ 1.4923e+00, -2.8869e+00, -2.1747e+00],\n",
       "         [ 2.7381e+00, -2.6448e+00, -4.7441e+00],\n",
       "         [ 2.1291e+00, -1.2810e+00, -6.7332e+00],\n",
       "         [ 2.1935e+00, -2.2734e+00, -4.0522e+00],\n",
       "         [ 1.7462e+00, -2.7820e-01, -7.2310e+00],\n",
       "         [ 1.7351e+00,  1.9440e-01, -8.7519e+00],\n",
       "         [ 1.9528e+00, -2.4344e+00, -2.7880e+00],\n",
       "         [ 3.5110e+00, -3.0441e-01, -1.1136e+01],\n",
       "         [ 9.2348e-01,  1.0035e+00, -9.9741e+00],\n",
       "         [ 1.6332e+00, -2.4166e+00, -2.3378e+00],\n",
       "         [ 2.6931e+00, -9.2179e-01, -6.4642e+00],\n",
       "         [ 9.9779e-01, -4.2285e+00,  1.2910e+00],\n",
       "         [ 2.3204e+00, -7.1101e-01, -6.3330e+00],\n",
       "         [ 2.1189e+00, -8.4864e-01, -5.2008e+00],\n",
       "         [ 1.6999e+00, -2.6400e+00, -1.9056e+00],\n",
       "         [ 2.0659e+00, -5.2404e-01, -6.9022e+00],\n",
       "         [ 1.9014e+00, -8.5467e-01, -5.6065e+00],\n",
       "         [ 1.4835e+00, -2.1133e-01, -6.4105e+00],\n",
       "         [ 2.4075e+00, -8.6479e-01, -6.0016e+00],\n",
       "         [ 1.7771e+00, -7.2780e-01, -6.6667e+00],\n",
       "         [ 3.3249e+00, -7.3150e-01, -9.4405e+00],\n",
       "         [ 1.8902e+00, -8.6056e-01, -5.5813e+00],\n",
       "         [ 1.2021e+00, -3.0340e-01, -4.2243e+00],\n",
       "         [ 1.3431e+00, -1.1809e+00, -3.8546e+00],\n",
       "         [ 1.3021e+00,  5.5059e-02, -7.3755e+00],\n",
       "         [ 1.0981e+00, -4.6571e-01, -4.9951e+00],\n",
       "         [ 2.3244e+00, -1.8017e+00, -5.3504e+00],\n",
       "         [ 5.1848e+00, -1.2674e+00, -1.3677e+01],\n",
       "         [ 2.1433e+00, -8.7663e-02, -6.9704e+00],\n",
       "         [ 1.5187e+00, -1.8193e+00, -3.6624e+00],\n",
       "         [ 1.7353e+00, -1.1917e-01, -6.7939e+00],\n",
       "         [ 5.0058e+00, -6.2642e-01, -1.3668e+01],\n",
       "         [ 2.1401e+00, -1.3440e+00, -6.6344e+00],\n",
       "         [ 1.4022e+00, -9.3339e-01, -4.5661e+00],\n",
       "         [ 1.6880e+00, -2.2158e+00, -2.9852e+00],\n",
       "         [ 2.7372e+00, -2.6572e+00, -4.7209e+00],\n",
       "         [ 1.3853e+00,  1.9635e-01, -6.4902e+00],\n",
       "         [ 2.2938e+00, -1.4498e+00, -4.8498e+00],\n",
       "         [ 2.2997e+00, -1.4372e+00, -4.8990e+00],\n",
       "         [ 2.0331e+00, -2.0292e+00, -3.9772e+00],\n",
       "         [ 2.0960e+00, -3.6459e-01, -6.8309e+00],\n",
       "         [ 1.6200e+00, -8.2163e-01, -5.5323e+00],\n",
       "         [ 1.9221e+00, -3.6090e-01, -7.4970e+00],\n",
       "         [ 1.6959e+00, -6.1312e-01, -4.8555e+00],\n",
       "         [ 2.5155e+00, -1.2068e+00, -6.9765e+00],\n",
       "         [ 1.6863e+00, -2.2208e+00, -2.9703e+00],\n",
       "         [ 1.0827e+00,  1.4891e+00, -1.1716e+01],\n",
       "         [ 1.9302e+00, -2.6489e+00, -2.7809e+00],\n",
       "         [ 1.1869e+00, -8.6671e-01, -4.4535e+00],\n",
       "         [ 1.9093e+00, -4.5039e+00,  8.7801e-02],\n",
       "         [ 3.0278e+00, -5.6914e-02, -9.9111e+00],\n",
       "         [ 1.6809e+00, -2.2064e+00, -2.9674e+00],\n",
       "         [ 2.1748e+00, -2.7228e+00, -3.7531e+00],\n",
       "         [ 2.3279e+00, -7.5636e-01, -6.2679e+00],\n",
       "         [ 1.4819e+00, -2.1840e-01, -6.3705e+00],\n",
       "         [ 2.1018e+00, -5.7044e-01, -6.3739e+00],\n",
       "         [ 1.9609e+00,  4.7947e-01, -1.0039e+01],\n",
       "         [ 1.5021e+00, -1.8795e+00, -3.4931e+00]]),\n",
       " tensor([[  2.0153,  -2.0268,  -3.9770],\n",
       "         [  1.4883,  -0.2117,  -6.4382],\n",
       "         [  2.5183,  -1.2250,  -6.9288],\n",
       "         [  1.6437,  -0.4824,  -5.5166],\n",
       "         [  1.4041,  -0.9144,  -4.5988],\n",
       "         [  1.8949,  -0.3478,  -6.4216],\n",
       "         [  1.2259,  -0.8552,  -3.9689],\n",
       "         [  1.1733,  -1.3890,  -4.0085],\n",
       "         [  2.3282,  -0.7671,  -7.6733],\n",
       "         [  2.7428,  -2.6197,  -4.7995],\n",
       "         [  1.8334,  -1.7167,  -4.0151],\n",
       "         [  2.7468,  -0.9063,  -7.1001],\n",
       "         [  1.7363,  -0.4988,  -5.8494],\n",
       "         [  1.8884,  -0.3708,  -7.1850],\n",
       "         [  2.6921,  -0.9296,  -6.4429],\n",
       "         [  1.3568,  -0.6150,  -4.7700],\n",
       "         [  1.7718,  -0.3529,  -6.9256],\n",
       "         [  2.1986,  -2.2718,  -4.0632],\n",
       "         [  2.1206,  -1.3013,  -6.6698],\n",
       "         [  3.9189,  -0.5629, -10.1904],\n",
       "         [  2.6562,  -0.7176,  -8.0873],\n",
       "         [  1.9727,  -0.3694,  -6.3850],\n",
       "         [  1.4451,  -0.2427,  -6.0637],\n",
       "         [  2.0310,  -0.1165,  -7.9937],\n",
       "         [  1.6043,  -1.9008,  -3.3519],\n",
       "         [  1.6870,  -1.6017,  -4.9857],\n",
       "         [  2.7325,  -2.6262,  -4.7457],\n",
       "         [  1.9208,  -2.6581,  -2.7475],\n",
       "         [  0.8248,  -0.1675,  -3.0754],\n",
       "         [  1.9989,  -3.0990,  -2.3561],\n",
       "         [  1.9221,  -2.6815,  -2.7077],\n",
       "         [  3.0493,  -3.7014,  -3.9282],\n",
       "         [  1.6444,  -0.0359,  -8.4713],\n",
       "         [  2.6799,  -0.6641,  -8.3989],\n",
       "         [  2.0540,  -3.2516,  -2.1136],\n",
       "         [  2.6592,  -0.6414,  -8.3685],\n",
       "         [  2.6743,  -0.6647,  -8.3770],\n",
       "         [  2.0527,  -3.2589,  -2.0994],\n",
       "         [  1.4947,  -2.7968,  -1.2553],\n",
       "         [  1.4920,  -2.8138,  -1.2246],\n",
       "         [  1.4932,  -2.8487,  -1.1526],\n",
       "         [  1.4885,  -2.8183,  -1.2210],\n",
       "         [  1.4844,  -2.8223,  -1.1477],\n",
       "         [  1.6810,   0.4296,  -8.5228],\n",
       "         [  2.7316,  -2.6365,  -4.7410],\n",
       "         [  2.5732,  -0.2805,  -9.8038],\n",
       "         [  2.0319,  -0.5475,  -7.6925],\n",
       "         [  5.1701,  -1.2585, -13.6391],\n",
       "         [  1.2337,  -0.4896,  -5.4258],\n",
       "         [  2.0928,  -0.3908,  -6.7832],\n",
       "         [  1.4901,  -0.2346,  -6.3577],\n",
       "         [  0.3016,  -1.0810,  -5.9336],\n",
       "         [  0.8916,  -1.7678,  -1.5743],\n",
       "         [  1.6839,  -2.2373,  -2.9408],\n",
       "         [  2.3038,  -1.4603,  -4.8603],\n",
       "         [  1.2236,  -0.8467,  -3.9837],\n",
       "         [  2.0930,  -0.5554,  -6.3998],\n",
       "         [  1.7315,   0.4218,  -8.5955],\n",
       "         [  1.7935,   0.2379,  -8.5688],\n",
       "         [  1.6986,   0.4258,  -8.5549],\n",
       "         [  0.6151,  -3.0185,   1.0448],\n",
       "         [  1.8388,  -0.6887,  -6.0765],\n",
       "         [  1.3325,  -2.3620,  -2.6261],\n",
       "         [  1.5564,  -1.1451,  -5.3485]]),\n",
       " tensor([[ 1.6493e+00, -1.7274e-01, -7.5198e+00],\n",
       "         [ 5.9663e-01, -3.3952e+00,  1.3084e+00],\n",
       "         [ 1.6853e+00, -1.7957e+00, -4.4277e+00],\n",
       "         [ 1.3242e+00, -2.8425e-01, -5.3389e+00],\n",
       "         [ 1.7197e+00, -4.9355e-01, -5.7244e+00],\n",
       "         [ 1.3591e+00, -6.2048e-01, -4.7628e+00],\n",
       "         [ 2.3151e+00, -7.2169e-01, -6.3109e+00],\n",
       "         [ 2.9889e+00, -6.3973e-01, -8.4373e+00],\n",
       "         [ 2.6726e+00, -6.6503e-01, -8.3778e+00],\n",
       "         [ 3.7118e+00, -9.2127e-01, -8.8315e+00],\n",
       "         [ 1.9227e+00, -2.6699e+00, -2.7319e+00],\n",
       "         [ 3.0899e-01, -1.0565e+00, -6.0286e+00],\n",
       "         [ 1.9200e+00, -2.7056e+00, -2.6192e+00],\n",
       "         [ 1.8536e+00, -2.4518e+00, -3.8166e+00],\n",
       "         [ 2.7532e+00, -1.8961e+00, -5.8706e+00],\n",
       "         [ 2.1826e+00, -2.2854e+00, -4.0092e+00],\n",
       "         [ 2.6880e+00, -9.3312e-01, -6.4184e+00],\n",
       "         [ 1.6862e+00,  4.6336e-01, -8.6127e+00],\n",
       "         [ 1.6591e+00, -2.1139e-02, -6.6608e+00],\n",
       "         [ 2.3746e+00, -2.9995e+00, -4.2746e+00],\n",
       "         [ 1.7022e+00, -1.2989e+00, -6.4049e+00],\n",
       "         [ 2.7354e+00, -2.6692e+00, -4.6886e+00],\n",
       "         [ 2.5327e+00, -5.4016e-02, -8.8335e+00],\n",
       "         [ 9.9156e-01,  4.7195e-01, -7.6831e+00],\n",
       "         [ 1.3146e+00,  9.6321e-03, -6.8451e+00],\n",
       "         [ 2.1248e+00, -7.8596e-02, -8.8660e+00],\n",
       "         [ 3.0658e+00, -3.9986e+00, -3.4386e+00],\n",
       "         [ 1.8087e+00, -1.8076e-01, -6.9769e+00],\n",
       "         [ 1.6922e+00, -2.2125e+00, -3.0124e+00],\n",
       "         [ 1.6872e+00, -2.2256e+00, -2.9644e+00],\n",
       "         [ 1.6920e+00,  4.3894e-01, -8.5673e+00],\n",
       "         [ 2.0470e+00, -5.4381e+00,  6.5877e-01],\n",
       "         [ 1.6085e+00, -1.3972e+00, -4.0372e+00],\n",
       "         [ 1.4916e+00,  4.7458e-01, -8.5301e+00],\n",
       "         [ 2.1237e+00, -7.4093e-02, -8.8762e+00],\n",
       "         [ 1.7285e+00, -1.1475e-01, -6.7924e+00],\n",
       "         [ 1.2138e+00,  6.7027e-01, -8.5644e+00],\n",
       "         [ 1.6586e+00, -1.3054e+00, -4.7183e+00],\n",
       "         [ 1.4742e+00, -1.7710e+00, -3.2382e+00],\n",
       "         [ 1.1086e+00, -9.1534e-01, -5.5442e+00],\n",
       "         [ 1.4275e+00, -1.2745e+00, -4.1352e+00],\n",
       "         [ 1.7120e+00, -4.6434e-01, -5.8863e+00],\n",
       "         [ 1.1755e+00, -2.8835e-01, -4.2164e+00],\n",
       "         [ 2.0333e+00, -2.5941e+00, -2.9885e+00],\n",
       "         [ 2.1734e+00, -1.9400e+00, -4.4591e+00],\n",
       "         [ 3.6445e+00, -1.5748e+00, -6.7187e+00],\n",
       "         [ 1.7877e+00, -6.4421e-01, -4.9267e+00],\n",
       "         [ 2.5124e+00, -3.6953e-01, -8.4791e+00],\n",
       "         [ 1.4850e+00, -2.1604e-01, -6.4011e+00],\n",
       "         [ 3.5719e+00, -1.8456e+00, -6.7276e+00],\n",
       "         [ 1.2370e+00, -3.1795e+00, -1.4606e+00],\n",
       "         [ 1.6791e+00, -9.8915e-01, -4.5475e+00],\n",
       "         [ 1.7788e+00, -1.6777e+00, -5.9315e+00],\n",
       "         [ 2.3166e+00, -7.4527e-01, -6.2688e+00],\n",
       "         [ 1.6903e+00, -1.7986e+00, -4.4252e+00],\n",
       "         [ 2.3903e+00, -8.6460e-01, -5.9711e+00],\n",
       "         [ 2.4561e+00, -5.4206e-01, -8.6742e+00],\n",
       "         [ 1.9835e+00, -2.0719e+00, -4.9218e+00],\n",
       "         [ 2.0755e+00, -3.5727e-01, -6.8266e+00],\n",
       "         [ 1.6255e+00, -2.4389e+00, -2.2837e+00],\n",
       "         [ 2.4657e+00, -2.8534e+00, -3.9420e+00],\n",
       "         [ 2.2404e+00,  4.4468e-02, -9.5938e+00],\n",
       "         [ 3.6194e+00, -8.2601e-01, -9.7269e+00],\n",
       "         [ 1.1368e+00, -4.7366e-02, -5.6314e+00]]),\n",
       " tensor([[ 1.6464, -1.3419, -4.6184],\n",
       "         [ 2.5094, -0.6082, -6.7935],\n",
       "         [ 2.0578, -2.6805, -2.8484],\n",
       "         [ 1.8885,  0.1824, -9.1687],\n",
       "         [ 2.7352, -2.6372, -4.7423],\n",
       "         [ 1.4752, -0.0926, -6.3977],\n",
       "         [ 2.0597, -0.3462, -6.8189],\n",
       "         [ 1.5065, -2.8447, -2.2412],\n",
       "         [ 1.4910, -2.8357, -1.1888],\n",
       "         [ 1.4823, -2.8797, -1.0826],\n",
       "         [ 1.4956, -2.8293, -1.2066],\n",
       "         [ 1.7329,  0.1772, -8.6462],\n",
       "         [ 1.2292, -1.3124, -3.9101],\n",
       "         [ 2.0136, -2.0211, -3.9903],\n",
       "         [ 2.1908, -2.2851, -4.0212],\n",
       "         [ 1.4887, -2.8512, -1.1776],\n",
       "         [ 1.4934, -2.8521, -1.1708],\n",
       "         [ 1.8531, -2.4157, -3.8603],\n",
       "         [ 2.6605, -0.8472, -7.1682],\n",
       "         [ 2.5726, -0.2837, -9.7886],\n",
       "         [ 1.5031, -1.8608, -3.5190],\n",
       "         [ 2.5175, -1.2081, -6.9628],\n",
       "         [ 1.4111, -2.8236, -1.1159],\n",
       "         [ 2.1481, -2.2953, -4.1518],\n",
       "         [ 2.7397, -2.6352, -4.7694],\n",
       "         [ 1.7068,  0.4209, -8.5650],\n",
       "         [ 1.3808, -3.0310, -1.0819],\n",
       "         [ 1.5899,  0.0931, -8.7007],\n",
       "         [ 1.4842, -0.2240, -6.3633],\n",
       "         [ 1.9928, -3.1353, -2.2909],\n",
       "         [ 1.6839, -2.1967, -2.9773],\n",
       "         [ 0.7187,  0.7729, -8.6672],\n",
       "         [ 1.7063, -1.2950, -6.4365],\n",
       "         [ 1.7062, -1.3068, -6.4038],\n",
       "         [ 2.2781, -4.1058, -1.4390],\n",
       "         [ 1.6497, -1.3671, -4.6055],\n",
       "         [ 2.5717, -0.2778, -9.8039],\n",
       "         [ 3.8503, -0.8587, -9.5629],\n",
       "         [ 1.4875, -0.2182, -6.4003],\n",
       "         [ 2.0026, -0.6929, -5.6573],\n",
       "         [ 1.7086, -0.0988, -7.0671],\n",
       "         [ 1.2273, -0.8508, -3.9620],\n",
       "         [ 2.2032, -2.2627, -4.0882],\n",
       "         [ 1.3334, -2.3880, -2.5765],\n",
       "         [ 1.3315, -2.3787, -2.5842],\n",
       "         [ 1.8927,  0.1812, -9.1742],\n",
       "         [ 1.8029,  0.2285, -8.5450],\n",
       "         [ 1.5887, -1.4673, -4.5972],\n",
       "         [ 2.0497, -3.2652, -2.0680],\n",
       "         [ 2.3739, -0.2896, -8.6272],\n",
       "         [ 2.3492, -0.3040, -8.5051],\n",
       "         [ 1.6161, -3.9513, -0.5338],\n",
       "         [ 1.0973, -2.0372, -2.7282],\n",
       "         [ 3.0525, -3.6644, -3.9752],\n",
       "         [ 3.0502, -3.7260, -3.8603],\n",
       "         [ 3.0525, -3.7091, -3.9035],\n",
       "         [ 2.8517, -1.3279, -5.8880],\n",
       "         [ 2.1187, -0.8470, -5.2037],\n",
       "         [ 2.1519, -0.6172, -6.4726],\n",
       "         [ 2.5175, -1.2081, -6.9628],\n",
       "         [ 2.5083, -1.1943, -7.0139],\n",
       "         [ 2.5137, -1.1713, -7.0417],\n",
       "         [ 1.3388, -1.1594, -3.8726],\n",
       "         [ 1.4673, -2.1548, -3.1750]]),\n",
       " tensor([[ 2.2951e+00, -1.4451e+00, -4.8701e+00],\n",
       "         [ 2.3083e+00, -1.4258e+00, -4.9313e+00],\n",
       "         [ 2.3049e+00, -1.4436e+00, -4.8948e+00],\n",
       "         [ 3.1742e-01, -1.0613e+00, -6.0304e+00],\n",
       "         [ 2.3968e+00, -8.5680e-01, -5.9903e+00],\n",
       "         [ 1.4039e+00, -9.3356e-01, -4.5733e+00],\n",
       "         [ 1.4905e+00, -2.8650e+00, -2.2056e+00],\n",
       "         [ 1.5257e+00, -1.3239e+00, -3.4906e+00],\n",
       "         [ 2.1297e+00, -1.3471e+00, -6.6116e+00],\n",
       "         [ 2.1297e+00, -6.5438e-02, -8.9233e+00],\n",
       "         [ 2.9944e+00, -6.4430e-01, -8.4375e+00],\n",
       "         [ 2.9955e+00, -6.4500e-01, -8.4412e+00],\n",
       "         [ 2.9938e+00, -6.4289e-01, -8.4417e+00],\n",
       "         [ 2.9824e+00, -6.3987e-01, -8.4147e+00],\n",
       "         [ 1.7080e+00, -9.8602e-02, -7.0631e+00],\n",
       "         [ 1.6879e+00, -2.1943e+00, -3.0150e+00],\n",
       "         [ 3.0688e+00, -4.0104e+00, -3.4321e+00],\n",
       "         [ 3.3087e+00,  1.2228e-01, -8.9417e+00],\n",
       "         [ 1.6383e+00, -1.9563e+00, -2.6774e+00],\n",
       "         [ 1.6426e+00, -1.9636e+00, -2.6762e+00],\n",
       "         [ 1.3089e+00,  9.9520e-03, -6.8375e+00],\n",
       "         [ 1.9540e+00,  4.5750e-01, -9.9415e+00],\n",
       "         [ 1.7358e+00,  2.0539e-01, -8.8153e+00],\n",
       "         [ 1.7922e+00, -1.5224e-01, -7.0337e+00],\n",
       "         [ 6.4682e-01,  9.9838e-01, -7.6316e+00],\n",
       "         [ 2.6302e+00, -1.0470e+00, -5.6146e+00],\n",
       "         [ 2.6508e+00, -1.0565e+00, -5.6401e+00],\n",
       "         [ 2.6320e+00, -1.0409e+00, -5.6447e+00],\n",
       "         [ 1.4493e+00, -1.4466e-01, -5.6030e+00],\n",
       "         [ 1.7703e+00, -1.1274e-01, -7.8509e+00],\n",
       "         [ 2.2611e+00, -2.0530e+00, -4.1398e+00],\n",
       "         [ 1.4419e+00, -2.2630e-01, -6.1113e+00],\n",
       "         [ 2.0200e+00, -6.3406e-01, -5.9556e+00],\n",
       "         [ 2.4064e+00, -2.6274e-01, -9.5378e+00],\n",
       "         [ 1.2337e+00, -1.2730e+00, -3.9897e+00],\n",
       "         [ 9.0989e-01, -2.4450e+00,  7.4336e-02],\n",
       "         [ 2.2790e+00, -2.3144e+00, -4.6887e+00],\n",
       "         [ 1.0880e+00, -7.7109e-01, -4.0319e+00],\n",
       "         [ 1.0842e+00, -7.9102e-01, -3.9901e+00],\n",
       "         [ 1.0891e+00, -7.8847e-01, -4.0074e+00],\n",
       "         [ 1.8514e+00, -1.6191e+00, -4.9647e+00],\n",
       "         [ 1.8967e+00, -1.5796e-01, -8.0682e+00],\n",
       "         [ 1.5649e+00, -1.4275e+00, -4.6431e+00],\n",
       "         [ 1.9262e+00, -2.7194e+00, -2.6138e+00],\n",
       "         [ 1.9210e+00, -2.7304e+00, -2.5840e+00],\n",
       "         [ 1.5243e+00, -1.8236e+00, -3.6286e+00],\n",
       "         [ 1.2905e+00, -3.0972e+00, -5.4493e-01],\n",
       "         [ 2.9485e+00, -7.4116e-01, -8.1107e+00],\n",
       "         [ 1.9631e+00, -4.2121e-01, -6.6894e+00],\n",
       "         [ 1.9273e+00, -2.6390e+00, -2.7821e+00],\n",
       "         [ 3.0251e+00, -6.1476e-02, -9.8936e+00],\n",
       "         [ 2.1859e+00, -2.2889e+00, -4.0083e+00],\n",
       "         [ 1.6843e+00, -2.2345e+00, -2.9310e+00],\n",
       "         [ 2.1272e+00, -4.0045e-01, -7.1522e+00],\n",
       "         [ 1.5840e+00, -9.6005e-01, -4.8439e+00],\n",
       "         [ 1.2326e+00, -8.9218e-01, -4.4398e+00],\n",
       "         [ 7.0499e-01,  7.5362e-01, -6.6142e+00],\n",
       "         [ 2.5240e+00, -4.8624e-02, -8.8131e+00],\n",
       "         [ 1.7632e+00, -1.3622e+00, -4.7628e+00],\n",
       "         [ 2.1170e+00, -4.3536e-01, -7.0293e+00],\n",
       "         [ 5.2248e+00, -1.2743e+00, -1.3750e+01],\n",
       "         [ 1.4881e+00, -2.7833e+00, -1.2522e+00],\n",
       "         [ 1.8479e+00, -4.0255e+00, -6.7964e-01],\n",
       "         [ 1.7046e+00, -1.3001e+00, -6.4228e+00]]),\n",
       " tensor([[  1.9154,  -0.9275,  -6.3466],\n",
       "         [  6.2528,  -0.8351, -17.3458],\n",
       "         [  0.9379,  -2.2265,  -0.5143],\n",
       "         [  2.7457,  -2.0599,  -4.2020],\n",
       "         [  1.5617,  -0.6908,  -6.1467],\n",
       "         [  1.4736,   0.5084,  -8.6103],\n",
       "         [  1.4775,   0.4914,  -8.5530],\n",
       "         [  1.6569,  -0.1613,  -7.6012],\n",
       "         [  2.2833,  -2.0446,  -4.3586],\n",
       "         [  2.3133,  -1.8001,  -5.3451],\n",
       "         [  1.7322,  -1.2141,  -6.0097],\n",
       "         [  1.5617,  -1.0012,  -5.3560],\n",
       "         [  1.4839,   0.4848,  -8.5509],\n",
       "         [  2.1107,  -0.6737,  -7.2410],\n",
       "         [  4.0437,  -0.7097, -11.3663],\n",
       "         [  4.1582,  -1.4731, -10.9295],\n",
       "         [  1.9019,  -1.5568,  -4.0434],\n",
       "         [  2.1097,  -0.6499,  -7.2798],\n",
       "         [  4.1513,  -1.4823, -10.8970],\n",
       "         [  1.2888,   0.9482,  -8.6355],\n",
       "         [  2.2075,  -2.0292,  -4.6479],\n",
       "         [  2.0109,  -0.2553,  -7.9533],\n",
       "         [  1.6507,  -0.0290,  -8.5315],\n",
       "         [  1.2724,  -0.2774,  -5.1277],\n",
       "         [  1.6394,  -0.0251,  -8.4884],\n",
       "         [  1.6445,  -0.0294,  -8.5012],\n",
       "         [  2.2500,   0.5373, -11.1998],\n",
       "         [  1.7591,   0.3293,  -9.8088],\n",
       "         [  1.7673,  -1.3927,  -4.7220],\n",
       "         [  3.0324,  -2.1501,  -6.9773],\n",
       "         [  1.9552,  -2.4898,  -3.6402],\n",
       "         [  1.2400,  -0.9815,  -4.2850],\n",
       "         [  1.7248,   0.4289,  -8.6068],\n",
       "         [  1.7659,  -0.6988,  -4.7542],\n",
       "         [  1.5521,  -1.0276,  -5.2919],\n",
       "         [  1.0925,   0.5405,  -8.4197],\n",
       "         [  1.8248,  -3.8877,  -0.7267],\n",
       "         [  2.0137,  -4.3600,  -0.6090],\n",
       "         [  2.0115,  -4.3320,  -0.6345],\n",
       "         [  1.2358,  -0.9348,  -4.3713],\n",
       "         [  2.1108,  -0.9559,  -4.9738],\n",
       "         [  2.9010,  -0.8878,  -7.8569],\n",
       "         [  2.0033,  -4.3924,  -0.5326],\n",
       "         [  2.4582,  -0.6000,  -8.5812],\n",
       "         [  1.4833,   0.4886,  -8.5512],\n",
       "         [  3.1627,  -0.2870, -10.1900],\n",
       "         [  2.2862,  -2.3145,  -4.7159],\n",
       "         [  2.0680,  -2.2788,  -3.2657],\n",
       "         [  2.8046,  -0.0711, -11.5515],\n",
       "         [  2.1045,  -0.6348,  -7.3374],\n",
       "         [  1.5579,  -1.2431,  -5.1607],\n",
       "         [  1.5752,  -1.4439,  -3.9223],\n",
       "         [  1.7118,   0.3871,  -9.9108],\n",
       "         [  2.1880,  -0.5371,  -7.0922],\n",
       "         [  1.4915,  -2.8545,  -1.1667],\n",
       "         [  1.4669,  -3.4638,  -2.5022],\n",
       "         [  2.3204,  -0.7306,  -6.2975],\n",
       "         [  2.0198,  -4.3389,  -0.6493],\n",
       "         [  1.1121,   0.8746, -12.3069],\n",
       "         [  2.2827,  -2.3091,  -4.7075],\n",
       "         [  3.0299,  -2.1250,  -7.0135],\n",
       "         [  1.1780,   0.8270, -12.5817],\n",
       "         [  1.4816,   0.4925,  -8.5760],\n",
       "         [  3.0885,  -0.2947, -10.1175]]),\n",
       " tensor([[  1.7195,   0.3836,  -9.9010],\n",
       "         [  1.6431,  -0.0427,  -8.4359],\n",
       "         [  1.4544,  -3.8330,  -0.4579],\n",
       "         [  1.8056,  -3.8035,  -0.8070],\n",
       "         [  1.6478,  -0.0220,  -8.5466],\n",
       "         [  2.0030,  -4.3563,  -0.5855],\n",
       "         [  3.1111,  -0.2843, -10.2310],\n",
       "         [  2.4645,  -0.5900,  -8.6027],\n",
       "         [  1.4853,   0.4812,  -8.5389],\n",
       "         [  1.7331,  -3.6420,  -0.6268],\n",
       "         [  2.4516,  -0.5617,  -8.6290],\n",
       "         [  2.4635,  -0.6057,  -8.5430],\n",
       "         [  1.1792,   0.8373, -12.6309],\n",
       "         [  1.5812,  -0.2107,  -6.6897],\n",
       "         [  2.0719,  -2.2958,  -3.2470],\n",
       "         [  1.5533,  -1.0290,  -5.3003],\n",
       "         [  1.4641,   0.5078,  -8.5668],\n",
       "         [  3.0115,  -0.9600,  -7.4552],\n",
       "         [  2.1084,  -0.6303,  -7.3390],\n",
       "         [  1.7819,  -1.6993,  -5.8889],\n",
       "         [  2.5002,  -0.4057,  -7.7158],\n",
       "         [  1.9590,  -2.4446,  -2.8283],\n",
       "         [  1.6434,  -0.0139,  -8.5616],\n",
       "         [  1.6504,  -0.0237,  -8.5481],\n",
       "         [  2.2843,  -2.3013,  -4.7278],\n",
       "         [  2.3026,  -1.4687,  -4.8501],\n",
       "         [  2.1109,  -0.6558,  -7.3020],\n",
       "         [  1.6483,  -0.0302,  -8.5035],\n",
       "         [  2.8047,  -0.0717, -11.5473],\n",
       "         [  2.0085,  -4.3435,  -0.6175],\n",
       "         [  1.4789,   0.5035,  -8.6054],\n",
       "         [  1.4758,   0.5229,  -8.6805],\n",
       "         [  1.7589,  -1.3089,  -4.8680],\n",
       "         [  2.4544,  -0.5631,  -8.6480],\n",
       "         [  2.9812,  -0.7656,  -8.1394],\n",
       "         [  2.8083,  -0.0716, -11.5624],\n",
       "         [  1.7555,  -1.3140,  -4.8418],\n",
       "         [  1.7555,  -1.3400,  -4.7624],\n",
       "         [  1.8854,  -0.8567,  -5.5851],\n",
       "         [  0.9367,  -2.2309,  -0.5134],\n",
       "         [  2.0141,  -4.3544,  -0.6050],\n",
       "         [  1.6926,  -2.4100,  -2.3443],\n",
       "         [  1.4692,  -1.7610,  -3.2252],\n",
       "         [  1.4690,   0.5144,  -8.6247],\n",
       "         [  1.4982,   0.4578,  -8.4645],\n",
       "         [  1.7621,  -1.3555,  -4.7704],\n",
       "         [  1.4793,   0.4969,  -8.5817],\n",
       "         [  1.8193,  -0.7401,  -3.6585],\n",
       "         [  1.7317,   0.3877,  -8.5385],\n",
       "         [  1.4690,   0.5144,  -8.6247],\n",
       "         [  2.1058,  -0.6076,  -7.4561],\n",
       "         [  1.2845,  -3.0708,  -0.5438],\n",
       "         [  2.2758,  -0.4991,  -6.7330],\n",
       "         [  1.5953,  -0.1359,  -5.6320],\n",
       "         [  1.5624,  -1.0103,  -5.3487],\n",
       "         [  1.5662,  -0.0252,  -7.4579],\n",
       "         [  2.0201,  -4.3593,  -0.6222],\n",
       "         [  1.4723,  -1.7741,  -3.2237],\n",
       "         [  1.4798,  -2.3868,  -1.1685],\n",
       "         [  1.5286,  -2.1205,  -2.4783],\n",
       "         [  1.9436,   0.5478, -10.5958],\n",
       "         [  1.9532,   0.5268, -10.5499],\n",
       "         [  1.1169,   0.6652,  -9.4004],\n",
       "         [  1.9569,   0.5244, -10.5576]]),\n",
       " tensor([[  1.8428,  -3.9498,  -0.7951],\n",
       "         [  1.0539,  -2.4515,  -1.3434],\n",
       "         [  1.0530,  -2.4751,  -1.3081],\n",
       "         [  1.0539,  -2.4291,  -1.3625],\n",
       "         [  1.4812,   0.4913,  -8.5691],\n",
       "         [  1.2003,   0.4118,  -8.0728],\n",
       "         [  1.7465,  -1.3496,  -4.7502],\n",
       "         [  2.1104,  -0.6405,  -7.3269],\n",
       "         [  1.7349,   0.3256,  -9.7125],\n",
       "         [  1.8124,   0.3316,  -8.4416],\n",
       "         [  2.2836,  -2.2936,  -4.7379],\n",
       "         [  1.4362,  -0.2657,  -6.2746],\n",
       "         [  1.4867,   0.4847,  -8.5497],\n",
       "         [  3.1077,  -0.3195, -10.0443],\n",
       "         [  2.1107,  -0.6619,  -7.2906],\n",
       "         [  2.0210,  -4.3491,  -0.6446],\n",
       "         [  1.6461,  -0.0266,  -8.5189],\n",
       "         [  1.6482,  -0.0297,  -8.5145],\n",
       "         [  2.0321,  -4.3647,  -0.6451],\n",
       "         [  1.7590,  -1.3643,  -4.7494],\n",
       "         [  1.2333,  -0.9296,  -4.3700],\n",
       "         [  1.4820,   0.5296,  -8.7458],\n",
       "         [  1.7602,  -1.3478,  -4.8002],\n",
       "         [  1.7576,  -1.3497,  -4.7586],\n",
       "         [  1.6817,   0.4666,  -8.5951],\n",
       "         [  1.7641,  -1.3680,  -4.7870],\n",
       "         [  1.3972,  -0.2379,  -5.6024],\n",
       "         [  2.1076,  -0.6421,  -7.3046],\n",
       "         [  1.5585,  -1.0068,  -5.3483],\n",
       "         [  2.2829,  -2.3209,  -4.6902],\n",
       "         [  2.3146,  -0.7316,  -6.2722],\n",
       "         [  4.7199,  -1.0074, -12.5576],\n",
       "         [  2.1336,  -1.3204,  -6.6876],\n",
       "         [  2.4306,  -0.2599,  -9.6052],\n",
       "         [  2.3322,  -0.7831,  -7.6219],\n",
       "         [  2.0015,  -3.1261,  -2.3330],\n",
       "         [  1.6186,  -3.9485,  -0.5576],\n",
       "         [  1.5379,  -1.9583,  -3.3192],\n",
       "         [  1.7641,  -0.9174,  -4.9569],\n",
       "         [  2.3011,  -1.4763,  -4.8309],\n",
       "         [  0.8257,  -0.1802,  -3.0232],\n",
       "         [  3.0438,  -3.6879,  -3.9325],\n",
       "         [  3.0239,  -0.0357,  -9.9991],\n",
       "         [  2.3031,  -1.4631,  -4.8449],\n",
       "         [  2.2365,   0.7382, -11.8209],\n",
       "         [  1.8451,  -4.0052,  -0.7215],\n",
       "         [  0.8261,   0.5397,  -5.3275],\n",
       "         [  1.3407,  -1.1992,  -3.8316],\n",
       "         [  1.6535,  -0.5791,  -4.8017],\n",
       "         [  1.4515,  -0.2451,  -6.0488],\n",
       "         [  4.5670,  -0.8937, -13.0719],\n",
       "         [  2.2789,  -2.0000,  -4.4113],\n",
       "         [  1.7029,  -1.3130,  -6.4066],\n",
       "         [  0.7819,   1.3258,  -9.8344],\n",
       "         [  1.2290,  -1.2903,  -3.9334],\n",
       "         [  2.0209,  -2.0550,  -3.9311],\n",
       "         [  2.5670,  -0.2799,  -9.7767],\n",
       "         [  2.5193,  -1.2159,  -6.9603],\n",
       "         [  1.9197,  -2.7358,  -2.5716],\n",
       "         [  1.4671,  -2.1655,  -3.1544],\n",
       "         [  1.6872,  -2.2267,  -2.9575],\n",
       "         [  3.7979,  -0.7182, -10.2810],\n",
       "         [  4.5460,  -0.8970, -13.0061],\n",
       "         [  1.2276,  -0.8762,  -3.9537]]),\n",
       " tensor([[  1.7201,   0.0810,  -8.9444],\n",
       "         [  4.5637,  -0.9072, -13.0241],\n",
       "         [  1.7126,   0.0671,  -8.8665],\n",
       "         [  1.7055,  -1.3033,  -6.4274],\n",
       "         [  1.4391,  -0.1085,  -7.6788],\n",
       "         [  2.9874,  -0.6388,  -8.4346],\n",
       "         [  2.3026,  -0.0512,  -9.2432],\n",
       "         [  2.6787,  -0.6842,  -8.3594],\n",
       "         [  1.5022,  -2.0248,  -2.9869],\n",
       "         [  4.5362,  -0.8974, -12.9728],\n",
       "         [  4.5561,  -0.8974, -13.0344],\n",
       "         [  2.7492,  -0.9977,  -6.4100],\n",
       "         [  1.9125,  -0.8488,  -6.5720],\n",
       "         [  4.1728,  -1.1670, -10.8584],\n",
       "         [  1.6265,  -2.4444,  -2.2878],\n",
       "         [  2.2939,  -1.4465,  -4.8607],\n",
       "         [  4.5511,  -0.9128, -12.9746],\n",
       "         [  2.5876,  -0.7355,  -7.7555],\n",
       "         [  0.7155,  -1.8037,  -1.0886],\n",
       "         [  1.7279,  -0.2563,  -7.2779],\n",
       "         [  3.0848,  -0.0282, -10.6844],\n",
       "         [  1.9404,  -0.5033,  -6.5804],\n",
       "         [  1.9283,  -0.9245,  -4.2525],\n",
       "         [  1.2837,  -1.8539,  -2.8874],\n",
       "         [  1.7766,  -2.4529,  -2.8150],\n",
       "         [  2.0292,  -0.5326,  -7.7216],\n",
       "         [  2.0249,  -0.0987,  -8.0676],\n",
       "         [  4.1728,  -1.1684, -10.8455],\n",
       "         [  2.6397,  -1.0466,  -5.6436],\n",
       "         [  2.1643,  -1.9721,  -4.3976],\n",
       "         [  1.1010,  -0.8510,  -5.6857],\n",
       "         [  1.4650,  -2.1680,  -3.1392],\n",
       "         [  2.5719,  -0.2883,  -9.7728],\n",
       "         [  2.3573,  -0.2784,  -8.6123],\n",
       "         [  1.5665,  -1.4501,  -4.6021],\n",
       "         [  1.8232,  -0.8907,  -4.3714],\n",
       "         [  2.0593,  -0.3400,  -6.8240],\n",
       "         [  3.4310,  -0.1013, -10.5000],\n",
       "         [  1.9728,  -2.0568,  -3.4790],\n",
       "         [  2.2689,  -0.0391,  -9.2214],\n",
       "         [  2.2930,  -1.4336,  -4.8876],\n",
       "         [  1.6973,  -0.7572,  -5.6786],\n",
       "         [  2.0850,  -0.4250,  -7.6435],\n",
       "         [  1.8575,  -1.6084,  -4.9666],\n",
       "         [  1.6391,  -0.7334,  -4.7677],\n",
       "         [  1.4451,  -0.2413,  -6.0742],\n",
       "         [  2.1674,  -1.9400,  -4.4428],\n",
       "         [  1.7109,  -0.1037,  -7.0625],\n",
       "         [  2.6661,  -1.0602,  -5.6745],\n",
       "         [  3.9105,  -0.5577, -10.1844],\n",
       "         [  1.9245,  -1.5750,  -4.0414],\n",
       "         [  1.9563,  -0.5015,  -6.6227],\n",
       "         [  1.5112,  -0.4233,  -5.7037],\n",
       "         [  1.6291,   0.0801,  -7.9894],\n",
       "         [  1.9256,  -2.6619,  -2.7438],\n",
       "         [  2.3406,  -0.8457,  -6.9743],\n",
       "         [  2.3485,  -0.8413,  -6.9882],\n",
       "         [  2.0274,  -0.4816,  -7.7965],\n",
       "         [ -0.2038,  -1.4705,  -5.8154],\n",
       "         [  2.3472,  -0.8552,  -6.9812],\n",
       "         [  2.0721,   0.0171,  -8.9754],\n",
       "         [  2.9993,  -0.6476,  -8.4287],\n",
       "         [  1.8927,  -0.8553,  -5.6018],\n",
       "         [  2.6559,  -0.6537,  -8.3471]]),\n",
       " tensor([[ 2.4342e+00,  9.0605e-01, -8.9789e+00],\n",
       "         [ 2.6077e+00, -8.2175e-01, -4.8558e+00],\n",
       "         [ 2.5669e+00, -2.5732e-01, -9.8608e+00],\n",
       "         [ 1.9811e+00, -3.8479e-01, -6.3666e+00],\n",
       "         [ 1.2257e+00, -1.2878e+00, -3.9222e+00],\n",
       "         [ 7.1957e-01, -1.8007e+00, -1.0993e+00],\n",
       "         [ 1.6165e+00, -2.4738e+00, -2.2177e+00],\n",
       "         [ 1.2282e+00, -1.2583e+00, -3.9896e+00],\n",
       "         [ 1.6334e+00, -2.4605e+00, -2.2794e+00],\n",
       "         [ 2.4107e+00, -8.9117e-01, -5.9677e+00],\n",
       "         [ 1.5213e+00, -1.8176e+00, -3.6465e+00],\n",
       "         [ 3.4546e+00, -5.4079e-01, -9.4785e+00],\n",
       "         [ 3.0652e+00, -4.0020e+00, -3.4370e+00],\n",
       "         [ 1.6004e+00, -1.8913e+00, -3.3522e+00],\n",
       "         [-5.3433e+00,  7.1875e-01, -2.8508e+00],\n",
       "         [ 1.5806e+00, -1.3987e+00, -4.6709e+00],\n",
       "         [ 2.3643e+00, -3.3555e-01, -8.4518e+00],\n",
       "         [ 1.5672e+00, -1.4466e+00, -4.6334e+00],\n",
       "         [ 1.4667e+00, -2.1831e+00, -3.1274e+00],\n",
       "         [ 1.2224e+00, -8.8259e-01, -3.9387e+00],\n",
       "         [ 1.8286e+00, -1.6376e+00, -4.9093e+00],\n",
       "         [ 2.7572e+00, -9.9898e-01, -6.4281e+00],\n",
       "         [ 1.6829e+00, -2.2150e+00, -2.9579e+00],\n",
       "         [ 1.4986e+00, -1.3958e+00, -3.3987e+00],\n",
       "         [ 3.7970e+00, -7.0642e-01, -1.0321e+01],\n",
       "         [ 1.4289e+00, -1.2475e+00, -4.1506e+00],\n",
       "         [ 1.7190e+00, -4.8599e-01, -5.7093e+00],\n",
       "         [ 2.5720e+00, -2.7281e-01, -9.8281e+00],\n",
       "         [ 2.5772e+00, -2.7844e-01, -9.8240e+00],\n",
       "         [ 1.0812e+00, -2.7558e-01, -5.2946e+00],\n",
       "         [ 9.1791e-01,  1.0112e+00, -1.0004e+01],\n",
       "         [ 2.4241e+00, -2.6926e-01, -9.5618e+00],\n",
       "         [ 2.0716e+00, -3.5023e-01, -6.8150e+00],\n",
       "         [ 3.4340e+00, -1.0085e-01, -1.0555e+01],\n",
       "         [ 1.6837e+00, -7.2617e-01, -5.6954e+00],\n",
       "         [ 1.4039e+00, -9.2609e-01, -4.5841e+00],\n",
       "         [ 1.8549e+00, -1.5680e+00, -5.0416e+00],\n",
       "         [ 2.0348e+00, -1.0636e-01, -8.0532e+00],\n",
       "         [ 5.8114e+00, -2.5760e+00, -1.2471e+01],\n",
       "         [ 2.3339e+00, -7.9096e-01, -7.5972e+00],\n",
       "         [ 1.5565e+00, -1.4020e-01, -6.2855e+00],\n",
       "         [ 1.5677e+00, -1.4606e+00, -4.6149e+00],\n",
       "         [ 1.5707e+00, -1.3943e+00, -4.7247e+00],\n",
       "         [ 1.5433e+00, -1.1782e-02, -5.9845e+00],\n",
       "         [ 1.4648e+00, -2.1641e+00, -3.1435e+00],\n",
       "         [ 3.0499e+00, -3.6569e+00, -3.9907e+00],\n",
       "         [ 4.9520e+00, -4.7987e-01, -1.3803e+01],\n",
       "         [ 2.6578e+00, -6.5746e-01, -8.3416e+00],\n",
       "         [ 1.6848e+00, -2.2198e+00, -2.9596e+00],\n",
       "         [ 1.7089e+00, -8.7063e-02, -7.1019e+00],\n",
       "         [ 1.2349e+00, -1.2762e+00, -3.9856e+00],\n",
       "         [ 3.0283e+00, -2.0263e+00, -6.6377e+00],\n",
       "         [ 1.5226e+00, -1.8174e+00, -3.6356e+00],\n",
       "         [ 2.0448e+00, -1.0291e-01, -8.0965e+00],\n",
       "         [ 2.7795e+00, -1.0033e+00, -6.4741e+00],\n",
       "         [ 1.9248e+00, -2.6906e+00, -2.7011e+00],\n",
       "         [ 2.8604e+00, -1.3315e+00, -5.9096e+00],\n",
       "         [ 1.8387e+00, -6.3272e-01, -5.0209e+00],\n",
       "         [ 2.9933e+00, -6.4496e-01, -8.4375e+00],\n",
       "         [ 7.1717e-01, -1.8139e+00, -1.0843e+00],\n",
       "         [ 2.3067e+00, -1.4324e+00, -4.9083e+00],\n",
       "         [ 2.3429e+00, -8.2970e-01, -7.0008e+00],\n",
       "         [ 2.3465e+00, -8.5898e-01, -6.9773e+00],\n",
       "         [ 1.7337e+00, -1.1282e-01, -6.8149e+00]]),\n",
       " tensor([[  2.5737,  -0.2754,  -9.8235],\n",
       "         [  0.8651,  -0.0505,  -5.6703],\n",
       "         [  2.9901,  -0.6435,  -8.4318],\n",
       "         [  4.6119,   0.0799, -14.9597],\n",
       "         [  4.1715,  -1.1580, -10.8807],\n",
       "         [  2.7484,  -1.8622,  -5.9219],\n",
       "         [  1.7042,  -1.3155,  -6.3959],\n",
       "         [  1.7091,  -0.1066,  -7.0251],\n",
       "         [  2.3123,  -0.7537,  -6.2312],\n",
       "         [  1.9598,   0.5358, -10.6067],\n",
       "         [  1.2666,  -0.1754,  -5.9210],\n",
       "         [  3.0536,  -3.7048,  -3.9412],\n",
       "         [  2.4275,  -0.5867,  -6.9664],\n",
       "         [  1.6767,  -2.2009,  -2.9530],\n",
       "         [  1.4740,  -2.3383,  -3.6070],\n",
       "         [  2.0023,  -3.1578,  -2.2854],\n",
       "         [  2.0027,  -3.1750,  -2.2594],\n",
       "         [  1.5653,  -0.0199,  -7.4970],\n",
       "         [  1.3955,  -0.2423,  -5.5779],\n",
       "         [  2.3141,  -0.7269,  -6.2959],\n",
       "         [  1.5088,  -0.7658,  -4.9366],\n",
       "         [  0.9422,  -2.2402,  -0.5091],\n",
       "         [  1.1675,  -1.3467,  -4.0762],\n",
       "         [  1.7763,  -0.4507,  -6.5674],\n",
       "         [  1.8613,  -2.1968,  -3.4738],\n",
       "         [  2.5334,  -0.6048,  -6.8722],\n",
       "         [  2.5048,  -0.5997,  -6.7189],\n",
       "         [  2.5489,  -0.6162,  -6.8825],\n",
       "         [  2.5489,  -0.6070,  -6.8672],\n",
       "         [  2.1470,  -2.1474,  -4.3677],\n",
       "         [  2.6524,  -2.4222,  -5.4161],\n",
       "         [  1.7737,  -0.7311,  -6.6464],\n",
       "         [  1.7674,  -0.7368,  -6.6099],\n",
       "         [  1.1287,  -0.0197,  -5.7085],\n",
       "         [  1.1304,  -0.0244,  -5.7020],\n",
       "         [  3.4013,  -1.3722,  -7.0985],\n",
       "         [  1.5817,   0.2758,  -8.2503],\n",
       "         [  2.7447,  -2.6223,  -4.8120],\n",
       "         [  2.7388,  -2.6205,  -4.7883],\n",
       "         [  2.5999,  -1.0659,  -6.3521],\n",
       "         [  0.6235,  -3.0539,   1.0598],\n",
       "         [  1.0868,  -0.3522,  -5.1699],\n",
       "         [  1.8407,  -0.6905,  -6.0653],\n",
       "         [  1.8380,  -0.6883,  -6.0680],\n",
       "         [  1.8466,   0.0874,  -6.1721],\n",
       "         [  1.7052,  -1.2978,  -6.4323],\n",
       "         [  3.4809,   0.0402,  -9.3022],\n",
       "         [  3.2532,   0.2093,  -8.9553],\n",
       "         [  3.3826,   0.1142,  -9.0928],\n",
       "         [  3.5288,   0.0246,  -9.3840],\n",
       "         [  2.9728,  -1.6832,  -4.7470],\n",
       "         [  2.0995,  -0.1134,  -9.2397],\n",
       "         [  1.8523,  -2.3903,  -3.8870],\n",
       "         [  1.4342,  -0.2291,  -6.1154],\n",
       "         [  1.4508,  -0.2238,  -6.0979],\n",
       "         [  2.2025,  -2.2803,  -4.0623],\n",
       "         [  1.0497,  -0.8463,  -3.2224],\n",
       "         [  1.9308,  -2.7231,  -2.6224],\n",
       "         [  2.0332,  -0.5145,  -7.7545],\n",
       "         [  2.0332,  -0.5436,  -7.6919],\n",
       "         [  5.2103,  -1.2622, -13.7430],\n",
       "         [  5.1960,  -1.2543, -13.7396],\n",
       "         [  3.8591,  -0.8672,  -9.5545],\n",
       "         [  3.8784,  -1.0363,  -8.1545]]),\n",
       " tensor([[  1.4277,  -0.3295,  -3.5462],\n",
       "         [  2.2785,  -4.1198,  -1.4216],\n",
       "         [  2.3948,  -0.1293,  -9.8200],\n",
       "         [  2.3642,  -2.9965,  -4.2562],\n",
       "         [  2.3690,  -2.9825,  -4.2796],\n",
       "         [  1.0240,  -4.2490,   1.2700],\n",
       "         [  2.3405,  -1.1257,  -5.4081],\n",
       "         [  2.3397,  -1.1381,  -5.3963],\n",
       "         [  2.7560,  -2.0681,  -4.2404],\n",
       "         [  1.6869,  -1.5903,  -5.0040],\n",
       "         [  2.8054,  -0.0758, -11.5361],\n",
       "         [  2.8094,  -0.0656, -11.5934],\n",
       "         [  3.6153,   0.6378, -15.2931],\n",
       "         [  3.5684,   0.7070, -15.3944],\n",
       "         [  3.5843,   0.7052, -15.3694],\n",
       "         [  3.0238,  -0.9812,  -7.4191],\n",
       "         [  3.0629,  -3.9919,  -3.4419],\n",
       "         [  1.9832,  -0.4589,  -6.6597],\n",
       "         [  1.4398,  -0.2557,  -6.3163],\n",
       "         [  2.6779,  -0.6584,  -8.3990],\n",
       "         [  3.6280,  -0.8341,  -9.7248],\n",
       "         [  3.6309,  -0.8254,  -9.7573],\n",
       "         [  3.1677,  -1.6186,  -7.2685],\n",
       "         [  1.1530,  -0.1091,  -5.1389],\n",
       "         [  1.7681,  -0.1105,  -7.8453],\n",
       "         [  1.7648,   0.2753,  -9.2456],\n",
       "         [  1.7487,   0.2402,  -9.0353],\n",
       "         [  1.4827,  -0.2077,  -6.4201],\n",
       "         [  1.4839,  -0.2093,  -6.4193],\n",
       "         [  1.4898,  -0.2215,  -6.3977],\n",
       "         [  1.9006,  -2.6454,  -3.1999],\n",
       "         [  1.7931,  -0.1367,  -7.1238],\n",
       "         [  1.8138,  -0.1890,  -6.9541],\n",
       "         [  1.7999,  -0.1468,  -7.1004],\n",
       "         [  1.4124,  -0.3770,  -5.4341],\n",
       "         [  1.8984,   0.1741,  -9.1638],\n",
       "         [  1.3581,  -0.6107,  -4.7818],\n",
       "         [  1.7976,  -0.6567,  -4.9184],\n",
       "         [  1.6292,   0.4747,  -8.5032],\n",
       "         [  2.1157,  -2.7714,  -3.3165],\n",
       "         [  1.4009,  -0.9594,  -4.5210],\n",
       "         [  1.8642,  -1.8697,  -4.1321],\n",
       "         [  3.7479,  -1.4766,  -7.5866],\n",
       "         [  1.6159,  -0.7657,  -5.6165],\n",
       "         [  1.6224,  -0.7941,  -5.5889],\n",
       "         [  1.6136,  -0.8403,  -5.4605],\n",
       "         [  2.0947,  -1.2780,  -5.1204],\n",
       "         [  1.6921,  -2.2453,  -2.9544],\n",
       "         [  1.6838,  -2.2204,  -2.9595],\n",
       "         [  2.3428,  -0.7927,  -7.6184],\n",
       "         [  2.3300,  -0.7654,  -7.6816],\n",
       "         [  1.3330,  -2.3790,  -2.5891],\n",
       "         [  1.9542,  -0.5150,  -6.5428],\n",
       "         [  1.3871,  -0.2741,  -5.7588],\n",
       "         [  6.2259,  -1.6776, -17.8034],\n",
       "         [  3.1102,  -0.0472, -10.7137],\n",
       "         [  2.6965,  -0.9410,  -6.4175],\n",
       "         [  2.6872,  -0.9310,  -6.4165],\n",
       "         [  2.6865,  -0.9298,  -6.4200],\n",
       "         [  2.6874,  -0.9285,  -6.4288],\n",
       "         [  1.4731,  -1.7724,  -3.2290],\n",
       "         [  2.5137,  -1.2048,  -6.9718],\n",
       "         [  1.9547,  -2.4356,  -2.7782],\n",
       "         [  2.5683,  -0.2816,  -9.7768]]),\n",
       " tensor([[ 2.5681e+00, -2.8383e-01, -9.7706e+00],\n",
       "         [ 2.5736e+00, -2.8118e-01, -9.8024e+00],\n",
       "         [ 1.9769e+00, -3.7429e-01, -6.3823e+00],\n",
       "         [ 1.9866e+00, -3.7687e-01, -6.4247e+00],\n",
       "         [ 2.0011e+00, -4.0632e-01, -6.3822e+00],\n",
       "         [ 2.1463e+00, -1.6273e+00, -5.5995e+00],\n",
       "         [ 4.4251e+00, -9.3589e-01, -9.6885e+00],\n",
       "         [ 2.1239e+00, -4.3575e-01, -7.0713e+00],\n",
       "         [ 2.0276e+00, -1.1424e-01, -8.0074e+00],\n",
       "         [ 1.3110e+00, -1.0116e-02, -6.7407e+00],\n",
       "         [ 1.8453e+00,  2.8056e-01, -8.3939e+00],\n",
       "         [ 1.7494e+00, -1.3610e+00, -4.5569e+00],\n",
       "         [ 9.3888e-01, -2.2335e+00, -5.0859e-01],\n",
       "         [ 1.4823e+00, -2.8522e+00, -1.1680e+00],\n",
       "         [ 1.4970e+00, -2.8473e+00, -1.1741e+00],\n",
       "         [ 2.0634e+00, -2.6895e+00, -2.8461e+00],\n",
       "         [ 2.0548e+00, -2.6886e+00, -2.8284e+00],\n",
       "         [ 1.6633e+00, -4.7741e-01, -4.5775e+00],\n",
       "         [ 2.5273e+00, -6.1677e-01, -6.8077e+00],\n",
       "         [ 2.0682e+00, -3.4644e-01, -6.8517e+00],\n",
       "         [ 1.5977e+00, -1.8535e+00, -3.4319e+00],\n",
       "         [ 2.3861e+00,  2.3591e-02, -1.0709e+01],\n",
       "         [ 2.3841e+00,  2.5069e-02, -1.0710e+01],\n",
       "         [ 2.3784e+00,  3.8556e-02, -1.0739e+01],\n",
       "         [ 6.1421e-01, -3.0372e+00,  1.0575e+00],\n",
       "         [ 1.8407e+00, -7.1429e-01, -5.9689e+00],\n",
       "         [ 3.0312e+00, -2.1466e+00, -6.9824e+00],\n",
       "         [ 2.5686e+00, -2.7740e-01, -9.7952e+00],\n",
       "         [ 2.9924e+00, -6.4188e-01, -8.4370e+00],\n",
       "         [ 1.4919e+00, -2.8359e+00, -1.1977e+00],\n",
       "         [ 2.2948e+00, -1.4700e+00, -4.8214e+00],\n",
       "         [ 2.5157e+00, -1.1916e+00, -6.9922e+00],\n",
       "         [ 2.3019e+00, -1.4470e+00, -4.8757e+00],\n",
       "         [ 2.0580e+00, -2.6942e+00, -2.8425e+00],\n",
       "         [ 9.5646e-01,  5.3059e-01, -7.8085e+00],\n",
       "         [ 1.0656e+00, -1.2107e+00, -3.2606e+00],\n",
       "         [ 3.3412e+00, -7.3242e-01, -9.4884e+00],\n",
       "         [ 2.3908e+00, -1.2271e-01, -9.8120e+00],\n",
       "         [ 1.4524e+00, -3.4718e-01, -3.5648e+00],\n",
       "         [ 2.1432e+00, -2.3367e+00, -4.0738e+00],\n",
       "         [ 1.7766e+00, -7.2332e-01, -6.6682e+00],\n",
       "         [ 1.7768e+00, -7.2333e-01, -6.6701e+00],\n",
       "         [ 1.3431e+00, -1.1809e+00, -3.8546e+00],\n",
       "         [ 2.3219e+00, -7.1430e-01, -6.2989e+00],\n",
       "         [ 9.4074e-01, -2.2321e+00, -5.1141e-01],\n",
       "         [ 9.3834e-01, -2.2364e+00, -5.1435e-01],\n",
       "         [ 1.7972e+00, -2.0599e+00, -5.7906e+00],\n",
       "         [ 1.4605e+00, -1.5492e+00, -3.0955e+00],\n",
       "         [ 6.2028e-01, -3.0453e+00,  1.0570e+00],\n",
       "         [ 6.1817e-01, -3.0298e+00,  1.0480e+00],\n",
       "         [ 2.5125e+00, -2.7966e-01, -8.2176e+00],\n",
       "         [ 1.7625e+00, -1.7572e-01, -7.7446e+00],\n",
       "         [ 1.7568e+00, -1.3402e+00, -4.7909e+00],\n",
       "         [ 1.6960e+00, -4.4727e-01, -5.8965e+00],\n",
       "         [ 1.8990e+00, -8.6947e-01, -5.5794e+00],\n",
       "         [ 2.1253e+00, -6.5677e-02, -8.8917e+00],\n",
       "         [ 1.7021e+00, -1.3103e+00, -6.3986e+00],\n",
       "         [ 1.7028e+00, -1.3286e+00, -6.3829e+00],\n",
       "         [ 2.9951e+00, -6.7247e-02, -1.0119e+01],\n",
       "         [ 2.5785e+00, -2.8401e-01, -9.8122e+00],\n",
       "         [ 2.5717e+00, -2.7211e-01, -9.8263e+00],\n",
       "         [ 1.7665e+00, -8.3086e-01, -4.7419e+00],\n",
       "         [ 2.1046e+00, -1.0037e-01, -9.3101e+00],\n",
       "         [ 3.0704e+00, -1.5601e-02, -1.0675e+01]]),\n",
       " tensor([[  2.8054,  -0.0644, -11.5747],\n",
       "         [  3.4603,  -0.4811,  -9.7058],\n",
       "         [  3.4609,  -0.4780,  -9.7100],\n",
       "         [  3.4498,  -0.4892,  -9.6682],\n",
       "         [  1.9798,  -0.4106,  -7.3228],\n",
       "         [  3.8432,  -1.2171,  -9.3908],\n",
       "         [  3.8482,  -1.1896,  -9.4843],\n",
       "         [  2.5136,  -1.1658,  -7.0534],\n",
       "         [  5.2884,  -0.4505, -16.7403],\n",
       "         [  1.3594,  -0.6241,  -4.7688],\n",
       "         [  2.1887,  -0.5550,  -7.0405],\n",
       "         [  2.1615,  -0.5426,  -6.9286],\n",
       "         [  1.7509,  -1.3547,  -4.5770],\n",
       "         [  3.7640,  -1.4894,  -7.5770],\n",
       "         [  1.4841,  -0.2175,  -6.3888],\n",
       "         [  1.4864,  -0.2170,  -6.4043],\n",
       "         [  1.4545,  -0.2358,  -6.0869],\n",
       "         [  1.6884,  -2.2109,  -3.0028],\n",
       "         [  3.0251,  -0.0554,  -9.9207],\n",
       "         [  1.8922,   0.1817,  -9.1750],\n",
       "         [  1.9005,   0.1689,  -9.1631],\n",
       "         [  1.6994,   0.4458,  -8.6016],\n",
       "         [  0.9796,  -0.2790,  -5.4861],\n",
       "         [  1.4819,  -2.8381,  -1.1630],\n",
       "         [  1.4912,  -2.8688,  -1.1098],\n",
       "         [  1.4900,  -2.8169,  -1.2037],\n",
       "         [  1.4892,  -2.8274,  -1.2208],\n",
       "         [  1.4840,  -2.8262,  -1.2028],\n",
       "         [  1.1588,  -2.7205,  -0.7024],\n",
       "         [  1.9268,  -1.5573,  -4.0322],\n",
       "         [  1.7419,  -1.1408,  -6.1481],\n",
       "         [  1.7369,  -1.2050,  -6.0589],\n",
       "         [  2.3452,  -0.8014,  -7.5894],\n",
       "         [  1.3317,  -2.3805,  -2.5795],\n",
       "         [  2.6948,  -0.9292,  -6.4501],\n",
       "         [  3.1488,  -0.2822, -10.1707],\n",
       "         [  0.9803,   0.4920,  -7.7274],\n",
       "         [  4.2331,  -0.8431,  -8.9962],\n",
       "         [  2.2867,  -2.3237,  -4.6995],\n",
       "         [  0.9996,   1.4994, -11.6463],\n",
       "         [  2.1676,  -2.7129,  -3.1741],\n",
       "         [  1.6159,  -0.7719,  -5.5959],\n",
       "         [  1.0899,  -0.2824,  -5.2964],\n",
       "         [  1.5724,   0.2663,  -8.1856],\n",
       "         [  1.6819,  -0.0253,  -6.7285],\n",
       "         [  2.0514,  -3.2407,  -2.1186],\n",
       "         [  1.6291,   0.0801,  -7.9894],\n",
       "         [  1.4821,  -0.2070,  -6.4231],\n",
       "         [  1.7928,  -4.4025,   0.0194],\n",
       "         [  2.6947,  -0.9346,  -6.4250],\n",
       "         [  3.9097,  -0.5583, -10.1746],\n",
       "         [  2.1843,  -0.5123,  -3.6281],\n",
       "         [  1.0609,  -1.2424,  -3.1936],\n",
       "         [ -1.1350,  -1.2178,   1.9883],\n",
       "         [  1.4071,  -2.8259,  -1.1061],\n",
       "         [  3.3784,  -1.3574,  -7.0788],\n",
       "         [  2.0765,  -0.5591,  -6.3316],\n",
       "         [  1.7791,  -1.8498,  -3.0981],\n",
       "         [  1.4038,  -0.9286,  -4.5803],\n",
       "         [  2.3000,  -1.4203,  -4.9219],\n",
       "         [  1.6094,   0.0839,  -7.9261],\n",
       "         [  4.7282,  -1.0214, -12.5545],\n",
       "         [  2.3075,  -1.4593,  -4.8688],\n",
       "         [  2.3122,  -1.4259,  -4.9335]]),\n",
       " tensor([[ 6.1627e-01, -3.0234e+00,  1.0469e+00],\n",
       "         [ 2.9838e+00, -6.4276e-01, -8.4139e+00],\n",
       "         [ 3.0322e+00, -6.9943e-02, -9.8853e+00],\n",
       "         [ 2.3877e+00, -8.5551e-01, -5.9845e+00],\n",
       "         [ 1.4159e+00, -2.8243e+00, -1.1273e+00],\n",
       "         [ 1.7518e+00, -7.0923e-01, -4.7172e+00],\n",
       "         [ 1.9349e+00, -2.6657e+00, -2.7639e+00],\n",
       "         [ 3.4536e+00, -4.7252e-01, -9.6317e+00],\n",
       "         [ 2.1013e+00, -1.1005e-01, -9.2617e+00],\n",
       "         [ 1.9252e+00, -1.4620e+00, -5.1970e+00],\n",
       "         [ 2.1270e+00,  4.3646e-02, -8.2823e+00],\n",
       "         [ 1.9842e+00, -1.3501e+00, -5.4075e+00],\n",
       "         [ 2.0327e+00, -4.7434e-01, -6.2854e+00],\n",
       "         [ 4.6346e+00,  8.3793e-02, -1.5045e+01],\n",
       "         [ 3.1813e+00, -5.6589e-01, -1.0143e+01],\n",
       "         [ 2.1297e+00,  5.1810e-02, -8.3262e+00],\n",
       "         [ 3.1853e-01, -1.0725e+00, -6.0451e+00],\n",
       "         [ 2.5693e+00, -2.7867e-01, -9.7918e+00],\n",
       "         [ 2.5715e+00, -2.6417e-01, -9.8535e+00],\n",
       "         [ 1.5629e+00,  1.6745e-01, -8.9254e+00],\n",
       "         [ 2.0240e+00, -6.3409e-01, -5.9588e+00],\n",
       "         [ 3.8418e+00, -1.2141e+00, -9.3934e+00],\n",
       "         [ 3.8497e+00, -1.2046e+00, -9.4502e+00],\n",
       "         [ 1.5101e+00, -8.3148e-02, -6.2814e+00],\n",
       "         [ 1.9368e+00, -3.6018e-01, -6.6256e+00],\n",
       "         [ 1.5703e+00, -1.3054e+00, -4.8477e+00],\n",
       "         [ 4.4219e+00, -9.6346e-01, -9.6740e+00],\n",
       "         [ 4.4827e+00, -9.6581e-01, -9.8558e+00],\n",
       "         [ 2.5315e+00, -1.0806e+00, -5.8492e+00],\n",
       "         [ 1.3244e+00,  7.2533e-03, -6.8800e+00],\n",
       "         [ 9.4806e-01,  4.6305e-01, -7.1759e+00],\n",
       "         [ 9.0228e-01,  5.8950e-01, -7.5677e+00],\n",
       "         [ 2.6707e+00, -6.5654e-01, -8.3774e+00],\n",
       "         [ 2.4523e+00, -7.1611e-01, -3.9290e+00],\n",
       "         [ 1.4859e+00, -2.1236e-01, -6.4115e+00],\n",
       "         [ 4.5448e+00, -9.0693e-01, -1.2973e+01],\n",
       "         [ 2.1998e+00, -2.2553e+00, -4.0940e+00],\n",
       "         [ 1.9699e+00, -4.3812e-01, -6.6673e+00],\n",
       "         [ 1.9717e+00, -4.3897e-01, -6.6664e+00],\n",
       "         [ 1.8521e+00, -2.4200e+00, -3.8364e+00],\n",
       "         [ 1.6316e+00, -1.9603e+00, -2.6584e+00],\n",
       "         [ 1.6242e+00, -1.9688e+00, -2.6279e+00],\n",
       "         [ 2.0640e+00, -2.2840e+00, -3.2440e+00],\n",
       "         [ 1.4036e+00, -9.2934e-01, -4.5763e+00],\n",
       "         [ 2.0905e+00, -1.2868e+00, -5.0954e+00],\n",
       "         [ 1.6906e+00, -2.2050e+00, -3.0131e+00],\n",
       "         [ 1.6876e+00, -2.2244e+00, -2.9702e+00],\n",
       "         [ 1.6896e+00, -2.2275e+00, -2.9752e+00],\n",
       "         [ 1.6806e+00, -2.1798e+00, -3.0043e+00],\n",
       "         [ 1.6896e+00, -2.2007e+00, -3.0127e+00],\n",
       "         [ 1.6910e+00, -2.2263e+00, -2.9826e+00],\n",
       "         [ 1.4868e+00, -2.8391e+00, -1.1775e+00],\n",
       "         [ 1.4899e+00, -2.8457e+00, -1.1532e+00],\n",
       "         [ 1.4800e+00, -2.8211e+00, -1.1871e+00],\n",
       "         [ 1.4758e+00, -2.7998e+00, -1.2116e+00],\n",
       "         [ 1.4722e+00, -2.8470e+00, -1.1433e+00],\n",
       "         [ 1.4928e+00, -2.8068e+00, -1.2383e+00],\n",
       "         [ 1.4864e+00, -2.8019e+00, -1.2351e+00],\n",
       "         [ 1.4864e+00, -2.8337e+00, -1.1796e+00],\n",
       "         [ 1.4875e+00, -2.8180e+00, -1.2080e+00],\n",
       "         [ 1.5106e+00, -2.8327e+00, -1.1789e+00],\n",
       "         [ 2.3297e+00, -7.9077e-01, -7.5836e+00],\n",
       "         [ 2.2442e+00,  7.3262e-01, -1.1835e+01],\n",
       "         [ 2.0583e+00, -3.4271e-01, -6.8517e+00]]),\n",
       " tensor([[ 2.0838e+00, -5.5501e-01, -6.3617e+00],\n",
       "         [ 1.3308e+00, -2.3651e+00, -2.6043e+00],\n",
       "         [ 1.3324e+00, -2.3812e+00, -2.5865e+00],\n",
       "         [ 1.9183e+00, -1.5502e+00, -4.0559e+00],\n",
       "         [ 1.9358e+00, -1.5754e+00, -4.0502e+00],\n",
       "         [ 2.2256e+00,  7.3730e-02, -9.6849e+00],\n",
       "         [ 2.0558e+00, -2.6793e+00, -2.8463e+00],\n",
       "         [ 2.4634e+00, -2.8780e+00, -3.6555e+00],\n",
       "         [ 2.4597e+00, -2.8698e+00, -3.6582e+00],\n",
       "         [ 2.3667e+00, -2.9907e+00, -4.2856e+00],\n",
       "         [ 1.4843e+00, -2.8492e+00, -2.1811e+00],\n",
       "         [ 2.0596e+00, -2.6956e+00, -2.8320e+00],\n",
       "         [ 2.0464e+00, -2.6813e+00, -2.8333e+00],\n",
       "         [ 2.0472e+00, -2.6920e+00, -2.8249e+00],\n",
       "         [ 2.0558e+00, -2.6879e+00, -2.8331e+00],\n",
       "         [ 1.9935e+00, -4.3550e+00, -5.7242e-01],\n",
       "         [ 2.1124e+00, -8.4982e-01, -5.1939e+00],\n",
       "         [ 1.5889e+00, -2.6254e-01, -6.9725e+00],\n",
       "         [ 1.7424e+00, -2.4012e+00, -2.5361e+00],\n",
       "         [ 2.7411e+00, -2.0668e+00, -4.2182e+00],\n",
       "         [ 9.4606e-01,  9.6283e-01, -9.9213e+00],\n",
       "         [ 2.1914e+00, -3.3697e-02, -9.9427e+00],\n",
       "         [ 2.1987e+00, -4.2918e-02, -9.9196e+00],\n",
       "         [ 2.2055e+00, -4.2416e-02, -9.9589e+00],\n",
       "         [ 2.2000e+00, -2.5525e-02, -1.0014e+01],\n",
       "         [ 2.1319e+00, -1.3278e+00, -6.6667e+00],\n",
       "         [ 3.3768e+00, -1.3586e+00, -7.0643e+00],\n",
       "         [ 1.8361e+00, -1.7401e+00, -3.9992e+00],\n",
       "         [ 2.0016e+00, -3.2160e+00, -2.1933e+00],\n",
       "         [ 2.0030e+00, -3.1448e+00, -2.3108e+00],\n",
       "         [ 1.8944e+00, -4.5011e+00,  1.2587e-01],\n",
       "         [ 2.1142e+00, -1.7205e+00, -4.9034e+00],\n",
       "         [ 1.9704e+00, -2.0574e+00, -3.4754e+00],\n",
       "         [ 2.7383e+00, -2.6013e+00, -4.8150e+00],\n",
       "         [ 2.7409e+00, -2.6312e+00, -4.7777e+00],\n",
       "         [ 3.0537e+00, -3.7123e+00, -3.9195e+00],\n",
       "         [ 2.7675e+00, -1.2885e+00, -6.0238e+00],\n",
       "         [ 2.0407e+00, -2.6600e+00, -2.9035e+00],\n",
       "         [ 1.6424e+00, -1.2262e-02, -6.6385e+00],\n",
       "         [ 1.9497e+00,  5.1979e-01, -1.0521e+01],\n",
       "         [ 1.9503e+00,  5.2574e-01, -1.0542e+01],\n",
       "         [ 1.9426e+00,  5.4168e-01, -1.0541e+01],\n",
       "         [ 1.8615e+00, -2.1799e+00, -3.4835e+00],\n",
       "         [ 1.5191e+00, -4.8493e-01, -5.6068e+00],\n",
       "         [ 1.4147e+00, -1.4705e-01, -6.5263e+00],\n",
       "         [ 1.4102e+00, -1.3355e-01, -6.5732e+00],\n",
       "         [ 1.9603e+00, -2.5396e+00, -3.5825e+00],\n",
       "         [ 1.9578e+00, -2.5025e+00, -3.6330e+00],\n",
       "         [ 1.9577e+00, -2.4821e+00, -3.6596e+00],\n",
       "         [ 3.1769e+00, -2.6143e-01, -7.0604e+00],\n",
       "         [ 5.8553e+00, -2.6554e+00, -1.2510e+01],\n",
       "         [ 1.2398e+00, -4.8557e-01, -5.4809e+00],\n",
       "         [ 1.2394e+00, -5.1296e-01, -5.3879e+00],\n",
       "         [ 1.8457e+00, -7.0157e-01, -6.0436e+00],\n",
       "         [ 1.7008e+00, -1.2683e+00, -6.4309e+00],\n",
       "         [ 2.1209e+00, -9.4919e-01, -6.3256e+00],\n",
       "         [ 1.8982e+00, -8.4193e-01, -5.6375e+00],\n",
       "         [ 1.8983e+00, -8.6762e-01, -5.5907e+00],\n",
       "         [ 2.9834e+00, -1.3224e-02, -1.0277e+01],\n",
       "         [ 3.0130e+00, -5.8231e-02, -1.0197e+01],\n",
       "         [ 2.9942e+00, -6.1426e-02, -1.0147e+01],\n",
       "         [ 2.2546e+00, -2.0012e+00, -4.8734e+00],\n",
       "         [ 3.3800e+00,  9.7543e-02, -9.1392e+00],\n",
       "         [ 2.2202e+00, -2.0145e+00, -4.7287e+00]]),\n",
       " tensor([[ 2.1291, -0.0683, -8.9106],\n",
       "         [ 2.1169, -0.0702, -8.8544]])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "366394d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7362])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_list = torch.cat(pred_label,dim=-1)\n",
    "pred_label_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c43e733",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7362, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_prob_list = torch.cat(pred_prob,dim=0)\n",
    "pred_prob_list.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3bafe611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7362"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_label = [y for X,y in testing_data]\n",
    "len(true_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a62ec3ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "      <th>true</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7357</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7358</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7359</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7360</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7361</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7362 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred  true\n",
       "0        0     2\n",
       "1        0     2\n",
       "2        0     2\n",
       "3        0     2\n",
       "4        0     1\n",
       "...    ...   ...\n",
       "7357     0     0\n",
       "7358     0     0\n",
       "7359     0     0\n",
       "7360     0     0\n",
       "7361     0     0\n",
       "\n",
       "[7362 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_comp = pd.DataFrame({'pred': pred_label_list.tolist(),'true':true_label})\n",
    "pred_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b61919c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>true</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3521</td>\n",
       "      <td>1978</td>\n",
       "      <td>572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33</td>\n",
       "      <td>1085</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57</td>\n",
       "      <td>22</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "true     0     1    2\n",
       "pred                 \n",
       "0     3521  1978  572\n",
       "1       33  1085   62\n",
       "2       57    22   32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(pred_comp['pred'],pred_comp['true'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a18eb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_comp.to_csv('pred_results.300epochs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85aac82",
   "metadata": {},
   "source": [
    "## Test for each epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3731087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, optimizer, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    patho_size = len([y for x,y in dataloader.dataset if y > 0])\n",
    "    n_total_steps = len(dataloader)\n",
    "    step_loss = []\n",
    "    train_loss, correct, patho_correct = 0, 0, 0\n",
    "    for i, (X,y) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred,y)\n",
    "\n",
    "        # Backprop and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #record loss\n",
    "        #step_loss.append(loss.item())\n",
    "        #if (i+1) % 100 == 0:\n",
    "        #    print(f'Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "        nonzero_indices = torch.nonzero(y,as_tuple=True)\n",
    "        #print(y)\n",
    "        #print(nonzero_indices)\n",
    "        #print((pred.argmax(1) == y).type(torch.float)[nonzero_indices])\n",
    "        #print((pred.argmax(1) == y).type(torch.float))\n",
    "        patho_correct += (pred.argmax(1) == y).type(torch.float)[nonzero_indices].sum().item()\n",
    "        \n",
    "    train_loss /= n_total_steps\n",
    "    correct /= size\n",
    "    patho_correct /= patho_size\n",
    "    print(f\"Train Error: \\n Accuracy: {(100*correct):>0.1f}%, Patho accuracy: {(100*patho_correct):>0.1f}%, Avg loss: {train_loss:>8f} \\n\")            \n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0f71dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    patho_size = len([y for x,y in dataloader.dataset if y > 0])\n",
    "    model.eval()\n",
    "    test_loss, correct, patho_correct = 0, 0, 0\n",
    "    #pred_prob, pred_label = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            nonzero_indices = torch.nonzero(y,as_tuple=True)\n",
    "            patho_correct += (pred.argmax(1) == y).type(torch.float)[nonzero_indices].sum().item()\n",
    "            #pred_prob.append(pred)\n",
    "            #pred_label.append(pred.argmax(1))\n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    patho_correct /= patho_size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Patho accuracy: {(100*patho_correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6790deba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_predictions(prefix, dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    pred_prob, pred_label = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            pred_prob.append(pred)\n",
    "            pred_label.append(pred.argmax(1))\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    \n",
    "    pred_label_list = torch.cat(pred_label,dim=-1)\n",
    "    pred_prob_list = torch.cat(pred_prob,dim=0)\n",
    "    pred_comp = pd.DataFrame({'pred': pred_label_list.tolist(),'true':[y for X,y in dataloader.dataset]})\n",
    "    pred_comp.to_csv('pred_results.%s.csv' % prefix,index=False)\n",
    "    pred_prob = pd.DataFrame(pred_prob_list.numpy(),columns=['Benign','LoF','GoF'])\n",
    "    pred_prob.to_csv('pred_proba.%s.csv' % prefix,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c346b3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, optimizer, loss\n",
    "model = NeuralNetwork(INPUT_DIM, Z_DIM, H1_DIM, H2_DIM).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR_RATE)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1fa2de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 74.9%, Patho accuracy: 90.9%, Avg loss: 45.938483 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Patho accuracy: 0.0%, Avg loss: 94.593339 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 54.6%, Patho accuracy: 26.8%, Avg loss: 57.414889 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Patho accuracy: 0.0%, Avg loss: 80.302278 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 55.3%, Patho accuracy: 26.0%, Avg loss: 54.396529 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Patho accuracy: 0.0%, Avg loss: 75.676187 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 56.6%, Patho accuracy: 26.1%, Avg loss: 52.549842 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Patho accuracy: 0.0%, Avg loss: 74.461877 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 57.3%, Patho accuracy: 26.4%, Avg loss: 51.983817 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Patho accuracy: 0.0%, Avg loss: 75.250164 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 58.4%, Patho accuracy: 29.3%, Avg loss: 51.523089 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Patho accuracy: 0.0%, Avg loss: 72.152503 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 59.8%, Patho accuracy: 27.7%, Avg loss: 50.243538 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.0%, Patho accuracy: 0.0%, Avg loss: 75.209044 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 60.9%, Patho accuracy: 29.5%, Avg loss: 51.033554 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.2%, Patho accuracy: 0.3%, Avg loss: 69.933699 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 62.1%, Patho accuracy: 32.0%, Avg loss: 49.257000 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 49.7%, Patho accuracy: 1.3%, Avg loss: 69.926336 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 32.9%, Avg loss: 48.796915 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 50.4%, Patho accuracy: 2.6%, Avg loss: 69.532933 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 35.3%, Avg loss: 48.201008 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.1%, Patho accuracy: 4.1%, Avg loss: 69.306112 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 36.5%, Avg loss: 47.702317 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 51.7%, Patho accuracy: 5.2%, Avg loss: 68.739852 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 64.9%, Patho accuracy: 38.2%, Avg loss: 47.176958 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Patho accuracy: 5.9%, Avg loss: 68.463233 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 65.4%, Patho accuracy: 39.4%, Avg loss: 46.638031 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.4%, Patho accuracy: 6.7%, Avg loss: 68.098127 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 66.0%, Patho accuracy: 40.3%, Avg loss: 45.961851 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 52.0%, Patho accuracy: 5.8%, Avg loss: 71.300736 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 66.2%, Patho accuracy: 42.7%, Avg loss: 46.181139 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.0%, Patho accuracy: 7.9%, Avg loss: 67.507731 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 67.0%, Patho accuracy: 42.4%, Avg loss: 44.897026 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 53.2%, Patho accuracy: 8.3%, Avg loss: 69.095888 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 67.4%, Patho accuracy: 43.8%, Avg loss: 44.965879 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.0%, Patho accuracy: 9.9%, Avg loss: 67.134879 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 67.8%, Patho accuracy: 43.9%, Avg loss: 43.986334 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.3%, Patho accuracy: 10.3%, Avg loss: 68.538220 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 68.1%, Patho accuracy: 44.8%, Avg loss: 43.831336 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.6%, Patho accuracy: 11.0%, Avg loss: 67.751146 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 69.2%, Patho accuracy: 46.6%, Avg loss: 43.157358 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 54.9%, Patho accuracy: 11.7%, Avg loss: 68.239459 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 69.8%, Patho accuracy: 48.3%, Avg loss: 42.836583 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.1%, Patho accuracy: 12.1%, Avg loss: 68.362431 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 70.9%, Patho accuracy: 50.5%, Avg loss: 42.400151 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Patho accuracy: 12.5%, Avg loss: 68.297587 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 71.3%, Patho accuracy: 51.0%, Avg loss: 41.869491 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.3%, Patho accuracy: 12.6%, Avg loss: 68.880304 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 71.6%, Patho accuracy: 51.4%, Avg loss: 41.540882 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.5%, Patho accuracy: 13.0%, Avg loss: 68.738352 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 71.9%, Patho accuracy: 51.8%, Avg loss: 41.160707 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.6%, Patho accuracy: 13.2%, Avg loss: 68.803159 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 72.2%, Patho accuracy: 51.8%, Avg loss: 40.642904 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.7%, Patho accuracy: 13.4%, Avg loss: 69.635021 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 72.7%, Patho accuracy: 52.7%, Avg loss: 40.377334 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.9%, Patho accuracy: 13.8%, Avg loss: 69.465850 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 72.9%, Patho accuracy: 53.2%, Avg loss: 40.049503 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.0%, Patho accuracy: 14.0%, Avg loss: 69.431785 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 73.3%, Patho accuracy: 53.9%, Avg loss: 39.550028 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.2%, Patho accuracy: 14.3%, Avg loss: 70.081143 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 73.6%, Patho accuracy: 54.5%, Avg loss: 38.949445 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 55.4%, Patho accuracy: 12.9%, Avg loss: 76.999350 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 73.9%, Patho accuracy: 55.4%, Avg loss: 39.663587 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.3%, Patho accuracy: 14.6%, Avg loss: 71.431913 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 74.1%, Patho accuracy: 55.9%, Avg loss: 38.629203 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Patho accuracy: 15.1%, Avg loss: 71.160599 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 74.4%, Patho accuracy: 56.2%, Avg loss: 38.205696 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.6%, Patho accuracy: 15.3%, Avg loss: 71.462695 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 74.5%, Patho accuracy: 56.4%, Avg loss: 37.875118 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.8%, Patho accuracy: 15.5%, Avg loss: 71.621228 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 74.7%, Patho accuracy: 56.7%, Avg loss: 37.642959 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Patho accuracy: 15.8%, Avg loss: 71.488248 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 74.9%, Patho accuracy: 57.1%, Avg loss: 37.302174 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 56.9%, Patho accuracy: 15.9%, Avg loss: 71.795304 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 75.1%, Patho accuracy: 57.5%, Avg loss: 37.043511 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Patho accuracy: 16.0%, Avg loss: 71.928245 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 75.3%, Patho accuracy: 57.9%, Avg loss: 36.719732 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.0%, Patho accuracy: 16.1%, Avg loss: 72.330564 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 75.4%, Patho accuracy: 58.1%, Avg loss: 36.463012 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Patho accuracy: 16.3%, Avg loss: 72.499506 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 75.5%, Patho accuracy: 58.3%, Avg loss: 36.149322 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.2%, Patho accuracy: 16.4%, Avg loss: 72.850765 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 75.6%, Patho accuracy: 58.7%, Avg loss: 35.921288 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Patho accuracy: 16.7%, Avg loss: 72.980574 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 75.8%, Patho accuracy: 59.0%, Avg loss: 35.618035 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Patho accuracy: 16.8%, Avg loss: 73.249085 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 75.9%, Patho accuracy: 59.3%, Avg loss: 35.308577 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.5%, Patho accuracy: 17.0%, Avg loss: 73.884730 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 76.3%, Patho accuracy: 60.0%, Avg loss: 35.088252 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.6%, Patho accuracy: 17.1%, Avg loss: 73.865167 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 76.7%, Patho accuracy: 60.9%, Avg loss: 34.795660 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Patho accuracy: 17.3%, Avg loss: 74.539892 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 76.8%, Patho accuracy: 61.3%, Avg loss: 34.620862 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.7%, Patho accuracy: 17.4%, Avg loss: 74.774844 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 77.0%, Patho accuracy: 61.6%, Avg loss: 34.287349 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Patho accuracy: 17.5%, Avg loss: 75.034354 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 77.1%, Patho accuracy: 62.0%, Avg loss: 34.165750 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.8%, Patho accuracy: 17.6%, Avg loss: 74.996428 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 77.3%, Patho accuracy: 62.4%, Avg loss: 33.920270 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Patho accuracy: 17.8%, Avg loss: 75.401290 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 77.5%, Patho accuracy: 62.7%, Avg loss: 33.653495 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Patho accuracy: 17.9%, Avg loss: 75.667653 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 77.8%, Patho accuracy: 63.1%, Avg loss: 33.189761 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.4%, Patho accuracy: 16.8%, Avg loss: 82.420092 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 77.6%, Patho accuracy: 62.8%, Avg loss: 33.630773 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 57.9%, Patho accuracy: 17.8%, Avg loss: 77.936507 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 78.4%, Patho accuracy: 64.6%, Avg loss: 33.021771 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.0%, Patho accuracy: 18.0%, Avg loss: 77.780390 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 78.6%, Patho accuracy: 65.0%, Avg loss: 32.821671 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.1%, Patho accuracy: 18.2%, Avg loss: 77.929027 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 78.8%, Patho accuracy: 65.2%, Avg loss: 32.600607 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.2%, Patho accuracy: 18.3%, Avg loss: 78.371641 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 78.9%, Patho accuracy: 65.4%, Avg loss: 32.493158 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Patho accuracy: 18.5%, Avg loss: 78.380362 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 79.1%, Patho accuracy: 65.5%, Avg loss: 32.394554 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.4%, Patho accuracy: 18.7%, Avg loss: 78.299896 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 79.2%, Patho accuracy: 65.9%, Avg loss: 32.287999 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.3%, Patho accuracy: 18.8%, Avg loss: 78.408368 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 79.3%, Patho accuracy: 65.8%, Avg loss: 32.123867 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Patho accuracy: 19.1%, Avg loss: 78.647572 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 79.4%, Patho accuracy: 66.0%, Avg loss: 31.999866 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Patho accuracy: 19.3%, Avg loss: 78.849908 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 79.6%, Patho accuracy: 66.2%, Avg loss: 31.809109 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.5%, Patho accuracy: 19.4%, Avg loss: 79.031386 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 79.7%, Patho accuracy: 66.3%, Avg loss: 31.632614 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Patho accuracy: 19.5%, Avg loss: 79.444625 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 79.8%, Patho accuracy: 66.3%, Avg loss: 31.485600 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Patho accuracy: 19.5%, Avg loss: 79.713141 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 79.9%, Patho accuracy: 66.4%, Avg loss: 31.268221 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.6%, Patho accuracy: 19.6%, Avg loss: 80.015250 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.0%, Patho accuracy: 66.5%, Avg loss: 31.081939 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Patho accuracy: 19.6%, Avg loss: 80.343726 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.1%, Patho accuracy: 66.6%, Avg loss: 30.924349 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.7%, Patho accuracy: 19.8%, Avg loss: 80.641705 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.1%, Patho accuracy: 66.8%, Avg loss: 30.813095 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Patho accuracy: 19.9%, Avg loss: 80.799680 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.2%, Patho accuracy: 66.9%, Avg loss: 30.625716 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Patho accuracy: 19.9%, Avg loss: 81.235004 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.3%, Patho accuracy: 66.9%, Avg loss: 30.467631 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.8%, Patho accuracy: 20.0%, Avg loss: 81.596650 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.4%, Patho accuracy: 67.0%, Avg loss: 30.340721 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Patho accuracy: 20.0%, Avg loss: 81.725018 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.4%, Patho accuracy: 67.2%, Avg loss: 30.222918 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 58.9%, Patho accuracy: 20.2%, Avg loss: 81.808365 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.6%, Patho accuracy: 67.4%, Avg loss: 30.074155 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Patho accuracy: 20.3%, Avg loss: 82.125448 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.7%, Patho accuracy: 67.6%, Avg loss: 30.045577 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.0%, Patho accuracy: 20.4%, Avg loss: 81.980625 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.6%, Patho accuracy: 67.5%, Avg loss: 29.933424 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Patho accuracy: 20.4%, Avg loss: 82.330582 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.7%, Patho accuracy: 67.6%, Avg loss: 29.887232 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.1%, Patho accuracy: 20.6%, Avg loss: 82.330502 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.8%, Patho accuracy: 67.7%, Avg loss: 29.722462 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.2%, Patho accuracy: 20.6%, Avg loss: 82.647509 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 80.9%, Patho accuracy: 68.0%, Avg loss: 29.611911 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Patho accuracy: 20.8%, Avg loss: 82.813650 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.0%, Patho accuracy: 68.1%, Avg loss: 29.461088 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.3%, Patho accuracy: 20.9%, Avg loss: 83.168741 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.1%, Patho accuracy: 68.2%, Avg loss: 29.343390 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.4%, Patho accuracy: 21.1%, Avg loss: 83.321644 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.1%, Patho accuracy: 68.2%, Avg loss: 29.240469 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.5%, Patho accuracy: 21.3%, Avg loss: 83.615086 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.1%, Patho accuracy: 68.2%, Avg loss: 29.130421 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Patho accuracy: 21.4%, Avg loss: 83.823061 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.1%, Patho accuracy: 68.3%, Avg loss: 29.062117 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Patho accuracy: 21.5%, Avg loss: 84.127207 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.1%, Patho accuracy: 68.3%, Avg loss: 28.887501 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.6%, Patho accuracy: 21.5%, Avg loss: 84.784021 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.2%, Patho accuracy: 68.5%, Avg loss: 28.791119 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.7%, Patho accuracy: 21.8%, Avg loss: 85.006288 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.2%, Patho accuracy: 68.5%, Avg loss: 28.677570 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Patho accuracy: 21.8%, Avg loss: 85.261085 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 81.3%, Patho accuracy: 68.6%, Avg loss: 28.587267 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.8%, Patho accuracy: 21.9%, Avg loss: 85.501179 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.4%, Patho accuracy: 68.8%, Avg loss: 28.469662 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Patho accuracy: 22.0%, Avg loss: 85.847980 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.6%, Patho accuracy: 69.1%, Avg loss: 28.371982 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Patho accuracy: 22.1%, Avg loss: 86.173006 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.7%, Patho accuracy: 69.6%, Avg loss: 28.251731 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 59.9%, Patho accuracy: 22.1%, Avg loss: 86.664289 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.8%, Patho accuracy: 69.9%, Avg loss: 28.076727 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Patho accuracy: 22.3%, Avg loss: 87.148821 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.9%, Patho accuracy: 69.9%, Avg loss: 27.971087 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.0%, Patho accuracy: 22.3%, Avg loss: 87.312556 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 81.9%, Patho accuracy: 70.1%, Avg loss: 27.891780 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Patho accuracy: 22.4%, Avg loss: 87.260619 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.0%, Patho accuracy: 70.1%, Avg loss: 27.757968 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.1%, Patho accuracy: 22.5%, Avg loss: 87.872469 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.0%, Patho accuracy: 70.2%, Avg loss: 27.671336 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Patho accuracy: 22.6%, Avg loss: 88.274383 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.1%, Patho accuracy: 70.4%, Avg loss: 27.612433 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 22.9%, Avg loss: 88.376746 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.1%, Patho accuracy: 70.4%, Avg loss: 27.565308 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Patho accuracy: 23.0%, Avg loss: 88.483628 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.1%, Patho accuracy: 70.4%, Avg loss: 27.497649 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Patho accuracy: 23.1%, Avg loss: 88.723970 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.2%, Patho accuracy: 70.5%, Avg loss: 27.419361 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Patho accuracy: 23.2%, Avg loss: 88.723086 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.2%, Patho accuracy: 70.5%, Avg loss: 27.380533 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Patho accuracy: 23.3%, Avg loss: 88.850842 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.2%, Patho accuracy: 70.6%, Avg loss: 27.301963 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Patho accuracy: 23.3%, Avg loss: 88.954980 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.2%, Patho accuracy: 70.5%, Avg loss: 27.265375 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Patho accuracy: 23.4%, Avg loss: 89.368032 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.3%, Patho accuracy: 70.6%, Avg loss: 27.211044 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.4%, Avg loss: 89.284067 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.3%, Patho accuracy: 70.7%, Avg loss: 27.185126 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.5%, Avg loss: 89.328081 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.3%, Patho accuracy: 70.7%, Avg loss: 27.158492 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.6%, Avg loss: 89.210999 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.3%, Patho accuracy: 70.7%, Avg loss: 27.148715 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Patho accuracy: 23.8%, Avg loss: 89.042348 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.7%, Patho accuracy: 71.5%, Avg loss: 27.123649 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Patho accuracy: 23.9%, Avg loss: 88.960006 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.8%, Patho accuracy: 71.6%, Avg loss: 27.052917 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Patho accuracy: 24.1%, Avg loss: 88.783457 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.8%, Patho accuracy: 71.7%, Avg loss: 27.002498 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Patho accuracy: 24.1%, Avg loss: 88.967276 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.9%, Patho accuracy: 71.8%, Avg loss: 26.972898 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Patho accuracy: 24.3%, Avg loss: 88.546722 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 82.9%, Patho accuracy: 71.9%, Avg loss: 26.940771 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Patho accuracy: 24.2%, Avg loss: 88.477568 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 71.9%, Avg loss: 26.937699 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Patho accuracy: 24.3%, Avg loss: 88.030258 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 71.9%, Avg loss: 26.928517 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Patho accuracy: 24.2%, Avg loss: 87.889638 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 71.9%, Avg loss: 26.932192 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Patho accuracy: 24.3%, Avg loss: 87.243211 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 71.9%, Avg loss: 26.841975 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Patho accuracy: 24.4%, Avg loss: 87.596804 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.3%, Avg loss: 26.963588 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Patho accuracy: 24.6%, Avg loss: 86.779319 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.3%, Avg loss: 26.905546 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Patho accuracy: 24.6%, Avg loss: 86.550834 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 72.1%, Avg loss: 26.884544 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Patho accuracy: 24.6%, Avg loss: 86.459073 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 72.2%, Avg loss: 26.898256 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Patho accuracy: 24.8%, Avg loss: 86.257045 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.3%, Avg loss: 26.832513 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Patho accuracy: 24.8%, Avg loss: 86.308006 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.824378 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Patho accuracy: 24.9%, Avg loss: 86.092897 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.827155 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Patho accuracy: 24.9%, Avg loss: 85.533003 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.804343 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Patho accuracy: 25.0%, Avg loss: 85.537530 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.809739 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Patho accuracy: 25.1%, Avg loss: 85.114514 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.733688 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Patho accuracy: 25.1%, Avg loss: 85.317018 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.1%, Avg loss: 26.743948 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Patho accuracy: 25.1%, Avg loss: 85.083439 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.684455 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Patho accuracy: 25.1%, Avg loss: 85.467635 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.692799 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 25.3%, Avg loss: 85.330115 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.3%, Avg loss: 26.646426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 25.2%, Avg loss: 85.710611 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 72.1%, Avg loss: 26.611547 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 25.3%, Avg loss: 85.536887 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 72.0%, Avg loss: 26.559808 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Patho accuracy: 25.2%, Avg loss: 86.186792 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.560217 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Patho accuracy: 25.5%, Avg loss: 85.861690 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 71.9%, Avg loss: 26.461828 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Patho accuracy: 25.5%, Avg loss: 86.625903 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 72.0%, Avg loss: 26.380021 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.5%, Avg loss: 86.808547 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.2%, Avg loss: 26.401529 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.8%, Avg loss: 86.311862 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 72.0%, Avg loss: 26.269475 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.8%, Avg loss: 86.866486 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 72.0%, Avg loss: 26.184206 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.7%, Avg loss: 87.910982 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.2%, Patho accuracy: 72.3%, Avg loss: 26.157246 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.9%, Avg loss: 87.728839 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.1%, Avg loss: 26.016771 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.9%, Avg loss: 88.191318 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.1%, Avg loss: 25.933953 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 26.0%, Avg loss: 88.944437 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.0%, Patho accuracy: 71.9%, Avg loss: 25.886809 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.9%, Avg loss: 88.628167 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.0%, Avg loss: 25.837266 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 26.0%, Avg loss: 89.156479 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.0%, Avg loss: 25.758403 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 26.0%, Avg loss: 89.625716 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.1%, Patho accuracy: 72.0%, Avg loss: 25.664012 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.9%, Avg loss: 89.974132 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.2%, Patho accuracy: 72.2%, Avg loss: 25.611033 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.9%, Avg loss: 90.350450 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.2%, Patho accuracy: 72.2%, Avg loss: 25.489105 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 25.9%, Avg loss: 91.101422 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.3%, Patho accuracy: 72.4%, Avg loss: 25.385254 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Patho accuracy: 25.8%, Avg loss: 91.977992 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.2%, Patho accuracy: 72.4%, Avg loss: 25.228878 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 25.6%, Avg loss: 92.984352 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.3%, Patho accuracy: 72.5%, Avg loss: 25.138609 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 25.5%, Avg loss: 93.360791 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.4%, Patho accuracy: 72.6%, Avg loss: 25.057255 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.4%, Patho accuracy: 25.4%, Avg loss: 93.694528 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.5%, Patho accuracy: 72.9%, Avg loss: 24.965708 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.3%, Patho accuracy: 25.2%, Avg loss: 94.509305 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.6%, Patho accuracy: 73.1%, Avg loss: 24.822042 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.2%, Patho accuracy: 24.9%, Avg loss: 95.401697 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.7%, Patho accuracy: 73.1%, Avg loss: 24.699888 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Patho accuracy: 24.7%, Avg loss: 96.692257 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.7%, Patho accuracy: 73.3%, Avg loss: 24.561383 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Patho accuracy: 24.7%, Avg loss: 97.664562 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.8%, Patho accuracy: 73.4%, Avg loss: 24.387314 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Patho accuracy: 24.6%, Avg loss: 98.901705 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.8%, Patho accuracy: 73.5%, Avg loss: 24.248239 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.0%, Patho accuracy: 24.5%, Avg loss: 99.732525 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 83.8%, Patho accuracy: 73.6%, Avg loss: 24.062055 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Patho accuracy: 24.4%, Avg loss: 100.735822 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.0%, Patho accuracy: 74.1%, Avg loss: 23.836237 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Patho accuracy: 24.3%, Avg loss: 101.259000 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.1%, Patho accuracy: 74.1%, Avg loss: 23.669014 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Patho accuracy: 24.2%, Avg loss: 102.364506 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.2%, Patho accuracy: 74.3%, Avg loss: 23.515006 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Patho accuracy: 24.1%, Avg loss: 103.520241 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.3%, Patho accuracy: 74.6%, Avg loss: 23.386350 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Patho accuracy: 23.9%, Avg loss: 104.683660 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.3%, Patho accuracy: 74.7%, Avg loss: 23.233911 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.9%, Avg loss: 105.612464 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.4%, Patho accuracy: 74.8%, Avg loss: 23.165526 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.9%, Avg loss: 106.219098 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.4%, Patho accuracy: 74.9%, Avg loss: 23.012468 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.9%, Avg loss: 106.651519 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.6%, Patho accuracy: 75.1%, Avg loss: 22.790011 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.7%, Avg loss: 107.450183 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.7%, Patho accuracy: 75.3%, Avg loss: 22.651910 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Patho accuracy: 23.7%, Avg loss: 108.443324 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.9%, Patho accuracy: 75.6%, Avg loss: 22.509587 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.5%, Patho accuracy: 23.6%, Avg loss: 109.472774 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.9%, Patho accuracy: 75.9%, Avg loss: 22.297481 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Patho accuracy: 23.5%, Avg loss: 110.562755 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 76.2%, Avg loss: 22.102639 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.2%, Avg loss: 111.713130 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.2%, Patho accuracy: 76.6%, Avg loss: 22.018767 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Patho accuracy: 23.3%, Avg loss: 112.800301 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.3%, Patho accuracy: 76.9%, Avg loss: 21.932812 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.2%, Avg loss: 113.008998 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 85.4%, Patho accuracy: 77.0%, Avg loss: 21.877985 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.1%, Avg loss: 113.732512 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.5%, Patho accuracy: 77.5%, Avg loss: 21.833887 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.1%, Avg loss: 114.610797 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.6%, Patho accuracy: 77.5%, Avg loss: 21.773582 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.1%, Avg loss: 115.015094 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.7%, Patho accuracy: 77.6%, Avg loss: 21.750400 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.1%, Avg loss: 114.916438 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.7%, Patho accuracy: 77.7%, Avg loss: 21.700583 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.0%, Avg loss: 115.652379 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.8%, Patho accuracy: 77.9%, Avg loss: 21.632990 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.0%, Avg loss: 115.791280 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 77.9%, Avg loss: 21.598080 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.1%, Avg loss: 115.521977 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.8%, Patho accuracy: 77.4%, Avg loss: 21.595824 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Patho accuracy: 23.2%, Avg loss: 115.477080 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.8%, Avg loss: 21.599638 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.1%, Avg loss: 115.278324 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 77.8%, Avg loss: 21.573461 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Patho accuracy: 22.9%, Avg loss: 115.306481 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.6%, Avg loss: 21.553592 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Patho accuracy: 22.9%, Avg loss: 115.843264 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.5%, Avg loss: 21.535609 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Patho accuracy: 23.0%, Avg loss: 115.570074 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 77.5%, Avg loss: 21.614995 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.2%, Patho accuracy: 23.0%, Avg loss: 114.708581 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.4%, Avg loss: 21.641845 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.3%, Patho accuracy: 23.2%, Avg loss: 114.114733 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 77.1%, Avg loss: 21.729293 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.4%, Patho accuracy: 23.4%, Avg loss: 113.661765 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 77.1%, Avg loss: 21.792980 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.6%, Avg loss: 112.454077 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.8%, Patho accuracy: 77.0%, Avg loss: 21.912272 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.6%, Patho accuracy: 23.8%, Avg loss: 111.601156 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 76.8%, Avg loss: 21.973579 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.7%, Patho accuracy: 23.8%, Avg loss: 111.021692 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.7%, Patho accuracy: 76.6%, Avg loss: 22.107806 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Patho accuracy: 24.1%, Avg loss: 109.639840 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.7%, Patho accuracy: 76.5%, Avg loss: 22.178198 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.8%, Patho accuracy: 24.1%, Avg loss: 108.385272 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.6%, Patho accuracy: 76.2%, Avg loss: 22.399249 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 60.9%, Patho accuracy: 24.3%, Avg loss: 107.101158 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.6%, Patho accuracy: 76.2%, Avg loss: 22.530860 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.1%, Patho accuracy: 24.6%, Avg loss: 105.925335 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.6%, Patho accuracy: 76.2%, Avg loss: 22.587546 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Patho accuracy: 25.6%, Avg loss: 104.160345 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.3%, Patho accuracy: 75.7%, Avg loss: 23.010350 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Patho accuracy: 26.0%, Avg loss: 102.393103 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.3%, Patho accuracy: 75.5%, Avg loss: 23.011844 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Patho accuracy: 26.3%, Avg loss: 99.653601 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.3%, Avg loss: 23.398068 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Patho accuracy: 26.2%, Avg loss: 97.769191 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.2%, Patho accuracy: 75.3%, Avg loss: 23.510653 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.1%, Patho accuracy: 26.4%, Avg loss: 96.577464 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.0%, Avg loss: 23.575105 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Patho accuracy: 26.5%, Avg loss: 96.096973 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.9%, Avg loss: 23.723045 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Patho accuracy: 26.6%, Avg loss: 95.667451 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.7%, Avg loss: 23.744348 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Patho accuracy: 26.6%, Avg loss: 95.202089 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.9%, Patho accuracy: 74.7%, Avg loss: 23.802617 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Patho accuracy: 26.6%, Avg loss: 95.033264 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.4%, Avg loss: 23.800078 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Patho accuracy: 26.6%, Avg loss: 94.621983 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.5%, Avg loss: 23.828250 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Patho accuracy: 26.6%, Avg loss: 94.574659 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.4%, Avg loss: 23.930940 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Patho accuracy: 26.7%, Avg loss: 94.107187 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.4%, Avg loss: 23.905378 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Patho accuracy: 26.8%, Avg loss: 93.837621 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.4%, Avg loss: 23.918650 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Patho accuracy: 26.8%, Avg loss: 93.705398 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.4%, Avg loss: 23.888428 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Patho accuracy: 27.0%, Avg loss: 93.405109 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.4%, Avg loss: 24.004412 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Patho accuracy: 27.1%, Avg loss: 93.037405 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.5%, Avg loss: 23.907706 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 27.1%, Avg loss: 93.370705 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.5%, Avg loss: 23.960648 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 27.2%, Avg loss: 93.405900 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.5%, Avg loss: 23.957234 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 27.5%, Avg loss: 94.113673 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.2%, Patho accuracy: 74.7%, Avg loss: 23.935606 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Patho accuracy: 27.6%, Avg loss: 94.290927 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.6%, Avg loss: 23.908306 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 27.8%, Avg loss: 94.504907 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.2%, Patho accuracy: 74.5%, Avg loss: 23.891041 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 27.7%, Avg loss: 94.945078 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.5%, Avg loss: 23.902264 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 27.8%, Avg loss: 94.722143 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.5%, Avg loss: 23.843411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 27.9%, Avg loss: 94.706354 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.5%, Avg loss: 23.836964 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 27.8%, Avg loss: 94.747412 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.5%, Avg loss: 23.841543 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 27.9%, Avg loss: 94.901908 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.6%, Avg loss: 23.785373 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 28.0%, Avg loss: 94.803249 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.4%, Avg loss: 23.810081 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 28.1%, Avg loss: 94.879163 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.5%, Avg loss: 23.785281 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Patho accuracy: 28.2%, Avg loss: 94.873708 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.6%, Avg loss: 23.725127 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Patho accuracy: 28.2%, Avg loss: 95.196154 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.6%, Avg loss: 23.735744 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 28.3%, Avg loss: 94.982689 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.9%, Patho accuracy: 74.4%, Avg loss: 23.760932 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 28.4%, Avg loss: 95.325929 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.6%, Avg loss: 23.742300 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 28.4%, Avg loss: 95.296623 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.6%, Avg loss: 23.729057 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 28.5%, Avg loss: 95.317037 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.6%, Avg loss: 23.674665 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 28.6%, Avg loss: 95.587945 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.7%, Avg loss: 23.676704 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 28.6%, Avg loss: 95.423273 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.7%, Avg loss: 23.588936 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 28.7%, Avg loss: 95.828362 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.8%, Avg loss: 23.596960 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 28.8%, Avg loss: 95.882641 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 74.9%, Avg loss: 23.524939 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 28.9%, Avg loss: 96.050544 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.0%, Avg loss: 23.476040 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 29.0%, Avg loss: 96.038711 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.0%, Avg loss: 23.445254 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 28.9%, Avg loss: 96.381036 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.2%, Patho accuracy: 75.1%, Avg loss: 23.443788 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 28.9%, Avg loss: 96.183966 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.2%, Patho accuracy: 75.2%, Avg loss: 23.338884 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 28.9%, Avg loss: 96.233495 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.2%, Avg loss: 23.352267 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 29.0%, Avg loss: 96.221906 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.1%, Avg loss: 23.327040 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 29.2%, Avg loss: 96.421535 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.0%, Avg loss: 23.326780 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 29.3%, Avg loss: 96.183169 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.0%, Avg loss: 23.310349 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 29.3%, Avg loss: 96.367946 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.0%, Avg loss: 23.290139 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 29.5%, Avg loss: 96.275172 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.0%, Avg loss: 23.316636 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 29.6%, Avg loss: 96.016097 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 74.9%, Avg loss: 23.275826 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 29.6%, Avg loss: 96.065448 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.8%, Patho accuracy: 74.8%, Avg loss: 23.347292 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 29.8%, Avg loss: 95.947030 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.8%, Patho accuracy: 74.7%, Avg loss: 23.262509 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 29.9%, Avg loss: 96.105517 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.8%, Patho accuracy: 74.7%, Avg loss: 23.332971 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.1%, Avg loss: 95.676009 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.7%, Patho accuracy: 74.6%, Avg loss: 23.217051 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.2%, Avg loss: 95.944379 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.7%, Patho accuracy: 74.7%, Avg loss: 23.307560 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.3%, Avg loss: 95.592642 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.7%, Patho accuracy: 74.7%, Avg loss: 23.216544 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.2%, Avg loss: 95.566079 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.7%, Patho accuracy: 74.8%, Avg loss: 23.263997 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.3%, Avg loss: 95.524606 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.6%, Patho accuracy: 74.8%, Avg loss: 23.186834 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.3%, Avg loss: 96.219331 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.6%, Patho accuracy: 74.7%, Avg loss: 23.256006 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 30.4%, Avg loss: 95.770593 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.6%, Patho accuracy: 74.8%, Avg loss: 23.215581 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 30.2%, Avg loss: 96.179685 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.7%, Patho accuracy: 74.9%, Avg loss: 23.115221 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 30.3%, Avg loss: 96.551121 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.5%, Patho accuracy: 74.8%, Avg loss: 23.216336 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 30.3%, Avg loss: 96.379773 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.6%, Patho accuracy: 75.0%, Avg loss: 22.999862 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 30.3%, Avg loss: 97.706963 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 84.6%, Patho accuracy: 74.8%, Avg loss: 23.070451 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 30.2%, Avg loss: 97.463889 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.6%, Patho accuracy: 75.1%, Avg loss: 22.863593 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Patho accuracy: 30.0%, Avg loss: 98.331339 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 84.8%, Patho accuracy: 75.5%, Avg loss: 22.762763 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Patho accuracy: 30.1%, Avg loss: 98.442532 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.0%, Patho accuracy: 75.7%, Avg loss: 22.579864 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Patho accuracy: 30.1%, Avg loss: 99.922519 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.8%, Avg loss: 22.486777 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 30.1%, Avg loss: 100.445059 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.1%, Patho accuracy: 75.7%, Avg loss: 22.320232 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 30.0%, Avg loss: 101.299567 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.2%, Patho accuracy: 75.8%, Avg loss: 22.190646 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 30.0%, Avg loss: 101.728370 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.4%, Patho accuracy: 76.1%, Avg loss: 22.037249 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 29.9%, Avg loss: 102.513479 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.6%, Patho accuracy: 76.3%, Avg loss: 21.920685 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 29.7%, Avg loss: 103.147716 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.6%, Patho accuracy: 76.3%, Avg loss: 21.825003 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 29.6%, Avg loss: 103.798917 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.8%, Patho accuracy: 76.4%, Avg loss: 21.685309 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 29.6%, Avg loss: 104.071957 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.8%, Patho accuracy: 76.3%, Avg loss: 21.689788 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Patho accuracy: 29.4%, Avg loss: 104.494560 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 76.6%, Avg loss: 21.589571 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 29.4%, Avg loss: 104.283606 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 76.7%, Avg loss: 21.538268 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Patho accuracy: 29.1%, Avg loss: 104.836869 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 76.7%, Avg loss: 21.494333 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Patho accuracy: 29.1%, Avg loss: 105.101906 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 76.8%, Avg loss: 21.420687 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Patho accuracy: 29.2%, Avg loss: 104.576292 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 76.9%, Avg loss: 21.402503 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Patho accuracy: 29.2%, Avg loss: 105.084810 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.0%, Avg loss: 21.358964 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 29.3%, Avg loss: 105.398412 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 77.0%, Avg loss: 21.495434 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Patho accuracy: 29.2%, Avg loss: 105.020539 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.0%, Avg loss: 21.288357 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 29.4%, Avg loss: 104.939093 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 76.9%, Avg loss: 21.277350 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 29.5%, Avg loss: 104.895959 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 77.2%, Avg loss: 21.182002 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 29.4%, Avg loss: 105.177863 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 77.0%, Avg loss: 21.185411 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.5%, Patho accuracy: 29.4%, Avg loss: 105.051997 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 77.4%, Avg loss: 21.088995 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 29.5%, Avg loss: 104.228995 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 77.0%, Avg loss: 21.129077 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 29.6%, Avg loss: 104.816392 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.5%, Avg loss: 21.000538 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 29.5%, Avg loss: 104.214704 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.0%, Avg loss: 21.045619 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 29.7%, Avg loss: 103.974851 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.4%, Avg loss: 20.916430 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 29.5%, Avg loss: 103.311873 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.3%, Avg loss: 20.938338 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 29.7%, Avg loss: 103.399345 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.5%, Avg loss: 20.865362 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Patho accuracy: 29.7%, Avg loss: 103.512238 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 77.8%, Avg loss: 20.812873 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 29.8%, Avg loss: 103.608907 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 77.8%, Avg loss: 20.922789 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 29.9%, Avg loss: 103.169907 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 77.8%, Avg loss: 20.816012 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 29.9%, Avg loss: 103.325342 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.6%, Avg loss: 20.904221 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Patho accuracy: 30.2%, Avg loss: 103.353323 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 77.7%, Avg loss: 20.787710 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Patho accuracy: 30.1%, Avg loss: 103.784495 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.8%, Avg loss: 20.910036 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 30.4%, Avg loss: 103.305369 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.8%, Avg loss: 20.943100 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 30.4%, Avg loss: 103.490058 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 77.9%, Avg loss: 20.905603 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 30.4%, Avg loss: 103.394495 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.6%, Avg loss: 20.911644 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 30.4%, Avg loss: 103.626836 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.8%, Avg loss: 20.944832 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 30.4%, Avg loss: 103.197994 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 77.5%, Avg loss: 21.027146 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 30.4%, Avg loss: 102.685220 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.6%, Avg loss: 20.942125 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 30.1%, Avg loss: 102.034052 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 77.5%, Avg loss: 21.043273 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 30.1%, Avg loss: 100.819385 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.4%, Avg loss: 20.948176 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.4%, Avg loss: 101.074533 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.7%, Avg loss: 21.005337 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 30.3%, Avg loss: 100.236534 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 77.6%, Avg loss: 20.966245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 30.3%, Avg loss: 100.343699 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 76.9%, Avg loss: 21.029896 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 30.5%, Avg loss: 99.741501 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.3%, Avg loss: 20.943333 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 30.3%, Avg loss: 99.964802 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.5%, Avg loss: 20.811354 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 30.3%, Avg loss: 100.319595 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.6%, Avg loss: 20.867874 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.6%, Avg loss: 99.800683 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.5%, Avg loss: 20.792701 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 30.6%, Avg loss: 100.519909 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.7%, Avg loss: 20.813629 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 30.7%, Avg loss: 100.122826 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 77.9%, Avg loss: 20.796783 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 30.8%, Avg loss: 100.356428 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.6%, Avg loss: 20.846845 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Patho accuracy: 30.9%, Avg loss: 100.538709 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.6%, Avg loss: 20.902394 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 31.0%, Avg loss: 101.221085 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.5%, Avg loss: 20.920859 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Patho accuracy: 31.0%, Avg loss: 101.225971 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.7%, Avg loss: 20.949771 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Patho accuracy: 31.1%, Avg loss: 101.494754 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.7%, Avg loss: 20.984891 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 31.1%, Avg loss: 101.260233 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.7%, Avg loss: 21.000851 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 31.2%, Avg loss: 101.424139 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.5%, Avg loss: 20.972248 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 31.2%, Avg loss: 101.532611 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.5%, Avg loss: 21.014187 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 31.3%, Avg loss: 101.560426 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.7%, Avg loss: 21.023901 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 31.3%, Avg loss: 101.646178 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.8%, Avg loss: 21.063817 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 31.4%, Avg loss: 101.444390 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.9%, Avg loss: 21.088171 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 31.3%, Avg loss: 101.413151 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 78.0%, Avg loss: 21.067168 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 31.2%, Avg loss: 101.403206 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 77.5%, Avg loss: 21.117894 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 31.3%, Avg loss: 100.821357 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.4%, Avg loss: 21.088640 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 31.3%, Avg loss: 100.861122 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.4%, Avg loss: 21.072944 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 31.4%, Avg loss: 101.352319 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.4%, Avg loss: 21.062996 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 31.6%, Avg loss: 101.387499 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.1%, Avg loss: 20.958969 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 31.6%, Avg loss: 101.490748 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.3%, Avg loss: 21.000976 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 31.6%, Avg loss: 101.804093 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 77.3%, Avg loss: 20.987621 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 31.7%, Avg loss: 101.757089 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 77.9%, Avg loss: 21.010046 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 31.7%, Avg loss: 101.870687 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 78.2%, Avg loss: 20.960392 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 31.9%, Avg loss: 102.092364 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 78.0%, Avg loss: 20.909840 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 31.9%, Avg loss: 102.449350 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 78.3%, Avg loss: 20.967701 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 32.0%, Avg loss: 102.862993 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 78.6%, Avg loss: 20.976084 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 32.2%, Avg loss: 103.344036 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 78.7%, Avg loss: 20.929434 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 32.2%, Avg loss: 103.817397 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 78.5%, Avg loss: 20.997707 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 32.3%, Avg loss: 104.014595 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 78.7%, Avg loss: 21.049497 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 32.4%, Avg loss: 103.820016 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 78.4%, Avg loss: 21.067483 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 32.5%, Avg loss: 103.926915 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 78.2%, Avg loss: 21.116737 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 32.5%, Avg loss: 103.876702 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 78.6%, Avg loss: 21.068401 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 32.6%, Avg loss: 103.778100 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 78.6%, Avg loss: 21.118108 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 32.7%, Avg loss: 103.845444 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 78.6%, Avg loss: 21.128329 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 32.8%, Avg loss: 103.558106 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 78.1%, Avg loss: 21.204755 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 32.8%, Avg loss: 103.482115 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.7%, Avg loss: 21.201178 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 32.5%, Avg loss: 103.376891 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.5%, Avg loss: 21.176963 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 32.5%, Avg loss: 102.601907 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 77.5%, Avg loss: 21.140491 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 32.7%, Avg loss: 102.708574 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 78.0%, Avg loss: 21.129081 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 32.7%, Avg loss: 102.980957 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 78.2%, Avg loss: 21.063781 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 32.8%, Avg loss: 102.926246 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 77.8%, Avg loss: 21.010454 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 32.8%, Avg loss: 102.965889 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 78.3%, Avg loss: 20.986032 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 32.9%, Avg loss: 103.256983 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 78.5%, Avg loss: 21.005563 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 33.1%, Avg loss: 103.558937 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 78.8%, Avg loss: 20.981618 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Patho accuracy: 33.1%, Avg loss: 103.727485 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 78.9%, Avg loss: 20.967252 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 33.2%, Avg loss: 104.118260 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.1%, Avg loss: 20.937096 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 33.4%, Avg loss: 104.101502 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.1%, Avg loss: 20.952237 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Patho accuracy: 33.5%, Avg loss: 104.221264 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.2%, Avg loss: 20.968894 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Patho accuracy: 33.5%, Avg loss: 104.517231 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.3%, Avg loss: 20.982731 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Patho accuracy: 33.8%, Avg loss: 104.728846 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.3%, Avg loss: 21.056018 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Patho accuracy: 33.9%, Avg loss: 104.687295 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.3%, Avg loss: 21.048686 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Patho accuracy: 34.0%, Avg loss: 105.017413 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.3%, Avg loss: 21.025230 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Patho accuracy: 34.2%, Avg loss: 105.297562 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.3%, Avg loss: 21.027756 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Patho accuracy: 34.2%, Avg loss: 105.335700 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.1%, Avg loss: 21.115848 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Patho accuracy: 34.0%, Avg loss: 105.423051 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 78.9%, Avg loss: 21.117562 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Patho accuracy: 34.0%, Avg loss: 105.247825 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 78.5%, Avg loss: 21.205233 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Patho accuracy: 34.3%, Avg loss: 104.601945 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 78.7%, Avg loss: 21.188555 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Patho accuracy: 34.1%, Avg loss: 104.919597 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 78.1%, Avg loss: 21.245532 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Patho accuracy: 34.1%, Avg loss: 104.498757 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 78.0%, Avg loss: 21.256002 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Patho accuracy: 34.0%, Avg loss: 104.923582 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.8%, Patho accuracy: 78.0%, Avg loss: 21.271147 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Patho accuracy: 34.1%, Avg loss: 104.263328 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 78.0%, Avg loss: 21.258018 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Patho accuracy: 34.2%, Avg loss: 105.503889 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 78.3%, Avg loss: 21.241490 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 34.5%, Avg loss: 104.772049 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 78.4%, Avg loss: 21.276658 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Patho accuracy: 34.3%, Avg loss: 105.409561 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 78.3%, Avg loss: 21.229580 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Patho accuracy: 34.5%, Avg loss: 105.960369 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 78.8%, Avg loss: 21.187028 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Patho accuracy: 34.6%, Avg loss: 108.237227 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 78.8%, Avg loss: 21.245807 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 34.3%, Avg loss: 108.620320 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 79.0%, Avg loss: 21.233969 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 34.1%, Avg loss: 109.139992 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 78.7%, Avg loss: 21.422671 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 34.4%, Avg loss: 107.131173 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.8%, Patho accuracy: 78.5%, Avg loss: 21.136336 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 33.8%, Avg loss: 110.646276 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 79.0%, Avg loss: 21.186501 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 34.4%, Avg loss: 107.630680 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.8%, Patho accuracy: 78.4%, Avg loss: 21.032230 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 34.5%, Avg loss: 107.852713 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 78.8%, Avg loss: 20.980744 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 34.6%, Avg loss: 107.844855 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 85.9%, Patho accuracy: 78.5%, Avg loss: 20.915863 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 34.6%, Avg loss: 107.848895 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 78.9%, Avg loss: 20.883340 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Patho accuracy: 34.6%, Avg loss: 108.232477 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 78.9%, Avg loss: 20.908658 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 34.7%, Avg loss: 107.888845 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.0%, Avg loss: 20.773215 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 34.9%, Avg loss: 107.668959 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.3%, Avg loss: 20.720041 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 35.0%, Avg loss: 108.607748 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.3%, Avg loss: 20.678649 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 34.9%, Avg loss: 107.687498 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 78.6%, Avg loss: 20.588141 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 35.1%, Avg loss: 108.395728 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.4%, Avg loss: 20.605085 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 35.1%, Avg loss: 107.725738 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.4%, Avg loss: 20.579512 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 35.0%, Avg loss: 107.947059 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.1%, Avg loss: 20.561503 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 35.2%, Avg loss: 107.682540 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.3%, Avg loss: 20.589181 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 35.0%, Avg loss: 107.525997 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.0%, Avg loss: 20.608925 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Patho accuracy: 35.4%, Avg loss: 107.238874 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.5%, Avg loss: 20.560233 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Patho accuracy: 35.1%, Avg loss: 107.527176 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 78.9%, Avg loss: 20.608534 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.6%, Patho accuracy: 35.3%, Avg loss: 107.116361 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.0%, Avg loss: 20.533110 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 35.4%, Avg loss: 107.249432 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.2%, Avg loss: 20.520951 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 35.4%, Avg loss: 106.782794 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.0%, Avg loss: 20.435796 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Patho accuracy: 35.2%, Avg loss: 106.555703 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 78.9%, Avg loss: 20.485106 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 35.5%, Avg loss: 105.929347 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.0%, Avg loss: 20.366539 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 35.1%, Avg loss: 105.805874 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 78.8%, Avg loss: 20.280707 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 35.3%, Avg loss: 105.557480 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 78.9%, Avg loss: 20.248702 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 35.4%, Avg loss: 105.781629 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 78.9%, Avg loss: 20.253385 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 35.5%, Avg loss: 106.235195 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.0%, Avg loss: 20.248773 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 35.6%, Avg loss: 106.563883 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.9%, Patho accuracy: 79.5%, Avg loss: 20.257521 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 35.7%, Avg loss: 105.956511 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.9%, Patho accuracy: 79.6%, Avg loss: 20.218105 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 35.7%, Avg loss: 106.860232 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 79.9%, Avg loss: 20.242132 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 35.9%, Avg loss: 106.788798 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.1%, Avg loss: 20.232609 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 35.9%, Avg loss: 107.077036 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.4%, Avg loss: 20.227095 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 36.0%, Avg loss: 107.398927 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.4%, Avg loss: 20.277857 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.7%, Patho accuracy: 35.9%, Avg loss: 107.539054 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.4%, Avg loss: 20.296851 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 36.1%, Avg loss: 107.718051 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.4%, Avg loss: 20.261481 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.8%, Patho accuracy: 36.0%, Avg loss: 107.812847 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.5%, Avg loss: 20.296474 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 36.2%, Avg loss: 107.835164 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.3%, Avg loss: 20.247245 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 36.2%, Avg loss: 108.514339 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.2%, Avg loss: 20.344291 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 36.1%, Avg loss: 107.999985 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.0%, Avg loss: 20.310586 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 36.0%, Avg loss: 108.368255 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.0%, Avg loss: 20.314894 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 36.2%, Avg loss: 108.317247 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.0%, Patho accuracy: 79.9%, Avg loss: 20.311382 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 36.1%, Avg loss: 108.940721 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.0%, Patho accuracy: 80.0%, Avg loss: 20.364508 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 36.2%, Avg loss: 108.535494 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.0%, Patho accuracy: 79.8%, Avg loss: 20.366346 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 36.2%, Avg loss: 108.557064 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.0%, Patho accuracy: 80.0%, Avg loss: 20.371182 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 36.2%, Avg loss: 108.586931 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.0%, Patho accuracy: 79.8%, Avg loss: 20.375154 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 36.1%, Avg loss: 108.307190 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.9%, Patho accuracy: 79.7%, Avg loss: 20.345940 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 36.0%, Avg loss: 109.286370 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.9%, Patho accuracy: 79.5%, Avg loss: 20.372027 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 36.0%, Avg loss: 108.985316 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.4%, Avg loss: 20.416264 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 36.2%, Avg loss: 108.415326 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.4%, Avg loss: 20.417568 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Patho accuracy: 36.2%, Avg loss: 108.305386 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.4%, Avg loss: 20.465112 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 36.3%, Avg loss: 107.939010 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.3%, Avg loss: 20.488650 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.0%, Patho accuracy: 36.3%, Avg loss: 108.417815 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.1%, Avg loss: 20.544526 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 36.3%, Avg loss: 107.303007 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.1%, Avg loss: 20.523805 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 36.5%, Avg loss: 107.437859 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.2%, Avg loss: 20.558188 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Patho accuracy: 36.6%, Avg loss: 107.091263 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.1%, Avg loss: 20.371024 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 36.6%, Avg loss: 107.807845 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.1%, Avg loss: 20.371447 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 36.6%, Avg loss: 108.360303 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.3%, Avg loss: 20.372207 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 36.7%, Avg loss: 107.905898 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.2%, Avg loss: 20.133985 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 34.9%, Avg loss: 118.909945 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.1%, Avg loss: 20.906319 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.1%, Patho accuracy: 37.2%, Avg loss: 107.145600 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.2%, Avg loss: 20.504175 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 37.1%, Avg loss: 109.660675 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.2%, Avg loss: 20.340481 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Patho accuracy: 37.5%, Avg loss: 108.597864 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.4%, Avg loss: 20.306477 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 35.8%, Avg loss: 115.704474 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.7%, Avg loss: 20.469767 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 36.9%, Avg loss: 111.407684 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 79.2%, Avg loss: 20.493768 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Patho accuracy: 37.2%, Avg loss: 110.153804 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 79.3%, Avg loss: 20.370137 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Patho accuracy: 36.1%, Avg loss: 113.472354 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.5%, Avg loss: 20.359951 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Patho accuracy: 36.4%, Avg loss: 111.785414 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 79.2%, Avg loss: 20.469426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 37.3%, Avg loss: 111.335824 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.2%, Avg loss: 20.307453 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Patho accuracy: 36.4%, Avg loss: 111.782986 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.3%, Patho accuracy: 79.1%, Avg loss: 20.353392 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Patho accuracy: 37.2%, Avg loss: 111.393262 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 78.8%, Avg loss: 20.273829 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.6%, Patho accuracy: 36.9%, Avg loss: 111.396591 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.2%, Avg loss: 20.287398 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.7%, Patho accuracy: 37.2%, Avg loss: 111.167380 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.6%, Patho accuracy: 79.1%, Avg loss: 20.160362 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Patho accuracy: 37.3%, Avg loss: 110.701488 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.5%, Avg loss: 20.151733 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Patho accuracy: 37.6%, Avg loss: 110.653359 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.9%, Patho accuracy: 79.4%, Avg loss: 20.070453 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Patho accuracy: 37.5%, Avg loss: 110.114864 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.5%, Avg loss: 20.083792 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 37.6%, Avg loss: 110.276414 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.9%, Patho accuracy: 79.4%, Avg loss: 20.098558 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 37.7%, Avg loss: 109.762177 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.9%, Patho accuracy: 79.5%, Avg loss: 20.051681 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Patho accuracy: 37.2%, Avg loss: 109.316935 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.2%, Avg loss: 20.085615 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 37.2%, Avg loss: 109.133279 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.2%, Avg loss: 19.965499 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 37.1%, Avg loss: 109.091770 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.4%, Avg loss: 19.952127 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 37.1%, Avg loss: 108.955568 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.5%, Avg loss: 19.885543 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 36.9%, Avg loss: 108.777839 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.5%, Avg loss: 19.893800 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 36.9%, Avg loss: 109.005403 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.8%, Patho accuracy: 79.5%, Avg loss: 19.840406 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 37.5%, Avg loss: 108.909244 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.1%, Avg loss: 19.819668 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 37.5%, Avg loss: 108.763627 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.1%, Avg loss: 19.828505 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 37.5%, Avg loss: 108.487041 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.1%, Avg loss: 19.772349 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 37.5%, Avg loss: 108.838187 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.2%, Avg loss: 19.754009 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 37.5%, Avg loss: 108.939537 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.4%, Avg loss: 19.699251 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 37.2%, Avg loss: 109.303053 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.5%, Patho accuracy: 80.6%, Avg loss: 19.695012 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 37.4%, Avg loss: 109.410345 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.5%, Avg loss: 19.686048 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 37.7%, Avg loss: 109.969410 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.4%, Avg loss: 19.677484 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 38.0%, Avg loss: 109.857872 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.6%, Avg loss: 19.708501 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 37.9%, Avg loss: 110.220774 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.6%, Avg loss: 19.706700 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 38.1%, Avg loss: 110.072567 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.6%, Avg loss: 19.785042 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 38.2%, Avg loss: 110.227401 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.8%, Avg loss: 19.840856 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 38.1%, Avg loss: 110.579324 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.3%, Avg loss: 19.862040 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.4%, Patho accuracy: 37.9%, Avg loss: 111.706273 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.2%, Avg loss: 19.938650 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 38.2%, Avg loss: 110.954585 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.9%, Avg loss: 19.957570 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 37.5%, Avg loss: 114.954455 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.5%, Patho accuracy: 79.5%, Avg loss: 20.220356 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 37.5%, Avg loss: 114.182207 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.1%, Avg loss: 20.130595 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.2%, Patho accuracy: 36.3%, Avg loss: 114.878587 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.0%, Patho accuracy: 78.5%, Avg loss: 20.352427 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Patho accuracy: 35.2%, Avg loss: 117.654088 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.1%, Patho accuracy: 78.4%, Avg loss: 20.191032 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Patho accuracy: 35.9%, Avg loss: 115.998012 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.2%, Patho accuracy: 78.9%, Avg loss: 20.086601 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.0%, Patho accuracy: 35.8%, Avg loss: 116.397360 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 79.6%, Avg loss: 19.955512 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.8%, Patho accuracy: 35.4%, Avg loss: 118.987275 \n",
      "\n",
      "Epoch 480\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.4%, Patho accuracy: 78.8%, Avg loss: 19.884365 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 35.0%, Avg loss: 117.239074 \n",
      "\n",
      "Epoch 481\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.2%, Avg loss: 19.871690 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 35.1%, Avg loss: 117.907285 \n",
      "\n",
      "Epoch 482\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.3%, Avg loss: 19.678649 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 35.2%, Avg loss: 117.828038 \n",
      "\n",
      "Epoch 483\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 86.7%, Patho accuracy: 79.7%, Avg loss: 19.561482 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 35.1%, Avg loss: 118.342201 \n",
      "\n",
      "Epoch 484\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.0%, Patho accuracy: 79.9%, Avg loss: 19.505426 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 35.0%, Avg loss: 117.672681 \n",
      "\n",
      "Epoch 485\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.0%, Patho accuracy: 79.9%, Avg loss: 19.416687 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 35.0%, Avg loss: 117.815653 \n",
      "\n",
      "Epoch 486\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.0%, Avg loss: 19.346603 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.5%, Patho accuracy: 35.0%, Avg loss: 117.740924 \n",
      "\n",
      "Epoch 487\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.0%, Avg loss: 19.324300 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Patho accuracy: 35.0%, Avg loss: 117.975799 \n",
      "\n",
      "Epoch 488\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.1%, Avg loss: 19.305360 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.6%, Patho accuracy: 35.1%, Avg loss: 118.209808 \n",
      "\n",
      "Epoch 489\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.5%, Avg loss: 19.273840 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.7%, Patho accuracy: 35.4%, Avg loss: 117.077598 \n",
      "\n",
      "Epoch 490\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.5%, Avg loss: 19.249644 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 61.9%, Patho accuracy: 35.8%, Avg loss: 116.791826 \n",
      "\n",
      "Epoch 491\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.5%, Avg loss: 19.162544 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.3%, Patho accuracy: 36.3%, Avg loss: 114.688556 \n",
      "\n",
      "Epoch 492\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.5%, Avg loss: 19.204818 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.4%, Patho accuracy: 36.9%, Avg loss: 113.828789 \n",
      "\n",
      "Epoch 493\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.2%, Avg loss: 19.158421 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 62.9%, Patho accuracy: 37.3%, Avg loss: 113.234313 \n",
      "\n",
      "Epoch 494\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.6%, Avg loss: 19.245383 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.0%, Patho accuracy: 37.9%, Avg loss: 112.377641 \n",
      "\n",
      "Epoch 495\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.4%, Patho accuracy: 80.7%, Avg loss: 19.179356 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.2%, Patho accuracy: 37.9%, Avg loss: 112.606800 \n",
      "\n",
      "Epoch 496\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.3%, Patho accuracy: 80.5%, Avg loss: 19.285553 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.3%, Patho accuracy: 38.1%, Avg loss: 112.311703 \n",
      "\n",
      "Epoch 497\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.5%, Patho accuracy: 80.8%, Avg loss: 19.228024 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 63.5%, Patho accuracy: 38.1%, Avg loss: 112.053728 \n",
      "\n",
      "Epoch 498\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.1%, Patho accuracy: 80.1%, Avg loss: 19.279449 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Patho accuracy: 37.4%, Avg loss: 111.882442 \n",
      "\n",
      "Epoch 499\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.5%, Avg loss: 19.312391 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Patho accuracy: 37.4%, Avg loss: 111.855065 \n",
      "\n",
      "Epoch 500\n",
      "-------------------------------\n",
      "Train Error: \n",
      " Accuracy: 87.2%, Patho accuracy: 80.2%, Avg loss: 19.253913 \n",
      "\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Patho accuracy: 37.4%, Avg loss: 110.287434 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "trainingEpoch_loss = []\n",
    "testingEpoch_loss = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    trainLoss = train(train_dataloader, model, optimizer, loss_fn)\n",
    "    testLoss = test(test_dataloader, model, loss_fn)\n",
    "    trainingEpoch_loss.append(trainLoss)\n",
    "    testingEpoch_loss.append(testLoss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3eb5afb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f2921ee3150>]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,10))\n",
    "ax.plot(trainingEpoch_loss, label='train_loss', color='blue')\n",
    "ax.plot(testingEpoch_loss, label='test_loss', color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a1ad80ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 1.737754 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_predictions('128-16-500epochs',test_dataloader,model,loss_fn)\n",
    "torch.save(model.state_dict(), \"model.128-16-500epochs.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34db164",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "torch_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
